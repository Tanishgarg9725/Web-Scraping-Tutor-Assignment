{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13633087", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13633087", "key": "SPARK-54131", "fields": {"summary": "Update pandas to 2.3.3", "description": "New version, support for python 3.14\r\nhttps://pandas.pydata.org/pandas-docs/version/2.3/whatsnew/v2.3.3.html\r\n", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-11-01T16:54:40.000+0000", "created": "2025-11-01T16:54:40.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13633084", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13633084", "key": "SPARK-54130", "fields": {"summary": "Add detailed error messages for catalog assertion failures", "description": "There are many instances where we simply assert without providing a valid error message. This complicates debugging. We should implement proper error messages to make debugging easier.\u00a0", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-11-01T15:28:20.000+0000", "created": "2025-11-01T14:59:49.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13633078", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13633078", "key": "SPARK-54129", "fields": {"summary": "Extract dropping table from catalog from DataFrameWriter in case of overwriting table during creation", "description": "Currently, if we call DataFrameWriter.saveAsTable(tableName) with the SaveMode equal to Overwrite and the DataSource is V1, the table is dropped from catalog directly in the DataFrameWriter and only then the CreateTable command is created.\u00a0\r\n\r\nIn case if we want to somehow change the default behavior using Spark extensions, for instance, postpone the table dropping to decrease the time interval, when the old table is dropped and the new one is not created, such hardcoded behavior in DataFrameWriter makes it almost impossible if we want to use the original saveAsTable method. \r\n\r\nFor instance, if we call df.write.mode(\"overwrite\").insertInto(table_name) then it's possible to control the table removal logic if we provide our own implementation of the spark.sql.sources.commitProtocolClass. In case of saveAsTable it makes no sense, because a table is unconditionally dropped before handling the CreateTable command.\r\n\r\nTherefore, is it possible to extract the table dropping logic from DataFrameWriter to the corresponding LogicalPlan commands (as it was done for V2 Datasources) in order to give more flexibility to Spark extensions?", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-11-01T09:55:16.000+0000", "created": "2025-11-01T09:55:16.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13633075", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13633075", "key": "SPARK-54128", "fields": {"summary": "Improve Error Handling for Spark Connect", "description": "This is an umbrella Jira to migrate exceptions in Spark Connect from plain runtime exceptions to proper Spark Exceptions with error class.", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-11-01T08:47:45.000+0000", "created": "2025-11-01T08:47:45.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13633071", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13633071", "key": "SPARK-54127", "fields": {"summary": "Fix sbt inconsistent shading package", "description": null, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-11-01T04:29:06.000+0000", "created": "2025-11-01T04:20:38.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13633068", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13633068", "key": "SPARK-54126", "fields": {"summary": "SHOW TBLPROPERTIES AS JSON", "description": null, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-11-01T00:20:36.000+0000", "created": "2025-11-01T00:18:18.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13633067", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13633067", "key": "SPARK-54125", "fields": {"summary": "SHOW DATABASES AS JSON", "description": null, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-11-01T00:20:55.000+0000", "created": "2025-11-01T00:14:14.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13633066", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13633066", "key": "SPARK-54124", "fields": {"summary": "SHOW VIEWS AS JSON", "description": null, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-11-01T00:21:04.000+0000", "created": "2025-11-01T00:13:05.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13633065", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13633065", "key": "SPARK-54123", "fields": {"summary": "Add timezone to make timestamp absolute time.", "description": null, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-11-01T00:25:46.000+0000", "created": "2025-11-01T00:05:48.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13633064", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13633064", "key": "SPARK-54122", "fields": {"summary": "Improve the testing experience for TransformWithState", "description": "Currently it's hard to write unit tests for user's implementation of StatefulProcessor used in TransformWithState. User either needs to test it by running actual streaming query, or they need to refactor business logic into a separate function (that will be called from handleInputRows), and write unit tests for that function.\r\n\u00a0\r\nI propose to add a simple testing framework for TransformWithState that will allow users easily test their business logic inside StatefulProcessor.\r\n\u00a0\r\nOn high-level, it's a class TwsTester that takes StatefulProcessor and allows to feed in input rows and immediately returns what rows would be produced by TWS. It also allows to set and inspect state. This can be used in unit tests without having to run streaming query and it won't need RocksDB (it will use in-memory state store). I will start with implementing this for Scala users, potentially making it available for Python users later.", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-11-01T01:12:01.000+0000", "created": "2025-11-01T00:03:04.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13633061", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13633061", "key": "SPARK-54121", "fields": {"summary": "Automatic Snapshot Repair for State store", "description": "Today, the engine currently treats both the changelog and snapshot files with the same importance, so when a state store needs to be loaded it reads the latest snapshot and applies the subsequent changes on it. If the snapshot is bad or corrupted, then the query will fail and be completely down and blocked, needing manual intervention. This leads to user clearing their query checkpoint and having to do full recomputation.\r\n\r\nThis shouldn\u2019t be the case. The changelog should be treated as the \u201csource of truth\u201d and the snapshot is just a disposable materialization of the log.\r\n\r\nIntroducing Automatic snapshot repair, which will automatically repair the checkpoint by skipping bad snapshots and rebuilding the current state from the last good snapshot (works even if there\u2019s none) and applying the changelogs on it. This eliminates the need for manual intervention and unblocks the pipeline to keep it running.\r\n\r\nAlso emit metrics about number of state stores that were auto repaired in a given batch, so that you can build alert and dashboard for it.", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-11-01T04:54:12.000+0000", "created": "2025-10-31T22:49:00.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13633056", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13633056", "key": "SPARK-54120", "fields": {"summary": "Update assertGeneratedCRDMatchesHelmChart to include diff", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13633056/comment/18034718", "id": "18034718", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=ptoth", "name": "ptoth", "key": "ptoth", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Peter Toth", "active": true, "timeZone": "Europe/Budapest"}, "body": "Issue resolved by pull request 414\n[https://github.com/apache/spark-kubernetes-operator/pull/414]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=ptoth", "name": "ptoth", "key": "ptoth", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Peter Toth", "active": true, "timeZone": "Europe/Budapest"}, "created": "2025-11-01T09:44:03.686+0000", "updated": "2025-11-01T09:44:03.686+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-11-01T09:44:53.000+0000", "created": "2025-10-31T21:24:37.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13633054", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13633054", "key": "SPARK-54119", "fields": {"summary": "Metrics & semantic modeling in Spark", "description": "SPIP: https://docs.google.com/document/d/1xVTLijvDTJ90lZ_ujwzf9HvBJgWg0mY6cYM44Fcghl0/edit?tab=t.0#heading=h.4iogryr5qznc", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-31T21:33:37.000+0000", "created": "2025-10-31T20:49:11.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13633050", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13633050", "key": "SPARK-54118", "fields": {"summary": "Improve the put/merge operation in ListState when t here are multiple values", "description": "In SS TWS, when we do the {{put(array)}}\u00a0operation in liststate, we put the first element and then merge the remaining elements one by one. so if we want to put an array with 100 elements, it means we need do 1 put + 99 merges. This can result in worse performance than a single put operation for the entire array.\r\n\r\n\u00a0\r\n\r\nSimilar, we have the same issue in {{merge(array)}}\u00a0\r\n\u00a0", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13633050/comment/18034544", "id": "18034544", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=anishshri-db", "name": "anishshri-db", "key": "JIRAUSER287599", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"}, "displayName": "Anish Shrigondekar", "active": true, "timeZone": "Etc/UTC"}, "body": "Issue resolved by pull request 52820\n[https://github.com/apache/spark/pull/52820]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=anishshri-db", "name": "anishshri-db", "key": "JIRAUSER287599", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"}, "displayName": "Anish Shrigondekar", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-11-01T04:42:03.422+0000", "updated": "2025-11-01T04:42:03.422+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-11-01T04:42:03.000+0000", "created": "2025-10-31T20:24:12.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13633035", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13633035", "key": "SPARK-54117", "fields": {"summary": "Throw better error to indicate that TWS is only supported with RocksDB state store provider", "description": "When user tries to use TWS with HDFS as state store, they should get an error explicitly requiring to use RocksDB state provider.", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13633035/comment/18034453", "id": "18034453", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=fedimser", "name": "fedimser", "key": "JIRAUSER311356", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=JIRAUSER311356&avatarId=54569", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=JIRAUSER311356&avatarId=54569", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=JIRAUSER311356&avatarId=54569", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=JIRAUSER311356&avatarId=54569"}, "displayName": "Dmytro Fedoriaka", "active": true, "timeZone": "Etc/UTC"}, "body": "I am working on this.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=fedimser", "name": "fedimser", "key": "JIRAUSER311356", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=JIRAUSER311356&avatarId=54569", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=JIRAUSER311356&avatarId=54569", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=JIRAUSER311356&avatarId=54569", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=JIRAUSER311356&avatarId=54569"}, "displayName": "Dmytro Fedoriaka", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-31T17:13:42.321+0000", "updated": "2025-10-31T17:13:42.321+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-31T23:11:36.000+0000", "created": "2025-10-31T17:09:35.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632999", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632999", "key": "SPARK-54116", "fields": {"summary": "Add off-heap mode support for LongHashedRelation", "description": "LongHashedRelation now only supports on-heap mode. The task is for adding off-heap mode support for the relation.", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-31T12:24:50.000+0000", "created": "2025-10-31T12:13:28.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632988", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632988", "key": "SPARK-54115", "fields": {"summary": "Display connect server execution threads first in thread dump page", "description": null, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-31T17:04:26.000+0000", "created": "2025-10-31T09:50:13.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632982", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632982", "key": "SPARK-54114", "fields": {"summary": "Support getColumns for SparkConnectDatabaseMetaData", "description": null, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-31T09:03:00.000+0000", "created": "2025-10-31T09:03:00.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632981", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632981", "key": "SPARK-54113", "fields": {"summary": "Support getTables for SparkConnectDatabaseMetaData", "description": null, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-31T09:02:33.000+0000", "created": "2025-10-31T09:02:33.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632980", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632980", "key": "SPARK-54112", "fields": {"summary": "Support getSchemas for SparkConnectDatabaseMetaData", "description": null, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-31T17:46:19.000+0000", "created": "2025-10-31T09:02:03.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632979", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632979", "key": "SPARK-54111", "fields": {"summary": "Support getCatalogs for SparkConnectDatabaseMetaData", "description": null, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-31T09:34:27.000+0000", "created": "2025-10-31T09:01:36.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632975", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632975", "key": "SPARK-54110", "fields": {"summary": "Introduce type encoders for Geography and Geometry types", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632975/comment/18034343", "id": "18034343", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=uros-db", "name": "uros-db", "key": "JIRAUSER304339", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"}, "displayName": "Uro\u0161 Bojani\u0107", "active": true, "timeZone": "Etc/UTC"}, "body": "Work in progress: https://github.com/apache/spark/pull/52813.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=uros-db", "name": "uros-db", "key": "JIRAUSER304339", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"}, "displayName": "Uro\u0161 Bojani\u0107", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-31T08:39:14.369+0000", "updated": "2025-10-31T08:39:14.369+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-11-01T09:22:54.000+0000", "created": "2025-10-31T08:32:57.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632967", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632967", "key": "SPARK-54109", "fields": {"summary": "Avoid function conflicts in test_pandas_grouped_map", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632967/comment/18034364", "id": "18034364", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=podongfeng", "name": "podongfeng", "key": "podongfeng", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=podongfeng&avatarId=26870", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=podongfeng&avatarId=26870", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=podongfeng&avatarId=26870", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=podongfeng&avatarId=26870"}, "displayName": "Ruifeng Zheng", "active": true, "timeZone": "Asia/Hong_Kong"}, "body": "Issue resolved by pull request 52811\n[https://github.com/apache/spark/pull/52811]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=podongfeng", "name": "podongfeng", "key": "podongfeng", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=podongfeng&avatarId=26870", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=podongfeng&avatarId=26870", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=podongfeng&avatarId=26870", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=podongfeng&avatarId=26870"}, "displayName": "Ruifeng Zheng", "active": true, "timeZone": "Asia/Hong_Kong"}, "created": "2025-10-31T09:38:25.216+0000", "updated": "2025-10-31T09:38:25.216+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-31T09:38:25.000+0000", "created": "2025-10-31T07:47:22.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632957", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632957", "key": "SPARK-54108", "fields": {"summary": "Revise execute methods of SparkConnectStatement", "description": null, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-31T07:22:06.000+0000", "created": "2025-10-31T06:24:19.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632954", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632954", "key": "SPARK-54107", "fields": {"summary": "Use `4.1.0-preview3-java21-scala` image for preview examples ", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632954/comment/18034322", "id": "18034322", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 413\n[https://github.com/apache/spark-kubernetes-operator/pull/413]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-31T06:10:28.924+0000", "updated": "2025-10-31T06:10:28.924+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-31T06:10:40.000+0000", "created": "2025-10-31T05:41:26.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632952", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632952", "key": "SPARK-54106", "fields": {"summary": "State Store Row Checksum implementation", "description": "Introduce row checksum creation and verification for state store, both {*}HDFS and RocksDB state store{*}. This will help detect corruption at the row level and help prevent corruption.", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632952/comment/18034518", "id": "18034518", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=anishshri-db", "name": "anishshri-db", "key": "JIRAUSER287599", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"}, "displayName": "Anish Shrigondekar", "active": true, "timeZone": "Etc/UTC"}, "body": "-Issue resolved by pull request 52809-\r\n[https://github.com/apache/spark/pull/52809]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=sarutak", "name": "sarutak", "key": "sarutak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=sarutak&avatarId=20842", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=sarutak&avatarId=20842", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=sarutak&avatarId=20842", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=sarutak&avatarId=20842"}, "displayName": "Kousuke Saruta", "active": true, "timeZone": "Asia/Tokyo"}, "created": "2025-11-01T00:17:55.716+0000", "updated": "2025-11-01T17:33:42.212+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13632952/comment/18034767", "id": "18034767", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=sarutak", "name": "sarutak", "key": "sarutak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=sarutak&avatarId=20842", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=sarutak&avatarId=20842", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=sarutak&avatarId=20842", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=sarutak&avatarId=20842"}, "displayName": "Kousuke Saruta", "active": true, "timeZone": "Asia/Tokyo"}, "body": "Reverted.\r\nhttps://github.com/apache/spark/pull/52827", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=sarutak", "name": "sarutak", "key": "sarutak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=sarutak&avatarId=20842", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=sarutak&avatarId=20842", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=sarutak&avatarId=20842", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=sarutak&avatarId=20842"}, "displayName": "Kousuke Saruta", "active": true, "timeZone": "Asia/Tokyo"}, "created": "2025-11-01T17:30:23.587+0000", "updated": "2025-11-01T17:30:23.587+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-11-01T17:34:27.000+0000", "created": "2025-10-31T05:23:24.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632951", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632951", "key": "SPARK-54105", "fields": {"summary": "Update `integration-test-ubuntu-spark41` to use Spark `4.1.0-preview3-java21`", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632951/comment/18034321", "id": "18034321", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 259\n[https://github.com/apache/spark-connect-swift/pull/259]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-31T05:38:25.923+0000", "updated": "2025-10-31T05:38:25.923+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-31T05:38:38.000+0000", "created": "2025-10-31T05:17:51.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632944", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632944", "key": "SPARK-54104", "fields": {"summary": "Disallow casting geospatial types to/from other data types", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632944/comment/18034307", "id": "18034307", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=uros-db", "name": "uros-db", "key": "JIRAUSER304339", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"}, "displayName": "Uro\u0161 Bojani\u0107", "active": true, "timeZone": "Etc/UTC"}, "body": "Work in progress: https://github.com/apache/spark/pull/52806.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=uros-db", "name": "uros-db", "key": "JIRAUSER304339", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"}, "displayName": "Uro\u0161 Bojani\u0107", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-31T02:28:24.374+0000", "updated": "2025-10-31T02:28:24.374+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13632944/comment/18034421", "id": "18034421", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=cloud_fan", "name": "cloud_fan", "key": "cloud_fan", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=cloud_fan&avatarId=23649", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cloud_fan&avatarId=23649", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cloud_fan&avatarId=23649", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cloud_fan&avatarId=23649"}, "displayName": "Wenchen Fan", "active": true, "timeZone": "Asia/Shanghai"}, "body": "Issue resolved by pull request 52806\n[https://github.com/apache/spark/pull/52806]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=cloud_fan", "name": "cloud_fan", "key": "cloud_fan", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=cloud_fan&avatarId=23649", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cloud_fan&avatarId=23649", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cloud_fan&avatarId=23649", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cloud_fan&avatarId=23649"}, "displayName": "Wenchen Fan", "active": true, "timeZone": "Asia/Shanghai"}, "created": "2025-10-31T15:32:09.811+0000", "updated": "2025-10-31T15:32:09.811+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-10-31T15:32:09.000+0000", "created": "2025-10-31T02:25:41.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632938", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632938", "key": "SPARK-54103", "fields": {"summary": "Introduce client-side Geography and Geometry classes", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632938/comment/18034275", "id": "18034275", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=uros-db", "name": "uros-db", "key": "JIRAUSER304339", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"}, "displayName": "Uro\u0161 Bojani\u0107", "active": true, "timeZone": "Etc/UTC"}, "body": "Work in progress: https://github.com/apache/spark/pull/52804.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=uros-db", "name": "uros-db", "key": "JIRAUSER304339", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"}, "displayName": "Uro\u0161 Bojani\u0107", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-31T00:02:28.372+0000", "updated": "2025-10-31T00:02:28.372+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13632938/comment/18034329", "id": "18034329", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=cloud_fan", "name": "cloud_fan", "key": "cloud_fan", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=cloud_fan&avatarId=23649", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cloud_fan&avatarId=23649", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cloud_fan&avatarId=23649", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cloud_fan&avatarId=23649"}, "displayName": "Wenchen Fan", "active": true, "timeZone": "Asia/Shanghai"}, "body": "Issue resolved by pull request 52804\n[https://github.com/apache/spark/pull/52804]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=cloud_fan", "name": "cloud_fan", "key": "cloud_fan", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=cloud_fan&avatarId=23649", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cloud_fan&avatarId=23649", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cloud_fan&avatarId=23649", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cloud_fan&avatarId=23649"}, "displayName": "Wenchen Fan", "active": true, "timeZone": "Asia/Shanghai"}, "created": "2025-10-31T06:51:19.222+0000", "updated": "2025-10-31T06:51:19.222+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-10-31T06:51:19.000+0000", "created": "2025-10-31T00:00:31.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632935", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632935", "key": "SPARK-54102", "fields": {"summary": "Spark 4.0.1 still throws \"String length (20054016) exceeds the maximum length (20000000)\" and \"from_json\" fails on a very large JSON with a jackson_core parse error", "description": "Based on JIRA *SPARK-49872* and the implementation in [{{JsonProtocol.scala}}|https://github.com/apache/spark/blob/29434ea766b0fc3c3bf6eaadb43a8f931133649e/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala#L71], the relevant limit was removed in {*}4.0.1{*}. However, in our environment we can still reliably reproduce:\r\n # When generating/processing a very large string:\r\n\r\n{code:java}\r\nCaused by: com.fasterxml.jackson.core.exc.StreamConstraintsException: String value length (20040525) exceeds the maximum allowed (20000000, from `StreamReadConstraints.getMaxStringLength()`){code}\r\n # When using {{from_json}} on a _valid_ very large single-line JSON (no missing comma), Jackson throws at around column {*}20,271,838{*}:\r\n\r\n{code:java}\r\nCaused by: com.fasterxml.jackson.core.JsonParseException: Unexpected character ('1' (code 49)): was expecting comma to separate Object entries\r\n at [Source: UNKNOWN; line: 1, column: 20271838]{code}\r\nI'm sure this is not a formatting issue. If I truncate the JSON to below column {*}20,271,838{*}, it parses successfully.\r\nHere is my parsing code:\r\n\r\n{code:java}\r\nraw_df.withColumn(\"parsed_item\", f.from_json(f.col(\"item\"), my_schema){code}", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-30T23:41:04.000+0000", "created": "2025-10-30T23:41:04.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632933", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632933", "key": "SPARK-54101", "fields": {"summary": "Introduce the framework for adding ST functions in Scala", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632933/comment/18034272", "id": "18034272", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=uros-db", "name": "uros-db", "key": "JIRAUSER304339", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"}, "displayName": "Uro\u0161 Bojani\u0107", "active": true, "timeZone": "Etc/UTC"}, "body": "Work in progress: https://github.com/apache/spark/pull/52803.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=uros-db", "name": "uros-db", "key": "JIRAUSER304339", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"}, "displayName": "Uro\u0161 Bojani\u0107", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-30T22:53:45.876+0000", "updated": "2025-10-30T22:53:45.876+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13632933/comment/18034356", "id": "18034356", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=cloud_fan", "name": "cloud_fan", "key": "cloud_fan", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=cloud_fan&avatarId=23649", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cloud_fan&avatarId=23649", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cloud_fan&avatarId=23649", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cloud_fan&avatarId=23649"}, "displayName": "Wenchen Fan", "active": true, "timeZone": "Asia/Shanghai"}, "body": "Issue resolved by pull request 52803\n[https://github.com/apache/spark/pull/52803]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=cloud_fan", "name": "cloud_fan", "key": "cloud_fan", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=cloud_fan&avatarId=23649", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cloud_fan&avatarId=23649", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cloud_fan&avatarId=23649", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cloud_fan&avatarId=23649"}, "displayName": "Wenchen Fan", "active": true, "timeZone": "Asia/Shanghai"}, "created": "2025-10-31T09:20:51.012+0000", "updated": "2025-10-31T09:20:51.012+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-10-31T09:20:51.000+0000", "created": "2025-10-30T22:53:14.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632931", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632931", "key": "SPARK-54100", "fields": {"summary": "Remove `ignore.symbol.file` Javac option", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632931/comment/18034297", "id": "18034297", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 52802\n[https://github.com/apache/spark/pull/52802]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-31T00:58:29.845+0000", "updated": "2025-10-31T00:58:29.845+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-31T03:55:13.000+0000", "created": "2025-10-30T22:09:49.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632930", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632930", "key": "SPARK-54099", "fields": {"summary": "XML to Variant conversion throws ArithmeticException on decimals with extreme exponents", "description": "When parsing XML data with `parse_xml` that contains decimal numbers with very large\u00a0\r\nexponents (e.g., \"1E+2147483647\"), the conversion to Variant type fails with:\r\n\r\n```\r\njava.lang.ArithmeticException: BigInteger would overflow supported range\r\n\u00a0 \u00a0 at java.base/java.math.BigDecimal.setScale(BigDecimal.java:3000)\r\n\u00a0 \u00a0 at org.apache.spark.sql.catalyst.xml.StaxXmlParser$.org$apache$spark$sql$catalyst$xml$StaxXmlParser$$appendXMLCharacterToVariant(StaxXmlParser.scala:1285)\r\n```", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-31T00:30:02.000+0000", "created": "2025-10-30T21:30:34.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632929", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632929", "key": "SPARK-54098", "fields": {"summary": "Set `--sun-misc-unsafe-memory-access=allow` for `JavaExec` Gradle tasks", "description": null, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-30T21:45:58.000+0000", "created": "2025-10-30T21:25:47.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632928", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632928", "key": "SPARK-54097", "fields": {"summary": "Upgrade `Gradle` to 9.2.0", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632928/comment/18034265", "id": "18034265", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 411\n[https://github.com/apache/spark-kubernetes-operator/pull/411]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-30T21:23:53.100+0000", "updated": "2025-10-30T21:23:53.100+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-30T21:23:53.000+0000", "created": "2025-10-30T21:10:46.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632923", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632923", "key": "SPARK-54096", "fields": {"summary": "Support Spatial Reference System mapping in PySpark", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632923/comment/18034248", "id": "18034248", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=uros-db", "name": "uros-db", "key": "JIRAUSER304339", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"}, "displayName": "Uro\u0161 Bojani\u0107", "active": true, "timeZone": "Etc/UTC"}, "body": "Work in progress: https://github.com/apache/spark/pull/52799.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=uros-db", "name": "uros-db", "key": "JIRAUSER304339", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"}, "displayName": "Uro\u0161 Bojani\u0107", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-30T19:39:42.148+0000", "updated": "2025-10-30T19:39:42.148+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13632923/comment/18034336", "id": "18034336", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=podongfeng", "name": "podongfeng", "key": "podongfeng", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=podongfeng&avatarId=26870", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=podongfeng&avatarId=26870", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=podongfeng&avatarId=26870", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=podongfeng&avatarId=26870"}, "displayName": "Ruifeng Zheng", "active": true, "timeZone": "Asia/Hong_Kong"}, "body": "Issue resolved by pull request 52799\n[https://github.com/apache/spark/pull/52799]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=podongfeng", "name": "podongfeng", "key": "podongfeng", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=podongfeng&avatarId=26870", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=podongfeng&avatarId=26870", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=podongfeng&avatarId=26870", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=podongfeng&avatarId=26870"}, "displayName": "Ruifeng Zheng", "active": true, "timeZone": "Asia/Hong_Kong"}, "created": "2025-10-31T07:54:49.996+0000", "updated": "2025-10-31T07:54:49.996+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-10-31T07:54:59.000+0000", "created": "2025-10-30T19:29:49.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632914", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632914", "key": "SPARK-54095", "fields": {"summary": "Release Spark Connect Swift Client 0.6.0", "description": null, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-30T17:50:32.000+0000", "created": "2025-10-30T17:49:49.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632913", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632913", "key": "SPARK-54094", "fields": {"summary": "Extract common methods to KafkaOffsetReaderBase", "description": "When reviewing https://github.com/apache/spark/pull/52729, I found that KafkaOffsetReaderConsumer and KafkaOffsetReaderAdmin have the exactly same getOffsetRangesFromResolvedOffsets method. The method is actually long so seems good to extract them to common one.", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632913/comment/18034261", "id": "18034261", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=viirya", "name": "viirya", "key": "viirya", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=viirya&avatarId=40656", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=viirya&avatarId=40656", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=viirya&avatarId=40656", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=viirya&avatarId=40656"}, "displayName": "L. C. Hsieh", "active": true, "timeZone": "Asia/Shanghai"}, "body": "Issue resolved by pull request 52788\n[https://github.com/apache/spark/pull/52788]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=viirya", "name": "viirya", "key": "viirya", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=viirya&avatarId=40656", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=viirya&avatarId=40656", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=viirya&avatarId=40656", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=viirya&avatarId=40656"}, "displayName": "L. C. Hsieh", "active": true, "timeZone": "Asia/Shanghai"}, "created": "2025-10-30T21:05:07.609+0000", "updated": "2025-10-30T21:05:07.609+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-30T22:08:46.000+0000", "created": "2025-10-30T17:45:28.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632907", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632907", "key": "SPARK-54093", "fields": {"summary": "Release Spark Kubernetes Operator 0.7.0", "description": null, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-30T15:49:54.000+0000", "created": "2025-10-30T15:49:36.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632904", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632904", "key": "SPARK-54092", "fields": {"summary": "Use Java-friendly `KubernetesClientUtils` APIs", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632904/comment/18034213", "id": "18034213", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 410\n[https://github.com/apache/spark-kubernetes-operator/pull/410]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-30T16:07:40.733+0000", "updated": "2025-10-30T16:07:40.733+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-30T17:27:48.000+0000", "created": "2025-10-30T15:24:09.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632888", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632888", "key": "SPARK-54091", "fields": {"summary": "Implement the ST_Srid expression in SQL", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632888/comment/18034176", "id": "18034176", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=uros-db", "name": "uros-db", "key": "JIRAUSER304339", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"}, "displayName": "Uro\u0161 Bojani\u0107", "active": true, "timeZone": "Etc/UTC"}, "body": "Work in progress: https://github.com/apache/spark/pull/52795.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=uros-db", "name": "uros-db", "key": "JIRAUSER304339", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"}, "displayName": "Uro\u0161 Bojani\u0107", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-30T13:22:27.645+0000", "updated": "2025-10-30T13:22:27.645+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13632888/comment/18034318", "id": "18034318", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=cloud_fan", "name": "cloud_fan", "key": "cloud_fan", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=cloud_fan&avatarId=23649", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cloud_fan&avatarId=23649", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cloud_fan&avatarId=23649", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cloud_fan&avatarId=23649"}, "displayName": "Wenchen Fan", "active": true, "timeZone": "Asia/Shanghai"}, "body": "Issue resolved by pull request 52795\n[https://github.com/apache/spark/pull/52795]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=cloud_fan", "name": "cloud_fan", "key": "cloud_fan", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=cloud_fan&avatarId=23649", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cloud_fan&avatarId=23649", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cloud_fan&avatarId=23649", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cloud_fan&avatarId=23649"}, "displayName": "Wenchen Fan", "active": true, "timeZone": "Asia/Shanghai"}, "created": "2025-10-31T04:42:39.703+0000", "updated": "2025-10-31T04:42:39.703+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-10-31T04:42:39.000+0000", "created": "2025-10-30T13:20:13.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632875", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632875", "key": "SPARK-54090", "fields": {"summary": "AssertDataframeEqual carries rows when showing differences", "description": "When we try to do assertDataFrameEqual, on two dataframes that have not the same amount of rows, the output gets cascading from the difference till the end. Why this is happening:\r\n\r\n[https://github.com/apache/spark/blob/067969ff946712eeabf47040415f25000837cd87/python/pyspark/testing/utils.py#L1036]\r\n{code:java}\r\n\u00a0 \u00a0 def assert_rows_equal(\r\n\u00a0 \u00a0 \u00a0 \u00a0 rows1: List[Row], rows2: List[Row], maxErrors: int = None, showOnlyDiff: bool = False\r\n\u00a0 \u00a0 ):\r\n\u00a0 \u00a0 \u00a0 \u00a0 __tracebackhide__ = True\r\n\u00a0 \u00a0 \u00a0 \u00a0 zipped = list(zip_longest(rows1, rows2))\r\n\u00a0 \u00a0 \u00a0 \u00a0 diff_rows_cnt = 0\r\n\u00a0 \u00a0 \u00a0 \u00a0 diff_rows = []\r\n\u00a0 \u00a0 \u00a0 \u00a0 has_diff_rows = False\u00a0 \u00a0 \u00a0 \u00a0 \r\n        rows_str1 = \"\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 rows_str2 = \"\"\u00a0 \u00a0 \u00a0 \u00a0 \r\n        \r\n        # count different rows\r\n\u00a0 \u00a0 \u00a0 \u00a0 for r1, r2 in zipped:\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if not compare_rows(r1, r2):\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 diff_rows_cnt += 1\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 has_diff_rows = True\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if includeDiffRows:\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 diff_rows.append((r1, r2))\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 rows_str1 += str(r1) + \"\\n\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 rows_str2 += str(r2) + \"\\n\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if maxErrors is not None and diff_rows_cnt >= maxErrors:\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 break\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 elif not showOnlyDiff:\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 rows_str1 += str(r1) + \"\\n\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 rows_str2 += str(r2) + \"\\n\"\u00a0 \u00a0 \u00a0 \u00a0 \r\n        generated_diff = _context_diff(\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 actual=rows_str1.splitlines(), expected=rows_str2.splitlines(), n=len(zipped)\r\n\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \r\n        if has_diff_rows:\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 error_msg = \"Results do not match: \"\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 percent_diff = (diff_rows_cnt / len(zipped)) * 100\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 error_msg += \"( %.5f %% )\" % percent_diff\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 error_msg += \"\\n\" + \"\\n\".join(generated_diff)\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 data = diff_rows if includeDiffRows else None\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 raise PySparkAssertionError(\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 errorClass=\"DIFFERENT_ROWS\", messageParameters={\"error_msg\": error_msg}, data=data\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ){code}\r\nThe problem lies in the way that we zip the lines\r\n{code:java}\r\nzipped = list(zip_longest(rows1, rows2)){code}\r\nWith zip longest we assume that the rows are in order and we do position by position comparison but it does not work well with checkRowOrder which defaults to False.\r\n\r\nIf I have 1 line difference in 100 line dataframe the result percentage won't be 1% but the amount of rows that cascade towards on from that difference.\u00a0\r\n\r\nThe best solution here would be to have a set based comparison and return the percentage and the rows over that.\r\n\r\nA sample of what zip_longest is doing:\r\n{code:java}\r\nfrom itertools import zip_longest\r\nrows1 = [    'A',    'B',    'C',    'D',    'E']\r\nrows2 = [    'A',    'C',    'D',]\r\nzipped = list(zip_longest(rows1, rows2))zipped {code}\r\nResult:\r\n{code:java}\r\n[('A', 'A'), ('B', 'C'), ('C', 'D'), ('D', None), ('E', None)]{code}\r\nSo in this case we would have 80% rows failure.\r\n\r\nThis comes directly with the implementation of CheckRowOrder, when it is True we do not sort and it makes sense to use the zip_longest, when it is False it makes sense to use set based comparison since we have already sorted.\r\n\r\n\u00a0", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-30T13:09:58.000+0000", "created": "2025-10-30T10:44:10.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632871", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632871", "key": "SPARK-54089", "fields": {"summary": "Add off-heap mode support for on-heap-only memory consumers", "description": "There are a few memory consumers that only support on-heap mode. Including:\r\n\r\n# LongToUnsafeRowMap (for long key hash join)\r\n# ExternalSorter (for non-serializable sort-based shuffle)\r\n\r\nIt's ideal to make them support off-heap when Spark is running with off-heap memory mode. For 3rd accelerator plugins that leverage Spark off-heap memory, having the large allocations in vanilla Spark supporting off-heap mode will significantly ease the memory settings for running the mixed query plan that contains both vanilla Spark and offloaded computations. ", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-31T15:02:55.000+0000", "created": "2025-10-30T10:32:48.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632858", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632858", "key": "SPARK-54088", "fields": {"summary": "When TaskSchedulerImpl searches for an executor to schedule an unschedulable task, it should exclude executors that are pending to remove", "description": "Call kill executor\r\n\r\n!image-2025-10-30-17-22-25-127.png|width=1255,height=241!\r\n\r\nContainer 18 didn't kill it\r\n\u00a0\r\n!image-2025-10-30-17-24-03-632.png|width=1087,height=183!\r\n\r\n\u00a0\r\n\r\nPending container causing\u00a0 task can't be scheduled .\r\n\r\n\u00a0", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-11-01T07:25:22.000+0000", "created": "2025-10-30T09:14:40.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632855", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632855", "key": "SPARK-54087", "fields": {"summary": "Spark Executor launch task failed should return task killed message", "description": "When not enable speculation ,executor launch task failed caused by OOM or other issue, DAGScheduler won;t know task failed and won't scheduler again, causing application stuck\r\n\r\n!image-2025-10-30-16-52-22-524.png|width=589,height=233!", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-30T09:02:56.000+0000", "created": "2025-10-30T08:50:01.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632852", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632852", "key": "SPARK-54086", "fields": {"summary": "Support IO_URING Netty IO Mode", "description": null, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-31T22:47:01.000+0000", "created": "2025-10-30T08:30:49.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632846", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632846", "key": "SPARK-54085", "fields": {"summary": "Fix `initialize` to add `CREATE` option additionally in `DriverRunner`", "description": "When submitting jobs to standalone cluster using restful api, we get errors like:\r\n\r\n```\r\n25/10/30 16:02:59 INFO DriverRunner: Killing driver process!\r\n25/10/30 16:02:59 INFO CommandUtils: Redirection to /Users/ericgao/workspace/open_source/spark/work/driver-20251030160259-0020/stdout closed: Stream closed\r\n25/10/30 16:02:59 WARN Worker: Driver driver-20251030160259-0020 failed with unrecoverable exception: java.nio.file.NoSuchFileException: /Users/ericgao/workspace/open_source/spark/work/driver-20251030160259-0020/stderr\r\n\r\n```\r\n\r\nWe need to fix the usage of `Files.writeString` in DriverRunner", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632846/comment/18034114", "id": "18034114", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=chufenggao", "name": "chufenggao", "key": "JIRAUSER292643", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=JIRAUSER292643&avatarId=53237", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=JIRAUSER292643&avatarId=53237", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=JIRAUSER292643&avatarId=53237", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=JIRAUSER292643&avatarId=53237"}, "displayName": "Chufeng Gao", "active": true, "timeZone": "Etc/UTC"}, "body": "I'd like to fix it.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=chufenggao", "name": "chufenggao", "key": "JIRAUSER292643", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=JIRAUSER292643&avatarId=53237", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=JIRAUSER292643&avatarId=53237", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=JIRAUSER292643&avatarId=53237", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=JIRAUSER292643&avatarId=53237"}, "displayName": "Chufeng Gao", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-30T08:12:38.961+0000", "updated": "2025-10-30T08:12:38.961+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13632846/comment/18034203", "id": "18034203", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 52789\n[https://github.com/apache/spark/pull/52789]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-30T15:46:14.871+0000", "updated": "2025-10-30T15:46:14.871+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-10-30T15:52:51.000+0000", "created": "2025-10-30T08:12:22.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632831", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632831", "key": "SPARK-54084", "fields": {"summary": "Publish Apache Spark 4.1.0-preview3 to docker registry", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632831/comment/18034315", "id": "18034315", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "This is resolved via https://github.com/apache/spark-docker/pull/97", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-31T03:56:52.967+0000", "updated": "2025-10-31T03:56:52.967+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-31T03:59:01.000+0000", "created": "2025-10-30T05:22:24.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632830", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632830", "key": "SPARK-54083", "fields": {"summary": "Use `4.1.0-preview3` instead of `RC1`", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632830/comment/18034099", "id": "18034099", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 258\n[https://github.com/apache/spark-connect-swift/pull/258]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-30T05:57:43.848+0000", "updated": "2025-10-30T05:57:43.848+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-30T16:04:02.000+0000", "created": "2025-10-30T05:06:34.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632829", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632829", "key": "SPARK-54082", "fields": {"summary": "Upgrade Spark to `4.1.0-preview3`", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632829/comment/18034186", "id": "18034186", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 409\n[https://github.com/apache/spark-kubernetes-operator/pull/409]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-30T14:28:45.419+0000", "updated": "2025-10-30T14:28:45.419+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-30T14:28:45.000+0000", "created": "2025-10-30T04:59:24.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632818", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632818", "key": "SPARK-54081", "fields": {"summary": "Add `word-count-preview.yaml` Example", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632818/comment/18034085", "id": "18034085", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 408\n[https://github.com/apache/spark-kubernetes-operator/pull/408]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-30T04:15:37.557+0000", "updated": "2025-10-30T04:15:37.557+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-30T04:15:51.000+0000", "created": "2025-10-29T23:45:58.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632805", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632805", "key": "SPARK-54080", "fields": {"summary": "Use Swift 6.2 as the minimum supported version", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632805/comment/18033938", "id": "18033938", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 257\n[https://github.com/apache/spark-connect-swift/pull/257]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-29T18:32:26.042+0000", "updated": "2025-10-29T18:32:26.042+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-29T18:32:26.000+0000", "created": "2025-10-29T18:04:09.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632803", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632803", "key": "SPARK-54079", "fields": {"summary": "Introduce the framework for adding ST expressions in Catalyst", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632803/comment/18033925", "id": "18033925", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=uros-db", "name": "uros-db", "key": "JIRAUSER304339", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"}, "displayName": "Uro\u0161 Bojani\u0107", "active": true, "timeZone": "Etc/UTC"}, "body": "Work in progress: https://github.com/apache/spark/pull/52784.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=uros-db", "name": "uros-db", "key": "JIRAUSER304339", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"}, "displayName": "Uro\u0161 Bojani\u0107", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-29T17:53:47.537+0000", "updated": "2025-10-29T17:53:47.537+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13632803/comment/18034076", "id": "18034076", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=cloud_fan", "name": "cloud_fan", "key": "cloud_fan", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=cloud_fan&avatarId=23649", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cloud_fan&avatarId=23649", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cloud_fan&avatarId=23649", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cloud_fan&avatarId=23649"}, "displayName": "Wenchen Fan", "active": true, "timeZone": "Asia/Shanghai"}, "body": "Issue resolved by pull request 52784\n[https://github.com/apache/spark/pull/52784]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=cloud_fan", "name": "cloud_fan", "key": "cloud_fan", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=cloud_fan&avatarId=23649", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cloud_fan&avatarId=23649", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cloud_fan&avatarId=23649", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cloud_fan&avatarId=23649"}, "displayName": "Wenchen Fan", "active": true, "timeZone": "Asia/Shanghai"}, "created": "2025-10-30T02:22:59.190+0000", "updated": "2025-10-30T02:22:59.190+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-10-31T02:35:09.000+0000", "created": "2025-10-29T17:47:59.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632796", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632796", "key": "SPARK-54078", "fields": {"summary": "Deflake StateStoreSuite `SPARK-40492: maintenance before unload`", "description": "`SPARK-40492: maintenance before unload` in StateStoreSuite is flaky due to timing issues. Fix this by adding hooks to pause maintenance.", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632796/comment/18034221", "id": "18034221", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=anishshri-db", "name": "anishshri-db", "key": "JIRAUSER287599", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"}, "displayName": "Anish Shrigondekar", "active": true, "timeZone": "Etc/UTC"}, "body": "Issue resolved by pull request 52783\n[https://github.com/apache/spark/pull/52783]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=anishshri-db", "name": "anishshri-db", "key": "JIRAUSER287599", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"}, "displayName": "Anish Shrigondekar", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-30T17:13:36.877+0000", "updated": "2025-10-30T17:13:36.877+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-30T17:13:36.000+0000", "created": "2025-10-29T16:40:18.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632795", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632795", "key": "SPARK-54077", "fields": {"summary": "Enable all disabled tests on Linux", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632795/comment/18033910", "id": "18033910", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 256\n[https://github.com/apache/spark-connect-swift/pull/256]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-29T17:24:17.799+0000", "updated": "2025-10-29T17:24:17.799+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-29T17:24:39.000+0000", "created": "2025-10-29T16:37:05.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632785", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632785", "key": "SPARK-54076", "fields": {"summary": "Fix Flaky Test:`OracleJoinPushdownIntegrationSuite`", "description": "Recently, `OracleJoinPushdownIntegrationSuite` frequently fails.\r\n\r\nhttps://github.com/beliefer/spark/actions/runs/18904457292/job/53959322233#logs\r\n{code:java}\r\n[info] org.apache.spark.sql.jdbc.v2.join.OracleJoinPushdownIntegrationSuite *** ABORTED *** (10 minutes, 15 seconds)\r\n[info]   The code passed to eventually never returned normally. Attempted 599 times over 10.012648813883333 minutes. Last failure message: ORA-12541: Cannot connect. No listener at host 10.1.0.201 port 44551. (CONNECTION_ID=5rB4vZNTQR2sFHpk9xQC1A==)\r\n[info]   https://docs.oracle.com/error-help/db/ora-12541/. (DockerJDBCIntegrationSuite.scala:214)\r\n[info]   org.scalatest.exceptions.TestFailedDueToTimeoutException:\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:219)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.eventually(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$1(DockerJDBCIntegrationSuite.scala:214)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled(DockerIntegrationFunSuite.scala:49)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled$(DockerIntegrationFunSuite.scala:47)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.runIfTestsEnabled(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.beforeAll(DockerJDBCIntegrationSuite.scala:133)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n[info]   at java.base/java.lang.Thread.run(Thread.java:840)\r\n[info]   Cause: java.sql.SQLException: ORA-12541: Cannot connect. No listener at host 10.1.0.201 port 44551. (CONNECTION_ID=5rB4vZNTQR2sFHpk9xQC1A==)\r\n[info] https://docs.oracle.com/error-help/db/ora-12541/\r\n[info]   at oracle.jdbc.driver.T4CConnection.handleLogonNetException(T4CConnection.java:1631)\r\n[info]   at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:1151)\r\n[info]   at oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:1189)\r\n[info]   at oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:106)\r\n[info]   at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:895)\r\n[info]   at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:702)\r\n[info]   at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:681)\r\n[info]   at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:190)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.getConnection(DockerJDBCIntegrationSuite.scala:250)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$8(DockerJDBCIntegrationSuite.scala:215)\r\n[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.eventually(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$1(DockerJDBCIntegrationSuite.scala:214)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled(DockerIntegrationFunSuite.scala:49)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled$(DockerIntegrationFunSuite.scala:47)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.runIfTestsEnabled(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.beforeAll(DockerJDBCIntegrationSuite.scala:133)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n[info]   at java.base/java.lang.Thread.run(Thread.java:840)\r\n[info]   Cause: oracle.net.ns.NetException: ORA-12541: Cannot connect. No listener at host 10.1.0.201 port 44551. (CONNECTION_ID=5rB4vZNTQR2sFHpk9xQC1A==)\r\n[info] https://docs.oracle.com/error-help/db/ora-12541/\r\n[info]   at oracle.net.nt.TcpNTAdapter.handleEstablishSocketException(TcpNTAdapter.java:418)\r\n[info]   at oracle.net.nt.TcpNTAdapter.establishSocket(TcpNTAdapter.java:350)\r\n[info]   at oracle.net.nt.TcpNTAdapter.connect(TcpNTAdapter.java:228)\r\n[info]   at oracle.net.nt.ConnOption.connect(ConnOption.java:333)\r\n[info]   at oracle.net.nt.ConnStrategy.executeConnOption(ConnStrategy.java:1223)\r\n[info]   at oracle.net.nt.ConnStrategy.execute(ConnStrategy.java:762)\r\n[info]   at oracle.net.resolver.AddrResolution.resolveAndExecute(AddrResolution.java:712)\r\n[info]   at oracle.net.ns.NSProtocol.establishConnection(NSProtocol.java:960)\r\n[info]   at oracle.net.ns.NSProtocol.connect(NSProtocol.java:329)\r\n[info]   at oracle.jdbc.driver.T4CConnection.connectNetworkSessionProtocol(T4CConnection.java:3462)\r\n[info]   at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:1030)\r\n[info]   at oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:1189)\r\n[info]   at oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:106)\r\n[info]   at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:895)\r\n[info]   at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:702)\r\n[info]   at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:681)\r\n[info]   at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:190)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.getConnection(DockerJDBCIntegrationSuite.scala:250)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$8(DockerJDBCIntegrationSuite.scala:215)\r\n[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.eventually(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$1(DockerJDBCIntegrationSuite.scala:214)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled(DockerIntegrationFunSuite.scala:49)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled$(DockerIntegrationFunSuite.scala:47)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.runIfTestsEnabled(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.beforeAll(DockerJDBCIntegrationSuite.scala:133)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n[info]   at java.base/java.lang.Thread.run(Thread.java:840)\r\n[info]   Cause: java.net.ConnectException: Connection refused\r\n[info]   at java.base/sun.nio.ch.Net.pollConnect(Native Method)\r\n[info]   at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)\r\n[info]   at java.base/sun.nio.ch.SocketChannelImpl.finishTimedConnect(SocketChannelImpl.java:1141)\r\n[info]   at java.base/sun.nio.ch.SocketChannelImpl.blockingConnect(SocketChannelImpl.java:1183)\r\n[info]   at java.base/sun.nio.ch.SocketAdaptor.connect(SocketAdaptor.java:98)\r\n[info]   at oracle.net.nt.TimeoutSocketChannel.doConnect(TimeoutSocketChannel.java:289)\r\n[info]   at oracle.net.nt.TimeoutSocketChannel.initializeSocketChannel(TimeoutSocketChannel.java:269)\r\n[info]   at oracle.net.nt.TimeoutSocketChannel.connect(TimeoutSocketChannel.java:236)\r\n[info]   at oracle.net.nt.TimeoutSocketChannel.<init>(TimeoutSocketChannel.java:203)\r\n[info]   at oracle.net.nt.TcpNTAdapter.establishSocket(TcpNTAdapter.java:339)\r\n[info]   at oracle.net.nt.TcpNTAdapter.connect(TcpNTAdapter.java:228)\r\n[info]   at oracle.net.nt.ConnOption.connect(ConnOption.java:333)\r\n[info]   at oracle.net.nt.ConnStrategy.executeConnOption(ConnStrategy.java:1223)\r\n[info]   at oracle.net.nt.ConnStrategy.execute(ConnStrategy.java:762)\r\n[info]   at oracle.net.resolver.AddrResolution.resolveAndExecute(AddrResolution.java:712)\r\n[info]   at oracle.net.ns.NSProtocol.establishConnection(NSProtocol.java:960)\r\n[info]   at oracle.net.ns.NSProtocol.connect(NSProtocol.java:329)\r\n[info]   at oracle.jdbc.driver.T4CConnection.connectNetworkSessionProtocol(T4CConnection.java:3462)\r\n[info]   at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:1030)\r\n[info]   at oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:1189)\r\n[info]   at oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:106)\r\n[info]   at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:895)\r\n[info]   at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:702)\r\n[info]   at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:681)\r\n[info]   at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:190)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.getConnection(DockerJDBCIntegrationSuite.scala:250)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$8(DockerJDBCIntegrationSuite.scala:215)\r\n[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.eventually(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$1(DockerJDBCIntegrationSuite.scala:214)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled(DockerIntegrationFunSuite.scala:49)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled$(DockerIntegrationFunSuite.scala:47)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.runIfTestsEnabled(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.beforeAll(DockerJDBCIntegrationSuite.scala:133)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n[info]   at java.base/java.lang.Thread.run(Thread.java:840) {code}", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632785/comment/18033905", "id": "18033905", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 52780\n[https://github.com/apache/spark/pull/52780]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-29T17:17:16.164+0000", "updated": "2025-10-29T17:17:16.164+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-30T04:48:07.000+0000", "created": "2025-10-29T14:56:32.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632784", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632784", "key": "SPARK-54075", "fields": {"summary": "Make ResolvedCollation evaluable", "description": "In this PR I propose to make ResolvedCollation evaluable. By making ResolvedCollation evaluable, it can now pass the canEvaluateWithinJoin check, allowing the optimizer to use efficient hash joins for queries with inline COLLATE in join conditions (before this change we would fallback to BroadcastNestedLoopJoin).", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632784/comment/18033872", "id": "18033872", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Hi, [~mihailoale-db] .\r\n\r\nApache Spark community has a policy which manages `Fix Version` and `Target Version` like the following. So, please don't set it when you file a JIRA issue.\r\n\r\n- https://spark.apache.org/contributing.html\r\n\r\n{quote}Do not set the following fields:\r\n- Fix Version. This is assigned by committers only when resolved.\r\n- Target Version. This is assigned by committers to indicate a PR has been accepted for possible fix by the target version.\r\n{quote}", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-29T15:55:15.412+0000", "updated": "2025-10-29T15:55:15.412+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13632784/comment/18033921", "id": "18033921", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 52779\n[https://github.com/apache/spark/pull/52779]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-29T17:34:35.571+0000", "updated": "2025-10-29T17:34:35.571+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-10-29T17:35:50.000+0000", "created": "2025-10-29T14:48:46.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632750", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632750", "key": "SPARK-54074", "fields": {"summary": "Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when offset(s) in planned topic partitions got unavailable during subsequent batches`", "description": "Similar to `Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches` resolved in SPARK-54058, `Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite` also seems flaky.\r\n\r\nhttps://github.com/micheal-o/spark/actions/runs/18895266750/job/53931238163\r\n\r\n{code}\r\n[info] - Query with Trigger.AvailableNow should throw error when offset(s) in planned topic partitions got unavailable during subsequent batches *** FAILED *** (1 minute)\r\n[info]   java.lang.AssertionError: assertion failed: Exception tree doesn't contain the expected exception with message: Some of partitions in Kafka topic(s) report available offset which is less than end offset during running query with Trigger.AvailableNow.\r\n[info] org.scalatest.exceptions.TestFailedException: isPropagated was false Partition [topic-42, 1] metadata not propagated after timeout\r\n[info] \tat org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)\r\n[info] \tat org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)\r\n[info] \tat org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)\r\n[info] \tat org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.$anonfun$waitUntilMetadataIsPropagated$1(KafkaTestUtils.scala:614)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info] \tat org.scalatest.concurrent.Eventually.eventually(Eventually.scala:348)\r\n[info] \tat org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:347)\r\n[info] \tat org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:457)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.waitUntilMetadataIsPropagated(KafkaTestUtils.scala:613)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.$anonfun$createTopic$1(KafkaTestUtils.scala:378)\r\n[info] \tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:256)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.createTopic(KafkaTestUtils.scala:377)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaMicroBatchSourceSuiteBase.$anonfun$new$16(KafkaMicroBatchSourceSuite.scala:406)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaMicroBatchSourceSuiteBase.$anonfun$new$16$adapted(KafkaMicroBatchSourceSuite.scala:403)\r\n[info] \tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.callBatchWriter(ForeachBatchSink.scala:56)\r\n[info] \tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:49)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runBatch$19(MicroBatchExecution.scala:1063)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:176)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:284)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:138)\r\n[info] \tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n[info] \tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n[info] \tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n[info] \tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:138)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:307)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:137)\r\n[info] \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:91)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:249)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runBatch$18(MicroBatchExecution.scala:1054)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.ProgressContext.reportTimeTaken(ProgressReporter.scala:200)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.runBatch(MicroBatchExecution.scala:1054)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$executeOneBatch$2(MicroBatchExecution.scala:513)\r\n[info] \tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.ProgressContext.reportTimeTaken(ProgressReporter.scala:200)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.executeOneBatch(MicroBatchExecution.scala:478)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:458)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runActivatedStream$1$adapted(MicroBatchExecution.scala:458)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.TriggerExecutor.runOneBatch(TriggerExecutor.scala:40)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.TriggerExecutor.runOneBatch$(TriggerExecutor.scala:38)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MultiBatchExecutor.runOneBatch(TriggerExecutor.scala:60)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MultiBatchExecutor.execute(TriggerExecutor.scala:65)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:458)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:347)\r\n[info] \tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info] \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.StreamExecution.org$apache$spark$sql$execution$streaming$runtime$StreamExecution$$runStream(StreamExecution.scala:307)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.StreamExecution$$anon$1.run(StreamExecution.scala:230)\r\n[info]   at scala.Predef$.assert(Predef.scala:279)\r\n[info]   at org.apache.spark.TestUtils$.assertExceptionMsg(TestUtils.scala:198)\r\n[info]   at org.apache.spark.sql.kafka010.KafkaMicroBatchSourceSuiteBase.$anonfun$new$14(KafkaMicroBatchSourceSuite.scala:428)\r\n[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info]   at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)\r\n[info]   at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)\r\n[info]   at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)\r\n[info]   at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)\r\n[info]   at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:68)\r\n[info]   at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:154)\r\n[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)\r\n[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)\r\n[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\r\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)\r\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)\r\n[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:226)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)\r\n[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)\r\n[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)\r\n[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)\r\n[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)\r\n[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)\r\n[info]   at scala.collection.immutable.List.foreach(List.scala:323)\r\n[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)\r\n[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)\r\n[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)\r\n[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)\r\n[info]   at org.scalatest.Suite.run(Suite.scala:1114)\r\n[info]   at org.scalatest.Suite.run$(Suite.scala:1096)\r\n[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)\r\n[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)\r\n[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n[info]   at java.base/java.lang.Thread.run(Thread.java:840)\r\n{code}", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632750/comment/18033888", "id": "18033888", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 52777\n[https://github.com/apache/spark/pull/52777]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-29T16:26:32.062+0000", "updated": "2025-10-29T16:26:32.062+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-29T16:27:25.000+0000", "created": "2025-10-29T10:36:24.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632725", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632725", "key": "SPARK-54073", "fields": {"summary": "Improve `ConfOptionDocGenerator` to generate a sorted doc by config key", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632725/comment/18033750", "id": "18033750", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 407\n[https://github.com/apache/spark-kubernetes-operator/pull/407]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-29T06:01:19.443+0000", "updated": "2025-10-29T06:01:19.443+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-29T15:07:17.000+0000", "created": "2025-10-29T04:41:02.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632721", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632721", "key": "SPARK-54072", "fields": {"summary": "Make sure we don't upload empty files in RocksDB snapshot", "description": "This change is to make sure the RocksDB snapshot zip file doesn't contain empty files (except RocksDB log file). Other files such as OPTION, manifest, metadata are not expected to be empty in any situation.\r\n\r\nIntroduced a conf\u00a0{{verifyNonEmptyFilesInZip}}\u00a0to make sure we can turn this off if needed.", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632721/comment/18033961", "id": "18033961", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=anishshri-db", "name": "anishshri-db", "key": "JIRAUSER287599", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"}, "displayName": "Anish Shrigondekar", "active": true, "timeZone": "Etc/UTC"}, "body": "Issue resolved by pull request 52774\n[https://github.com/apache/spark/pull/52774]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=anishshri-db", "name": "anishshri-db", "key": "JIRAUSER287599", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"}, "displayName": "Anish Shrigondekar", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-29T20:58:41.430+0000", "updated": "2025-10-29T20:58:41.430+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-29T21:08:09.000+0000", "created": "2025-10-29T02:42:37.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632718", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632718", "key": "SPARK-54071", "fields": {"summary": "Spark Structured Streaming Filesink can not generate open lineage with output details", "description": "h2. Environment details\r\nh3. OpenLineage version\r\n\r\n{quote}io.openlineage:openlineage-spark_2.13:1.39.0\r\nTechnology and package versions\r\n\r\nPython: 3.13.3\r\nScala: 2.13.16\r\nJava: OpenJDK 64-Bit Server VM, 17.0.16\r\n\r\npip freeze\r\npy4j==0.10.9.9\r\npyspark==4.0.1{quote}\r\n\r\n\r\nFor the openlineage set up, I used the default setting:\r\n\r\n\r\n{quote}$ git clone https://github.com/MarquezProject/marquez.git && cd marquez\r\n\r\n$ ./docker/up.sh{quote}\r\n\r\nh3. Spark Deployment details\r\n\r\nI used native spark on local machine. There is no managed services involved.\r\nProblem details\r\n\r\nh2. Issue details\r\nWhen using Spark structured streaming to write parquet file to file systems,\r\n\r\n*     File sink will only generate openlineage event with streaming processing type with output information as empty.\r\n*     Foreachbatch sink will generate openlineage event with both streaming processing type and batch processing type. The batch processing type will have valid output information.\r\n\r\nThe bug is that File sink in Spark structured streaming does not generate open lineage event with output details.\r\n\r\nMore details about the sample code and sample events are following.\r\nFile sink:\r\nSample code:\r\n{quote}\r\nquery = streaming_df.writeStream \\\r\n    .format('parquet') \\\r\n    .outputMode('append') \\\r\n    .partitionBy('year', 'month', 'day') \\\r\n    .option('checkpointLocation', checkpoint_path) \\\r\n    .option('path', output_path) \\\r\n    .queryName('filesink') \\\r\n    .start()\r\n{quote}\r\nSample event for \"processingType\":\"STREAMING\"\r\n{quote}\r\n25/10/29 00:49:02 DEBUG wire: http-outgoing-52 >> \"{\"eventTime\":\"2025-10-28T13:45:34.282Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"019a2b14-4e9d-7574-95a9-55182f07591d\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-0/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"019a2b10-5218-7ba3-9cf4-1ce4b1800752\"},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"filesink\"},\"root\":{\"run\":{\"runId\":\"019a2b10-5218-7ba3-9cf4-1ce4b1800752\"},\"job\":{\"namespace\":\"airflow_namespace\",\"name\":\"airflow_dag.airflow_task\"}}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"filesink\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"4.0.1\",\"name\":\"spark\",\"openlineageAdapterVersion\":\"1.39.0\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"filesink.project\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-3/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"STREAMING\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\"{quote}\r\n\r\nforeachbatch sink\r\nsample code\r\n{quote}\r\ndef write_to_file(batch_df, batch_id):\r\n    if batch_df.count() > 0:\r\n        batch_df.write \\\r\n            .mode(\"append\") \\\r\n            .partitionBy(\"year\", \"month\", \"day\") \\\r\n            .parquet(output_path)\r\n{quote}\r\n{quote}\r\nquery = streaming_df \\\r\n    .writeStream \\\r\n    .outputMode(\"append\") \\\r\n    .foreachBatch(write_to_file) \\\r\n    .option(\"checkpointLocation\", checkpoint_path) \\\r\n    .trigger(processingTime='10 seconds') \\\r\n    .start()\r\n{quote}\r\n\r\nThe above code with generate both streaming and batch processing type event.\r\nSample streaming type event\r\n{quote}\r\n25/10/29 01:04:45 DEBUG wire: http-outgoing-1 >> \"{\"eventTime\":\"2025-10-28T14:04:43.373Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"019a2b22-b364-79ca-be87-2d173c25c16c\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-0/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"019a2b22-9e86-770e-8af9-f9136ec6594c\"},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"foreachbatchsink\"},\"root\":{\"run\":{\"runId\":\"019a2b22-9e86-770e-8af9-f9136ec6594c\"},\"job\":{\"namespace\":\"airflow_namespace\",\"name\":\"airflow_dag.airflow_task\"}}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"foreachbatchsink\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"4.0.1\",\"name\":\"spark\",\"openlineageAdapterVersion\":\"1.39.0\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"foreachbatchsink.project\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-3/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"STREAMING\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\"\r\n{quote}\r\nSample batch type event\r\n{quote}\r\n25/10/29 01:07:26 DEBUG wire: http-outgoing-33 >> \"{\"eventTime\":\"2025-10-28T14:07:20.711Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"019a2b25-28f6-7fa0-a4e2-2aaba4f61d7e\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-0/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"019a2b22-9e86-770e-8af9-f9136ec6594c\"},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"foreachbatchsink\"},\"root\":{\"run\":{\"runId\":\"019a2b22-9e86-770e-8af9-f9136ec6594c\"},\"job\":{\"namespace\":\"airflow_namespace\",\"name\":\"airflow_dag.airflow_task\"}}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"foreachbatchsink\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"4.0.1\",\"name\":\"spark\",\"openlineageAdapterVersion\":\"1.39.0\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"foreachbatchsink.execute_insert_into_hadoop_fs_relation_command.tests_output_foreachbatchsink\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-3/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"file\",\"name\":\"/Users/xxxx/venvs/tests/output_foreachbatchsink\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"file\",\"uri\":\"file\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"long\"},{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"timestamp\",\"type\":\"timestamp\"},{\"name\":\"value\",\"type\":\"double\"},{\"name\":\"year\",\"type\":\"integer\"},{\"name\":\"month\",\"type\":\"integer\"},{\"name\":\"day\",\"type\":\"integer\"}]}},\"outputFacets\":{\"outputStatistics\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-2/OutputStatisticsOutputDatasetFacet.json#/$defs/OutputStatisticsOutputDatasetFacet\",\"rowCount\":10,\"size\":13259,\"fileCount\":10}}}]}\"{quote}\r\n\r\nWhat you think should happen instead\r\n\r\nFile sink for spark structured streaming should create open lineage event with valid output details as the information is in the spark query logic plan.\r\n\r\nHere is the logic plan for streaming query using file sink.\r\n{quote}\r\n== Analyzed Logical Plan ==\r\nid: bigint, name: string, timestamp: timestamp, value: double, year: int, month: int, day: int\r\n~WriteToMicroBatchDataSourceV1 FileSink[file:/Users/xxx/venvs/tests/output_filesink], 753bdc9a-07cd-4788-a17d-27ff622ababc, [checkpointLocation=checkpoint_filesink, path=output_filesink, queryName=filesink], Append, 1\r\n+- ~Project [id#2L, name#3, timestamp#0, value#4, year#5, month#6, dayofmonth(cast(timestamp#0 as date)) AS day#7]\r\n   +- ~Project [id#2L, name#3, timestamp#0, value#4, year#5, month(cast(timestamp#0 as date)) AS month#6]\r\n      +- ~Project [id#2L, name#3, timestamp#0, value#4, year(cast(timestamp#0 as date)) AS year#5]\r\n         +- ~Project [(value#1L % cast(1000 as bigint)) AS id#2L, concat(user_, cast((value#1L % cast(100 as bigint)) as string)) AS name#3, timestamp#0, (rand(-1344458628259366487) * cast(100 as double)) AS value#4]\r\n            +- ~StreamingDataSourceV2ScanRelation[timestamp#0, value#1L] RateStream(rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=12)\r\n{quote}\r\nHow to reproduce\r\n\r\n*     step 1: install pyspark on your local machine https://spark.apache.org/docs/latest/api/python/getting_started/install.html\r\n*     step 2: install openlineage server on your local machine https://openlineage.io/getting-started\r\n*     step 3: refer to following spark-submit command to run file_sink.py and foreachbatch_sink.py. You will see the open lineage event in the debug logs.\r\n\r\n{quote}\r\nspark-submit   --packages io.openlineage:openlineage-spark_2.13:1.39.0   --conf \"spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener\"   --conf \"spark.openlineage.transport.type=http\"   --conf \"spark.openlineage.transport.url=http://localhost:5000\"   --conf \"spark.openlineage.namespace=spark_namespace\"   --conf \"spark.openlineage.parentJobNamespace=airflow_namespace\"   --conf \"spark.openlineage.parentJobName=airflow_dag.airflow_task\"   --conf \"spark.openlineage.parentRunId=xxxx-xxxx-xxxx-xxxx\"   [filename].py\r\n{quote}\r\n", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-29T00:01:44.000+0000", "created": "2025-10-28T23:15:44.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632715", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632715", "key": "SPARK-54070", "fields": {"summary": "Add spark.logConf configuration to operator docs", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632715/comment/18033747", "id": "18033747", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 406\n[https://github.com/apache/spark-kubernetes-operator/pull/406]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-29T04:10:35.504+0000", "updated": "2025-10-29T04:10:35.504+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-29T06:29:07.000+0000", "created": "2025-10-28T21:58:38.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632709", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632709", "key": "SPARK-54069", "fields": {"summary": "Skip `test_to_feather` in Python 3.14", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632709/comment/18033742", "id": "18033742", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 52771\n[https://github.com/apache/spark/pull/52771]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-29T03:15:29.005+0000", "updated": "2025-10-29T03:15:29.005+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-29T03:15:29.000+0000", "created": "2025-10-28T20:56:28.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632708", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632708", "key": "SPARK-54068", "fields": {"summary": "Fix `pyspark.pandas.tests.connect.io.test_parity_feather.FeatherParityTests.test_to_feather` in Python 3.14", "description": "{code}\r\n======================================================================\r\nERROR [1.960s]: test_to_feather (pyspark.pandas.tests.connect.io.test_parity_feather.FeatherParityTests.test_to_feather)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/__w/spark/spark/python/pyspark/pandas/tests/io/test_feather.py\", line 43, in test_to_feather\r\n    self.psdf.to_feather(path2)\r\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/pandas/frame.py\", line 2702, in to_feather\r\n    return validate_arguments_and_invoke_function(\r\n        self._to_internal_pandas(), self.to_feather, pd.DataFrame.to_feather, args\r\n    )\r\n  File \"/__w/spark/spark/python/pyspark/pandas/utils.py\", line 592, in validate_arguments_and_invoke_function\r\n    return pandas_func(**args)\r\n  File \"/usr/local/lib/python3.14/dist-packages/pandas/core/frame.py\", line 2949, in to_feather\r\n    to_feather(self, path, **kwargs)\r\n    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.14/dist-packages/pandas/io/feather_format.py\", line 65, in to_feather\r\n    feather.write_feather(df, handles.handle, **kwargs)\r\n    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.14/dist-packages/pyarrow/feather.py\", line 156, in write_feather\r\n    table = Table.from_pandas(df, preserve_index=preserve_index)\r\n  File \"pyarrow/table.pxi\", line 4795, in pyarrow.lib.Table.from_pandas\r\n  File \"/usr/local/lib/python3.14/dist-packages/pyarrow/pandas_compat.py\", line 663, in dataframe_to_arrays\r\n    pandas_metadata = construct_metadata(\r\n        columns_to_convert, df, column_names, index_columns, index_descriptors,\r\n        preserve_index, types, column_field_names=column_field_names\r\n    )\r\n  File \"/usr/local/lib/python3.14/dist-packages/pyarrow/pandas_compat.py\", line 281, in construct_metadata\r\n    b'pandas': json.dumps({\r\n               ~~~~~~~~~~^^\r\n        'index_columns': index_descriptors,\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    ...<7 lines>...\r\n        'pandas_version': _pandas_api.version\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    }).encode('utf8')\r\n    ^^\r\n  File \"/usr/lib/python3.14/json/__init__.py\", line 231, in dumps\r\n    return _default_encoder.encode(obj)\r\n           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\r\n  File \"/usr/lib/python3.14/json/encoder.py\", line 200, in encode\r\n    chunks = self.iterencode(o, _one_shot=True)\r\n  File \"/usr/lib/python3.14/json/encoder.py\", line 261, in iterencode\r\n    return _iterencode(o, 0)\r\n  File \"/usr/lib/python3.14/json/encoder.py\", line 180, in default\r\n    raise TypeError(f'Object of type {o.__class__.__name__} '\r\n                    f'is not JSON serializable')\r\nTypeError: Object of type PlanMetrics is not JSON serializable\r\nwhen serializing list item 0\r\nwhen serializing dict item 'metrics'\r\nwhen serializing dict item 'attributes'\r\n{code}", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-28T20:55:17.000+0000", "created": "2025-10-28T20:55:17.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632707", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632707", "key": "SPARK-54067", "fields": {"summary": "Improve `SparkSubmit` to invoke `exitFn` with the root cause instead of `SparkUserAppException`", "description": "I hit this\u00a0when I ran a pipeline that had no flows:\r\n```\r\norg.apache.spark.SparkUserAppException: User application exited with 1\r\n\tat org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:127)\r\n\tat org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1028)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:226)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:95)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1166)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1175)\r\n\tat org.apache.spark.deploy.SparkPipelines$.main(SparkPipelines.scala:42)\r\n\tat org.apache.spark.deploy.SparkPipelines.main(SparkPipelines.scala)\r\n```\r\n\r\nThis is not information that's relevant to the user.", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632707/comment/18034225", "id": "18034225", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 52770\n[https://github.com/apache/spark/pull/52770]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-30T17:45:39.666+0000", "updated": "2025-10-30T17:45:39.666+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13632707/comment/18034226", "id": "18034226", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "I collected this as a subtask of\u00a0SPARK-51727 in order to improve the visibility.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-30T17:47:37.774+0000", "updated": "2025-10-30T17:47:37.774+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-10-30T17:47:37.000+0000", "created": "2025-10-28T20:32:23.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632705", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632705", "key": "SPARK-54066", "fields": {"summary": "Skip `test_in_memory_data_source` in Python 3.14", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632705/comment/18033741", "id": "18033741", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 52769\n[https://github.com/apache/spark/pull/52769]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-29T03:14:33.000+0000", "updated": "2025-10-29T03:14:33.000+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-29T03:14:52.000+0000", "created": "2025-10-28T19:51:29.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632704", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632704", "key": "SPARK-54065", "fields": {"summary": "Fix `test_in_memory_data_source` in Python 3.14", "description": "{code}\r\n======================================================================\r\nERROR [0.007s]: test_in_memory_data_source (pyspark.sql.tests.test_python_datasource.PythonDataSourceTests.test_in_memory_data_source)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/__w/spark/spark/python/pyspark/serializers.py\", line 460, in dumps\r\n    return cloudpickle.dumps(obj, pickle_protocol)\r\n           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 1537, in dumps\r\n    cp.dump(obj)\r\n    ~~~~~~~^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 1303, in dump\r\n    return super().dump(obj)\r\n           ~~~~~~~~~~~~^^^^^\r\nTypeError: cannot pickle '_abc._abc_data' object\r\nwhen serializing dict item '_abc_impl'\r\nwhen serializing tuple item 0\r\nwhen serializing cell reconstructor arguments\r\nwhen serializing cell object\r\nwhen serializing tuple item 0\r\nwhen serializing dict item '__closure__'\r\nwhen serializing tuple item 1\r\nwhen serializing function state\r\nwhen serializing function object\r\nwhen serializing dict item '__annotate_func__'\r\nwhen serializing tuple item 0\r\nwhen serializing abc.ABCMeta state\r\nwhen serializing abc.ABCMeta object\r\nwhen serializing tuple item 0\r\nwhen serializing cell reconstructor arguments\r\nwhen serializing cell object\r\nwhen serializing tuple item 0\r\nwhen serializing dict item '__closure__'\r\nwhen serializing tuple item 1\r\nwhen serializing function state\r\nwhen serializing function object\r\nwhen serializing dict item 'reader'\r\nwhen serializing tuple item 0\r\nwhen serializing abc.ABCMeta state\r\nwhen serializing abc.ABCMeta object\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/__w/spark/spark/python/pyspark/sql/tests/test_python_datasource.py\", line 283, in test_in_memory_data_source\r\n    self.spark.dataSource.register(InMemoryDataSource)\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/datasource.py\", line 1197, in register\r\n    wrapped = _wrap_function(sc, dataSource)\r\n  File \"/__w/spark/spark/python/pyspark/sql/udf.py\", line 59, in _wrap_function\r\n    pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\r\n                                                     ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/core/rdd.py\", line 5121, in _prepare_for_python_RDD\r\n    pickled_command = ser.dumps(command)\r\n  File \"/__w/spark/spark/python/pyspark/serializers.py\", line 470, in dumps\r\n    raise pickle.PicklingError(msg)\r\n_pickle.PicklingError: Could not serialize object: TypeError: cannot pickle '_abc._abc_data' object\r\n{code}\r\n\r\n{code}\r\n======================================================================\r\nERROR [0.014s]: test_in_memory_data_source (pyspark.sql.tests.connect.test_parity_python_datasource.PythonDataSourceParityTests.test_in_memory_data_source)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/__w/spark/spark/python/pyspark/serializers.py\", line 460, in dumps\r\n    return cloudpickle.dumps(obj, pickle_protocol)\r\n           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 1537, in dumps\r\n    cp.dump(obj)\r\n    ~~~~~~~^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 1303, in dump\r\n    return super().dump(obj)\r\n           ~~~~~~~~~~~~^^^^^\r\nTypeError: cannot pickle '_abc._abc_data' object\r\nwhen serializing dict item '_abc_impl'\r\nwhen serializing tuple item 0\r\nwhen serializing cell reconstructor arguments\r\nwhen serializing cell object\r\nwhen serializing tuple item 0\r\nwhen serializing dict item '__closure__'\r\nwhen serializing tuple item 1\r\nwhen serializing function state\r\nwhen serializing function object\r\nwhen serializing dict item '__annotate_func__'\r\nwhen serializing tuple item 0\r\nwhen serializing abc.ABCMeta state\r\nwhen serializing abc.ABCMeta object\r\nwhen serializing tuple item 0\r\nwhen serializing cell reconstructor arguments\r\nwhen serializing cell object\r\nwhen serializing tuple item 0\r\nwhen serializing dict item '__closure__'\r\nwhen serializing tuple item 1\r\nwhen serializing function state\r\nwhen serializing function object\r\nwhen serializing dict item 'reader'\r\nwhen serializing tuple item 0\r\nwhen serializing abc.ABCMeta state\r\nwhen serializing abc.ABCMeta object\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/__w/spark/spark/python/pyspark/sql/tests/test_python_datasource.py\", line 283, in test_in_memory_data_source\r\n    self.spark.dataSource.register(InMemoryDataSource)\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/datasource.py\", line 45, in register\r\n    self.sparkSession._client.register_data_source(dataSource)\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/client/core.py\", line 863, in register_data_source\r\n    ).to_data_source_proto(self)\r\n      ~~~~~~~~~~~~~~~~~~~~^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/plan.py\", line 2833, in to_data_source_proto\r\n    plan.python_data_source.CopyFrom(self._data_source.to_plan(session))\r\n                                     ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/plan.py\", line 2807, in to_plan\r\n    ds.command = CloudPickleSerializer().dumps(self._data_source)\r\n                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/serializers.py\", line 470, in dumps\r\n    raise pickle.PicklingError(msg)\r\n_pickle.PicklingError: Could not serialize object: TypeError: cannot pickle '_abc._abc_data' object\r\n\r\n----------------------------------------------------------------------\r\n{code}", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-28T19:54:26.000+0000", "created": "2025-10-28T19:49:33.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632703", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632703", "key": "SPARK-54064", "fields": {"summary": "Simplify recursion detection of `cloudpickle`", "description": null, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-29T03:29:24.000+0000", "created": "2025-10-28T18:58:28.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632699", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632699", "key": "SPARK-54063", "fields": {"summary": "Trigger snapshot generation for next batch when lag is detected", "description": "We have done work to detect snapshot lag. Now we need to trigger snapshot generation for the next microbatch, when snapshot lag is detected.", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-31T20:44:51.000+0000", "created": "2025-10-28T18:32:08.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632698", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632698", "key": "SPARK-54062", "fields": {"summary": "MergeScalarSubqueries code cleanup", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632698/comment/18033848", "id": "18033848", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 52763\n[https://github.com/apache/spark/pull/52763]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-29T15:02:47.277+0000", "updated": "2025-10-29T15:02:47.277+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13632698/comment/18033849", "id": "18033849", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "I collected this as a subtask of SPARK-51166 in order to improve the visibility of this improvement.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-29T15:03:37.040+0000", "updated": "2025-10-29T15:03:37.040+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-10-29T17:56:04.000+0000", "created": "2025-10-28T17:47:04.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632696", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632696", "key": "SPARK-54061", "fields": {"summary": "Wrap IllegalArgumentException with proper error code for invalid datetime patterns", "description": "When `TO_TIMESTAMP` is called with an invalid datetime pattern (e.g., `'yyyyMMddHHMIss'` with invalid letter `'I'`), the `IllegalArgumentException` from Java's `DateTimeFormatterBuilder` escapes during constant folding optimization, resulting in a raw exception without proper error codes or `SQLSTATE`.", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632696/comment/18033616", "id": "18033616", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=milan.dankovic", "name": "milan.dankovic", "key": "JIRAUSER304529", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34061", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34061", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34061", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34061"}, "displayName": "Milan Dankovic", "active": true, "timeZone": "Etc/UTC"}, "body": "I am working ok this", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=milan.dankovic", "name": "milan.dankovic", "key": "JIRAUSER304529", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34061", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34061", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34061", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34061"}, "displayName": "Milan Dankovic", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T17:28:12.956+0000", "updated": "2025-10-28T17:28:12.956+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13632696/comment/18034094", "id": "18034094", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=cloud_fan", "name": "cloud_fan", "key": "cloud_fan", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=cloud_fan&avatarId=23649", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cloud_fan&avatarId=23649", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cloud_fan&avatarId=23649", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cloud_fan&avatarId=23649"}, "displayName": "Wenchen Fan", "active": true, "timeZone": "Asia/Shanghai"}, "body": "Issue resolved by pull request 52762\n[https://github.com/apache/spark/pull/52762]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=cloud_fan", "name": "cloud_fan", "key": "cloud_fan", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=cloud_fan&avatarId=23649", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cloud_fan&avatarId=23649", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cloud_fan&avatarId=23649", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cloud_fan&avatarId=23649"}, "displayName": "Wenchen Fan", "active": true, "timeZone": "Asia/Shanghai"}, "created": "2025-10-30T05:14:34.501+0000", "updated": "2025-10-30T05:14:34.501+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-10-30T05:14:34.000+0000", "created": "2025-10-28T17:28:02.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632692", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632692", "key": "SPARK-54060", "fields": {"summary": "Introduce Geometry and Geography in-memory wrapper formats", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632692/comment/18033890", "id": "18033890", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=uros-db", "name": "uros-db", "key": "JIRAUSER304339", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"}, "displayName": "Uro\u0161 Bojani\u0107", "active": true, "timeZone": "Etc/UTC"}, "body": "Issue is resolved by: [https://github.com/apache/spark/pull/52761.] cc [~cloud_fan]\u00a0", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=uros-db", "name": "uros-db", "key": "JIRAUSER304339", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"}, "displayName": "Uro\u0161 Bojani\u0107", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-29T16:32:08.613+0000", "updated": "2025-10-29T16:32:08.613+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-29T16:32:58.000+0000", "created": "2025-10-28T16:50:42.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632686", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632686", "key": "SPARK-54059", "fields": {"summary": "Reduce default page size by LONG_ARRAY_OFFSET if ZGC or ShenandoahGC and ON_HEAP are used", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632686/comment/18033588", "id": "18033588", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 52754\n[https://github.com/apache/spark/pull/52754]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-28T15:40:11.452+0000", "updated": "2025-10-28T15:40:11.452+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-29T09:51:00.000+0000", "created": "2025-10-28T15:37:39.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632675", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632675", "key": "SPARK-54058", "fields": {"summary": "Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches`", "description": "Recently, `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches` frequently fails.\r\n\r\n[https://github.com/apache/spark/actions/runs/18872699886/job/53854858890]\r\n\r\n\u00a0\r\n{code:java}\r\n[info] - Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches *** FAILED *** (1 minute)\r\n[info]   java.lang.AssertionError: assertion failed: Exception tree doesn't contain the expected exception with message: Some of partitions in Kafka topic(s) have been lost during running query with Trigger.AvailableNow.\r\n[info] org.scalatest.exceptions.TestFailedException: isPropagated was false Partition [topic-41, 1] metadata not propagated after timeout\r\n[info] \tat org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)\r\n[info] \tat org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)\r\n[info] \tat org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)\r\n[info] \tat org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.$anonfun$waitUntilMetadataIsPropagated$1(KafkaTestUtils.scala:614)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info] \tat org.scalatest.concurrent.Eventually.eventually(Eventually.scala:348)\r\n[info] \tat org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:347)\r\n[info] \tat org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:457)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.waitUntilMetadataIsPropagated(KafkaTestUtils.scala:613)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.$anonfun$createTopic$1(KafkaTestUtils.scala:378)\r\n[info] \tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:256)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.createTopic(KafkaTestUtils.scala:377)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaMicroBatchSourceSuiteBase.$anonfun$new$11(KafkaMicroBatchSourceSuite.scala:352)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaMicroBatchSourceSuiteBase.$anonfun$new$11$adapted(KafkaMicroBatchSourceSuite.scala:349)\r\n[info] \tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.callBatchWriter(ForeachBatchSink.scala:56)\r\n[info] \tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:49)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runBatch$19(MicroBatchExecution.scala:1063)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:176)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:284)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:138)\r\n[info] \tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n[info] \tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n[info] \tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n[info] \tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:138)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:307)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:137)\r\n[info] \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:91)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:249)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runBatch$18(MicroBatchExecution.scala:1054)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.ProgressContext.reportTimeTaken(ProgressReporter.scala:200)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.runBatch(MicroBatchExecution.scala:1054)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$executeOneBatch$2(MicroBatchExecution.scala:513)\r\n[info] \tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.ProgressContext.reportTimeTaken(ProgressReporter.scala:200)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.executeOneBatch(MicroBatchExecution.scala:478)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:458)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runActivatedStream$1$adapted(MicroBatchExecution.scala:458)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.TriggerExecutor.runOneBatch(TriggerExecutor.scala:40)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.TriggerExecutor.runOneBatch$(TriggerExecutor.scala:38)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MultiBatchExecutor.runOneBatch(TriggerExecutor.scala:60)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MultiBatchExecutor.execute(TriggerExecutor.scala:65)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:458)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:347)\r\n[info] \tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info] \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.StreamExecution.org$apache$spark$sql$execution$streaming$runtime$StreamExecution$$runStream(StreamExecution.scala:307)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.StreamExecution$$anon$1.run(StreamExecution.scala:230)\r\n[info]   at scala.Predef$.assert(Predef.scala:279)\r\n[info]   at org.apache.spark.TestUtils$.assertExceptionMsg(TestUtils.scala:198)\r\n[info]   at org.apache.spark.sql.kafka010.KafkaMicroBatchSourceSuiteBase.$anonfun$new$9(KafkaMicroBatchSourceSuite.scala:374)\r\n[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info]   at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)\r\n[info]   at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)\r\n[info]   at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)\r\n[info]   at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)\r\n[info]   at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:68)\r\n[info]   at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:154)\r\n[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)\r\n[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)\r\n[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\r\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)\r\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)\r\n[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:226)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)\r\n[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)\r\n[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)\r\n[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)\r\n[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)\r\n[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)\r\n[info]   at scala.collection.immutable.List.foreach(List.scala:323)\r\n[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)\r\n[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)\r\n[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)\r\n[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)\r\n[info]   at org.scalatest.Suite.run(Suite.scala:1114)\r\n[info]   at org.scalatest.Suite.run$(Suite.scala:1096)\r\n[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)\r\n[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)\r\n[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n[info]   at java.base/java.lang.Thread.run(Thread.java:840) {code}", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632675/comment/18033662", "id": "18033662", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 52766\n[https://github.com/apache/spark/pull/52766]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-28T19:07:45.864+0000", "updated": "2025-10-28T19:07:45.864+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-28T19:07:45.000+0000", "created": "2025-10-28T14:01:43.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632667", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632667", "key": "SPARK-54057", "fields": {"summary": "Retrieve dependency version from pom.xml for DependencyOverrides in SparkBuild.scala", "description": "Currently, version strings for some dependencies are hard coded in DependencyOverrides in SparkBuild, so developers need to keep consistent with version strings specified in pom.xml manually.", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632667/comment/18033585", "id": "18033585", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 52760\n[https://github.com/apache/spark/pull/52760]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-28T15:33:37.758+0000", "updated": "2025-10-28T15:33:37.758+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-28T15:33:55.000+0000", "created": "2025-10-28T12:10:10.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632664", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632664", "key": "SPARK-54056", "fields": {"summary": "Enable substitution for SQLConf settings", "description": "Values set for custom catalogs are not being substituted:\r\n\r\n{code:java}\r\nspark.sql.catalog.mssql=org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalog\r\nspark.sql.catalog.mssql.user=${env:MSSQL__USER}\r\n{code}\r\n\r\n\r\nFails with:\r\n{code:java}\r\nspark.sql(\"show tables in mssql\").show()\r\n\r\ncom.microsoft.sqlserver.jdbc.SQLServerException: Login failed for user '${env:MSSQL__USER}'\r\n{code}\r\n\r\n", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632664/comment/18033524", "id": "18033524", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=eyushin", "name": "eyushin", "key": "eyushin", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "eugen yushin", "active": true, "timeZone": "Etc/UTC"}, "body": "PR: https://github.com/apache/spark/pull/52759", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=eyushin", "name": "eyushin", "key": "eyushin", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "eugen yushin", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T11:14:28.391+0000", "updated": "2025-10-28T11:14:28.391+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-11-01T14:27:00.000+0000", "created": "2025-10-28T11:13:47.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632655", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632655", "key": "SPARK-54055", "fields": {"summary": "Spark Connect sessions leak pyspark UDF daemon processes and threads", "description": "Each Spark Connect session that uses Python UDFs seems to leak one PySpark `daemon` process. Over time, these can accumulate in the 100s until the corresponding node or container goes OOM (although it can take a while as each process doesn't use a lot of memory and some of those pages are shared).\r\n{code:java}\r\nspark \u00a0 \u00a0 \u00a0 \u00a0263 \u00a00.0 \u00a00.0 121424 59504 ? \u00a0 \u00a0 \u00a0 \u00a0S \u00a0 \u00a005:00 \u00a0 0:01 \u00a0\\_ /opt/spark/.venv/bin/python3 -m pyspark.daemon\r\nspark \u00a0 \u00a0 \u00a0 1515 \u00a00.0 \u00a00.0 121324 60148 ? \u00a0 \u00a0 \u00a0 \u00a0S \u00a0 \u00a005:04 \u00a0 0:01 \u00a0\\_ /opt/spark/.venv/bin/python3 -m pyspark.daemon\r\nspark \u00a0 \u00a0 \u00a0 1525 \u00a00.0 \u00a00.0 121324 60400 ? \u00a0 \u00a0 \u00a0 \u00a0S \u00a0 \u00a005:04 \u00a0 0:01 \u00a0\\_ /opt/spark/.venv/bin/python3 -m pyspark.daemon\r\nspark \u00a0 \u00a0 \u00a0 1568 \u00a00.0 \u00a00.0 121324 60280 ? \u00a0 \u00a0 \u00a0 \u00a0S \u00a0 \u00a005:04 \u00a0 0:01 \u00a0\\_ /opt/spark/.venv/bin/python3 -m pyspark.daemon{code}\r\nIn addition there are also threads leaking - e.g., here is a thread dump histogram from a sample executor with 200+ leaked daemon processes:\r\n{code:java}\r\n# These threads seem to be leaking\r\n226 threads   Idle Worker Monitor for /opt/spark/.venv/bin/python3\r\n226 threads \u00a0 process reaper\r\n226 threads \u00a0 stderr reader for /opt/spark/.venv/bin/python3\r\n226 threads   stdout reader for /opt/spark/.venv/bin/python3\r\n250 threads \u00a0 Worker Monitor for /opt/spark/.venv/bin/python3\r\n\r\n# These threads seem fine, Spark is configured with 24 cores/executor\r\n21 threads \u00a0  stdout writer for /opt/spark/.venv/bin/python3\r\n21 threads \u00a0  Writer Monitor for /opt/spark/.venv/bin/python3{code}\r\nWith Spark Connect, each session always has a `SPARK_JOB_ARTIFACT_UUID`, even if there are no artifacts, so the UDF environment built by `BasePythonRunner.compute` is always different, and each session ends up with its own `PythonWorkerFactory` and hence its own daemon process.\r\n\r\n`PythonWorkerFactory` has a `stop` method that stops the daemon, but there does not seem to be anyone that calls `PythonWorkerFactory.stop`, except at shutdown in `SparkEnv.stop`.\r\n\r\n\u00a0\r\n\r\nThis can be reproduced by running a bunch of Spark Connect sessions:\r\n{code:java}\r\nparallel -n0 .venv/bin/python3 dummy.py ::: {1..200} {code}\r\nwith `dummy.py`:\r\n{code:java}\r\nfrom collections.abc import Iterable\r\n\r\nimport pandas as pd\r\nfrom pyspark.sql import SparkSession\r\n\r\n\r\ndef _udf(iterator: Iterable[pd.DataFrame]) -> Iterable[pd.DataFrame]:\r\n    yield from iterator\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    spark = SparkSession.builder.remote(\"...\").getOrCreate()\r\n\r\n    df = spark.range(128)\r\n    df.mapInPandas(_udf, df.schema).count()\r\n {code}\r\n\u00a0", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-28T16:23:06.000+0000", "created": "2025-10-28T09:52:58.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632636", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632636", "key": "SPARK-54054", "fields": {"summary": "Support row position for SparkConnectResultSet", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632636/comment/18033756", "id": "18033756", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=sarutak", "name": "sarutak", "key": "sarutak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=sarutak&avatarId=20842", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=sarutak&avatarId=20842", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=sarutak&avatarId=20842", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=sarutak&avatarId=20842"}, "displayName": "Kousuke Saruta", "active": true, "timeZone": "Asia/Tokyo"}, "body": "Issue resolved in https://github.com/apache/spark/pull/52756", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=sarutak", "name": "sarutak", "key": "sarutak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=sarutak&avatarId=20842", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=sarutak&avatarId=20842", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=sarutak&avatarId=20842", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=sarutak&avatarId=20842"}, "displayName": "Kousuke Saruta", "active": true, "timeZone": "Asia/Tokyo"}, "created": "2025-10-29T06:50:21.728+0000", "updated": "2025-10-29T06:50:21.728+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-29T06:50:21.000+0000", "created": "2025-10-28T05:28:22.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632635", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632635", "key": "SPARK-54053", "fields": {"summary": "Use Swift 6.2 for all Linux CIs", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632635/comment/18033577", "id": "18033577", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 255\n[https://github.com/apache/spark-connect-swift/pull/255]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-28T14:47:51.570+0000", "updated": "2025-10-28T14:47:51.570+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-28T14:47:51.000+0000", "created": "2025-10-28T05:00:55.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632634", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632634", "key": "SPARK-54052", "fields": {"summary": "Add SparkThrowable wrapper to workaround Py4J limitation", "description": "We could have a wrapper class to work around the problem in https://issues.apache.org/jira/browse/SPARK-52022", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-30T20:51:33.000+0000", "created": "2025-10-28T03:42:55.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632633", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632633", "key": "SPARK-54051", "fields": {"summary": "Keep coverage data when running pip tests", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632633/comment/18033730", "id": "18033730", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=podongfeng", "name": "podongfeng", "key": "podongfeng", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=podongfeng&avatarId=26870", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=podongfeng&avatarId=26870", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=podongfeng&avatarId=26870", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=podongfeng&avatarId=26870"}, "displayName": "Ruifeng Zheng", "active": true, "timeZone": "Asia/Hong_Kong"}, "body": "Issue resolved by pull request 51552\n[https://github.com/apache/spark/pull/51552]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=podongfeng", "name": "podongfeng", "key": "podongfeng", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=podongfeng&avatarId=26870", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=podongfeng&avatarId=26870", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=podongfeng&avatarId=26870", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=podongfeng&avatarId=26870"}, "displayName": "Ruifeng Zheng", "active": true, "timeZone": "Asia/Hong_Kong"}, "created": "2025-10-29T00:35:08.434+0000", "updated": "2025-10-29T00:35:08.434+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-29T04:55:07.000+0000", "created": "2025-10-28T03:38:34.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632632", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632632", "key": "SPARK-54050", "fields": {"summary": "Update the documents of arrow-batching related configures", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632632/comment/18033435", "id": "18033435", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=podongfeng", "name": "podongfeng", "key": "podongfeng", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=podongfeng&avatarId=26870", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=podongfeng&avatarId=26870", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=podongfeng&avatarId=26870", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=podongfeng&avatarId=26870"}, "displayName": "Ruifeng Zheng", "active": true, "timeZone": "Asia/Hong_Kong"}, "body": "Issue resolved by pull request 52753\n[https://github.com/apache/spark/pull/52753]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=podongfeng", "name": "podongfeng", "key": "podongfeng", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=podongfeng&avatarId=26870", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=podongfeng&avatarId=26870", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=podongfeng&avatarId=26870", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=podongfeng&avatarId=26870"}, "displayName": "Ruifeng Zheng", "active": true, "timeZone": "Asia/Hong_Kong"}, "created": "2025-10-28T04:58:23.044+0000", "updated": "2025-10-28T04:58:23.044+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-28T04:58:38.000+0000", "created": "2025-10-28T03:02:22.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632629", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632629", "key": "SPARK-54049", "fields": {"summary": "spark-network-common no longer shades all of Guava", "description": "Shading of Guava in `spark-network-common` has changed from 3.x to 4.x such that conflicting classes prevent downstream code from using Guava. \r\n\r\nIn spark jars prior to 4.x, `com/google/thirdparty/publicsuffix/PublicSuffixPatterns.class` was not a class included in the jar. Post 4.x, Guava is still shaded to `sparkproject` but `com/google/thirdparty/publicsuffix/PublicSuffixPatterns.class`, used in Guava's `InternetDomainName` and elsewhere, is now present in the jar. \r\n{code:java}\r\n$ jar tf spark-network-common_2.13-4.0.0.jar | rg PublicSuffixPatterns \r\ncom/google/thirdparty/publicsuffix/PublicSuffixPatterns.class {code}\r\nIf a library depends on spark but also depends on Guava, calls to `InternetDomainName` that call `PublicSuffixPatterns` will fail with `Exception in thread \"main\" java.lang.NoSuchFieldError: EXACT`. \r\n\r\nInspecting the code locations in such a library via `classOf[InternetDomainName].getProtectionDomain.getCodeSource.getLocation` and `classOf[PublicSuffixPatterns].getProtectionDomain.getCodeSource.getLocation` reveals that `InternetDomainName` is sourced from Guava, `target/bg-jobs/sbt_2062a9c3/target/5fcb43b5/1685140132000/guava-32.0.0-jre.jar`, while `PublicSuffixPatterns` is sourced instead from spark jar, `target/bg-jobs/sbt_2062a9c3/target/db746978/1747651686000/spark-network-common_2.13-4.0.0.jar`.", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-11-01T15:50:54.000+0000", "created": "2025-10-28T01:04:16.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632625", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632625", "key": "SPARK-54048", "fields": {"summary": "Update `dev/requirements.txt` to install `torch/torchvision` in Python 3.14", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632625/comment/18033400", "id": "18033400", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 52750\n[https://github.com/apache/spark/pull/52750]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-28T01:46:41.928+0000", "updated": "2025-10-28T01:46:41.928+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-28T01:47:01.000+0000", "created": "2025-10-27T23:55:25.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632624", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632624", "key": "SPARK-54047", "fields": {"summary": "Use difference error message when kill on idle timeout", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632624/comment/18033709", "id": "18033709", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=ueshin", "name": "ueshin", "key": "ueshin", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=ueshin&avatarId=15644", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ueshin&avatarId=15644", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ueshin&avatarId=15644", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ueshin&avatarId=15644"}, "displayName": "Takuya Ueshin", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 52749\n[https://github.com/apache/spark/pull/52749]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=ueshin", "name": "ueshin", "key": "ueshin", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=ueshin&avatarId=15644", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ueshin&avatarId=15644", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ueshin&avatarId=15644", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ueshin&avatarId=15644"}, "displayName": "Takuya Ueshin", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-28T23:48:47.483+0000", "updated": "2025-10-28T23:48:47.483+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-28T23:48:47.000+0000", "created": "2025-10-27T23:19:35.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632623", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632623", "key": "SPARK-54046", "fields": {"summary": "Upgrade PyArrow to 22.0.0", "description": null, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-30T05:09:54.000+0000", "created": "2025-10-27T22:45:17.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632620", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632620", "key": "SPARK-54045", "fields": {"summary": "Upgrade `FlatBuffers` to v25.9.23", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632620/comment/18033426", "id": "18033426", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 254\n[https://github.com/apache/spark-connect-swift/pull/254]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-28T04:45:58.757+0000", "updated": "2025-10-28T04:45:58.757+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-28T04:45:59.000+0000", "created": "2025-10-27T22:24:51.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632617", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632617", "key": "SPARK-54044", "fields": {"summary": "Upgrade `gRPC Swift NIO Transport` to 2.2.0", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632617/comment/18033349", "id": "18033349", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 253\n[https://github.com/apache/spark-connect-swift/pull/253]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-27T22:20:09.920+0000", "updated": "2025-10-27T22:20:09.920+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-27T22:20:09.000+0000", "created": "2025-10-27T21:57:06.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632608", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632608", "key": "SPARK-54043", "fields": {"summary": "Update `Spark Connect`-generated `Swift` source code with `4.1.0-preview3` RC1", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632608/comment/18033328", "id": "18033328", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 252\n[https://github.com/apache/spark-connect-swift/pull/252]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-27T21:16:43.739+0000", "updated": "2025-10-27T21:16:43.739+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-27T21:16:43.000+0000", "created": "2025-10-27T20:05:42.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632593", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632593", "key": "SPARK-54042", "fields": {"summary": "Use 4.1.0-preview3 in `integration-test-(token|mac-spark41)`", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632593/comment/18033337", "id": "18033337", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 251\n[https://github.com/apache/spark-connect-swift/pull/251]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-27T21:46:39.552+0000", "updated": "2025-10-27T21:46:39.552+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-27T21:46:39.000+0000", "created": "2025-10-27T16:11:43.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632592", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632592", "key": "SPARK-54041", "fields": {"summary": "Refactor ParameterizedQuery arguments validation", "description": "* In this issue I propose to refactor ParameterizedQuery arguments validation to ParameterizedQueryArgumentsValidator so it can be reused between single-pass and fixed-point analyzer implementations. We also remove one redundant case from the ParameterizedQueryArgumentsValidator.isNotAllowed (Alias shouldn't call isNotAllowed again as it introduces unnecessary overhead to the method) to improve performance.", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632592/comment/18033808", "id": "18033808", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=cloud_fan", "name": "cloud_fan", "key": "cloud_fan", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=cloud_fan&avatarId=23649", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cloud_fan&avatarId=23649", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cloud_fan&avatarId=23649", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cloud_fan&avatarId=23649"}, "displayName": "Wenchen Fan", "active": true, "timeZone": "Asia/Shanghai"}, "body": "Issue resolved by pull request 52744\n[https://github.com/apache/spark/pull/52744]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=cloud_fan", "name": "cloud_fan", "key": "cloud_fan", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=cloud_fan&avatarId=23649", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cloud_fan&avatarId=23649", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cloud_fan&avatarId=23649", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cloud_fan&avatarId=23649"}, "displayName": "Wenchen Fan", "active": true, "timeZone": "Asia/Shanghai"}, "created": "2025-10-29T12:28:01.867+0000", "updated": "2025-10-29T12:28:01.867+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-29T12:28:01.000+0000", "created": "2025-10-27T16:09:06.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632587", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632587", "key": "SPARK-54040", "fields": {"summary": "Remove unused commons-collections 3.x", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632587/comment/18033306", "id": "18033306", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 52743\n[https://github.com/apache/spark/pull/52743]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-27T18:20:55.590+0000", "updated": "2025-10-27T18:20:55.590+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-27T18:21:49.000+0000", "created": "2025-10-27T14:47:43.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632571", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632571", "key": "SPARK-54039", "fields": {"summary": "SS | `Add TaskContext information to KafkaDataConsumer release() logs for better debugging`", "description": "Currently, log messages in the {{release()}} method of {{KafkaDataConsumer}} do not include Spark task context information (e.g., TaskID).\r\n\r\nThis change adds task context details to the {{release()}} logs to improve traceability and debugging of Kafka consumer lifecycle events.\r\n\r\nWe previously added similar context in {{{}KafkaBatchReaderFactory.createReader{}}}, which has proven useful for debugging. Including this information in {{release()}} helps correlate consumer release metrics with the specific Spark tasks that borrowed the consumer.", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632571/comment/18034345", "id": "18034345", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=kabhwan", "name": "kabhwan", "key": "kabhwan", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Jungtaek Lim", "active": true, "timeZone": "Asia/Seoul"}, "body": "Issue resolved by pull request 52745\n[https://github.com/apache/spark/pull/52745]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=kabhwan", "name": "kabhwan", "key": "kabhwan", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Jungtaek Lim", "active": true, "timeZone": "Asia/Seoul"}, "created": "2025-10-31T08:44:20.236+0000", "updated": "2025-10-31T08:44:20.236+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-31T08:44:20.000+0000", "created": "2025-10-27T12:15:05.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632550", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632550", "key": "SPARK-54038", "fields": {"summary": "Support getSQLKeywords for SparkConnectDatabaseMetaData", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632550/comment/18034105", "id": "18034105", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=LuciferYang", "name": "LuciferYang", "key": "luciferyang", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Yang Jie", "active": true, "timeZone": "Etc/UTC"}, "body": "Issue resolved by pull request 52757\n[https://github.com/apache/spark/pull/52757]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=LuciferYang", "name": "LuciferYang", "key": "luciferyang", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Yang Jie", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-30T07:26:22.928+0000", "updated": "2025-10-30T07:26:22.928+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-30T07:26:51.000+0000", "created": "2025-10-27T09:41:10.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632534", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632534", "key": "SPARK-54037", "fields": {"summary": "Throughput deteriorated after migration from spark 3.5.5 to spark 4.0.0", "description": "My team recently updated spark dependency version from 3.5.5 to 4.0.0\r\nThis included use of spark-4.0.0-bin-hadoop3.tgz, update in pom.xml files and change of import statements (org.apache.spark.sql -> org.apache.spark.sql.classic).\r\n\r\nAfter this change our throughput (calculated as rows transferred per second) has significantly dropped for our both scenarios: 1. read from file, write to database and 2. read from database, write to database.\r\n\r\nI have performed comparison between application versions with spark 3.5.5 and 4.0.0 in cluster mode, local mode and one comparison (with use of synthetic file) using spark-shell only.\r\nIn case of spark-shell I had more or less the same throughput for 3.5.5 and 4.0.0 but in case of our app used in cluster / local mode - both of these scenarios had better throughput with 3.5.5.\r\n\r\nI have observed that with 4.0.0 there are longer delays (when compared with 3.5.5) between log lines\r\n\"Running task x in stage y\"\r\nand\r\n\"Finished task x in stage y\".\r\n\r\nIs this throughput degradation a known issue? Could it be related to this task - [SPARK-48456] [M1] Performance benchmark - ASF JIRA ?\r\n\r\n(I'll also mention that we are using checkpointing (in case it might be important here))", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-27T08:21:59.000+0000", "created": "2025-10-27T08:21:21.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632516", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632516", "key": "SPARK-54036", "fields": {"summary": "Add `build_python_3.11_macos26.yml` GitHub Action job", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632516/comment/18033105", "id": "18033105", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 52740\n[https://github.com/apache/spark/pull/52740]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-27T03:19:34.050+0000", "updated": "2025-10-27T03:19:34.050+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-27T03:19:34.000+0000", "created": "2025-10-27T03:00:13.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632514", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632514", "key": "SPARK-54035", "fields": {"summary": "Construct FileStatus from the executor side directly", "description": "https://github.com/apache/spark/pull/50765#discussion_r2357607758", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-27T02:33:18.000+0000", "created": "2025-10-27T02:33:18.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632511", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632511", "key": "SPARK-54034", "fields": {"summary": "Fix `Utils.isBindCollision` to detect port conflict `NativeIoException` correctly", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632511/comment/18033087", "id": "18033087", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 52738\n[https://github.com/apache/spark/pull/52738]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-27T01:00:51.808+0000", "updated": "2025-10-27T01:00:51.808+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-27T15:29:41.000+0000", "created": "2025-10-26T22:27:03.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632510", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632510", "key": "SPARK-54033", "fields": {"summary": "Introduce Catalyst server-side geospatial execution classes", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632510/comment/18033060", "id": "18033060", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=uros-db", "name": "uros-db", "key": "JIRAUSER304339", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"}, "displayName": "Uro\u0161 Bojani\u0107", "active": true, "timeZone": "Etc/UTC"}, "body": "Work in progress: https://github.com/apache/spark/pull/52737.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=uros-db", "name": "uros-db", "key": "JIRAUSER304339", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"}, "displayName": "Uro\u0161 Bojani\u0107", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-26T21:09:58.555+0000", "updated": "2025-10-26T21:09:58.555+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13632510/comment/18033591", "id": "18033591", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=cloud_fan", "name": "cloud_fan", "key": "cloud_fan", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=cloud_fan&avatarId=23649", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cloud_fan&avatarId=23649", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cloud_fan&avatarId=23649", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cloud_fan&avatarId=23649"}, "displayName": "Wenchen Fan", "active": true, "timeZone": "Asia/Shanghai"}, "body": "Issue resolved by pull request 52737\n[https://github.com/apache/spark/pull/52737]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=cloud_fan", "name": "cloud_fan", "key": "cloud_fan", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=cloud_fan&avatarId=23649", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cloud_fan&avatarId=23649", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cloud_fan&avatarId=23649", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cloud_fan&avatarId=23649"}, "displayName": "Wenchen Fan", "active": true, "timeZone": "Asia/Shanghai"}, "created": "2025-10-28T15:46:25.349+0000", "updated": "2025-10-28T15:46:25.349+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-10-31T07:53:32.000+0000", "created": "2025-10-26T21:09:17.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632507", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632507", "key": "SPARK-54032", "fields": {"summary": "Prefer to use native Netty transports by default", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632507/comment/18033103", "id": "18033103", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 52736\n[https://github.com/apache/spark/pull/52736]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-27T02:42:11.111+0000", "updated": "2025-10-27T02:42:11.111+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-27T02:42:36.000+0000", "created": "2025-10-26T20:07:43.000+0000"}}
