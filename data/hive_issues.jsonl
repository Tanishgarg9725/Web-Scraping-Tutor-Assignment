{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13633006", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13633006", "key": "HIVE-29301", "fields": {"summary": "Missing histogram info in DESCRIBE FORMATTED when executing another DESCRIBE FORMATTED before setting metastore.stats.fetch.kll", "description": "I had already observed several times an issue with DESCRIBE FORMATTED: sometimes the \"histogram\" info is empty, even when setting hive.stats.kll.enable and metastore.stats.fetch.kll. This time I managed to create a MRE (minimum reproducible example):\r\n\r\nThe following qfile shows the wrong behavior ({{{}histogram\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0{}}}):\r\n{code:java}\r\nCREATE TABLE tab1 AS (SELECT 1 as key);\r\n\r\nDESCRIBE FORMATTED tab1 key;\r\nset metastore.stats.fetch.kll=true;\r\n\r\nCREATE TABLE tab2 AS (SELECT 1 as key);\r\n\r\nset hive.stats.kll.enable=true;\r\nANALYZE TABLE tab2 COMPUTE STATISTICS FOR COLUMNS;\r\nDESCRIBE FORMATTED tab2 key;\r\n {code}\r\nWhile the following qfile works as expected:\r\n{code:java}\r\nCREATE TABLE tab1 AS (SELECT 1 as key);\r\n\r\nset metastore.stats.fetch.kll=true;\r\nDESCRIBE FORMATTED tab1 key;\r\n\r\nCREATE TABLE tab2 AS (SELECT 1 as key);\r\n\r\nset hive.stats.kll.enable=true;\r\nANALYZE TABLE tab2 COMPUTE STATISTICS FOR COLUMNS;\r\nDESCRIBE FORMATTED tab2 key;\r\n{code}\r\nresulting in a {{histogram Q1: 1, Q2: 1, Q3: 1}}\r\n\r\nThe only change is the order of {{set metastore.stats.fetch.kll=true;}} and {{DESCRIBE FORMATTED tab1 key;}}. Please note that the interchanged DESCRIBE FORMATTED is a command on an unrelated table!", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-31T14:48:40.000+0000", "created": "2025-10-31T13:24:16.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632995", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632995", "key": "HIVE-29300", "fields": {"summary": "Wrong estimation for num rows in EXPLAIN with histogram statistics", "description": "Given a query {{{}SELECT 1 FROM sh2a WHERE k1 < 10 AND k2 < 250{}}}, with a selectivity of 0.02 for {{k1 < 10}} and 0.5 for {{{}k2 < 250{}}}, the combined selectivity should be 0.01 resulting in selecting 5 rows of the 500 rows of the table.\r\n\r\nIf we activate histograms, the combined selectivity is estimated only with the last clause as approx 0.5, so estimating that 247 rows of 500 rows are selected.\r\n\r\nReason: {{StatsRulesProcFactory.FilterStatsRule#evaluateComparatorWithHistogram}} needs to estimate the selectivity of the condition and multiply it by currNumRows.", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-31T14:48:28.000+0000", "created": "2025-10-31T11:01:04.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632973", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632973", "key": "HIVE-29299", "fields": {"summary": "Upgrade Spring to 6.2.12 and spring-ldap-core to 3.3.4 to resolve CVE-2025-41249", "description": "There is [CVE-2025-41249|http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2025-41249] for the current version of {*}spring-core 5.3.39{*}. Current version of *spring-ldap-core[2.4.4]* has spring-core-5.3.39 as dependency, therefore needs to be upgraded to *3.3.4* which is the next latest version not affected by this vulnerability ({*}depends on spring-core-6.2.12{*}).", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-31T10:13:44.000+0000", "created": "2025-10-31T08:31:53.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632959", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632959", "key": "HIVE-29298", "fields": {"summary": "Refactoring minor issues in profile output servlet", "description": null, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-31T08:59:28.000+0000", "created": "2025-10-31T06:53:14.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632851", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632851", "key": "HIVE-29297", "fields": {"summary": "The directory of the direct insert manifest files should be hidden from read queries", "description": "In HIVE-27536 the value of the \"tmpPrefix\" variable in the Utilities class was changed to \"-tmp.\" from \"_tmp.\". This prefix was used to create the temporary directory for the manifest files. As a side effect of this change, if there is an insert and the read query running at the same time on the same table, the manifest files are read by the the read query. This leads to exception like this\r\n{code:java}\r\nERROR : Failed with exception java.io.IOException:java.lang.RuntimeException: ORC split generation failed with exception: org.apache.orc.FileFormatException: Malformed ORC file hdfs://ccycloud-1.kuczoram731.root.comops.site:8020/warehouse/tablespace/managed/hive/acid_test/-tmp.delta_0000002_0000002_0000/000000_0.manifest. Invalid postscript.\r\njava.io.IOException: java.lang.RuntimeException: ORC split generation failed with exception: org.apache.orc.FileFormatException: Malformed ORC file hdfs://ccycloud-1.kuczoram731.root.comops.site:8020/warehouse/tablespace/managed/hive/acid_test/-tmp.delta_0000002_0000002_0000/000000_0.manifest. Invalid postscript.\r\n\tat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:646)\r\n\tat org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:553)\r\n\tat org.apache.hadoop.hive.ql.exec.FetchTask.executeInner(FetchTask.java:217)\r\n\tat org.apache.hadoop.hive.ql.exec.FetchTask.execute(FetchTask.java:114)\r\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:819)\r\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:547)\r\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:541)\r\n\tat org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:190)\r\n\tat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:236)\r\n\tat org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:92)\r\n\tat org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:341)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1910)\r\n\tat org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:361)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.RuntimeException: ORC split generation failed with exception: org.apache.orc.FileFormatException: Malformed ORC file hdfs://ccycloud-1.kuczoram731.root.comops.site:8020/warehouse/tablespace/managed/hive/acid_test/-tmp.delta_0000002_0000002_0000/000000_0.manifest. Invalid postscript.\r\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1891)\r\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1980)\r\n\tat org.apache.hadoop.hive.ql.exec.FetchOperator.generateWrappedSplits(FetchOperator.java:457)\r\n\tat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextSplits(FetchOperator.java:424)\r\n\tat org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:328)\r\n\tat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:584)\r\n\t... 21 more\r\nCaused by: java.util.concurrent.ExecutionException: org.apache.orc.FileFormatException: Malformed ORC file hdfs://ccycloud-1.kuczoram731.root.comops.site:8020/warehouse/tablespace/managed/hive/acid_test/-tmp.delta_0000002_0000002_0000/000000_0.manifest. Invalid postscript.\r\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\r\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\r\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1885)\r\n\t... 26 more\r\nCaused by: org.apache.orc.FileFormatException: Malformed ORC file hdfs://ccycloud-1.kuczoram731.root.comops.site:8020/warehouse/tablespace/managed/hive/acid_test/-tmp.delta_0000002_0000002_0000/000000_0.manifest. Invalid postscript.\r\n\tat org.apache.orc.impl.ReaderImpl.ensureOrcFooter(ReaderImpl.java:464)\r\n\tat org.apache.orc.impl.ReaderImpl.extractFileTail(ReaderImpl.java:812)\r\n\tat org.apache.orc.impl.ReaderImpl.<init>(ReaderImpl.java:567)\r\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:61)\r\n\tat org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:112)\r\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.populateAndCacheStripeDetails(OrcInputFormat.java:1686)\r\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.callInternal(OrcInputFormat.java:1574)\r\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.access$2900(OrcInputFormat.java:1357)\r\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator$1.run(OrcInputFormat.java:1546)\r\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator$1.run(OrcInputFormat.java:1543)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1910)\r\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:1543)\r\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:1357)\r\n\t... 4 more {code}", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632851/comment/18034494", "id": "18034494", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "Merged to master\r\n\r\nThanks for the fix, [~kuczoram] !", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-31T20:57:36.882+0000", "updated": "2025-10-31T20:57:36.882+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-31T20:57:44.000+0000", "created": "2025-10-30T08:26:28.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632825", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632825", "key": "HIVE-29296", "fields": {"summary": "Remove getPrimaryKeys codes from beeline module", "description": "If you send a select query from Beeline client with --color, you can see exception in HS2 logs:\r\n{code:java}\r\n./apache-hive-beeline-4.2.0-SNAPSHOT/bin/beeline -u \"jdbc:hive2://127.0.0.1:10000/default hive\" --color{code}\r\n2025-09-18T16:22:34,121 ERROR [HiveServer2-Handler-Pool: Thread-166] thrift.ThriftCLIService: Failed to get primary keys [request: TGetPrimaryKeysReq(sessionHandle:TSessionHandle(sessionId:THandleIdentifier(guid:56 22 37 10 00 2D 4B BB B6 FE 42 49 E2 25 75 1F, secret:C7 E6 BF 96 D1 82 4E F5 82 36 AC A7 A1 3E 7A A6)), catalogName:, tableName:test12)]\r\norg.apache.hive.service.cli.HiveSQLException: org.apache.thrift.protocol.TProtocolException: Required field 'db_name' is unset! Struct:PrimaryKeysRequest(db_name:null, tbl_name:test12, catName:hive)\r\n\u00a0\r\n\r\nAs i said in HIVE-29213 https://issues.apache.org/jira/browse/HIVE-29213?focusedCommentId=18033761&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-18033761\u00a0\r\n\r\n\u00a0\r\n\r\nI believe retrieving the primary key of a table on the beeline side is meaningless (the original purpose was only for coloring the beeline output).\r\n\r\n{color:#172b4d}We do not need to investigate how to retrieve the correct db_name to ensure the method _HiveDatabaseMetaData::getPrimaryKeys(String catalog, String schema, String table)_ executes properly. {*}I believe removing this code logic would suffice{*}.{color}", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-31T10:30:14.000+0000", "created": "2025-10-30T02:31:05.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632794", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632794", "key": "HIVE-29295", "fields": {"summary": "Improve SchemaTool features in HiveMetaStore", "description": "There are some improvements need to do, for Schema tool in the HiveMetaStore.", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-29T16:29:15.000+0000", "created": "2025-10-29T16:29:06.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632748", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632748", "key": "HIVE-29294", "fields": {"summary": "Credential vending for external system access", "description": "_Credential vending_\u00a0issues\u00a0*short-lived, scoped credentials* to the query engine for accessing table storage. Instead of requiring long-term cloud storage keys or broad access, the engine is \u201cvended\u201d a temporary credential just-in-time for the query.", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-29T09:52:06.000+0000", "created": "2025-10-29T09:42:39.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632622", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632622", "key": "HIVE-29293", "fields": {"summary": "Restrict using mapreduce.job.queuename config at tez session level", "description": "Queries with config 'mapreduce.job.queuename' is allowing access to any queue if tez.queue.name is null (which is the default behavior).\r\n\r\nIt is ideal to restrict 'mapreduce.job.queuename' from setting this at the Tez session.\u00a0", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632622/comment/18033687", "id": "18033687", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "body": "Hi [~hemanth619], can you please set affected version(s)?", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "created": "2025-10-28T22:05:41.451+0000", "updated": "2025-10-28T22:05:41.451+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-30T00:01:28.000+0000", "created": "2025-10-27T22:44:19.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632559", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632559", "key": "HIVE-29292", "fields": {"summary": "Do not call getMSC from client itself, this can lead to deadlock", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632559/comment/18033474", "id": "18033474", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "Merged to master\r\n\r\nThanks [~difin] for the review!", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T08:59:22.999+0000", "updated": "2025-10-28T08:59:22.999+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-28T08:59:30.000+0000", "created": "2025-10-27T10:21:34.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632556", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632556", "key": "HIVE-29291", "fields": {"summary": "Use minHistoryWriteId by default", "description": null, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-30T14:26:50.000+0000", "created": "2025-10-27T10:10:48.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632540", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632540", "key": "HIVE-29290", "fields": {"summary": "Enabling \"hive.merge.nway.joins\" returns wrong resutls", "description": "Enabling CBO and \"nway.joins\" returns wrong results.\r\n\r\n\u00a0\r\n\r\nset hive.merge.nway.joins=true;\r\nset hive.cbo.enable=true;\r\n\r\ncreate table taba(id string);\r\nINSERT INTO TABLE taba VALUES ('1'),('2');\r\ncreate table tabb(id string);\r\nINSERT INTO TABLE tabb VALUES ('1');\r\ncreate table tabc(id string);\r\nINSERT INTO TABLE tabc VALUES ('1'),('2'),('2');\r\n\r\n//Full data\r\nselect * from taba A left outer join tabb B on (A.id = B.id) left outer join tabc C on (C.id = A.id);\r\n\r\n{code}\r\n+-----+-------+-------+\r\n| id \u00a0| id_1 \u00a0| id_2 \u00a0|\r\n+-----+-------+-------+\r\n| 1 \u00a0 | 1 \u00a0 \u00a0 | 1 \u00a0 \u00a0 |\r\n| 2 \u00a0 | NULL \u00a0| 2 \u00a0 \u00a0 |\r\n| 2 \u00a0 | NULL \u00a0| 2 \u00a0 \u00a0 |\r\n+-----+-------+-------+\r\n{code}\r\n\u00a0\r\n\r\n//ID is not null\r\nselect * from taba A left outer join chinna.tabb B on (A.id = B.id) left outer join chinna.tabc C on (C.id = A.id) where B.id is not null;\r\n{code}\r\n+-----+-------+-------+\r\n| id \u00a0| id_1 \u00a0| id_2 \u00a0|\r\n+-----+-------+-------+\r\n| 1 \u00a0 | 1 \u00a0 \u00a0 | 1 \u00a0 \u00a0 |\r\n+-----+-------+-------+\r\n{code}\r\n\u00a0\r\n\r\n//ID is null\r\nselect * from taba A left outer join tabb B on (A.id = B.id) left outer join tabc C on (C.id = A.id) where B.id is null;\r\n{code}\r\n+-----+-------+-------+\r\n| id \u00a0| id_1 \u00a0| id_2 \u00a0|\r\n+-----+-------+-------+\r\n| 1 \u00a0 | NULL \u00a0| 1 \u00a0 \u00a0 |\r\n| 2 \u00a0 | NULL \u00a0| 2 \u00a0 \u00a0 |\r\n| 2 \u00a0 | NULL \u00a0| 2 \u00a0 \u00a0 |\r\n+-----+-------+-------+\r\n{code}\r\nIn this case B.id is not null for id=1, Correct output is\u00a0\r\n{code}\r\n+-----+-------+-------+\r\n| id \u00a0| id_1 \u00a0| id_2 \u00a0|\r\n+-----+-------+-------+\r\n| 2 \u00a0 | NULL \u00a0| 2 \u00a0 \u00a0 |\r\n| 2 \u00a0 | NULL \u00a0| 2 \u00a0 \u00a0 |\r\n+-----+-------+-------+\r\n{code}", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632540/comment/18033344", "id": "18033344", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "[~chinnalalam] please set the affected version", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T22:12:21.586+0000", "updated": "2025-10-27T22:12:21.586+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13632540/comment/18033772", "id": "18033772", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=Aggarwal_Raghav", "name": "Aggarwal_Raghav", "key": "JIRAUSER295901", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=JIRAUSER295901&avatarId=52707", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=JIRAUSER295901&avatarId=52707", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=JIRAUSER295901&avatarId=52707", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=JIRAUSER295901&avatarId=52707"}, "displayName": "Raghav Aggarwal", "active": true, "timeZone": "Etc/UTC"}, "body": "Haven't investigated enough but disabling anti-join is yielding expected result with the following set.\r\n{code:java}\r\nset hive.merge.nway.joins=true; \r\nset hive.auto.convert.anti.join=false;{code}", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=Aggarwal_Raghav", "name": "Aggarwal_Raghav", "key": "JIRAUSER295901", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=JIRAUSER295901&avatarId=52707", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=JIRAUSER295901&avatarId=52707", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=JIRAUSER295901&avatarId=52707", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=JIRAUSER295901&avatarId=52707"}, "displayName": "Raghav Aggarwal", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-29T09:02:51.260+0000", "updated": "2025-10-29T09:03:04.323+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13632540/comment/18033893", "id": "18033893", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "cc [~thomas.rebele] , [~kkasa]\u00a0", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-29T16:40:28.891+0000", "updated": "2025-10-29T16:40:28.891+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13632540/comment/18034366", "id": "18034366", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=thomas.rebele", "name": "thomas.rebele", "key": "thomas.rebele", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Thomas Rebele", "active": true, "timeZone": "Europe/Paris"}, "body": "I checked with the PR of HIVE-29176. I could reproduce it there as well, so it's a different problem.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=thomas.rebele", "name": "thomas.rebele", "key": "thomas.rebele", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Thomas Rebele", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-10-31T09:44:17.618+0000", "updated": "2025-10-31T09:44:17.618+0000"}], "maxResults": 4, "total": 4, "startAt": 0}, "updated": "2025-10-31T09:44:17.000+0000", "created": "2025-10-27T08:58:26.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632519", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632519", "key": "HIVE-29289", "fields": {"summary": "Remove Deprecated Metastore configuration from HiveConf", "description": "Goal is to remove the redundant configuration present in HiveConf and MetaStoreConf which are deprecated.\u00a0", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-27T04:21:06.000+0000", "created": "2025-10-27T04:21:06.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632512", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632512", "key": "HIVE-29288", "fields": {"summary": "Support alter table command for Z-ordering", "description": null, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-27T01:02:00.000+0000", "created": "2025-10-27T01:01:53.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632309", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632309", "key": "HIVE-29287", "fields": {"summary": "Variant Shredding", "description": null, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-11-01T12:30:03.000+0000", "created": "2025-10-23T14:02:51.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632190", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632190", "key": "HIVE-29286", "fields": {"summary": "Session close from different clients/threads leaks resources", "description": "Consider the following scenario where three clients/threads interact with HiveServer2 (HS2) using directly (not via JDBC/ODBC) the Thrift APIs. To showcase the problem we assume that the following properties are set.\r\n{code:sql}\r\nset hive.driver.parallel.compilation=true;\r\nset hive.driver.parallel.compilation.global.limit=1;\r\nset hive.server2.compile.lock.timeout=0s; \r\n{code}\r\n * C1: Connect with HS2 and start session with handle S1\r\n * C1: Send Q1, a slow-compilation query, using S1\r\n * HS2: Obtain the compilation lock and start compiling Q1\r\n * C2: Connect with HS2 and start session with handle S2\r\n * C2: Send Q2, a fast-compilation query, using S2\r\n * HS2: Q2 blocks, waiting for the compilation lock to become available.\r\n * C3: Connect with HS2 and close session with handle S2\r\n\r\nC1, C2, C3 are different Thrift connections so they are handled by separate HS2 threads. C3 will successfully close/kill/stop the session. However, since Q2 was blocked in compilation it can't be stopped immediately. When C2 finally obtains the compilation lock and finishes, the operation will error out since various session related entries have been cleared out.\r\n\r\nThread-152 handles the requests from C2, and it reaches the end of compilation it throws the following errors.\r\n{noformat}\r\n2025-10-22T03:05:32,929 ERROR [87a053ce-f5d8-4baa-bdbc-8fd7bc1eef31 HiveServer2-Handler-Pool: Thread-152] thrift.ThriftCLIService: Failed to execute statement [request: TExecuteStatementReq(sessionHandle:TSessionHandle(sessionId:THandleIdentifier(guid:87 A0 53 CE F5 D8 4B AA BD BC \r\n8F D7 BC 1E EF 31, secret:C5 A3 62 BB 51 0B 4A 76 B6 7A 04 0B 38 5C 0A 5A)), statement:EXPLAIN SELECT AVG(age) FROM person GROUP BY name, confOverlay:{}, runAsync:false, queryTimeout:0)]\r\njava.lang.RuntimeException: java.lang.NullPointerException: Cannot invoke \"org.apache.hadoop.hive.ql.session.SessionState.getSessionId()\" because the return value of \"org.apache.hive.service.cli.session.HiveSession.getSessionState()\" is null\r\n        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:89) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n        at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n        at java.base/java.security.AccessController.doPrivileged(AccessController.java:714) ~[?:?]\r\n        at java.base/javax.security.auth.Subject.doAs(Subject.java:525) ~[?:?]\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953) ~[hadoop-common-3.4.1.jar:?]\r\n        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n        at jdk.proxy2/jdk.proxy2.$Proxy43.executeStatement(Unknown Source) ~[?:?]\r\n        at org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:281) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n        at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:651) ~[hive-service-4.2.0-SNAPSHOT.jar:?]\r\n        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1670) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1650) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n        at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:250) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]\r\n        at java.base/java.lang.Thread.run(Thread.java:1583) [?:?]\r\nCaused by: java.lang.NullPointerException: Cannot invoke \"org.apache.hadoop.hive.ql.session.SessionState.getSessionId()\" because the return value of \"org.apache.hive.service.cli.session.HiveSession.getSessionState()\" is null\r\n        at org.apache.hive.service.cli.operation.Operation.afterRun(Operation.java:270) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n        at org.apache.hive.service.cli.operation.Operation.run(Operation.java:288) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:558) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:532) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n        at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) ~[?:?]\r\n        at java.base/java.lang.reflect.Method.invoke(Method.java:580) ~[?:?]\r\n        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n        ... 17 more\r\n2025-10-22T03:05:32,930 ERROR [87a053ce-f5d8-4baa-bdbc-8fd7bc1eef31 HiveServer2-Handler-Pool: Thread-152] thrift.ThriftCLIService: Failed to close the session\r\norg.apache.hive.service.cli.HiveSQLException: Session does not exist: SessionHandle [87a053ce-f5d8-4baa-bdbc-8fd7bc1eef31]\r\n        at org.apache.hive.service.cli.session.SessionManager.closeSession(SessionManager.java:625) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n        at org.apache.hive.service.cli.CLIService.closeSession(CLIService.java:240) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n        at org.apache.hive.service.cli.thrift.ThriftCLIService.CloseSession(ThriftCLIService.java:611) ~[hive-service-4.2.0-SNAPSHOT.jar:?]\r\n        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$CloseSession.getResult(TCLIService.java:1620) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$CloseSession.getResult(TCLIService.java:1600) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n        at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:250) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]\r\n        at java.base/java.lang.Thread.run(Thread.java:1583) [?:?]\r\n{noformat}\r\nThe fact that C3 closes the session that is open by another client leads to race conditions and may also cause resource leaks. The error may not always be the same with the one reported above since it really depends at which stage of compilation/execution a session is closed/killed.\r\n\r\nWhen the queries are over ACID tables the abrupt session termination of S2 will fail to close the transaction that is opened for Q2 leading to a transaction leak.\r\n\r\nThe scenario is inspired from [Hue|https://gethue.com/] workflows where session close can be initiated by user request or [configuration settings|https://docs.gethue.com/administrator/configuration/server/#idle-session-timeout].", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632190/comment/18032038", "id": "18032038", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "body": "The [^session-close-txn-leak.patch] contains a unit test that reproduces the problem on current master (commit ad55d58eadd6c1aabc7fed51e25dfffe158a44f6).", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-10-22T10:47:23.843+0000", "updated": "2025-10-22T10:47:23.843+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13632190/comment/18032042", "id": "18032042", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "body": "As explained also under HIVE-11402 the objects that are handling the sessions in HS2 are not really thread safe and there are many assumptions that only one thread is making use of the session at every point in time. The scenario described above violates these assumptions since we have many threads/clients operating on the same session thus the use-case is not officially supported by HS2.\u00a0\r\n\r\nHIVE-14227, is very similar to this issue reporting issues when sessions are used by different Thrift connections. The respective patch was never merged since there was no consensus to move forward.\r\n\r\nOverall, there is a general agreement that clients should avoid using sessions from different connections since the results in most cases are unpredictable.\r\n\r\n\u00a0", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-10-22T11:04:17.959+0000", "updated": "2025-10-22T11:04:17.959+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-10-22T11:11:12.000+0000", "created": "2025-10-22T10:41:03.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632147", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632147", "key": "HIVE-29285", "fields": {"summary": "Iceberg: Add docker-compose setups for REST Catalog integrations with Gravitino and Polaris", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632147/comment/18032006", "id": "18032006", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "body": "What's the motivation for adding and maintaining these examples in the Hive repository? It would be perfectly possible to keep them in a repository apart or contribute them to the Gravitino and Polaris repo respectively. I am raising this question since now Gravitino and Polaris specificities become part of our release cycle and Hive developers should take care of maintaining those keeping them up to date and working. Do we want this extra burden?", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-10-22T09:05:36.892+0000", "updated": "2025-10-22T09:05:36.892+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13632147/comment/18032045", "id": "18032045", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "I don\u2019t see an issue with keeping a Hive docker-compose setup along with Gravitino or Polaris services. I\u2019ve seen the same approach used in Trino \u2014 it simplifies local deployment and provides a practical user guide for integrating with external catalogs.\r\n\r\nPS: It\u2019s not part of the official Hive release Docker image.\r\n\r\ncc [~okumin], [~zhangbutao] \u00a0", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-22T11:21:49.748+0000", "updated": "2025-10-22T14:49:52.247+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13632147/comment/18032164", "id": "18032164", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "body": "The motivation for adding REST Catalog integration examples to Hive repo:\u00a0\r\n * Supporting community interest in REST Catalog integrations in Hive:\r\nMy recent LinkedIn post discussing the Medium article on the Hive REST Catalog client got a significant engagement: 136 likes and 9 reposts, many from data engineers across various companies. This is strong signal that indicates a real-world demand for understanding and using Hive with external REST catalogs. Providing examples directly addresses this expressed interest and helps to maintain interest in Apache Hive.\r\n\r\n * Precedent in other Database and Data Warehousing Open-Source Projects:\r\n ** Apache Doris: this repository includes a docker/thirdparties/docker-compose directory containing over 20 docker-compose-based integration examples with systems like Elasticsearch, Spark and even Hive. Link: [https://github.com/apache/doris/tree/master/docker/thirdparties/docker-compose]\r\n ** ClickHouse: they maintain numerous Docker integration setups within their ci/docker/integration directory, used for testing integrations with HMS Catalog, Unitity Catalog, Hive, PostgreSQL, MySQL, etc. Link: [https://github.com/ClickHouse/ClickHouse/tree/master/ci/docker/integration]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "created": "2025-10-22T16:06:06.029+0000", "updated": "2025-10-22T16:08:41.574+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13632147/comment/18032329", "id": "18032329", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "body": "I never object to integrating other open-source components into Hive, as long as such integrations benefit the Hive community. The only point I would emphasize is that any new feature integration must be accompanied by community-friendly documentation to help Hive users understand and test the functionality. This will assist the Hive community in gathering positive feedback from users. Otherwise, newly integrated features may go unnoticed and unused, and the related code blocks could become dead code.  \r\n\r\nI noticed that this integration includes some README.md files to help users get started quickly. However, I believe it would be more appropriate to incorporate the usage documentation into https://hive.apache.org/, which should be regarded as the community's sole entry point for user documentation.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-23T02:24:05.109+0000", "updated": "2025-10-23T02:24:05.109+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13632147/comment/18032410", "id": "18032410", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "body": "Just to clarify my previous comment. I am not questioning the usefulness of having such user-guides/examples. They are definitely very helpful for end-users and can expand Hive popularity. I am mainly trying to figure out if they should be part of the Hive repository.\r\n\r\nIf the motivation is to build HMS integration tests with Gravitino, Polaris, etc., then it makes perfect sense to have such files in the Hive repository. However, as it stands right now the docker-compose files and everything else are not used for testing. Their main role seems to be to document how the integration works and what configurations users should set in order to get things working.\r\n\r\nIf the motivation is to document how to setup HMS with other projects then maybe a better place for all these would be the Hive Website ([https://github.com/apache/hive-site]). The site will be indexed by search engines and the information will be easier to find there.\u00a0\r\n\r\nNote that almost everything that gets into [https://github.com/apache/hive] becomes part of the (source or binary) release. The files may not be in Docker hub but they still have to fulfill all the other release criteria such as licensing, cves, etc.\r\n\r\nAnyways, I am not actively involved in the development of the REST catalog so my comments are mostly advisory and not blocking. Feel free to continue with whatever path seems to be the best for the project and its end-users.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-10-23T08:34:05.462+0000", "updated": "2025-10-23T08:34:05.462+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13632147/comment/18032455", "id": "18032455", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "> If the motivation is to build HMS integration tests with Gravitino, Polaris, etc.\r\n\r\nJust to clarify, this setup fully replaces HMS with alternative metadata providers such as Gravitino or Polaris, and HS2 communicates with them via REST.\r\n\r\n\u00a0\r\n\r\nFrom my perspective, the motivation is to provide a simple one-liner script (docker compose) to initialize clusters with Gravitino or Polaris catalog integration in the local environment.\r\n\r\n\u00a0", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-23T11:34:25.375+0000", "updated": "2025-10-23T11:35:10.504+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13632147/comment/18032693", "id": "18032693", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=okumin", "name": "okumin", "key": "okumin", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=okumin&avatarId=36445", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=okumin&avatarId=36445", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=okumin&avatarId=36445", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=okumin&avatarId=36445"}, "displayName": "Shohei Okumiya", "active": true, "timeZone": "Asia/Tokyo"}, "body": "From the conversations, I assume Hive developers and users are expected customers. Additionally, I expect the Hive project to demonstrate how to set up HiveServer2 for external Iceberg ecosystems. It means the Hive project is not intended to define best practices for setting up Apache Gravitino or Polaris.\r\n\r\nHaving docker-compose.yaml or not: I agree that having docker-compose.yaml could be a good example or useful tool to develop related features. In that sense, I am slightly biased to start not with Gravitino/Polaris but with HMS REST with authentication=none. The REST catalog implementation is opaque to HS2, and we may not need to maintain docker-compose.yaml for every external project. We can add a variant when we need to develop, for instance, Gravitino-specific features aggressively.\r\n\r\napache/hive vs apache/hive-site: In my opinion, it would be convenient for Hive developers if we had docker-compose.yaml in apache/hive. As docker-compose settings can express how to build a Docker image, it might make local dev easier. For Hive users, apache/hive or apache/hive-site might not make a difference.\r\n\r\nI agree that we should have a document to integrate HiveServer2 with an external catalog for Hive users anyway. I don't mind whether it is part of HIVE-29285 or a separate ticket, but we probably don't have a strong reason to disagree.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=okumin", "name": "okumin", "key": "okumin", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=okumin&avatarId=36445", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=okumin&avatarId=36445", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=okumin&avatarId=36445", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=okumin&avatarId=36445"}, "displayName": "Shohei Okumiya", "active": true, "timeZone": "Asia/Tokyo"}, "created": "2025-10-24T07:48:11.199+0000", "updated": "2025-10-24T07:48:11.199+0000"}], "maxResults": 7, "total": 7, "startAt": 0}, "updated": "2025-11-01T01:03:29.000+0000", "created": "2025-10-21T20:40:54.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632099", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632099", "key": "HIVE-29284", "fields": {"summary": "Decouple hard hadoop-yarn-registry dependency from hive-exec", "description": "I am tracking down some transitive dependency issues in {{spark}}, and I have found a potential tighter-than-necessary coupling between {{hive-exec}} and {{hadoop-yarn-registry}}.\r\n\r\n{{hadoop-yarn-registry}} is definitely used from the LLAP components, but {{hive-exec}} seems to depend on it only at a single point, in\r\n{noformat}\r\nql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java\r\n{noformat}\r\n{noformat}\r\nif (llapMode) {\r\n      // localize llap client jars\r\n      addJarLRByClass(LlapTaskSchedulerService.class, commonLocalResources);\r\n      addJarLRByClass(LlapProtocolClientImpl.class, commonLocalResources);\r\n      addJarLRByClass(LlapProtocolClientProxy.class, commonLocalResources);\r\n      addJarLRByClass(RegistryOperations.class, commonLocalResources);\r\n    }\r\n{noformat}\r\n\r\nSince spark uses hive-exec but does not use LLAP, my spark side dependency issues would go away, if the reference to this RegistryOperations.class was not resolved at compile time, but only at runtime. (== removing the compile time dependency to registry).\r\n\r\nI'll provide a simple patch to demonstrate the idea.\r\n\r\n\r\n", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632099/comment/18031416", "id": "18031416", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=ishnagy", "name": "ishnagy", "key": "JIRAUSER302340", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34058", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34058", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34058", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34058"}, "displayName": "Ish Nagy", "active": true, "timeZone": "Europe/Budapest"}, "body": "(proposed patch linked above)", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=ishnagy", "name": "ishnagy", "key": "JIRAUSER302340", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34058", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34058", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34058", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34058"}, "displayName": "Ish Nagy", "active": true, "timeZone": "Europe/Budapest"}, "created": "2025-10-21T12:33:22.592+0000", "updated": "2025-10-21T12:33:22.592+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-22T11:52:42.000+0000", "created": "2025-10-21T12:20:16.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632074", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632074", "key": "HIVE-29283", "fields": {"summary": "Display reason/txn id that is causing ready for cleaning state of a compaction in SHOW COMPACTIONS output", "description": "*Problem Statement:*\r\n\r\nWe see compaction requests are showing 'ready for cleaning' state. And sometimes, it will not finish and stuck there. There could be multiple reasons like a {*}long running transaction{*}, Obsolete directories were already cleaned, Cleaner stopped the attempt to clean for this specific compaction request etc.\r\n\r\nSo the specific reason about why a compaction is in ready for cleaning can only be known after looking at the logs which is very difficult and would take a lot of time to get to a conclusion on the RCA\r\n\r\nAnother case is where Initiator not able to initiate a compaction request for some reason like file not found etc.. We can capture this one as well and display\r\n\r\n*Proposal:*\r\nDisplay the reason for 'ready for cleaning/failed to initiate' state of a compaction request.\r\n\r\nIf it is due to long running txn, we should display that txn id, so that if that txn happens to be stale/invalid/unwanted etc, then that can be aborted so that compaction can continue\r\n\r\n\u00a0", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-21T08:24:50.000+0000", "created": "2025-10-21T08:24:13.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632073", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632073", "key": "HIVE-29282", "fields": {"summary": "Make function ddl task work with catalog", "description": "The code related to function DDL does not have catalog attributes. We need to add catalog attributes to function-related interfaces so that functions can work simultaneously across multiple catalogs.\r\n\r\n[https://github.com/apache/hive/tree/master/ql/src/java/org/apache/hadoop/hive/ql/ddl/function]\u00a0", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-21T08:18:52.000+0000", "created": "2025-10-21T08:18:19.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632072", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632072", "key": "HIVE-29281", "fields": {"summary": "Make proactive cache eviction work with catalog", "description": "Proactive LLAP cache eviction was introduced in HIVE-22820. But this feature didn't add catalog field.\r\n\r\nWe need to consider to add catalog field to it to make it work in case of there are multiple catalogs.\r\n\r\nProactive LLAP cache eviction was used several places. such as\u00a0\r\n\r\n[https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropDatabaseHandler.java]\r\n{code:java}\r\n\u00a0 \u00a0 \u00a0 if (LlapHiveUtils.isLlapMode(context.getConf())) {\r\n\u00a0 \u00a0 \u00a0 \u00a0 ProactiveEviction.Request.Builder llapEvictRequestBuilder = ProactiveEviction.Request.Builder.create();\r\n\u00a0 \u00a0 \u00a0 \u00a0 llapEvictRequestBuilder.addDb(dbName);\r\n\u00a0 \u00a0 \u00a0 \u00a0 ProactiveEviction.evict(context.getConf(), llapEvictRequestBuilder.build());\r\n\u00a0 \u00a0 \u00a0 } {code}\r\n\u00a0", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-21T08:06:42.000+0000", "created": "2025-10-21T08:06:42.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632068", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632068", "key": "HIVE-29280", "fields": {"summary": "Drop deprecated methods from Metastore", "description": "In RawStore,\u00a0AlterHandler and IHMSHandler, we have many methods marked as deprecated even since 2.2.0. It's good to drop them off in the next major release to keep the code clean and maintainable.", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-29T14:33:26.000+0000", "created": "2025-10-21T07:31:31.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632065", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632065", "key": "HIVE-29279", "fields": {"summary": "Make table ddl task work with catalog", "description": "\u00a0Table related ddl task should respect catalog. Such as these syntax:\r\n\r\n\u00a0\r\n{code:java}\r\ncreate table cat.db.tbl(id int);\r\nalter table cat.db.tbl set ..\r\n{code}\r\n\u00a0", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-21T06:46:34.000+0000", "created": "2025-10-21T06:46:34.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632055", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632055", "key": "HIVE-29278", "fields": {"summary": "Make Replicate functions work with catalog", "description": "*Replicate functions* was added in HIVE-16145. Its some interfaces can not work with catalog. Such as:\r\n\r\n[https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropDatabaseHandler.java]\r\n\r\n[https://github.com/apache/hive/blob/|https://github.com/apache/hive/blob/rel/release-4.1.0/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AlterDatabaseHandler.java] [master|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropDatabaseHandler.java] [/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AlterDatabaseHandler.java|https://github.com/apache/hive/blob/rel/release-4.1.0/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AlterDatabaseHandler.java]\r\n\r\n[https://github.com/apache/hive/blob/|https://github.com/apache/hive/blob/rel/release-4.1.0/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/CreateDatabaseHandler.java] [master|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropDatabaseHandler.java] [/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/CreateDatabaseHandler.java|https://github.com/apache/hive/blob/rel/release-4.1.0/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/CreateDatabaseHandler.java]\r\n\r\n[https://github.com/apache/hive/blob/|https://github.com/apache/hive/blob/rel/release-4.1.0/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/incremental/IncrementalLoadTasksBuilder.java] [master|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropDatabaseHandler.java] [/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/incremental/IncrementalLoadTasksBuilder.java|https://github.com/apache/hive/blob/rel/release-4.1.0/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/incremental/IncrementalLoadTasksBuilder.java]\r\n\r\n[https://github.com/apache/hive/blob/|https://github.com/apache/hive/blob/rel/release-4.1.0/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplLoadWork.java] [master|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropDatabaseHandler.java] [/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplLoadWork.java|https://github.com/apache/hive/blob/rel/release-4.1.0/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplLoadWork.java]\r\n\r\n\u00a0\r\n\r\nThese interfaces were found in [HIVE-29177|https://github.com/apache/hive/pull/6088#top] , and you can check the code blocks which need to add catalog by searching keyword {*}TODO catalog{*}. We need to add catalog to these interfaces to make this repl function work with catalog.", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-21T06:13:01.000+0000", "created": "2025-10-21T03:14:44.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13632017", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13632017", "key": "HIVE-29277", "fields": {"summary": "Invalid index should return null for get_json_object ", "description": "{{get_json_object}} index is numeric, but current implement will return the entire array string for invalid index like {*}$[1 ]{*}.\r\n{code:sql}\r\nhive> select get_json_object('[1,2,3]', \"$[1 ]\");\r\n[1,2,3]\r\n{code}", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13632017/comment/18033688", "id": "18033688", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "body": "Hi [~wechar], can you please set affected version(s)?", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "created": "2025-10-28T22:06:13.505+0000", "updated": "2025-10-28T22:06:13.505+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-29T04:05:24.000+0000", "created": "2025-10-20T16:15:28.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13631961", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13631961", "key": "HIVE-29276", "fields": {"summary": "Support major OAuth 2 Authorization Server implementation", "description": "HIVE-29020 added OAuth 2 support based on the latest best practice, such as \"RFC 9068: JSON Web Token (JWT) Profile for OAuth 2.0 Access Tokens\". Some real IdPs don't follow some of the new best practices. In this ticket, we will update the OAuth 2 configurations so that we can support major identity platforms, such as Microsoft Entra ID or Okta.\r\n\r\n\u00a0\r\n\r\nFor example, RFC 9068 recommends including `typ: at+jwt` in a JWT header so that a malicious user can't inject a JWT that is not for OAuth 2 protected resources. However, Okta's access tokens don't always have the type parameter and there is no tuning knob to add it.\r\n\r\nhttps://developer.okta.com/docs/api/openapi/okta-oauth/guides/overview/#id-token-header", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13631961/comment/18033108", "id": "18033108", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=okumin", "name": "okumin", "key": "okumin", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=okumin&avatarId=36445", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=okumin&avatarId=36445", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=okumin&avatarId=36445", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=okumin&avatarId=36445"}, "displayName": "Shohei Okumiya", "active": true, "timeZone": "Asia/Tokyo"}, "body": "I'm learning Microsoft Entra ID, and its OAuth 2 access token is unique... This is a sample payload.\r\n{code:java}\r\n{\r\n\u00a0 \"typ\": \"JWT\",\r\n\u00a0 \"alg\": \"RS256\",\r\n\u00a0 \"kid\": \"{kid}\"\r\n}\r\n{\r\n\u00a0 \"aud\": \"{App ID, UUID}\",\r\n\u00a0 \"iss\": \"https://login.microsoftonline.com/{tenant id}/v2.0\",\r\n\u00a0 \"iat\": 1761535310,\r\n\u00a0 \"nbf\": 1761535310,\r\n\u00a0 \"exp\": 1761539210,\r\n\u00a0 \"aio\": \"...\",\r\n\u00a0 \"azp\": \"...\",\r\n\u00a0 \"azpacr\": \"1\",\r\n\u00a0 \"oid\": \"...\",\r\n\u00a0 \"rh\": \"...\",\r\n\u00a0 \"roles\": [\r\n\u00a0 \u00a0 \"catalog2\"\r\n\u00a0 ],\r\n\u00a0 \"sub\": \"{UUID}\",\r\n\u00a0 \"tid\": \"{tenant ID}\",\r\n\u00a0 \"uti\": \"...\",\r\n\u00a0 \"ver\": \"2.0\",\r\n\u00a0 \"xms_ftd\": \"...\"\r\n}{code}\r\nIt uses \"JWT\" for \"typ\" and service accounts(= machine user) use \"roles\" instead of \"scope\"", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=okumin", "name": "okumin", "key": "okumin", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=okumin&avatarId=36445", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=okumin&avatarId=36445", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=okumin&avatarId=36445", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=okumin&avatarId=36445"}, "displayName": "Shohei Okumiya", "active": true, "timeZone": "Asia/Tokyo"}, "created": "2025-10-27T03:57:05.775+0000", "updated": "2025-10-27T03:57:05.775+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-27T03:57:05.000+0000", "created": "2025-10-20T06:18:52.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13631797", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13631797", "key": "HIVE-29275", "fields": {"summary": "Stats autogather calculates the min statistic incorrectly", "description": "\u00a0In stats_histogram.q autogather gets enabled and then rows are inserted into a newly created table. The minimum value for column e is [-123.2|https://github.com/apache/hive/blob/55d9ab7d6b00fa510be791b9de55974f61c90519/ql/src/test/queries/clientpositive/stats_histogram.q#L20]. However, {{DESCRIBE FORMATTED test_stats e}} shows [-10.2|https://github.com/apache/hive/blob/55d9ab7d6b00fa510be791b9de55974f61c90519/ql/src/test/results/clientpositive/llap/stats_histogram.q.out#L364] as the minimum value.\r\n\r\nWhen executing {{ANALYZE TABLE test_stats COMPUTE STATISTICS FOR COLUMNS;}} before the {{DESCRIBE FORMATTED test_stats e}} command, the [min value is -123.2|https://github.com/thomasrebele/hive/commit/2be9bef2851028678fa6752f7482080b3d201a51#diff-436ceeced7ea88c3ad4d931cfbf3d09feb838eef368a74ca8106d378209b1209L262-L364] as expected.", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13631797/comment/18031492", "id": "18031492", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=thomas.rebele", "name": "thomas.rebele", "key": "thomas.rebele", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Thomas Rebele", "active": true, "timeZone": "Europe/Paris"}, "body": "[~krisztiankasa] suggested to try it without vectorization to track down the bug.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=thomas.rebele", "name": "thomas.rebele", "key": "thomas.rebele", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Thomas Rebele", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-10-21T17:08:06.854+0000", "updated": "2025-10-21T17:08:06.854+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13631797/comment/18033345", "id": "18033345", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "[~thomas.rebele] please set the affected version", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T22:15:12.692+0000", "updated": "2025-10-27T22:15:12.692+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-10-30T19:26:40.000+0000", "created": "2025-10-17T07:37:55.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13631740", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13631740", "key": "HIVE-29274", "fields": {"summary": "Flaky TestMetastoreLeaseLeader#testHouseKeepingThreads", "description": "[https://ci.hive.apache.org/job/hive-flaky-check/906/]\r\n\r\n\u00a0\r\n\r\nmaster:\r\n\r\n[https://ci.hive.apache.org/job/hive-precommit/job/master/2706/testReport/junit/org.apache.hadoop.hive.metastore/TestMetastoreLeaseNonLeader/Testing___split_04___PostProcess___testHouseKeepingThreads/]\r\n\r\n\u00a0\r\n\r\ngithub PR:\r\n\r\n[https://ci.hive.apache.org/job/hive-precommit/job/PR-6135/1/testReport/junit/org.apache.hadoop.hive.metastore/TestMetastoreLeaseLeader/Testing___split_01___PostProcess___testHouseKeepingThreads/]", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13631740/comment/18030397", "id": "18030397", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "[~zabetak] should we disable it right away, wdyt?\r\n\r\ncc [~dengzh]\u00a0\r\n\r\nit might be a code issue as well:\r\n{code:java}\r\njavax.jdo.JDOUserException: Transaction is still active. You should always close your transactions correctly using commit() or rollback(). FailedObject:org.datanucleus.api.jdo.JDOPersistenceManager@2d5b55be at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.close(JDOPersistenceManagerFactory.java:644) at org.apache.hadoop.hive.metastore.PersistenceManagerProvider.closePmfInternal(PersistenceManagerProvider.java:151){code}", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-16T15:38:18.566+0000", "updated": "2025-10-16T15:57:37.673+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13631740/comment/18030410", "id": "18030410", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "body": "From my perspective if it is too flaky we can disable ASAP and search for the root cause a bit later.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-10-16T16:30:35.063+0000", "updated": "2025-10-16T16:30:35.063+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13631740/comment/18033215", "id": "18033215", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dengzh", "name": "dengzh", "key": "dengzh", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Zhihua Deng", "active": true, "timeZone": "Etc/UTC"}, "body": "Fix has been pushed to master. Thank you [~dkuzmenko] for the review!\r\n\r\nFlaky check: https://ci.hive.apache.org/job/hive-flaky-check/927/", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dengzh", "name": "dengzh", "key": "dengzh", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Zhihua Deng", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T12:40:39.071+0000", "updated": "2025-10-27T12:40:39.071+0000"}], "maxResults": 3, "total": 3, "startAt": 0}, "updated": "2025-10-28T03:08:33.000+0000", "created": "2025-10-16T15:37:00.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13631713", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13631713", "key": "HIVE-29273", "fields": {"summary": "Default port appended should respect transport mode when host is specified without port", "description": "As of now, when the port is not specified in the JDBC URI, 1000 gets appended by default irrespective of transport mode being binary/http.\r\n\r\nWhen HS2 is running in http mode and user tries to connect without specifying the port explicitly, it tries to connect to port 10000, running into error\r\n\r\nSample error:\r\n{code:java}\r\nError: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:10000/default;transportMode=http;httpPath=cliservice: Could not establish connection to jdbc:hive2://localhost:10000/default;transportMode=http;httpPath=cliservice: org.apache.http.conn.HttpHostConnectException: Connect to localhost:10000 {code}", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13631713/comment/18031060", "id": "18031060", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=okumin", "name": "okumin", "key": "okumin", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=okumin&avatarId=36445", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=okumin&avatarId=36445", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=okumin&avatarId=36445", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=okumin&avatarId=36445"}, "displayName": "Shohei Okumiya", "active": true, "timeZone": "Asia/Tokyo"}, "body": "Merged. [~tanishqchugh] Thanks for your contribution!\r\n\r\nhttps://github.com/apache/hive/pull/6137", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=okumin", "name": "okumin", "key": "okumin", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=okumin&avatarId=36445", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=okumin&avatarId=36445", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=okumin&avatarId=36445", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=okumin&avatarId=36445"}, "displayName": "Shohei Okumiya", "active": true, "timeZone": "Asia/Tokyo"}, "created": "2025-10-20T05:44:48.248+0000", "updated": "2025-10-20T05:44:48.248+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-20T05:44:48.000+0000", "created": "2025-10-16T09:53:04.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13631693", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13631693", "key": "HIVE-29272", "fields": {"summary": "Query-based MINOR compaction should not consider minOpenWriteId", "description": "In certain scenarios the query-based MINOR compaction produces empty delta file. In ACID tables it will be automatically cleaned up like no compaction happened, but on insert-only tables it causes data loss.\r\n\r\nThis issue happens if there is an aborted and an open transaction on the compacted table.\r\n\r\nLet\u2019s see an example:\r\n * Run an insert which creates delta_0000001_0000001 (writeId=1)\r\n * Start an insert and abort the transaction (writeId=2)\r\n * Run an insert which creates delta_0000003_0000003 (writeId=3)\r\n * Run an insert which creates delta_0000004_0000004 (writeId=4), but before it finishes, start the MINOR compaction\r\n * When the compaction is finished the table will contain the following files:\r\ndelta_0000001_0000003\r\ndelta_0000004_0000004\r\ndelta_0000004_0000004/000000_0\r\ndelta_0000001_0000001\r\ndelta_0000001_0000001/000000_0\r\ndelta_0000003_0000003\r\ndelta_0000001_0000001/000000_0\r\n * It can be seen that the delta_0000001_0000003 directory (which was produced by the compactor) is empty.\r\n * When the Cleaner runs, it will remove delta_0000001_0000001 and delta_0000003_0000003, so the data in them will be lost.\r\n\r\nThis happens because of this check in the MINOR compaction:\r\n\r\n\u00a0\r\n{code:java}\r\nlong minWriteID = validWriteIdList.getMinOpenWriteId() == null ? 1 : validWriteIdList.getMinOpenWriteId();\r\nlong highWatermark = validWriteIdList.getHighWatermark();\r\nList<AcidUtils.ParsedDelta> deltas = dir.getCurrentDirectories().stream().filter(\r\n        delta -> delta.isDeleteDelta() == isDeleteDelta && delta.getMaxWriteId() <= highWatermark && delta.getMinWriteId() >= minWriteID)\r\n    .collect(Collectors.toList());\r\nif (deltas.isEmpty()) {\r\n  query.setLength(0); // no alter query needed; clear StringBuilder\r\n  return;\r\n} {code}\r\nIf the table has aborted and open transactions, the minOpenWriteId will be set. In the example it will be 4.\u00a0\r\nWhen the ValidCompactorWriteIdList is created in the TxnUtils.createValidCompactWriteIdList the highWaterMark will be set to minOpenWriteId-1, so this will ensure that the compaction range is below the minOpenWriteId.\r\nBut in the minor compaction's code the minOpenWriteId is considered as the lower limit, so it wants to compact deltas which are above this values. This is not correct, it seems like a misunderstanding this minOpenWriteId values. In the example the compaction should consider delta_1 and delta_3, but none of them fulfills the conditions \"delta.getMinWriteId() >= minWriteID\" as the minWriteID=minOpenWriteId=4 here.\r\n\r\nThis check in the MINOR compaction code is not correct, I think it is safe to leave out checking against the minOpenWriteId as the highWatermark already adjusted to it.\r\n\r\n\u00a0", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13631693/comment/18033689", "id": "18033689", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "body": "Hi [~kuczoram], can you please set affected version(s)?", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "created": "2025-10-28T22:06:54.821+0000", "updated": "2025-10-28T22:06:54.821+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13631693/comment/18034430", "id": "18034430", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "Merged to master\r\n\r\nThanks [~kuczoram] for the fix!", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-31T15:55:11.985+0000", "updated": "2025-10-31T15:55:11.985+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13631693/comment/18034449", "id": "18034449", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=kuczoram", "name": "kuczoram", "key": "kuczoram", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Marta Kuczora", "active": true, "timeZone": "Etc/UTC"}, "body": "Thanks a lot [~dkuzmenko] for the review and for merging the fix.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=kuczoram", "name": "kuczoram", "key": "kuczoram", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Marta Kuczora", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-31T16:48:15.917+0000", "updated": "2025-10-31T16:48:15.917+0000"}], "maxResults": 3, "total": 3, "startAt": 0}, "updated": "2025-10-31T16:48:15.000+0000", "created": "2025-10-16T08:14:50.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13631618", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13631618", "key": "HIVE-29271", "fields": {"summary": "Skip corrupted files while reading an Orc table", "description": "*Scenario:*\r\n\r\nThere are large number of corrupted files scattered across multiple partitions. They were created by some external tools. Now when we query the table, exceptions like below are thrown\r\n{noformat}\r\nCaused by: com.google.protobuf.InvalidProtocolBufferException: Protocol message contained an invalid tag (zero).\r\n\u00a0 \u00a0 at com.google.protobuf.InvalidProtocolBufferException.invalidTag(InvalidProtocolBufferException.java:89)\r\n\u00a0 \u00a0 at com.google.protobuf.CodedInputStream.readTag(CodedInputStream.java:108)\r\n\u00a0 \u00a0 at org.apache.orc.OrcProto$PostScript.<init>(OrcProto.java:30246)\r\n\u00a0 \u00a0 at org.apache.orc.OrcProto$PostScript.<init>(OrcProto.java:30210)\r\n\u00a0 \u00a0 at org.apache.orc.OrcProto$PostScript$1.parsePartialFrom(OrcProto.java:30353)\r\n\u00a0 \u00a0 at org.apache.orc.OrcProto$PostScript$1.parsePartialFrom(OrcProto.java:30348)\r\n\u00a0 \u00a0 at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:89)\r\n\u00a0 \u00a0 at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:95)\r\n\u00a0 \u00a0 at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:49)\r\n\u00a0 \u00a0 at org.apache.orc.OrcProto$PostScript.parseFrom(OrcProto.java:30791)\r\n\u00a0 \u00a0 at org.apache.orc.impl.ReaderImpl.extractPostScript(ReaderImpl.java:644)\r\n\u00a0 \u00a0 at org.apache.orc.impl.ReaderImpl.extractFileTail(ReaderImpl.java:814)\r\n\u00a0 \u00a0 at org.apache.orc.impl.ReaderImpl.<init>(ReaderImpl.java:567)\r\n\u00a0 \u00a0 at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:61){noformat}\r\nSo, it is not possible to query the data from good files. The only way available today is to identify corrupted files from the table and remove them.\r\n\r\nOrc-tools is taking a lot of time to find out the corrupt files as it will traverse each file sequentially and show errors for corrupt file.\u00a0\r\n\r\n*Proposal:*\r\n\r\nIn spark we have a config, *ignoreCorruptFiles*\u00a0using which we can read data from rest of the files skipping corrupt files.\r\n\r\nCan we also implement something like this in Hive as well?\r\n\r\nWe can have a flag to enable this feature which is disabled by default.\r\n\r\n\u00a0\r\n\r\n*Issues:*\r\n\r\nIf we do not fail the queries, corrupt files may accumulate and may cause issues later like size of the table, incorrect results etc..\r\n\r\n\u00a0\r\n\r\nThe reason behind requesting this feature is that it is very difficult to identify faulty/corrupt files easily in a large table/s.\u00a0\r\n\r\nSo it is also good if we can list all the corrupt files using a simple Hive query, so that they can be deleted without disturbing the actual Hive query flow to skip them", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-15T13:04:28.000+0000", "created": "2025-10-15T13:04:28.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13631600", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13631600", "key": "HIVE-29270", "fields": {"summary": "Remove HMS compactor workers deprecated functionality", "description": "The metastore compactor workers functionality was deprecated as part of [HIVE-26443|https://issues.apache.org/jira/browse/HIVE-26443]", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13631600/comment/18031086", "id": "18031086", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "Merged to master\r\n\r\nThanks for the patch [~tanishqchugh]\u00a0 and [~dengzh] for the review!", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-20T08:10:18.290+0000", "updated": "2025-10-20T08:10:18.290+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-20T08:11:06.000+0000", "created": "2025-10-15T10:06:35.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13631572", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13631572", "key": "HIVE-29268", "fields": {"summary": "Iceberg: Close catalog in Catalogs.loadTable() to fix resource leak from open ResolvingFileIO", "description": "`Catalogs.loadTable()` loads an Iceberg catalog and then loads a table from that catalog by calling the`loadTable()` method on the loaded catalog.\r\n\r\nIt doesn't close the catalog before existing the function which causes resource leaks with exceptions like following in the hive.log:\r\n{code:java}\r\n2025-10-14 16:52:12 2025-10-14T20:52:12,131\u00a0 INFO [HiveServer2-Background-Pool: Thread-150] tez.TezSessionState: Opening new Tez Session (id: c8064268-e392-40ee-a062-008ade6a3b8e, scratch dir: file:/tmp/hive/hive/_tez_session_dir/c8064268-e392-40ee-a062-008ade6a3b8e)\r\n2025-10-14 16:52:12 2025-10-14T20:52:12,142\u00a0 INFO [HiveServer2-Background-Pool: Thread-150] client.TezClient: Session mode. Starting session.\r\n2025-10-14 16:52:12 2025-10-14T20:52:12,171\u00a0 INFO [HiveServer2-Background-Pool: Thread-150] client.TezClientUtils: Ignoring 'tez.lib.uris' since\u00a0 'tez.ignore.lib.uris' is set to true\r\n2025-10-14 16:52:12 2025-10-14T20:52:12,184\u00a0 INFO [HiveServer2-Background-Pool: Thread-150] client.TezClient: Tez system stage directory file:/tmp/hive/hive/_tez_session_dir/c8064268-e392-40ee-a062-008ade6a3b8e/.tez/application_1760475132141_0001 doesn't exist and is created\r\n2025-10-14 16:52:12 2025-10-14T20:52:12,222\u00a0 INFO [HiveServer2-Background-Pool: Thread-150] conf.Configuration: resource-types.xml not found\r\n2025-10-14 16:52:12 2025-10-14T20:52:12,222\u00a0 INFO [HiveServer2-Background-Pool: Thread-150] resource.ResourceUtils: Unable to find 'resource-types.xml'.\r\n2025-10-14 16:52:12 2025-10-14T20:52:12,236\u00a0 INFO [HiveServer2-Background-Pool: Thread-150] common.TezYARNUtils: Ignoring 'tez.lib.uris' since\u00a0 'tez.ignore.lib.uris' is set to true \r\n2025-10-14 16:52:12 2025-10-14T20:52:12,243\u00a0 INFO [HiveServer2-Background-Pool: Thread-150] Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\r\n2025-10-14 16:52:12 2025-10-14T20:52:12,323\u00a0 INFO [HiveServer2-Background-Pool: Thread-150] client.LocalClient: DAGAppMaster thread has been created\r\n2025-10-14 16:52:12 2025-10-14T20:52:12,323\u00a0 INFO [HiveServer2-Background-Pool: Thread-150] client.LocalClient: DAGAppMaster is not created wait for 100ms...\r\n2025-10-14 16:52:12 2025-10-14T20:52:12,323\u00a0 INFO [DAGAppMaster Thread] client.LocalClient: Using working directory: /tmp/hive/hive/_tez_session_dir/c8064268-e392-40ee-a062-008ade6a3b8e/.tez/application_1760475132141_0001_wd\r\n2025-10-14 16:52:12 2025-10-14T20:52:12,364\u00a0 WARN [Finalizer] io.ResolvingFileIO: Unclosed ResolvingFileIO instance created by:\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.iceberg.io.ResolvingFileIO.<init>(ResolvingFileIO.java:84)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(Unknown Source)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.iceberg.common.DynConstructors$Ctor.newInstanceChecked(DynConstructors.java:51)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.iceberg.common.DynConstructors$Ctor.newInstance(DynConstructors.java:64)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:391)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.iceberg.rest.RESTSessionCatalog.newFileIO(RESTSessionCatalog.java:983)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.iceberg.rest.RESTSessionCatalog.newFileIO(RESTSessionCatalog.java:974)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:233)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:82)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:277)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:331)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.iceberg.mr.Catalogs.loadCatalog(Catalogs.java:237)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.iceberg.mr.Catalogs.loadTable(Catalogs.java:110)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.iceberg.mr.Catalogs.loadTable(Catalogs.java:104)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.iceberg.mr.hive.IcebergTableUtil.lambda$getTable$1(IcebergTableUtil.java:178)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.iceberg.mr.hive.IcebergTableUtil.getTable(IcebergTableUtil.java:184)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.iceberg.mr.hive.BaseHiveIcebergMetaHook.preCreateTable(BaseHiveIcebergMetaHook.java:140)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.hadoop.hive.metastore.client.HookEnabledMetaStoreClient.createTable(HookEnabledMetaStoreClient.java:113)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(Unknown Source)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.hadoop.hive.metastore.client.SynchronizedMetaStoreClient$SynchronizedHandler.invoke(SynchronizedMetaStoreClient.java:69)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 jdk.proxy2/jdk.proxy2.$Proxy24.createTable(Unknown Source)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.hadoop.hive.metastore.client.MetaStoreClientWrapper.createTable(MetaStoreClientWrapper.java:445)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(Unknown Source)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:232)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 jdk.proxy2/jdk.proxy2.$Proxy24.createTable(Unknown Source)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1419)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1431)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:158)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:116)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:354)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:327)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:244)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.hadoop.hive.ql.Executor.execute(Executor.java:105)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.hadoop.hive.ql.Driver.execute(Driver.java:345)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:189)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.hadoop.hive.ql.Driver.run(Driver.java:142)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.hadoop.hive.ql.Driver.run(Driver.java:137)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:190)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:234)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:334)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 java.base/java.security.AccessController.doPrivileged(Unknown Source)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 java.base/javax.security.auth.Subject.doAs(Unknown Source)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:354)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 java.base/java.util.concurrent.FutureTask.run(Unknown Source)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n2025-10-14 16:52:12 \u00a0 \u00a0 java.base/java.lang.Thread.run(Unknown Source) {code}", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-29T14:46:25.000+0000", "created": "2025-10-15T00:47:19.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13631545", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13631545", "key": "HIVE-29267", "fields": {"summary": "Fix NPE on Grouping Sets Optimizer for UNION ALL Queries", "description": "Steps to reproduce \r\ncreate table grp_set_test (key string, value string, col0 int, col1 int, col2 int, col3 int);\r\nset hive.optimize.grouping.set.threshold=1;\r\nwith ssr as (select col2 as sales from grp_set_test)\r\nselect  channel , sum(sales) as sales\r\nfrom\r\n    (select 'store channel' as channel, sales from   ssr union all\r\n     select 'catalog channel' as channel, sales from  ssr\r\n    ) x\r\ngroup by channel with rollup;\r\n\r\n*Actual Error:*\r\nCaused by: java.lang.NullPointerException: Cannot invoke \"java.util.Map.containsKey(Object)\" because the return value of \"org.apache.hadoop.hive.ql.exec.Operator.getColumnExprMap()\" is null\r\n        at org.apache.hadoop.hive.ql.optimizer.GroupingSetOptimizer$GroupingSetProcessor.selectPartitionColumn(GroupingSetOptimizer.java:230) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4]\r\n        at org.apache.hadoop.hive.ql.optimizer.GroupingSetOptimizer$GroupingSetProcessor.process(GroupingSetOptimizer.java:100) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4]\r\n        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4]\r\n        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4]\r\n        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4]\r\n        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:158) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4]\r\n        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:120) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4]\r\n        at org.apache.hadoop.hive.ql.optimizer.GroupingSetOptimizer.transform(GroupingSetOptimizer.java:374) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4]\r\n        at org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsDependentOptimizations(TezCompiler.java:497) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4]\r\n        at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:227) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4]\r\n        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:182) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4]\r\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.compilePlan(SemanticAnalyzer.java:13150) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4]\r\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:13378) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4]\r\n        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:481) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4]\r\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:332) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4]\r\n        at org.apache.hadoop.hive.ql.Compiler.analyze(Compiler.java:224) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4]\r\n        at org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:109) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4]\r\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:508) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4]\r\n        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:460) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4]\r\n        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:424) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4]\r\n        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:418) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4]\r\n        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:121) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4]\r\n        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:207) ~[hive-service-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4]\r\n        ... 26 more", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13631545/comment/18030541", "id": "18030541", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=okumin", "name": "okumin", "key": "okumin", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=okumin&avatarId=36445", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=okumin&avatarId=36445", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=okumin&avatarId=36445", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=okumin&avatarId=36445"}, "displayName": "Shohei Okumiya", "active": true, "timeZone": "Asia/Tokyo"}, "body": "Merged.\r\n\r\n[https://github.com/apache/hive/pull/6128]\r\n\r\n\u00a0\r\n\r\n[~Indhumathi27] Thanks for your contribution, and [~ngsg] thanks for your review!", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=okumin", "name": "okumin", "key": "okumin", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=okumin&avatarId=36445", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=okumin&avatarId=36445", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=okumin&avatarId=36445", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=okumin&avatarId=36445"}, "displayName": "Shohei Okumiya", "active": true, "timeZone": "Asia/Tokyo"}, "created": "2025-10-17T06:08:59.677+0000", "updated": "2025-10-17T06:08:59.677+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-17T06:08:59.000+0000", "created": "2025-10-14T17:58:20.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13631485", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13631485", "key": "HIVE-29266", "fields": {"summary": "Add hive authorization support for catalog", "description": "We need to add relevant authorization capabilities to the catalog. Authorization features exist on both the HiveServer2 and HMS sides, referring to HIVE-25214 and HIVE-26245.\r\n\r\nThis ticket documents all authorization-related issues.", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-14T09:05:35.000+0000", "created": "2025-10-14T09:03:40.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13631484", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13631484", "key": "HIVE-29265", "fields": {"summary": "UnsupportedDoubleException could leave the stale column marker in COLUMN_STATS_ACCURATE", "description": "Take the schema_evol_orc_nonvec_part.q as an example,\u00a0\r\n{code:java}\r\nCREATE TABLE part_change_lower_to_higher_numeric_group_decimal_to_float_n7(insert_num int,\r\n           c1 decimal(38,18), c2 decimal(38,18),\r\n           c3 float,\r\n           b STRING) PARTITIONED BY(part INT);\r\ninsert into table part_change_lower_to_higher_numeric_group_decimal_to_float_n7 partition(part=1) SELECT insert_num,\r\n           decimal1, decimal1,\r\n           float1,\r\n          'original' FROM schema_evolution_data_n25; {code}\r\nfor column c3, the above query will throw UnsupportedDoubleException on gathering the column stats, as a result this column stats is ignored, we couldn't find the stats entry in part_col_stats. While in partition_params, the column stats c3 is marked as true: \\{\"BASIC_STATS\":\"true\",\"COLUMN_STATS\":{\"b\":\"true\",\"c1\":\"true\",\"c2\":\"true\",\"c3\":\"true\",\"insert_num\":\"true\"}}\r\n\r\nIf a valid insert happens afterwards, the new column stats for c3 will take over, this would make the c3 stats incorrect.\r\n\r\n\u00a0", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13631484/comment/18033348", "id": "18033348", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "[~dengzh] please set the affected version", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T22:19:24.816+0000", "updated": "2025-10-27T22:19:24.816+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-28T03:09:12.000+0000", "created": "2025-10-14T09:02:45.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13631429", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13631429", "key": "HIVE-29264", "fields": {"summary": "Use metaconf prefix to honor metastore configurations for direct sql in q-files", "description": "In *alter_table_column_stats.q,* the\u00a0_hive.metastore.try.direct.sql=false_ for partitioned table and following command is running:\r\n{code:java}\r\nanalyze table testpart0 compute statistics for columns; {code}\r\nbut the JDO implementation for\u00a0_get_aggr_stats_for_\u00a0is to throw MetaException but the exception is not thrown. As soon I replace it with set metaconf:metastore.try.direct.sql=false then exception is observed in\u00a0\r\n{code:java}\r\nitests/qtest/target/surefire-reports/org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver-output.txt {code}\r\nbut q file is not failing.\r\n\r\nIn short,\r\n # replace hivename with varname for metaconfs\r\n # investigate whether error be thrown if JDO fails.\r\n\r\nAttaching TestMiniLlapLocalCliDriver-output.txt showing\u00a0\r\n{code:java}\r\nmetastore.RetryingHMSHandler: MetaException(message:Jdo path is not implemented for stats aggr.) {code}\r\n\u00a0\r\n\r\n*Stacktrace:*\r\n{code:java}\r\n2025-10-10T00:04:54,397 ERROR [dc1b1868-d07a-4dcc-9cbd-6a0b7eca7e46 main] metastore.RetryingHMSHandler: MetaException(message:Jdo path is not implemented for stats aggr.)\tat org.apache.hadoop.hive.metastore.ObjectStore$28.getJdoResult(ObjectStore.java:10076)\tat org.apache.hadoop.hive.metastore.ObjectStore$28.getJdoResult(ObjectStore.java:10063)\tat org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:4415)\tat org.apache.hadoop.hive.metastore.ObjectStore.get_aggr_stats_for(ObjectStore.java:10082)\tat org.apache.hadoop.hive.metastore.ObjectStore.get_aggr_stats_for(ObjectStore.java:10050)\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\tat org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)\tat jdk.proxy2/jdk.proxy2.$Proxy55.get_aggr_stats_for(Unknown Source)\tat org.apache.hadoop.hive.metastore.HMSHandler.get_aggr_stats_for(HMSHandler.java:9409)\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:91)\tat org.apache.hadoop.hive.metastore.AbstractHMSHandlerProxy.invoke(AbstractHMSHandlerProxy.java:82)\tat jdk.proxy2/jdk.proxy2.$Proxy56.get_aggr_stats_for(Unknown Source)\tat org.apache.hadoop.hive.metastore.client.ThriftHiveMetaStoreClient.getAggrColStatsFor(ThriftHiveMetaStoreClient.java:1167)\tat org.apache.hadoop.hive.ql.metadata.HiveMetaStoreClientWithLocalCache.getAggrColStatsFor(HiveMetaStoreClientWithLocalCache.java:400)\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getAggrColStatsFor(SessionHiveMetaStoreClient.java:2383)\tat org.apache.hadoop.hive.metastore.client.MetaStoreClientWrapper.getAggrColStatsFor(MetaStoreClientWrapper.java:1032)\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\tat org.apache.hadoop.hive.metastore.client.SynchronizedMetaStoreClient$SynchronizedHandler.invoke(SynchronizedMetaStoreClient.java:69)\tat jdk.proxy2/jdk.proxy2.$Proxy57.getAggrColStatsFor(Unknown Source)\tat org.apache.hadoop.hive.metastore.client.MetaStoreClientWrapper.getAggrColStatsFor(MetaStoreClientWrapper.java:1032)\tat org.apache.hadoop.hive.metastore.client.BaseMetaStoreClient.getAggrColStatsFor(BaseMetaStoreClient.java:769)\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:232)\tat jdk.proxy2/jdk.proxy2.$Proxy57.getAggrColStatsFor(Unknown Source)\tat org.apache.hadoop.hive.ql.metadata.Hive.getAggrColStatsFor(Hive.java:6244)\tat org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:379)\tat org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:194)\tat org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:182)\tat org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:177)\tat org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)\tat org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:148)\tat org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:125)\tat org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:84)\tat org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsAnnotation(TezCompiler.java:472)\tat org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:202)\tat org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:182)\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.compilePlan(SemanticAnalyzer.java:13150)\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:13378)\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12725)\tat org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.analyze(ColumnStatsSemanticAnalyzer.java:671)\tat org.apache.hadoop.hive.ql.Compiler.analyze(Compiler.java:224)\tat org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:109)\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:498)\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:450)\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:414)\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:408)\tat org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)\tat org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:234)\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)\tat org.apache.hadoop.hive.cli.CliDriver.processCmd1(CliDriver.java:203)\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:129)\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:430)\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:358)\tat org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:760)\tat org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:730)\tat org.apache.hadoop.hive.cli.control.CoreCliDriver.runTest(CoreCliDriver.java:115)\tat org.apache.hadoop.hive.cli.control.CliAdapter.runTest(CliAdapter.java:139)\tat org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver(TestMiniLlapLocalCliDriver.java:62)\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\tat org.apache.hadoop.hive.cli.control.CliAdapter$2$1.evaluate(CliAdapter.java:118)\tat org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)\tat org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)\tat org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)\tat org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)\tat org.junit.runners.ParentRunner.run(ParentRunner.java:413)\tat org.junit.runners.Suite.runChild(Suite.java:128)\tat org.junit.runners.Suite.runChild(Suite.java:27)\tat org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)\tat org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)\tat org.apache.hadoop.hive.cli.control.CliAdapter$1$1.evaluate(CliAdapter.java:89)\tat org.junit.rules.RunRules.evaluate(RunRules.java:20)\tat org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)\tat org.junit.runners.ParentRunner.run(ParentRunner.java:413)\tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:316)\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:240)\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:214)\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:155)\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:385)\tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)\tat org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:507)\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:495)\r\n {code}", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13631429/comment/18029596", "id": "18029596", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=Aggarwal_Raghav", "name": "Aggarwal_Raghav", "key": "JIRAUSER295901", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=JIRAUSER295901&avatarId=52707", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=JIRAUSER295901&avatarId=52707", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=JIRAUSER295901&avatarId=52707", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=JIRAUSER295901&avatarId=52707"}, "displayName": "Raghav Aggarwal", "active": true, "timeZone": "Etc/UTC"}, "body": "To repro:\r\n\r\nRun the attached {_}alter_table_column_stats{_}.q with (modified line 137 with metaconf prefix)\u00a0\r\n{code:java}\r\nmvn clean test -Dtest=TestMiniLlapLocalCliDriver -Dqfile=alter_table_column_stats.q -Drat.skip -Pitests -pl itests/qtest {code}\r\nand check the output of _itests/qtest/target/surefire-reports/org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver-output.txt_", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=Aggarwal_Raghav", "name": "Aggarwal_Raghav", "key": "JIRAUSER295901", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=JIRAUSER295901&avatarId=52707", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=JIRAUSER295901&avatarId=52707", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=JIRAUSER295901&avatarId=52707", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=JIRAUSER295901&avatarId=52707"}, "displayName": "Raghav Aggarwal", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-13T19:09:14.766+0000", "updated": "2025-10-13T19:09:14.766+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13631429/comment/18030906", "id": "18030906", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=okumin", "name": "okumin", "key": "okumin", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=okumin&avatarId=36445", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=okumin&avatarId=36445", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=okumin&avatarId=36445", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=okumin&avatarId=36445"}, "displayName": "Shohei Okumiya", "active": true, "timeZone": "Asia/Tokyo"}, "body": "Merged. [~Aggarwal_Raghav] Thanks for your contribution!\r\n\r\n[https://github.com/apache/hive/pull/6129]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=okumin", "name": "okumin", "key": "okumin", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=okumin&avatarId=36445", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=okumin&avatarId=36445", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=okumin&avatarId=36445", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=okumin&avatarId=36445"}, "displayName": "Shohei Okumiya", "active": true, "timeZone": "Asia/Tokyo"}, "created": "2025-10-19T03:08:14.390+0000", "updated": "2025-10-19T03:08:14.390+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-10-19T03:08:14.000+0000", "created": "2025-10-13T19:03:27.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13631419", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13631419", "key": "HIVE-29263", "fields": {"summary": "NPE in desc catalog command using CommandAuthorizerV1", "description": "Environment: Local setup without kerberos, Hive (master branch (2fa252a04748e97e6581b7685f8dcc93825eb7f9)\r\n\r\nSteps to repro:\r\n{code:java}\r\ncreate catalog test_new_catalog location '/tmp/test_new_catalog/';\r\ndesc catalog test_new_catalog; {code}\r\nStacktrace:\r\n{code:java}\r\n2025-10-13 23:26:07,359 ERROR thrift.ThriftCLIService: Failed to execute statement [request: TExecuteStatementReq(sessionHandle:TSessionHandle(sessionId:THandleIdentifier(guid:72 92 FC 4B 3F 5B 4A 7D 96 8E 33 77 72 F0 78 83, secret:47 79 DA 4B 80 D4 42 F1 80 6D 6B BE 25 E3 0D 07)), statement:desc catalog test_new_catalog, confOverlay:{}, runAsync:true, queryTimeout:0)]org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: NullPointerException Cannot invoke \"org.apache.hadoop.hive.ql.metadata.Table.isView()\" because \"tbl\" is null; Query ID: raghav_20251013232607_6ba0bd75-1c78-4ab7-8791-f125e57a7d74\tat org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:376)\tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:212)\tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:268)\tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:286)\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:558)\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:543)\tat org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:311)\tat org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:650)\tat org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1670)\tat org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1650)\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\tat org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:250)\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\tat java.base/java.lang.Thread.run(Thread.java:1583)Caused by: java.lang.NullPointerException: Cannot invoke \"org.apache.hadoop.hive.ql.metadata.Table.isView()\" because \"tbl\" is null\tat org.apache.hadoop.hive.ql.security.authorization.command.CommandAuthorizerV1.authorizeInputs(CommandAuthorizerV1.java:145)\tat org.apache.hadoop.hive.ql.security.authorization.command.CommandAuthorizerV1.doAuthorization(CommandAuthorizerV1.java:67)\tat org.apache.hadoop.hive.ql.security.authorization.command.CommandAuthorizer.doAuthorization(CommandAuthorizer.java:56)\tat org.apache.hadoop.hive.ql.Compiler.authorize(Compiler.java:440)\tat org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:122)\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:498)\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:450)\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:414)\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:408)\tat org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)\tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:205)\t... 15 more {code}", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13631419/comment/18029582", "id": "18029582", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=Aggarwal_Raghav", "name": "Aggarwal_Raghav", "key": "JIRAUSER295901", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=JIRAUSER295901&avatarId=52707", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=JIRAUSER295901&avatarId=52707", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=JIRAUSER295901&avatarId=52707", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=JIRAUSER295901&avatarId=52707"}, "displayName": "Raghav Aggarwal", "active": true, "timeZone": "Etc/UTC"}, "body": "In CommandAuthorizerV2 as well i.e.\r\n\r\n_org.apache.hadoop.hive.ql.security.authorization.command.CommandAuthorizerV2#addHivePrivObject()_ I don't see any CASE statement for CATALOG. We need to handle it from RANGER project as well or am I missing something?\r\n\r\n\u00a0\r\n\r\nCC [~zhangbutao] [~okumin] [~dkuzmenko]\u00a0", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=Aggarwal_Raghav", "name": "Aggarwal_Raghav", "key": "JIRAUSER295901", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=JIRAUSER295901&avatarId=52707", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=JIRAUSER295901&avatarId=52707", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=JIRAUSER295901&avatarId=52707", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=JIRAUSER295901&avatarId=52707"}, "displayName": "Raghav Aggarwal", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-13T18:10:26.157+0000", "updated": "2025-10-13T18:10:26.157+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13631419/comment/18029584", "id": "18029584", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=Aggarwal_Raghav", "name": "Aggarwal_Raghav", "key": "JIRAUSER295901", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=JIRAUSER295901&avatarId=52707", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=JIRAUSER295901&avatarId=52707", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=JIRAUSER295901&avatarId=52707", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=JIRAUSER295901&avatarId=52707"}, "displayName": "Raghav Aggarwal", "active": true, "timeZone": "Etc/UTC"}, "body": "When I ran _*catalog.q*_ with\u00a0 _*--! qt:authorizer*_\u00a0\r\n{code:java}\r\n[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 6.241 s <<< FAILURE! -- in org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver[ERROR] org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[catalog] -- Time elapsed: 1.568 s <<< FAILURE!java.lang.AssertionError: Unexpected object type\tat org.apache.hadoop.hive.ql.security.authorization.command.CommandAuthorizerV2.addHivePrivObject(CommandAuthorizerV2.java:252)\tat org.apache.hadoop.hive.ql.security.authorization.command.CommandAuthorizerV2.getHivePrivObjects(CommandAuthorizerV2.java:152)\tat org.apache.hadoop.hive.ql.security.authorization.command.CommandAuthorizerV2.doAuthorization(CommandAuthorizerV2.java:77)\tat org.apache.hadoop.hive.ql.security.authorization.command.CommandAuthorizer.doAuthorization(CommandAuthorizer.java:58)\tat org.apache.hadoop.hive.ql.Compiler.authorize(Compiler.java:440)\tat org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:122)\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:498)\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:450)\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:414) {code}", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=Aggarwal_Raghav", "name": "Aggarwal_Raghav", "key": "JIRAUSER295901", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=JIRAUSER295901&avatarId=52707", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=JIRAUSER295901&avatarId=52707", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=JIRAUSER295901&avatarId=52707", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=JIRAUSER295901&avatarId=52707"}, "displayName": "Raghav Aggarwal", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-13T18:12:11.723+0000", "updated": "2025-10-13T18:12:11.723+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13631419/comment/18029663", "id": "18029663", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=okumin", "name": "okumin", "key": "okumin", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=okumin&avatarId=36445", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=okumin&avatarId=36445", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=okumin&avatarId=36445", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=okumin&avatarId=36445"}, "displayName": "Shohei Okumiya", "active": true, "timeZone": "Asia/Tokyo"}, "body": "Taking a glance at Ranger 2.7, I believe Ranger is not aware of Hive Metastore's catalog; it assumes any catalog is \"hive\". We might have a chance to update it.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=okumin", "name": "okumin", "key": "okumin", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=okumin&avatarId=36445", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=okumin&avatarId=36445", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=okumin&avatarId=36445", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=okumin&avatarId=36445"}, "displayName": "Shohei Okumiya", "active": true, "timeZone": "Asia/Tokyo"}, "created": "2025-10-14T04:57:31.977+0000", "updated": "2025-10-14T04:57:31.977+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13631419/comment/18029716", "id": "18029716", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "body": "Yes, we need to add hive authorization support for catalog, like data connector HIVE-25214.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-14T08:58:14.638+0000", "updated": "2025-10-14T08:58:14.638+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13631419/comment/18029723", "id": "18029723", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=Aggarwal_Raghav", "name": "Aggarwal_Raghav", "key": "JIRAUSER295901", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=JIRAUSER295901&avatarId=52707", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=JIRAUSER295901&avatarId=52707", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=JIRAUSER295901&avatarId=52707", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=JIRAUSER295901&avatarId=52707"}, "displayName": "Raghav Aggarwal", "active": true, "timeZone": "Etc/UTC"}, "body": "Thanks for the reply [~okumin] [~zhangbutao] . I'll look into this.\u00a0", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=Aggarwal_Raghav", "name": "Aggarwal_Raghav", "key": "JIRAUSER295901", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=JIRAUSER295901&avatarId=52707", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=JIRAUSER295901&avatarId=52707", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=JIRAUSER295901&avatarId=52707", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=JIRAUSER295901&avatarId=52707"}, "displayName": "Raghav Aggarwal", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-14T09:20:30.095+0000", "updated": "2025-10-14T09:20:30.095+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13631419/comment/18033690", "id": "18033690", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "body": "Hi [~Aggarwal_Raghav] , can you please set affected version(s)?", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "created": "2025-10-28T22:07:30.185+0000", "updated": "2025-10-28T22:07:30.185+0000"}], "maxResults": 6, "total": 6, "startAt": 0}, "updated": "2025-10-30T19:13:26.000+0000", "created": "2025-10-13T17:57:57.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13631366", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13631366", "key": "HIVE-29262", "fields": {"summary": "LEAD/LAG functions shows incorrect data when vectorization enabled", "description": "\u00a0\r\n\r\nPlease use this leadlag_Vectorization.q\u00a0 file to repro this scenario.\r\n\r\nThis case happnes when table has more columns.\r\n\r\nmvn test -Dtest=TestMiniLlapLocalCliDriver -Dqfile=leadlag_Vectorization.q\u00a0", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13631366/comment/18033350", "id": "18033350", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "[~rkirtir] please set the affected version", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T22:20:17.257+0000", "updated": "2025-10-27T22:20:17.257+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13631366/comment/18034240", "id": "18034240", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "body": "[~rkirtir] please provide more details - what data exactly is incorrect when running:\r\n\r\n\r\n{code:java}\r\nmvn test -Dtest=TestMiniLlapLocalCliDriver -Dqfile=leadlag_Vectorization.q\u00a0{code}", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "created": "2025-10-30T18:19:54.142+0000", "updated": "2025-10-30T18:19:54.142+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13631366/comment/18034313", "id": "18034313", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=rkirtir", "name": "rkirtir", "key": "JIRAUSER294595", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "KIRTI RUGE", "active": true, "timeZone": "Etc/UTC"}, "body": "If you see below query\u00a0\r\n\r\nSELECT\r\n*ws_bill_customer_sk,*\r\n *ws_item_sk,*\r\nws_sold_date_sk,\r\nws_sales_price,\r\nLEAD(ws_sales_price) OVER (\r\n*PARTITION BY ws_item_sk, ws_bill_customer_sk*\r\nORDER BY ws_sold_date_sk\r\n) AS next_sales_price,\r\nLEAD(ws_sales_price) OVER (\r\nPARTITION BY ws_bill_customer_sk, ws_item_sk\r\nORDER BY ws_sold_date_sk\r\n) - ws_sales_price AS sales_price_diff\r\nFROM\r\nweb_sales_txt\r\n\r\n\r\nThe order of columns in SELECT clause and PARTITION BY is different. In this case result gets messed up. Attached both output files [^leadlag_Vectorization_ve_false.q.out]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=rkirtir", "name": "rkirtir", "key": "JIRAUSER294595", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "KIRTI RUGE", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-31T03:24:37.669+0000", "updated": "2025-10-31T03:24:37.669+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13631366/comment/18034463", "id": "18034463", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "body": "happens in 4.0.1 also", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "created": "2025-10-31T17:49:56.410+0000", "updated": "2025-10-31T17:49:56.410+0000"}], "maxResults": 4, "total": 4, "startAt": 0}, "updated": "2025-10-31T17:50:23.000+0000", "created": "2025-10-13T09:28:45.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13631280", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13631280", "key": "HIVE-29261", "fields": {"summary": "Clean up GitHub Actions for Docker Release", "description": "The current workflow has two jobs, `buildFromArchive` for manual triggers and `buildFromSource` for automation. Both have the most in common.\r\n\r\n[https://github.com/apache/hive/blob/master/.github/workflows/docker-GA-images.yml]\r\n\r\n\u00a0\r\n\r\nBefore adding new steps in HIVE-29260, we would make them converge so that we don't need to replicate new steps.", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13631280/comment/18031955", "id": "18031955", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=okumin", "name": "okumin", "key": "okumin", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=okumin&avatarId=36445", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=okumin&avatarId=36445", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=okumin&avatarId=36445", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=okumin&avatarId=36445"}, "displayName": "Shohei Okumiya", "active": true, "timeZone": "Asia/Tokyo"}, "body": "Merged. [~simhadri-g] [~dkuzmenko] Thanks for your reviews!\r\n\r\nhttps://github.com/apache/hive/pull/6127", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=okumin", "name": "okumin", "key": "okumin", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=okumin&avatarId=36445", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=okumin&avatarId=36445", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=okumin&avatarId=36445", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=okumin&avatarId=36445"}, "displayName": "Shohei Okumiya", "active": true, "timeZone": "Asia/Tokyo"}, "created": "2025-10-22T04:19:23.093+0000", "updated": "2025-10-22T04:19:23.093+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-22T04:19:23.000+0000", "created": "2025-10-11T01:32:22.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13631279", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13631279", "key": "HIVE-29260", "fields": {"summary": "Add smoke tests for Docker images", "description": "The current workflow immediately pushes images once the build phase succeeds. So, we may push images with a wrong hive-site.xml or wrong Docker setup.\r\n\r\n[https://github.com/apache/hive/blob/master/.github/workflows/docker-GA-images.yml]\r\n\r\n\u00a0\r\n\r\nThis ticket would add testing the minimal configurations and connectivity.", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13631279/comment/18030018", "id": "18030018", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "body": "It would be awesome to have some tests around the images. I think that we already build the images during the precommit run so ideally the tests should run as part of that run as well. In fact I believe that we could create some java test classes somewhere in itest module and have some logic there that spins up the necessary containers and runs some queries via a JDBC/ODBC connection. Possibly, we could even use some .q files as input.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-10-15T09:17:06.775+0000", "updated": "2025-10-15T09:17:06.775+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-22T04:19:33.000+0000", "created": "2025-10-11T01:26:43.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13631220", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13631220", "key": "HIVE-29259", "fields": {"summary": "The result format of the desc catalog is messy", "description": "At present, you can find the result format of desc catalog is messy & wrong, e.g:\r\n{code:java}\r\n0: jdbc:hive2://127.0.0.1:10000/default> desc catalog hive;\r\n+------------------+---------------------------------------------------------+------------+\r\n| \u00a0 cat_name \u00a0   | \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0comment \u00a0 \u00a0 \u00a0        | location \u00a0|\r\n+------------------+-------------------------------------------------------- +-------------+\r\n| Catalog Name \u00a0  | hive \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0    | NULL \u00a0\u00a0\u00a0|\r\n| Comment \u00a0 \u00a0 \u00a0 | Default catalog for Hive \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0     | NULL \u00a0\u00a0\u00a0|\r\n| Location \u00a0 \u00a0 \u00a0| hdfs://127.0.0.1:8028/user/hive/warehouse/hiveicetest   | NULL \u00a0  \u00a0|\r\n+------------------+---------------------------------------------------------+------------+{code}\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n*It should output the result in this format:*\r\n{code:java}\r\n0: jdbc:hive2://127.0.0.1:10000/default> desc catalog hive;\r\n+------------------+-------------------------------------------------------+-------------------------------------------------------------+\r\n| \u00a0 cat_name \u00a0   | \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0comment \u00a0 \u00a0 \u00a0             |              location                                     \u00a0|\r\n+------------------+-------------------------------------------------------+-------------------------------------------------------------+\r\n|      hive \u00a0     |  \u00a0 \u00a0 \u00a0Default catalog for Hive \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0 | hdfs://127.0.0.1:8028/user/hive/warehouse/hiveicetest  \u00a0   |\r\n+------------------+-------------------------------------------------------+-------------------------------------------------------------+{code}\r\n\u00a0", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-10T09:50:51.000+0000", "created": "2025-10-10T09:38:22.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13631214", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13631214", "key": "HIVE-29258", "fields": {"summary": "Remove Class.forName() for driver loading", "description": "For example in HiveSchemaHelper.java,\u00a0Class.forName(driver) is explicitly used.\u00a0\r\n\r\n\u00a0\r\nhttps://github.com/apache/hive/blob/4f7140f0b324b5f2abde10acb671fd1f2ced9517/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/HiveSchemaHelper.java#L85\r\n\u00a0\r\nBut with JDBC 4+ DriverManager should automatically take care of classloading.\u00a0", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-10T09:07:01.000+0000", "created": "2025-10-10T09:00:24.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13631133", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13631133", "key": "HIVE-29257", "fields": {"summary": "Remove OPTIMIZED SQL entry from EXPLAIN EXTENDED", "description": "The {{EXPLAIN EXTENDED}} statement among other things prints a serialization of the CBO plan in the form of SQL (so called OPTIMIZED SQL). This feature was introduced in HIVE-19360 for diagnosability purposes to assist in the understanding of the DAG plan with respect to join ordering.\r\n\r\nHowever, in most cases when developers want to understand the join order they look into the CBO plan and not in the OPTIMIZED SQL output. Determining the join order from SQL becomes challenging for queries that contain more than a few tables and nested SQL constructs make the situation worse. The CBO RelNode tree is usually easier to navigate.\r\n\r\nIn various cases the SQL serialization is an invalid SQL query. This led people to believe that the CBO plan is wrong. As a consequence, they attempt to fix the CBO plan while in fact the plan is perfectly valid. The bug usually lies only in the serialization part but the latter does not have any effect on the execution of the query.\r\n\r\nUsers (and sometimes developers) believe that the OPTIMIZED SQL is the query that is actually run by Hive. There have been various cases where people copy-paste the OPTIMIZED SQL entry and try to run it when they have issues with the original query and when that fails as well it adds up to the confusion.\r\n\r\nIn addition, since the serializer is buggy it sometimes raises exceptions and the OPTIMIZED SQL does not appear in the output. People will raise bug fixes and invest effort in a feature that is not very useful.\r\n\r\nCurrently, there are 282 .q.out files that use {{EXPLAIN EXTENDED}} and thus contain an entry of OPTIMIZED SQL. Even minor changes in the CBO plan do affect the OPTIMIZED SQL necessitating updates and reviews on multiple files. Typically calcite upgrades trigger many changes in OPTIMIZED SQL.\r\n\r\n{noformat}\r\n$ find . -name \"*.q.out\" -exec grep -a \"OPTIMIZED SQL\" {} \\; | wc -l\r\n820\r\n$ find . -name \"*.q.out\" -exec grep -l \"OPTIMIZED SQL\" {} \\; | wc -l\r\n282\r\n{noformat}\r\n\r\nI propose to remove the {{OPTIMIZED SQL}} feature and related code to reduce:\r\n* maintenance overhead\r\n* plan/file (e.g., .q.out) size\r\n* users/dev confusion about its meaning/usage\r\n", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13631133/comment/18028760", "id": "18028760", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=thomas.rebele", "name": "thomas.rebele", "key": "thomas.rebele", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Thomas Rebele", "active": true, "timeZone": "Europe/Paris"}, "body": "I agree with the reasoning, printing an invalid SQL query leads to confusion.\r\n\r\nDo you think it makes sense to replace OPTIMIZED SQL by something else, e.g., OPTIMIZED PLAN, printing the CBO plan? Not sure whether that's helpful, as the CBO plan can already be obtained by EXPLAIN CBO.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=thomas.rebele", "name": "thomas.rebele", "key": "thomas.rebele", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Thomas Rebele", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-10-09T12:58:46.686+0000", "updated": "2025-10-09T12:58:46.686+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13631133/comment/18029429", "id": "18029429", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "body": "[~thomas.rebele] Since we already have options to display the CBO plan (as text and json) via other EXPLAIN commands, I would refrain from adding more redundancy in EXPLAIN EXTENDED. By doing this we also avoid too much coupling between \"logical\" and \"physical\" plans.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-10-13T08:31:38.998+0000", "updated": "2025-10-13T08:31:38.998+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13631133/comment/18029486", "id": "18029486", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=krisztiankasa", "name": "krisztiankasa", "key": "krisztiankasa", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Krisztian Kasa", "active": true, "timeZone": "Etc/UTC"}, "body": "+1 for removing OPTIMIZED SQL entry, especially because\r\n\r\n{quote}In various cases the SQL serialization is an invalid SQL query{quote}\r\n\r\nand checking the join order is easier by the CBO plan.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=krisztiankasa", "name": "krisztiankasa", "key": "krisztiankasa", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Krisztian Kasa", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-13T12:26:33.434+0000", "updated": "2025-10-13T12:26:33.434+0000"}], "maxResults": 3, "total": 3, "startAt": 0}, "updated": "2025-10-13T12:26:33.000+0000", "created": "2025-10-09T12:12:12.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13631112", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13631112", "key": "HIVE-29256", "fields": {"summary": "[DOC] Discard invalid installation documentation", "description": "Currently, the installation documentation for HIVE is very disorganized, with multiple installation documents for HIVE on the homepage. I suggest we should use [Apache Hive : Manual Installation|https://hive.apache.org/docs/latest/admin/manual-installation/] as the standard and discard other invalid installation documents.\r\n\r\nBecause this document has been rewritten since the release of HIVE version 4.0, we should follow it as the standard compared to other outdated and invalid installation documents.\r\n\r\nOr, at the very least, it should be placed in a prominent location within the documentation, rather than being buried in outdated and invalid documents.\r\n\r\n\u00a0", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13631112/comment/18028605", "id": "18028605", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=lisoda", "name": "lisoda", "key": "lisoda", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34043", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34043", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34043", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34043"}, "displayName": "yongzhi.shao", "active": true, "timeZone": "Etc/UTC"}, "body": "[~zhangbutao] \u00a0[~zabetak]\u00a0\r\n\r\nhello,sir,what do you think?", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=lisoda", "name": "lisoda", "key": "lisoda", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34043", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34043", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34043", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34043"}, "displayName": "yongzhi.shao", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-09T09:46:42.137+0000", "updated": "2025-10-09T09:46:42.137+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13631112/comment/18028613", "id": "18028613", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "body": "Although we've made some optimizations to the web documentation, the overall structure still looks quite troublesome. I agree with your opinion \u2014 we should remove some invalid&outdated installation documents, such as [https://hive.apache.org/docs/latest/admin/adminmanual-installation/].", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-09T10:14:42.837+0000", "updated": "2025-10-09T10:14:42.837+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13631112/comment/18028714", "id": "18028714", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "body": "[~lisoda] If you have ideas on how to improve the documentation feel free to raise a pull request.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-10-09T10:44:58.451+0000", "updated": "2025-10-09T10:44:58.451+0000"}], "maxResults": 3, "total": 3, "startAt": 0}, "updated": "2025-10-09T10:44:58.000+0000", "created": "2025-10-09T09:39:36.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13631009", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13631009", "key": "HIVE-29255", "fields": {"summary": "Document effects of JDK/tzdata on operations involving DATETIME and TIMESTAMP data types", "description": "Various functions and operations involving DATETIME and TIMESTAMP data types rely on JDK APIs in {{java.time}} package. The results of these APIs are strongly affected by changes in the [Time Zone Database|https://www.iana.org/time-zones](tzdata). Different JDK providers and versions package different tzdata versions thus changing the JDK and/or  Timezone provider can lead to different results in SQL queries and elsewhere. Although, JDK/tzdata changes are outside the control of Hive we should document this potential behavior change somewhere so that users are aware. ", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-08T14:18:11.000+0000", "created": "2025-10-08T14:17:31.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13631007", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13631007", "key": "HIVE-29254", "fields": {"summary": "Display TxnId associated with the query in show processlist command", "description": "Today we don't have a way to identify the query id along with the associated txn id.\r\n\r\nShow transactions provide\r\n{noformat}\r\n+-----------------+--------------------+----------------+----------------------+----------------------------------------------------+--------------------------------------------+\r\n| \u00a0 \u00a0 \u00a0txnid \u00a0 \u00a0 \u00a0| \u00a0 \u00a0 \u00a0 state \u00a0 \u00a0 \u00a0 \u00a0| \u00a0startedtime \u00a0 | \u00a0lastheartbeattime \u00a0 | \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0user \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0host \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0|\r\n+-----------------+--------------------+----------------+----------------------+----------------------------------------------------+--------------------------------------------+\r\n| Transaction ID \u00a0| Transaction State \u00a0| Started Time \u00a0 | Last Heartbeat Time \u00a0| User \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | Hostname \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 |\r\n| 37 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| OPEN \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 1759478145278 \u00a0| 1759478145278 \u00a0 \u00a0 \u00a0 \u00a0| hive/ip-10-17-78-194.support.fuse.cloudera.com@SUPPORT.FUSE.CLOUDERA.COM | ip-10-17-78-194.support.fuse.cloudera.com \u00a0|\r\n+-----------------+--------------------+----------------+----------------------+----------------------------------------------------+--------------------------------------------+{noformat}\r\nHere we don't know the query id and not sure if we retrieve it and display in the above command\r\n\r\n\u00a0\r\n\r\nShow processlist gives below data\r\n{noformat}\r\n0: jdbc:hive2://localhost:10000> show processlist;\r\n+------------+------------+-------------------+---------------------------------------+--------------------------+------------------------+----------------------------------------------------+-----------+-------------------+-------------------+---------------+-----------------+\r\n| User Name \u00a0| \u00a0Ip Addr \u00a0 | Execution Engine \u00a0| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Session Id \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | Session Active Time (s) \u00a0| Session Idle Time (s) \u00a0| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Query ID \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| \u00a0 State \u00a0 | Opened Timestamp \u00a0| Elapsed Time (s) \u00a0| \u00a0Runtime (s) \u00a0| \u00a0 \u00a0 \u00a0Query \u00a0 \u00a0 \u00a0|\r\n+------------+------------+-------------------+---------------------------------------+--------------------------+------------------------+----------------------------------------------------+-----------+-------------------+-------------------+---------------+-----------------+\r\n| hive \u00a0 \u00a0 \u00a0 | 127.0.0.1 \u00a0| mr \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| 6cdf027f-90e1-48af-b4d8-a11aac71ca1b \u00a0| 154 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| 19 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | rtrivedi_20240624193743_fb4b2bf8-a02a-4b76-a92c-4e3ee4bb6e9e | FINISHED \u00a0| 1719275863493 \u00a0 \u00a0 | 102 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 83 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| show tables \u00a0 \u00a0 |\r\n| hive \u00a0 \u00a0 \u00a0 | 127.0.0.1 \u00a0| mr \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| 43945f54-d65c-424d-b523-08e7675d8223 \u00a0| 165 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| 67 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | rtrivedi_20240624193826_42bff3ed-fb8d-4478-9500-fc6ff2173041 | RUNNING \u00a0 | 1719275906721 \u00a0 \u00a0 | 59 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| Not finished \u00a0| show databases \u00a0|\r\n+------------+------------+-------------------+---------------------------------------+--------------------------+------------------------+----------------------------------------------------+-----------+-------------------+-------------------+---------------+-----------------+\r\n2 rows selected (4.149 seconds){noformat}\r\nWe can try to display transaction id in this command, so that we can map transaction details from the show transactions output\r\n\r\n\u00a0\r\n\r\nCustomer use case:\r\n\r\nSome transactions are hanging for long, but they don't know the query corresponding to that txn id before aborting that manually\r\n\r\n\u00a0", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13631007/comment/18028382", "id": "18028382", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "body": "Having a way to correlate a query with a transaction is a good idea.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-10-08T14:25:02.221+0000", "updated": "2025-10-08T14:25:02.221+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-28T11:52:11.000+0000", "created": "2025-10-08T14:11:41.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630955", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630955", "key": "HIVE-29253", "fields": {"summary": "Bump netty version to 4.1.127.Final due to CVE-2025-58057, CVE-2025-58056 ", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13630955/comment/18030310", "id": "18030310", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "Merged to master\r\n\r\nThanks for the upgrade [~ramitg254] and [~Aggarwal_Raghav] , [~InvisibleProgrammer] for the review!", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-16T10:18:42.490+0000", "updated": "2025-10-16T10:18:42.490+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-16T10:19:23.000+0000", "created": "2025-10-08T06:26:23.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630926", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630926", "key": "HIVE-29252", "fields": {"summary": "Iceberg: Add support for Column Defaults with Alter commands", "description": "Allow column defaults with Alter Table Commands", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13630926/comment/18030859", "id": "18030859", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn", "name": "ayushtkn", "key": "ayushtkn", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Ayush Saxena", "active": true, "timeZone": "Asia/Kolkata"}, "body": "Committed to master.\r\n\r\nThanx [~difin] for the review!!!", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn", "name": "ayushtkn", "key": "ayushtkn", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Ayush Saxena", "active": true, "timeZone": "Asia/Kolkata"}, "created": "2025-10-18T14:05:49.832+0000", "updated": "2025-10-18T14:05:49.832+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-18T14:05:59.000+0000", "created": "2025-10-07T22:36:44.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630899", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630899", "key": "HIVE-29251", "fields": {"summary": "Hive ACID: HiveConf object shouldn't be shared between multiple cleanup task threads", "description": "The {{HiveConf}} object is shared across multiple cleanup tasks. When the Cleaner runs with multiple threads, this can lead to a race condition.\r\n\r\nAt the end of a cleanup task, the open transaction cap is lifted to check for any deltas that may become eligible for cleanup once the open transaction completes. If none are found, the cleanup request is marked complete; otherwise, it\u2019s returned to the queue with a \u201cready-for-cleaning\u201d status.\r\n\u00a0\r\n{code:java}\r\nconf.set(ValidTxnList.VALID_TXNS_KEY, new ValidReadTxnList().toString());\r\ndir = AcidUtils.getAcidState(fs, path, conf,\r\n\u00a0 new ValidReaderWriteIdList(info.getFullTableName(), new long[0], new BitSet(), info.highestWriteId, Long.MAX_VALUE),\r\n\u00a0 Ref.from(false), false,\r\n\u00a0 dirSnapshots);\r\n\r\nList<Path> remained = subtract(CompactorUtil.getObsoleteDirs(dir, isDynPartAbort), deleted);\r\nif (!remained.isEmpty()) {\r\n\u00a0 LOG.warn(\"Remained {} obsolete directories from {}. {}\",\r\n\u00a0 remained.size(), location, CompactorUtil.getDebugInfo(remained));\r\n} else {\r\n\u00a0 LOG.debug(\"All cleared below the watermark: {} from {}\", info.highestWriteId, location);\r\n\u00a0 success = true;\r\n}{code}\r\nthat might override the `{{{}ValidTxnList.VALID_TXNS_KEY`{}}} set in a concurrent task, leading to premature cleanup of data still in use by an active transaction.\r\n{code:java}\r\nprivate void cleanUsingAcidDir(CompactionInfo ci, String location, long minOpenTxn) throws Exception {\r\n\u00a0 ValidTxnList validTxnList =\r\n\u00a0 \u00a0 TxnUtils.createValidTxnListForCleaner(txnHandler.getOpenTxns(), minOpenTxn, false);\r\n\u00a0 //save it so that getAcidState() sees it\r\n\u00a0 conf.set(ValidTxnList.VALID_TXNS_KEY, validTxnList.writeToString());{code}", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13630899/comment/18029427", "id": "18029427", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "Merged to master.\r\n\r\nThank you v much for the review, [~sbadhya] !\u00a0", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-13T08:28:47.223+0000", "updated": "2025-10-13T08:28:47.223+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-13T08:29:02.000+0000", "created": "2025-10-07T16:24:18.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630871", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630871", "key": "HIVE-29250", "fields": {"summary": "Display effects of PlanModifierForASTConv in EXPLAIN CBO output", "description": "The EXPLAIN CBO clause outputs the plan generated by the cost-based (Calcite) optimizer. Although it is not explicitly defined which state of the CBO plan should be displayed to the end-users someone would expect that it is the final plan after applying all kind of transformations that are relevant and can affect the shape of the physical plan. \r\n\r\nHowever, at the moment the transformations that happen inside PlanModifierForASTConv are not *always* reflected to the plan. These transformations are important cause they can affect the physical plan both in terms of correctness and performance. Some transformations from PlanModifierForASTConv are already visible mostly as a side-effect of modifying directly the object reference that is passed as input.", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-13T12:14:35.000+0000", "created": "2025-10-07T11:43:28.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630852", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630852", "key": "HIVE-29249", "fields": {"summary": "RuntimeException in PlanModifierForASTConv.introduceDerivedTable for queries with self joins", "description": "Various queries containing more than 2 joins of the same relation (table, materialized view, CTE) fail at compile time.\r\n\r\n{code:sql}\r\ncreate table t1 (key int, value int);\r\ncreate table t2 (a string, b string);\r\nexplain cbo\r\nwith cte as\r\n(select key, value, BLOCK__OFFSET__INSIDE__FILE, INPUT__FILE__NAME, ROW__ID, ROW__IS__DELETED from t1)\r\nselect * from cte a join t2 b join cte c\r\n{code}\r\n\r\n The query fails with the following stacktrace:\r\n{noformat}\r\njava.lang.RuntimeException: Couldn't find child node in parent's inputs\r\n\tat org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForASTConv.introduceDerivedTable(PlanModifierForASTConv.java:341)\r\n\tat org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForASTConv$SelfJoinHandler.visit(PlanModifierForASTConv.java:236)\r\n\tat org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveJoin.accept(HiveJoin.java:229)\r\n\tat org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelShuttleImpl.visitChild(HiveRelShuttleImpl.java:60)\r\n\tat org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelShuttleImpl.visit(HiveRelShuttleImpl.java:114)\r\n\tat org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForASTConv$SelfJoinHandler.visit(PlanModifierForASTConv.java:242)\r\n\tat org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveProject.accept(HiveProject.java:134)\r\n\tat org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForASTConv.convertOpTree(PlanModifierForASTConv.java:109)\r\n\tat org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.convert(ASTConverter.java:136)\r\n\tat org.apache.hadoop.hive.ql.parse.CalcitePlanner.getOptimizedAST(CalcitePlanner.java:1405)\r\n\tat org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:609)\r\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:13218)\r\n\tat org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:482)\r\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:359)\r\n\tat org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:187)\r\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:359)\r\n\tat org.apache.hadoop.hive.ql.Compiler.analyze(Compiler.java:224)\r\n\tat org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:109)\r\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:498)\r\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:450)\r\n\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:414)\r\n\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:408)\r\n\tat org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)\r\n\tat org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:234)\r\n\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)\r\n\tat org.apache.hadoop.hive.cli.CliDriver.processCmd1(CliDriver.java:203)\r\n\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:129)\r\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:430)\r\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:358)\r\n\tat org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:760)\r\n\tat org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:730)\r\n\tat org.apache.hadoop.hive.cli.control.CoreCliDriver.runTest(CoreCliDriver.java:115)\r\n\tat org.apache.hadoop.hive.cli.control.CliAdapter.runTest(CliAdapter.java:139)\r\n\tat org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver(TestMiniLlapLocalCliDriver.java:62)\r\n{noformat}\r\n\r\nThe repro above requires all technical columns (e.g., {{BLOCK__OFFSET__INSIDE__FILE}}) to be present in the SELECT clause which is rather rare in real SQL queries. However, the problem can still appear in other use-cases/scenarios when using the {{hive.optimize.cte}} features and/or materialized views and the same view or CTE is used multiple times in the query.", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13630852/comment/18028005", "id": "18028005", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "body": "This is caused by HIVE-28222 or rather a follow-up of the latter. Problem is reproducible using TestMiniLlapLocalCliDriver on current master branch (commit d46d900cba2b484f033772ec42e62e8f96e0c07a).", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-10-07T08:37:09.246+0000", "updated": "2025-10-07T08:37:09.246+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13630852/comment/18033691", "id": "18033691", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "body": "Hi [~zabetak] , can you please set the affected version(s)?", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "created": "2025-10-28T22:08:21.770+0000", "updated": "2025-10-28T22:08:21.770+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-10-30T19:20:18.000+0000", "created": "2025-10-07T08:35:03.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630835", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630835", "key": "HIVE-29248", "fields": {"summary": "HMS Iceberg REST API should return 403 on HiveAccessControlException", "description": "The current implementation does not handle permission errors and returns a 500 error. This is the exception when I integrated HMS Iceberg REST Catalog with Apache Ranger.\r\n{code:java}\r\n2025-10-07T02:26:57,248 ERROR [qtp100805003-49] rest.HMSCatalogServlet: Error processing REST request\r\norg.apache.iceberg.exceptions.RESTException: Unhandled error: ErrorResponse(code=500, type=RuntimeException, message=Failed to list namespace under namespace: default in Hive Metastore)\r\njava.lang.RuntimeException: Failed to list namespace under namespace: default in Hive Metastore\r\n\u00a0 \u00a0 \u00a0 \u00a0at org.apache.iceberg.hive.HiveCatalog.loadNamespaceMetadata(HiveCatalog.java:632)\r\n\u00a0 \u00a0 \u00a0 \u00a0at org.apache.iceberg.catalog.SupportsNamespaces.namespaceExists(SupportsNamespaces.java:159)\r\n\u00a0 \u00a0 \u00a0 \u00a0at org.apache.iceberg.rest.CatalogHandlers.namespaceExists(CatalogHandlers.java:167)\r\n\u00a0 \u00a0 \u00a0 \u00a0at org.apache.iceberg.rest.HMSCatalogAdapter.namespaceExists(HMSCatalogAdapter.java:249)\r\n\u00a0 \u00a0 \u00a0 \u00a0at org.apache.iceberg.rest.HMSCatalogAdapter.handleRequest(HMSCatalogAdapter.java:441)\r\n\u00a0 \u00a0 \u00a0 \u00a0at org.apache.iceberg.rest.HMSCatalogAdapter.execute(HMSCatalogAdapter.java:524)\r\n\u00a0 \u00a0 \u00a0 \u00a0at org.apache.iceberg.rest.HMSCatalogServlet.service(HMSCatalogServlet.java:75)\r\n\u00a0 \u00a0 \u00a0 \u00a0at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\r\n...\r\nCaused by: MetaException(message:Permission denied: user [trino] does not have [USE] privilege on [default])\r\n\u00a0 \u00a0 \u00a0 \u00a0at org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.newMetaException(MetaStoreUtils.java:229)\r\n\u00a0 \u00a0 \u00a0 \u00a0at org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.newMetaException(MetaStoreUtils.java:219)\r\n\u00a0 \u00a0 \u00a0 \u00a0at org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.HiveMetaStoreAuthorizer.onEvent(HiveMetaStoreAuthorizer.java:137)\r\n\u00a0 \u00a0 \u00a0 \u00a0at org.apache.hadoop.hive.metastore.HMSHandler.firePreEvent(HMSHandler.java:4133)\r\n\u00a0 \u00a0 \u00a0 \u00a0at org.apache.hadoop.hive.metastore.HMSHandler.get_database_req(HMSHandler.java:1475)\r\n...\r\nCaused by: org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAccessControlException: Permission denied: user [trino] does not have [USE] privil\r\nege on [default]\r\n\u00a0 \u00a0 \u00a0 \u00a0at org.apache.ranger.authorization.hive.authorizer.RangerHiveAuthorizer.checkPrivileges(RangerHiveAuthorizer.java:1155)\r\n\u00a0 \u00a0 \u00a0 \u00a0at org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.HiveMetaStoreAuthorizer.checkPrivileges(HiveMetaStoreAuthorizer.java:701)\r\n\u00a0 \u00a0 \u00a0 \u00a0at org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.HiveMetaStoreAuthorizer.onEvent(HiveMetaStoreAuthorizer.java:133)\r\n\u00a0 \u00a0 \u00a0 \u00a0... 69 more\r\n\u00a0 \u00a0 \u00a0 \u00a0at org.apache.iceberg.rest.HMSCatalogAdapter.execute(HMSCatalogAdapter.java:537) ~[hive-standalone-metastore-rest-catalog-4.2.0-SNAPSHOT.jar:4.2.0\u00a0 \u00a0 \u00a0 \u00a0at org.apache.iceberg.rest.HMSCatalogServlet.service(HMSCatalogServlet.java:75) ~[hive-standalone-metastore-rest-catalog-4.2.0-SNAPSHOT.jar:4.2.0-\r\nSNAPSHOT] {code}", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13630835/comment/18033133", "id": "18033133", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=okumin", "name": "okumin", "key": "okumin", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=okumin&avatarId=36445", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=okumin&avatarId=36445", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=okumin&avatarId=36445", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=okumin&avatarId=36445"}, "displayName": "Shohei Okumiya", "active": true, "timeZone": "Asia/Tokyo"}, "body": "As HiveAccessControlException belongs to hive-exec(ql), it turned out that HiveCatalog cannot correctly classify the error category. This is not a REST problem but a HiveCatalog problem. I guess authorization-related modules should belong to storage-api or standalone-metastore-common because HMS or external modules are involved. I'm still checking the best way.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=okumin", "name": "okumin", "key": "okumin", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=okumin&avatarId=36445", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=okumin&avatarId=36445", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=okumin&avatarId=36445", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=okumin&avatarId=36445"}, "displayName": "Shohei Okumiya", "active": true, "timeZone": "Asia/Tokyo"}, "created": "2025-10-27T08:18:52.864+0000", "updated": "2025-10-27T08:18:52.864+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13630835/comment/18033470", "id": "18033470", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=okumin", "name": "okumin", "key": "okumin", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=okumin&avatarId=36445", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=okumin&avatarId=36445", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=okumin&avatarId=36445", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=okumin&avatarId=36445"}, "displayName": "Shohei Okumiya", "active": true, "timeZone": "Asia/Tokyo"}, "body": "A sample patch. This doesn't work unless `iceberg-catalog` depends on `hive-exec`, which is probably not a good idea.\r\n\r\nhttps://github.com/okumin/hive/commit/af242c85eff5aedd5716b4879c3e92d0500734ae", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=okumin", "name": "okumin", "key": "okumin", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=okumin&avatarId=36445", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=okumin&avatarId=36445", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=okumin&avatarId=36445", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=okumin&avatarId=36445"}, "displayName": "Shohei Okumiya", "active": true, "timeZone": "Asia/Tokyo"}, "created": "2025-10-28T08:41:01.034+0000", "updated": "2025-10-28T08:41:01.034+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-10-28T08:41:01.000+0000", "created": "2025-10-07T02:34:30.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630803", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630803", "key": "HIVE-29247", "fields": {"summary": "Iceberg: Add authentication support to HiveRESTCatalogClient", "description": null, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-07T14:48:31.000+0000", "created": "2025-10-06T19:24:35.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630751", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630751", "key": "HIVE-29246", "fields": {"summary": "Upgrade derby version to 10.17.1.0", "description": "As hive master branch is on JDK 21, upgrade of derby to 10.17.1.0 is possible [https://db.apache.org/derby/derby_downloads.html]\r\n\r\nThis will help with CVE's as well [CVE-2022-46337|https://www.cve.org/CVERecord?id=CVE-2022-46337]\u00a0\r\n\r\n[https://mvnrepository.com/artifact/org.apache.derby/derby/10.14.2.0]\r\n\r\n\u00a0\r\n\r\nDerby project has done refactoring:\r\n # org.apache.derby.jdbc.EmbeddedDriver is now part of\u00a0*derbytools*\u00a0jar\r\n # org.apache.derby.security.SystemPermission \u2192 org.apache.derby.shared.common.security.SystemPermission (this is part of derbyshared jar which is compile time dependency for org.apache.derby:derby)\r\n # org.apache.derby.jdbc.AutoloadedDriver \u2192 org.apache.derby.iapi.jdbc.AutoloadedDriver", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13630751/comment/18029424", "id": "18029424", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "body": "Fixed in [https://github.com/apache/hive/commit/009dea2b09eceba05bc96282842311b27e0d8c07]\r\n\r\n\u00a0\r\n\r\nThanks for the PR [~Aggarwal_Raghav] !\u00a0", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-10-13T08:25:18.094+0000", "updated": "2025-10-13T08:25:18.094+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13630751/comment/18030043", "id": "18030043", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "body": "Documentation update in https://github.com/apache/hive-site/commit/07b3afd1f4aad11ff07565459ce9c49c64b09ded", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-10-15T10:15:06.193+0000", "updated": "2025-10-15T10:15:06.193+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-10-15T10:15:06.000+0000", "created": "2025-10-06T08:08:23.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630700", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630700", "key": "HIVE-29245", "fields": {"summary": "Normalize Java license headers", "description": "SonarQube often reports that license headers in new files don't have the expected format. It is probably because some existing files are not compliant with the asf.header, and contributors copy-and-paste the wrong format of headers.\r\n\r\n> Line does not match expected header line of ' * http://www.apache.org/licenses/LICENSE-2.0'.\r\nHeader external_checkstyle:header.HeaderCheck\r\n\r\n[https://sonarcloud.io/project/issues?sinceLeakPeriod=true&issueStatuses=OPEN%2CCONFIRMED&pullRequest=6108&id=apache_hive&open=AZmX8jO00mHwehPdTX0p]\r\n\r\n\u00a0\r\n\r\nWe can locally check those violations with the following configuration and command.\r\n{code:java}\r\n$ cat checkstyle/checkstyle.xml \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\r\n<?xml version=\"1.0\"?>\r\n<!DOCTYPE module PUBLIC\r\n\u00a0 \u00a0 \"-//Puppy Crawl//DTD Check Configuration 1.2//EN\"\r\n\u00a0 \u00a0 \"http://www.puppycrawl.com/dtds/configuration_1_2.dtd\">\r\n<module name=\"Checker\">\r\n\u00a0 <module name=\"Header\">\r\n\u00a0 \u00a0 <property name=\"headerFile\" value=\"${config_loc}/asf.header\"/>\r\n\u00a0 </module>\r\n</module>\r\n$ mvn checkstyle:check{code}\r\n\u00a0\r\n\r\nIn this ticket, we will rewrite incompatible license headers.", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-17T09:25:37.000+0000", "created": "2025-10-05T04:58:22.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630638", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630638", "key": "HIVE-29244", "fields": {"summary": "Add catalog field into ShowLocksRequest", "description": "[https://github.com/apache/hive/blob/744a0d6869e3a93f5d1f9afc151677ab432709d1/standalone-metastore/metastore-common/src/main/thrift/hive_metastore.thrift#L1292-L1297]\r\n\r\n\u00a0\r\n\r\nAt present, {*}show locks dbName{*}could only get result from default hive catalog. We need to add catalog field into ShowLocksRequest, so that we can get locks info from different catalogs by new syntax {*}show locks catName.dbName{*}.", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-03T15:16:48.000+0000", "created": "2025-10-03T15:16:48.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630633", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630633", "key": "HIVE-29243", "fields": {"summary": "Evaluate whether certain functions can be enabled under non hive default catalogs", "description": "I have noticed that some features are currently limited to the default 'hive' catalog and are not available for newly created catalogs. Such as: HIVE-19898\u00a0 and HIVE-24127 and HIVE-22291\u00a0\r\n\r\nSince we aim to fully develop the capabilities of catalog, we need to evaluate whether these features should also be enabled for non default 'hive' catalogs. This is to prevent potential difficulties in using newly created catalogs due to the absence of these functions in the future.\r\n\r\n\u00a0\r\n\r\nBTW, Some task can only do in NATIVE catalog, like ReploadTask. As for external catalogs(e.g., JDBC, REST), we need to skip those tasks that are exclusive to native catalogs.", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-27T13:52:55.000+0000", "created": "2025-10-03T15:01:05.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630631", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630631", "key": "HIVE-29242", "fields": {"summary": "Add catalog for transaction module", "description": "A previous WIP ticket could be referred to: https://issues.apache.org/jira/browse/HIVE-18973 _Make transaction system work with catalogs._\r\n\r\n\u00a0\r\n\r\nWe may need to continue this work to make transaction module work with catalog.", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-03T14:50:56.000+0000", "created": "2025-10-03T14:50:21.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630630", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630630", "key": "HIVE-29241", "fields": {"summary": "Add managedLocationUri field for catalog", "description": "HIVE-22995 introduced the concept of managedLocationUri. This means that created databases will have two locations.\r\n\r\nCurrently, when creating a catalog, there is only the location attribute, without managedLocationUri. We should also add the managedLocationUri field to the catalog, so that when users create a database from the new created catalog, it can inherit the two locations from the catalog.\r\n\r\nAdditionally, to avoid disrupting the location of the current default catalog 'hive' (which is controlled by the properties *metastore.warehouse.dir* and {*}metastore.warehouse.external.dir{*}), the locations for newly created catalogs should be separated from the default 'hive' catalog. Two new parameters can be added, such as *metastore.warehouse.catalog.dir* and {*}metastore.warehouse.catalog.external.dir{*}.\r\n\r\nMoreover, the location for each newly created catalog should have the catalog name appended at the end. For example, if *metastore.warehouse.catalog.dir* is *hdfs://ns1/testdir,* then the location for a newly created catalog named testcat would be {*}hdfs://ns1/testdir/testcat{*}. Consequently, the default path for a database like testdb created under this catalog would be {*}hdfs://ns1/testdir/testcat/testdb{*}.", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-03T14:40:19.000+0000", "created": "2025-10-03T14:39:04.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630626", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630626", "key": "HIVE-29240", "fields": {"summary": "Increase GKE node pool capacity to run more PR jobs concurrently in CI", "description": "Currently the CI is using an auto-scalable node pool ([slave-pool9|https://console.cloud.google.com/kubernetes/nodepool/us-central1-c/hive-test-kube/slave-pool9]) that can span up to 8 nodes. Based on the current configuration 2 pull requests can run in parallel every ~3 hours. \r\n\r\nThis ticket aims to increase the number of nodes for the slave-pool9 node pool to 16 essentially doubling the number of pull requests that can run concurrently in CI. ", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13630626/comment/18024499", "id": "18024499", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "body": "There is a high chance that the change will also affect the monthly cost of the CI infrastructure. We will monitor the consumption and if cost remains at reasonable levels the changes will be retained otherwise we may rollback to the previous values.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-10-03T14:07:30.707+0000", "updated": "2025-10-03T14:07:30.707+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13630626/comment/18024502", "id": "18024502", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "body": "The changes were implemented via the GKE console. As of now the node pool can scale up to 16 nodes.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-10-03T14:14:30.830+0000", "updated": "2025-10-03T14:14:30.830+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-10-03T14:14:30.000+0000", "created": "2025-10-03T14:06:58.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630578", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630578", "key": "HIVE-29239", "fields": {"summary": "Upgrade checkstyle to 11.1.0", "description": "Since we reintroduced checkstyle in HIVE-29196, I have seen checkstyle violations in PRs when using modern java features. We should upgrade checkstyle to the latest version of 11.1.0 for better language support.\u00a0", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13630578/comment/18024405", "id": "18024405", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "body": "[~soumyakanti.das] Please provide some examples with the violations that you observed. I believe that the problems you encountered are not violations but rather crashes and bugs in the checkstyle library.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-10-03T07:04:54.027+0000", "updated": "2025-10-03T07:04:54.027+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13630578/comment/18024869", "id": "18024869", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "body": "Fixed in https://github.com/apache/hive/commit/0e8749ee85bf202e69f5b4d3e8d325a01a5a2033", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-10-06T09:07:25.609+0000", "updated": "2025-10-06T09:07:25.609+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-10-06T09:07:25.000+0000", "created": "2025-10-02T22:24:19.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630516", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630516", "key": "HIVE-29238", "fields": {"summary": "upgrade kafka version to fix CVE-2024-31141 and CVE-2021-38153", "description": null, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-27T12:11:47.000+0000", "created": "2025-10-02T10:09:53.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630510", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630510", "key": "HIVE-29237", "fields": {"summary": "TestHiveCommitLocks flakyness", "description": "TestHiveCommitLocks is flaky on downstream. I couldn't reproduce the issue on upstream but I'm pretty sure it can happen any time - as the test cases are reusing resources.\u00a0\r\n\r\nUpdate:\u00a0\r\nWith the help of [~dkuzmenko] , the root cause is identified:\r\n\r\nIt seems Hive: Refactor commit lock mechanism from HiveTableOperations (#6648) port from Iceberg had a mistake and left two\u00a0\r\ndoNothing().when(spyOps).doUnlock(any()); calls in the test class.\u00a0\r\n\u00a0", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13630510/comment/18028003", "id": "18028003", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "Merged to master.\r\n\r\nThanks [~InvisibleProgrammer] \u00a0for handling that.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-07T08:10:17.149+0000", "updated": "2025-10-07T08:10:17.149+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-07T08:10:17.000+0000", "created": "2025-10-02T08:19:35.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630458", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630458", "key": "HIVE-29236", "fields": {"summary": "hive-site: The link to the organization of the community member is invalid", "description": "[https://github.com/apache/hive-site/blob/b5d187552c34e8ffc7bf7ac4afaf985bb82bf3e2/content/community/people.md?plain=1#L39]\r\n\r\n\u00a0\r\n\r\nThe organization of the community member is invalid if you check the website: [https://hive.apache.org/community/people/]\u00a0\r\n\r\n\u00a0", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-01T15:55:05.000+0000", "created": "2025-10-01T15:55:05.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630432", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630432", "key": "HIVE-29235", "fields": {"summary": "Iceberg returns incorrect count value", "description": "For iceberg table, Hive tries to read partitionStatistics.\r\n\r\nBut if the table doesn't have them, Hive calculates using default statistics, which is incorrect.\r\n\r\nWe are using Hive 4.1.0.\r\n\r\nSELECT count(*), log_date FROM db1.tbl1 GROUP BY 2;\r\n+----------+-------------+\r\n| _c0 | log_date |\r\n+----------+-------------+\r\n| 343662 | 2025-09-29 |\r\n| 2513459 | 2025-09-30 | \u00a0\r\n\u00a0\r\n\u00a0\r\nSELECT count(*) FROM db1.tb1 WHERE log_date = '2025-09-29'; // 2857121\r\nSELECT count(*) FROM db1.tb1 WHERE log_date = '2025-09-30'; // 2857121\r\n\u00a0", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13630432/comment/18024017", "id": "18024017", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "body": "Hi [~jyoo94] . Thanks for reporting this issue!\r\n\r\nCould you please give a more specific test example to help others to reproduce this issue? \u00a0For example, how to create a table and insert a small amount of data to reproduce this issue?", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-01T10:40:05.693+0000", "updated": "2025-10-01T10:40:05.693+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13630432/comment/18024054", "id": "18024054", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=jyoo94", "name": "jyoo94", "key": "jyoo94", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34055", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34055", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34055", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34055"}, "displayName": "Jaeho Yoo", "active": true, "timeZone": "Asia/Seoul"}, "body": "[~zhangbutao]\u00a0\r\nSo we first used Flink to produced data in iceberg format (without the flink option of `write.metadata.statistics.enabled`), with 2 partitions (and about 30 other columns) with log_date as partition field (string type)\r\n\r\nThen, simply do `SELECT count(*) FROM db1.tb1 WHERE log_date = '2025-09-29'`", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=jyoo94", "name": "jyoo94", "key": "jyoo94", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34055", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34055", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34055", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34055"}, "displayName": "Jaeho Yoo", "active": true, "timeZone": "Asia/Seoul"}, "created": "2025-10-01T13:50:27.361+0000", "updated": "2025-10-01T13:50:27.361+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13630432/comment/18024123", "id": "18024123", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "> For iceberg table, Hive tries to read partitionStatistics. But if the table doesn't have them, Hive calculates using default statistics, which is incorrect.\r\n\r\n[~jyoo94] what do you mean by default statistics? If partitionStatistics is not available\r\nHiveIcebergStorageHandler#canComputeQueryUsingStats should returns false and trigger the query execution.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-01T18:45:52.857+0000", "updated": "2025-10-01T18:46:09.349+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13630432/comment/18024161", "id": "18024161", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=jyoo94", "name": "jyoo94", "key": "jyoo94", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34055", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34055", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34055", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34055"}, "displayName": "Jaeho Yoo", "active": true, "timeZone": "Asia/Seoul"}, "body": "[~dkuzmenko]\u00a0\r\nIt's basicStats. It estimates stats in absense of statistics.\u00a0\r\n[https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java#L275]\r\n\r\n\u00a0\r\n\r\nAlso, when in optimizer, when it tries to get row count, it sums up all row count for each partitions.\r\nhttps://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java#L933-L958", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=jyoo94", "name": "jyoo94", "key": "jyoo94", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34055", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34055", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34055", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34055"}, "displayName": "Jaeho Yoo", "active": true, "timeZone": "Asia/Seoul"}, "created": "2025-10-01T23:08:44.272+0000", "updated": "2025-10-01T23:15:56.738+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13630432/comment/18024319", "id": "18024319", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "[~jyoo94] estimated stats are used for query planning, not query output. Do you see tasks being launched, or is the result available instantly?\r\n\r\nwhat is the value of `hive.iceberg.stats.source` in hive-site.xml ?", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-02T18:08:36.358+0000", "updated": "2025-10-02T18:17:03.181+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13630432/comment/18029348", "id": "18029348", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=jyoo94", "name": "jyoo94", "key": "jyoo94", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34055", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34055", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34055", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34055"}, "displayName": "Jaeho Yoo", "active": true, "timeZone": "Asia/Seoul"}, "body": "Sry I was AFK due to Korean Thanksgiving day. [~dkuzmenko]\u00a0\r\nWe didn't set any value for the key `hive.iceberg.stats.source`", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=jyoo94", "name": "jyoo94", "key": "jyoo94", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34055", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34055", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34055", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34055"}, "displayName": "Jaeho Yoo", "active": true, "timeZone": "Asia/Seoul"}, "created": "2025-10-12T23:45:33.580+0000", "updated": "2025-10-12T23:45:33.580+0000"}], "maxResults": 6, "total": 6, "startAt": 0}, "updated": "2025-10-27T22:21:00.000+0000", "created": "2025-10-01T09:51:46.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630342", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630342", "key": "HIVE-29234", "fields": {"summary": "Iceberg: Validate HMS REST Catalog Client with OAuth2", "description": "Add OAuth 2 support to HiveRESTCatalogClient, covering initial token acquisition and automatic refresh handling.\r\n\r\n\u00a0\r\n\r\nOAuth2 authentication manager enhancement discussion:\r\n\r\n[https://lists.apache.org/thread/6fvfq2wx6joxqnc4qboq5wfv07hdlv0p]\r\n\r\n[https://github.com/dremio/iceberg-auth-manager]", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13630342/comment/18029812", "id": "18029812", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "body": "Thanks for reviewing this PR, [~okumin] !", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "created": "2025-10-14T15:35:05.049+0000", "updated": "2025-10-14T15:35:05.049+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-14T15:35:05.000+0000", "created": "2025-09-30T12:28:27.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630275", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630275", "key": "HIVE-29233", "fields": {"summary": "Iceberg: Validate HiveRESTCatalogClient with external RESTCatalogs like Gravitino", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13630275/comment/18027895", "id": "18027895", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "body": "Thanks for the code review, [~okumin] and [~dkuzmenko] !", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "created": "2025-10-06T19:14:06.783+0000", "updated": "2025-10-06T19:14:06.783+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-06T19:14:06.000+0000", "created": "2025-09-29T22:39:42.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630232", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630232", "key": "HIVE-29232", "fields": {"summary": "set hive.exec.parallel=true; \u5f15\u53d1\u540c\u4e00\u4e2asql\uff0c\u5e76\u884cstage\u7684map.xml\u6587\u4ef6\u8def\u5f84\u91cd\u590d\uff0c\u5bfc\u81f4\u4efb\u52a1\u5076\u73b0\u51fa\u9519", "description": "int pathId=10000\r\n\r\n\u00a0\r\n\r\npublic Path getMRTmpPath() {\r\n\r\nreturn new Path(getMRScratchDir(), MR_PREFIX + nextPathId());\r\n\r\n}\r\n\r\n\u00a0\r\n\r\n// \u8fd9\u6bb5\u4ee3\u7801\u5728\u5e76\u53d1\u6267\u884c\u65f6\u53ef\u80fd\u51fa\u73b0\u8fd4\u56de\u503c\u4e00\u6837\u7684\u60c5\u51b5\r\n\r\nprivate String nextPathId() {\r\n\r\nreturn Integer.toString(pathid++);\r\n\r\n}", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13630232/comment/18024016", "id": "18024016", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "body": "Hi [~liukailong123] . Thanks for your reporting this issue. Please note that JIRA descriptions must be written in English to ensure all developers can understand them.\r\n\r\nOne question: are you using the MR engine or the Tez engine? The Hive community has deprecated the MR engine, and the current primary code maintenance is focused on the Tez engine module.\r\n\r\nAdditionally, Apache Hive no longer uses patch files for code contributions. Please submit your code fixes via PR to [https://github.com/apache/hive/pulls] .\r\n\r\nThanks.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-01T10:35:46.190+0000", "updated": "2025-10-01T10:35:46.190+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-01T10:35:46.000+0000", "created": "2025-09-29T13:17:46.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630186", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630186", "key": "HIVE-29231", "fields": {"summary": "Fix flaky TestEmbeddedHiveMetaStore.testDatabase by enforcing deterministic database lookup", "description": "The test org.apache.hadoop.hive.metastore.TestEmbeddedHiveMetaStore.testDatabase(standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java) passed using normal maven-test, but showed Non-deterministic behavior under NonDex(https://github.com/TestingResearchIllinois/NonDex) and thus failed. NonDex is a tool for detecting hidden assumptions in code by exploring non-deterministic behaviors of specifications that allow multiple valid implementations.\r\n\r\n\u00a0\r\n\r\nSome of the error messages are:\r\n{code:java}\r\nBuild Failure observed:\r\njava.lang.AssertionError: first database is not testdb1 in list: []{code}\r\n\u00a0\r\n\r\nSteps to reproduce:\r\n{code:java}\r\nmvn -pl standalone-metastore/metastore-server edu.illinois:nondex-maven-plugin:2.2.1:nondex -Dtest=org.apache.hadoop.hive.metastore.TestEmbeddedHiveMetaStore#testDatabase {code}\r\n\u00a0\r\n\r\nRoot cause:\r\nThe test asserted database existence using getDatabases(\".*\") which relies on cached database lists that may return stale results when nondex randomizes test execution. This led to nondeterministic test outcomes where databases existed but were not found in the cached list.\r\n\r\n\u00a0\r\n\r\nFix:\r\nReplace getDatabases(\".*\") with direct getDatabase(name) calls in TestHiveMetaStore class's testDatabase method. This ensures consistent behavior regardless of test shuffling. Additionally, I believe this improve the harness of tests without altering and damaging the core database operations being tested(add, find, drop).\r\n\r\nConfirm Fixed:\r\nre-run\r\n{code:java}\r\nmvn -pl standalone-metastore/metastore-server edu.illinois:nondex-maven-plugin:2.2.1:nondex -Dtest=org.apache.hadoop.hive.metastore.TestEmbeddedHiveMetaStore#testDatabase {code}\r\nAnd the result is Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, BUILD SUCCESS", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-09-30T20:52:06.000+0000", "created": "2025-09-29T00:17:01.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630139", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630139", "key": "HIVE-29230", "fields": {"summary": "Iceberg: Reads fails after Schema evolution with complex type columns", "description": "If we add a complex type column to an existing table with data the reads fails\r\n{noformat}\r\nCaused by: java.lang.ClassCastException: optional binary point is not a group \u00a0 \u00a0 \u00a0 \u00a0 at \r\norg.apache.parquet.schema.Type.asGroupType(Type.java:247) \u00a0 \u00a0 \u00a0 \u00a0 \r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.buildVectorizedParquetReader(VectorizedParquetRecordReader.java:541) \u00a0 \u00a0 \u00a0 \u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:455) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:415) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.next(VectorizedParquetRecordReader.java:353) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.next(VectorizedParquetRecordReader.java:97) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.iceberg.mr.hive.vector.HiveBatchIterator.advance(HiveBatchIterator.java:75) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.iceberg.mr.hive.vector.HiveBatchIterator.hasNext(HiveBatchIterator.java:143) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.iceberg.mr.mapreduce.IcebergRecordReader.nextKeyValue(IcebergRecordReader.java:119) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.iceberg.mr.hive.vector.HiveIcebergVectorizedRecordReader.next(HiveIcebergVectorizedRecordReader.java:48) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.iceberg.mr.hive.vector.HiveIcebergVectorizedRecordReader.next(HiveIcebergVectorizedRecordReader.java:34) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:373) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:82) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:118) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:58) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:208) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:75) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:414) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:293) \u00a0 {noformat}", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13630139/comment/18023976", "id": "18023976", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn", "name": "ayushtkn", "key": "ayushtkn", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Ayush Saxena", "active": true, "timeZone": "Asia/Kolkata"}, "body": "Committed to master.\r\n\r\nThanx [~difin] & [~dkuzmenko] for the reviews!!!", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn", "name": "ayushtkn", "key": "ayushtkn", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Ayush Saxena", "active": true, "timeZone": "Asia/Kolkata"}, "created": "2025-10-01T06:01:40.429+0000", "updated": "2025-10-01T06:01:40.429+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-01T06:01:49.000+0000", "created": "2025-09-27T16:53:30.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630116", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630116", "key": "HIVE-29229", "fields": {"summary": "Consider deprecating and removing the TestMiniLlapCliDriver", "description": "Tested against master branch, commit id: https://github.com/apache/hive/commit/fda96f8a0db51207bc3ea8f95a9cd19bc5d94260\r\n\r\n\u00a0\r\n{code:java}\r\nmvn test -Dtest=TestMiniLlapLocalCliDriver -Dqfile=catalog.q -Dtest.output.overwrite=true -pl itests/qtest -Pitests{code}\r\n\u00a0\r\n{code:java}\r\nmvn test -Dtest=TestMiniLlapCliDriver -Dqfile=catalog.q -Dtest.output.overwrite=true -pl itests/qtest -Pitests{code}\r\n\u00a0\r\n\r\nI encountered a confusing discrepancy between the results of running\u00a0{{catalog.q}}\u00a0with\u00a0{{TestMiniLlapLocalCliDriver}}\u00a0and\u00a0{{{}TestMiniLlapCliDriver{}}}. I haven't investigated the root cause yet.\r\n\r\nSince our CI has been defaulting to\u00a0{{TestMiniLlapLocalCliDriver}}\u00a0since HIVE-23138, should we consider deprecating and removing\u00a0{{{}TestMiniLlapCliDriver{}}}? This would prevent confusion for Hive developers and help streamline our test module code.", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13630116/comment/18024124", "id": "18024124", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "TestMiniLlapLocalCliDriver helps to debug code executed on llap daemons.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-01T18:49:14.202+0000", "updated": "2025-10-01T18:49:14.202+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13630116/comment/18024304", "id": "18024304", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "body": ">> TestMiniLlapLocalCliDriver helps to debug code executed on llap daemons.\r\n\u00a0\r\nTestMiniLlapLocalCliDriver is beneficial for debugging code and is also the current default test class.\r\n\r\nSo, should we consider to remove TestMiniLlapCliDriver? I'm not sure what practical value TestMiniLlapCliDriver has.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-02T15:54:18.242+0000", "updated": "2025-10-02T15:54:18.242+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-10-02T15:54:18.000+0000", "created": "2025-09-27T07:37:58.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630114", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630114", "key": "HIVE-29228", "fields": {"summary": "Support Credential Vending in Iceberg REST Catalog", "description": "Iceberg REST API spec allows LoadTable or [/v1/\\{prefix}/namespaces/\\{namespace}/tables/\\{table}/credentials|https://github.com/apache/iceberg/blob/apache-iceberg-1.10.0/open-api/rest-catalog-open-api.yaml#L1184-L1220] to issue temporary credentials.\r\n\r\nRelated PRs and discussions..\r\n * [apache/iceberg#10722 OpenAPI: Standardize credentials in loadTable/loadView responses|https://github.com/apache/iceberg/pull/10722]\r\n * [apache/iceberg#11118 REST: Standardize vended credentials used in loadTable / loadView responses|https://github.com/apache/iceberg/issues/11118]\r\n * Iceberg dev ML: [DISCUSS] REST: Standardize vended credentials in Spec: [https://lists.apache.org/thread/jmklpnywnghg7qwmwr14zj2k6tnxmdo4]\r\n * Google Docs: [https://docs.google.com/document/d/1lySd_5hMZNtISLKsOvAq7xiNzdXU6TAoHF_yrOXWQvM/edit?tab=t.0#heading=h.hs6r9d26w1y2]\r\n\r\n\u00a0", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-10-29T09:52:06.000+0000", "created": "2025-09-27T04:32:57.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630056", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630056", "key": "HIVE-29227", "fields": {"summary": "Apply masking in memory instead of rewriting the file in QOutProcessor#maskPatterns", "description": "After HIVE-29226 has been merged, the masking is applied in various places. Refactor it so that the masking is only applied once, in memory, before the SORT_QUERY_RESULTS is applied.\r\n\r\nQFile also defines some patterns for masking. It would be a good to have a single place for all these mask rules. Probably a good opportunity for refactoring.", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-09-26T11:07:27.000+0000", "created": "2025-09-26T11:07:27.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13630055", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13630055", "key": "HIVE-29226", "fields": {"summary": "Make order of qfile query results deterministic when masking and sorting", "description": "Apply the masking before sorting in QTestResultProcessor to make the query results deterministic.", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13630055/comment/18023835", "id": "18023835", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "Merged to master.\r\n\r\nThanks for the fix, [~thomas.rebele] !\u00a0", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-30T15:18:01.938+0000", "updated": "2025-09-30T15:18:01.938+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-08T13:09:37.000+0000", "created": "2025-09-26T11:02:50.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13629768", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13629768", "key": "HIVE-29225", "fields": {"summary": "Premature deletion of scratch directories during output streaming", "description": "Once a job or application finishes, the corresponding lock file is released, and YARN no longer reports any active jobs or applications. At this point, Hive assumes the associated scratch directory is no longer needed and proceeds to delete it upon *ClearDanglingScratchDir* service is invoked.\r\n\r\nHowever, in some cases, Hive may still be streaming output to the client after the application is marked as finished. This causes the scratch directory to be deleted prematurely, even though it is still required for ongoing output.\r\n\r\nAs a result, queries can fail with *IOException* errors because the scratch directory is removed while Hive is still using it.\r\n{code:java}\r\norg.apache.hive.service.cli.HiveSQLException: java.io.IOException: java.io.IOException: 2049.323.265264 /user/mapr/tmp/hive/mapr/ecfdf7a4-91ba-4832-a408-8d459f90ac4b/hive_2025-07-23_12-57-08_793_7535333864129536266-1/-mr-10001/.hive-staging_hive_2025-07-23_12-57-08_793_7535333864129536266-1/-ext-10002/000008_0 (Input/output error)\r\n{code}\r\n\r\n---\r\n\r\n*STEPS TO REPRODUCE:*\r\n1. Add the following properties into hive-site.xml and restart Hive services:\r\n{code:java}\r\n<property>\r\n  <name>hive.scratchdir.lock</name>\r\n  <value>true</value>\r\n</property>\r\n<property>\r\n  <name>tez.session.am.dag.submit.timeout.secs</name>\r\n  <value>10</value>\r\n</property>\r\n{code}\r\n\r\n2. Generate the data using the following commands:\r\n{code:java}\r\nfor i in {1..1000000}; do echo \"\" >> /tmp/file1.txt; done\r\nfor i in {1..8}; do cat /tmp/file1.txt >> /tmp/file2.txt; cat /tmp/file2.txt >> /tmp/file1.txt; done\r\n{code}\r\n\r\n3. Create Hive table and load data into it:\r\n{code:java}\r\nCREATE TABLE i (id INT);\r\nLOAD DATA LOCAL INPATH '/tmp/file2.txt' INTO TABLE i;\r\n{code}\r\n\r\n4. Connect to HiveServer2 using Beeline and execute the following queries:\r\n{code:java}\r\nSET hive.fetch.task.conversion=none;\r\nSELECT * FROM i;\r\n{code}\r\n\r\n5. Open a new session to the same host. After the query from step 4 starts returning the results, wait 30 seconds and execute the command below:\r\n{code:java}\r\nhive --service cleardanglingscratchdir -v\r\n{code}\r\n\r\n6. \u201dhive --service cleardanglingscratchdir -v\u201d deletes the scratchdir used by the query and the query fails during the next 20-30 seconds with the following error:\r\n{code:java}\r\n...\r\n| NULL  |\r\norg.apache.hive.service.cli.HiveSQLException: java.io.IOException: java.io.IOException: 2049.323.265264 /user/mapr/tmp/hive/mapr/ecfdf7a4-91ba-4832-a408-8d459f90ac4b/hive_2025-07-23_12-57-08_793_7535333864129536266-1/-mr-10001/.hive-staging_hive_2025-07-23_12-57-08_793_7535333864129536266-1/-ext-10002/000008_0 (Input/output error)\r\n0: jdbc:hive2://node1:10000/>\r\n{code}\r\n\r\n", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13629768/comment/18033692", "id": "18033692", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "body": "Hi [~sercan.tekin], please set the affected version(s).", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "created": "2025-10-28T22:09:29.522+0000", "updated": "2025-10-28T22:09:29.522+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-30T00:09:41.000+0000", "created": "2025-09-24T02:07:06.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13629678", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13629678", "key": "HIVE-29224", "fields": {"summary": "Move TPC-DS CTE suggestion tests in new test suite", "description": "HIVE-28259 introduced a new experimental feature for finding and materializing common table expressions (CTEs) using CBO. The effect of the suggester on TPC-DS queries was tested using the TestTezTPCDS30TBPerfCliDriver.\r\n\r\nAs it is right now we cannot easily fine-tune other CTE related configurations (e.g., hive.optimize.cte.materialize.threshold) without affecting existing query plans and doing so may decrease test coverage for certain parts of the engine.\r\n\r\nI propose to move the suggester based (\"hive.optimize.cte.suggester.class\") tests under a new test suite (new TestTPCDSCteCliDriver) relying on the TPC-DS 30TB stats focusing on CTE materialization/suggestion features.\r\n\r\n*Pros*\u00a0\r\n * Increase in test coverage\r\n * No coupling of experimental features with main plan regression suite\r\n\r\n*Cons*\r\n * Increase in test runtime\r\n * Redundancy in golden files (*.q.out)\r\n\r\nOnce the CTE feature(s) are stabilized and become part of the default configuration it will no longer be necessary to maintain a separate suite.", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13629678/comment/18024407", "id": "18024407", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "body": "Fixed in https://github.com/apache/hive/commit/008f4d11e230f30103dc156247f97bf4d18e0b33", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-10-03T07:15:04.379+0000", "updated": "2025-10-03T07:15:04.379+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-03T07:15:04.000+0000", "created": "2025-09-23T08:49:53.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13629673", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13629673", "key": "HIVE-29223", "fields": {"summary": " The BUG is found in apache/hive:4.1.0", "description": "*The BUG is found in apache/hive:4.1.0*\r\n\r\n_Query 20250923_071049_00000_ht8pd failed: java.lang.NumberFormatException: For input string: \"60s\"_\r\n\r\n_project with docker containers explains_\r\n\u00a0\r\n*_https://github.com/Repinoid/hive410bug_*", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13629673/comment/18024125", "id": "18024125", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "hi [~naeel] , could you please add a cleaner bug description and repro steps.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-01T18:55:24.507+0000", "updated": "2025-10-01T18:55:24.507+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13629673/comment/18029150", "id": "18029150", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=naeel", "name": "naeel", "key": "JIRAUSER311011", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34056", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34056", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34056", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34056"}, "displayName": "Naeel", "active": true, "timeZone": "Europe/Moscow"}, "body": "I tried to describe this bug\u00a0 on [https://github.com/Repinoid/hive410bug] readme\r\n[https://github.com/Repinoid/hive410bug/blob/main/README.md]\r\nIt is necessary to clone & deploy\r\n\r\nWith metastore apache/hive Version 4.0.1 - no errors\r\nWhen using latest Version - 4.1.0 the bug\u00a0\r\n\u00a0\r\n_Query 20250923_071049_00000_ht8pd failed: java.lang.NumberFormatException: For input string: \"60s\"_\r\n\r\n{{occurs with Trino *SCREATE SCHEMA* command\u00a0}}\r\n\u00a0\r\n\r\n_trino> CREATE SCHEMA minio_catalog.mini WITH (location = 's3a://tiny/');_", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=naeel", "name": "naeel", "key": "JIRAUSER311011", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34056", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34056", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34056", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34056"}, "displayName": "Naeel", "active": true, "timeZone": "Europe/Moscow"}, "created": "2025-10-11T03:54:18.567+0000", "updated": "2025-10-11T03:57:48.136+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13629673/comment/18029273", "id": "18029273", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "body": "[~naeel] Could you please post the error stacktrace of HMS 4.1.0?", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-12T04:28:39.622+0000", "updated": "2025-10-12T04:28:39.622+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13629673/comment/18033293", "id": "18033293", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "{code:java}\r\n2025-10-27 20:06:53,824 ERROR [Metastore-Handler-Pool: Thread-59] metastore.RetryingHMSHandler (RetryingHMSHandler.java:invokeInternal(144)) - MetaException(message:java.lang.NumberFormatException: For input string: \"60s\")\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.ExceptionHandler.newMetaException(ExceptionHandler.java:152)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.ExceptionHandler.defaultMetaException(ExceptionHandler.java:168)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.HMSHandler.create_database_req(HMSHandler.java:1429)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.HMSHandler.create_database(HMSHandler.java:1368)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:91)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.AbstractHMSHandlerProxy.invoke(AbstractHMSHandlerProxy.java:82)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at jdk.proxy2/jdk.proxy2.$Proxy26.create_database(Unknown Source)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$create_database.getResult(ThriftHiveMetastore.java:19312)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$create_database.getResult(ThriftHiveMetastore.java:19291)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:104)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:250)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.NumberFormatException: For input string: \"60s\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.lang.NumberFormatException.forInputString(Unknown Source)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.lang.Long.parseLong(Unknown Source)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.lang.Long.parseLong(Unknown Source)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.conf.Configuration.getLong(Configuration.java:1607)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.fs.s3a.S3AUtils.longOption(S3AUtils.java:985)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.fs.s3a.S3AFileSystem.initThreadPools(S3AFileSystem.java:785)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:555)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3615)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.checkPermissions(StorageBasedAuthorizationProvider.java:409)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.authorize(StorageBasedAuthorizationProvider.java:381)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.authorize(StorageBasedAuthorizationProvider.java:176)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.authorizeCreateDatabase(AuthorizationPreEventListener.java:238)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.onEvent(AuthorizationPreEventListener.java:165)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.HMSHandler.firePreEvent(HMSHandler.java:4130)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.HMSHandler.create_database_core(HMSHandler.java:1217)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.HMSHandler.create_database_req(HMSHandler.java:1421){code}\r\nHas nothing todo with HMS. make sure to use proper version of Hadoop (3.4.1) as specified in RN: https://hive.apache.org/general/downloads/#31-july-2025--release-410-available\r\n\r\nwget -P ./jars [https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.4.1/hadoop-aws-3.4.1.jar]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T17:10:06.906+0000", "updated": "2025-10-27T17:23:35.919+0000"}], "maxResults": 4, "total": 4, "startAt": 0}, "updated": "2025-10-27T17:23:35.000+0000", "created": "2025-09-23T08:08:58.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13629669", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13629669", "key": "HIVE-29222", "fields": {"summary": "Compilation failure when running with thrift profile", "description": "Compilation failure post running with thrift profile\r\n\r\nThis issue\u00a0 is happening post HIVE-29184: Iceberg: Basic Variant type support in Hive\r\n\r\n\u00a0\r\n\r\nHIVE-29184 adds VARIANT_TYPE_NAME directly in serdeConstants.java, but it is thrift generated code. The same changes should have been made in serde.thrift. Compiling with thrift profile reverts this change causing compilation issue\r\n\r\n\u00a0", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13629669/comment/18023322", "id": "18023322", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn", "name": "ayushtkn", "key": "ayushtkn", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Ayush Saxena", "active": true, "timeZone": "Asia/Kolkata"}, "body": "Committed to master.\r\n\r\nThanx [~vikramahuja_] for the contribution!!!", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn", "name": "ayushtkn", "key": "ayushtkn", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Ayush Saxena", "active": true, "timeZone": "Asia/Kolkata"}, "created": "2025-09-27T16:49:45.017+0000", "updated": "2025-09-27T16:49:45.017+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-09-27T16:49:51.000+0000", "created": "2025-09-23T06:29:16.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13629624", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13629624", "key": "HIVE-29221", "fields": {"summary": "Upgrade bouncycastle to 1.79", "description": "We should upgrade bouncycastle to 1.79 for [https://www.wiz.io/vulnerability-database/cve/cve-2025-8916].", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13629624/comment/18021905", "id": "18021905", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=fangyurao", "name": "fangyurao", "key": "fangyurao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"}, "displayName": "Fang-Yu Rao", "active": true, "timeZone": "Etc/UTC"}, "body": "Close this since Apache Hive is already using 1.82.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=fangyurao", "name": "fangyurao", "key": "fangyurao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"}, "displayName": "Fang-Yu Rao", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-22T17:07:17.531+0000", "updated": "2025-09-22T17:07:17.531+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-09-22T17:07:17.000+0000", "created": "2025-09-22T16:52:01.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13629606", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13629606", "key": "HIVE-29220", "fields": {"summary": "Iceberg: Enable write operations with REST Catalog HMS Client", "description": null, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13629606/comment/18023249", "id": "18023249", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "body": "Thanks a lot for the code review and valuable comments, [~okumin] !", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "created": "2025-09-27T01:28:06.764+0000", "updated": "2025-09-27T01:28:06.764+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-09-27T01:28:06.000+0000", "created": "2025-09-22T13:53:59.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13629540", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13629540", "key": "HIVE-29219", "fields": {"summary": "Upgrade Jersey to 2.x", "description": null, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-09-22T06:04:25.000+0000", "created": "2025-09-22T06:01:23.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13629488", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13629488", "key": "HIVE-29218", "fields": {"summary": "LOAD OVERWRITE PARTITION on muti-level partititoned external Iceberg table may unexpectedly delete other partitions", "description": "When using {{appendFile}} as the implementation of {{{}LOAD{}}}, there is an issue with handling {{LOAD OVERWRITE}} on multi-level partitioned Iceberg tables. The overwrite logic may incorrectly delete partitions that should not be affected.\r\n\r\n*Steps to Reproduce*\r\n\r\n1. Create an Iceberg table partitioned by two columns, {{pcol1}} and {{{}pcol2{}}}.\r\n{code:java}\r\ncreate external table ice_parquet_multi_partitioned (\r\n    strcol string,\r\n    intcol integer\r\n) partitioned by (pcol1 string, pcol2 string)\r\nstored by iceberg; {code}\r\n2. Insert data into partition {{{}pcol1=x/pcol2=y{}}}.\r\n{code:java}\r\nLOAD DATA LOCAL INPATH '/path/to/pcol1=x_pcol2=y.parquet' INTO TABLE ice_parquet_multi_partitioned PARTITION (pcol1='x', pcol2='y');{code}\r\n3. Run a {{LOAD OVERWRITE}} into another partition, e.g., {{{}pcol1=x/pcol2=z{}}}.\r\n{code:java}\r\nLOAD DATA LOCAL INPATH '/path/to/pcol1=x_pcol2=z.parquet' OVERWRITE INTO TABLE\r\nice_parquet_multi_partitioned PARTITION (pcol1='x', pcol2='z');{code}\r\n*Expected Behavior*\r\nOnly the target partition ({{{}pcol1=x/pcol2=z{}}}) should be overwritten. Existing partitions ({{{}pcol1=x/pcol2=y{}}}) should remain intact.\r\n\r\n*Actual Behavior*\r\nThe existing partition {{pcol1=x/pcol2=y}} is unexpectedly deleted when overwriting another partition with the same {{pcol1}} value but different {{{}pcol2{}}}.", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13629488/comment/18023028", "id": "18023028", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "Merged to master\r\nThanks for the fix and refactor, [~bravoqq] and [~ayushsaxena] for the review!", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-26T08:12:51.619+0000", "updated": "2025-09-26T08:12:51.619+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-09-26T08:13:22.000+0000", "created": "2025-09-20T15:06:49.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13629447", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13629447", "key": "HIVE-29217", "fields": {"summary": "Add configuration to choose materialization strategy for CTEs", "description": "Currently there are two ways for materializing common table expressions (CTEs) that are present in SQL queries.\r\n\r\n+SQL/AST based materialization+\r\n\r\nIt was introduced in HIVE-11752 and relies on the syntax of SQL queries specifically in presence of WITH clauses. It is performed early on during the semantic analysis and triggers before any kind of optimization.\r\n\r\n+CBO based materialization+\r\n\r\nIt was introduced in HIVE-28259 and relies on plan equivalences. It is performed during the cost based optimization phase and after the initial analysis.\r\n\r\nCurrently the two strategies interfere with each other and in various cases if they are both enabled they can affect negatively the performance due to excessive materialization.\r\n\r\nAnother drawback is that if a query contains a WITH clause we cannot bypass the AST materialization (and let the materialization decision entirely on CBO) so we are risking a non-optimal choice purely based on the syntax of the query.\r\n\r\nI propose adding a new configuration property for explicitly selecting between AST and CBO materialization allowing each materialization strategy to be applied independently of the other.\r\n\r\nIn order to avoid changes in behavior and retain backward compatibility the default materialization strategy will use the old AST based approach.", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13629447/comment/18028444", "id": "18028444", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=soumyakanti.das", "name": "soumyakanti.das", "key": "soumyakanti.das", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34055", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34055", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34055", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34055"}, "displayName": "Soumyakanti Das", "active": true, "timeZone": "Etc/UTC"}, "body": "[~zabetak]\u00a0\r\n\r\nDo we know if one of them is better than the other? If CBO based materialization is better then we could probably set it as default.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=soumyakanti.das", "name": "soumyakanti.das", "key": "soumyakanti.das", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34055", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34055", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34055", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34055"}, "displayName": "Soumyakanti Das", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-08T18:59:08.904+0000", "updated": "2025-10-08T18:59:08.904+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13629447/comment/18028586", "id": "18028586", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "body": "[~soumyakanti.das] Without adding a new configuration it is not possible to set CBO materialization as default. The main goal of this change is to make the two materialization strategies independent of each other. Other than that the CBO strategy is still experimental so it cannot be the default at this point in time.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-10-09T08:00:31.470+0000", "updated": "2025-10-09T08:00:31.470+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-10-09T08:00:31.000+0000", "created": "2025-09-19T15:19:30.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13629389", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13629389", "key": "HIVE-29216", "fields": {"summary": "DirectSQL disables strict checking in MySQL, allowing undroppable partitions", "description": "[https://github.com/apache/hive/blob/77d0d8d92c3257fb056337e5757f0f9bd8c34f02/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/DatabaseProduct.java#L314]\r\n\r\n\u00a0\r\n\r\nWhen `SET @@session.sql_mode=ANSI_QUOTES` is written, it clears all other MySQL defaults, namely `STRICT_TRANS_TABLES`.\r\n\r\n\u00a0\r\n\r\nWhen inserting a partition with a value > 256 characters, it is successfully inserted when it should be rejected in MySQL. This leads to a scenario where you can insert a partition, the `PARTITIONS` table has a `PART_NAME` with > 256 chars, but the `PART_KEY_VALS` is silently truncated to 256 chars.\r\n\r\n\u00a0\r\n\r\nSo when you attempt to drop a table/cleanup partitions, it will never succeed, as the partition returned to the client is the truncated one, which then 404s on attempted deletion (correctly).\r\n\r\n\u00a0\r\n\r\nIdeally there is a validation check on partition values to ensure that it will insert into the DB, or we don't clear the default session mode (unless required)", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-09-18T22:28:36.000+0000", "created": "2025-09-18T22:28:19.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13629363", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13629363", "key": "HIVE-29215", "fields": {"summary": "Owner info for view is incorrect for alter view in the authorization events", "description": "Alter view query is sending in the wrong information regarding the 'owner', which can lead to incorrect authorization.\r\n\r\nMore details to be added.", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13629363/comment/18033693", "id": "18033693", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "body": "Hi [~hemanth619] , please set the affected version(s) and repro steps.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=difin", "name": "difin", "key": "JIRAUSER295017", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34054", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34054", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34054", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34054"}, "displayName": "Dmitriy Fingerman", "active": true, "timeZone": "America/Toronto"}, "created": "2025-10-28T22:10:21.983+0000", "updated": "2025-10-30T19:08:33.146+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-31T16:22:49.000+0000", "created": "2025-09-18T17:46:12.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13629328", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13629328", "key": "HIVE-29214", "fields": {"summary": "SELECT * from view failing with SemanticException when we created with column aliases.", "description": "SELECT * from view failing with SemanticException when we created with column aliases.\r\n\r\nReproduction steps:\r\n{noformat}\r\nCREATE TABLE t0(c0 bigint );\r\nINSERT INTO t0 VALUES(10),(20),(30);\r\nCREATE VIEW v0(col1) AS (SELECT * FROM t0);\r\n\r\nSELECT * FROM v0;{noformat}\r\nException Stacktrace:\r\n{noformat}\r\norg.apache.hadoop.hive.ql.parse.SemanticException: line 1:66 cannot recognize input near ')' 'v0' '<EOF>' in subquery source in definition of VIEW v0 [\r\nSELECT `c0` AS `col1` FROM ((select `t0`.`c0` from `default`.`t0`)) `v0`\r\n] used as v0 at Line 2:14\r\n    at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.replaceViewReferenceWithDefinition(SemanticAnalyzer.java:2909)\r\n    at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:2438)\r\n    at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:2328)\r\n    at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeAndResolveChildTree(SemanticAnalyzer.java:13052)\r\n    at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genResolvedParseTree(SemanticAnalyzer.java:13029)\r\n    at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:13159)\r\n    at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:481)\r\n    at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:360)\r\n    at org.apache.hadoop.hive.ql.Compiler.analyze(Compiler.java:224)\r\n    at org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:109)\r\n    at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:498)\r\n    at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:450)\r\n    at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:414)\r\n    at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:408)\r\n    at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)\r\n    at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:234)\r\n    at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)\r\n    at org.apache.hadoop.hive.cli.CliDriver.processCmd1(CliDriver.java:203)\r\n    at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:129)\r\n    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:427)\r\n    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:358)\r\n    at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:746)\r\n    at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:716)\r\n    at org.apache.hadoop.hive.cli.control.CoreCliDriver.runTest(CoreCliDriver.java:115)\r\n    at org.apache.hadoop.hive.cli.control.CliAdapter.runTest(CliAdapter.java:139)\r\n    at org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver(TestMiniLlapLocalCliDriver.java:62)\r\n    at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n    at java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)\r\n    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)\r\n    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n    at org.apache.hadoop.hive.cli.control.CliAdapter$2$1.evaluate(CliAdapter.java:118)\r\n    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)\r\n    at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)\r\n    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)\r\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)\r\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)\r\n    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)\r\n    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)\r\n    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)\r\n    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)\r\n    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)\r\n    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)\r\n    at org.junit.runners.Suite.runChild(Suite.java:128)\r\n    at org.junit.runners.Suite.runChild(Suite.java:27)\r\n    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)\r\n    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)\r\n    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)\r\n    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)\r\n    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)\r\n    at org.apache.hadoop.hive.cli.control.CliAdapter$1$1.evaluate(CliAdapter.java:89)\r\n    at org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)\r\n    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)\r\n    at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:316)\r\n    at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:240)\r\n    at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:214)\r\n    at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:155)\r\n    at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:385)\r\n    at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)\r\n    at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:507)\r\n    at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:495)\r\nCaused by: org.apache.hadoop.hive.ql.parse.ParseException: line 1:66 cannot recognize input near ')' 'v0' '<EOF>' in subquery source\r\n    at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:125)\r\n    at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:101)\r\n    at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.replaceViewReferenceWithDefinition(SemanticAnalyzer.java:2879)\r\n    ... 62 more\r\nCaused by: NoViableAltException(431@[])\r\n    at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.identifier(HiveParser_IdentifiersParser.java:15227)\r\n    at org.apache.hadoop.hive.ql.parse.HiveParser.identifier(HiveParser.java:43054)\r\n    at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.subQuerySource(HiveParser_FromClauseParser.java:5422)\r\n    at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.atomjoinSource(HiveParser_FromClauseParser.java:1932)\r\n    at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.joinSource(HiveParser_FromClauseParser.java:2186)\r\n    at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.atomjoinSource(HiveParser_FromClauseParser.java:2121)\r\n    at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.joinSource(HiveParser_FromClauseParser.java:2186)\r\n    at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.fromSource(HiveParser_FromClauseParser.java:1761)\r\n    at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.fromClause(HiveParser_FromClauseParser.java:1604)\r\n    at org.apache.hadoop.hive.ql.parse.HiveParser.fromClause(HiveParser.java:43312)\r\n    at org.apache.hadoop.hive.ql.parse.HiveParser.atomSelectStatement(HiveParser.java:36792)\r\n    at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:37085)\r\n    at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:36678)\r\n    at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:35940)\r\n    at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:35828)\r\n    at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:2776)\r\n    at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1655)\r\n    at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:123)\r\n    ... 64 more{noformat}", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13629328/comment/18021586", "id": "18021586", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=Dayakar", "name": "Dayakar", "key": "dayakar", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Dayakar M", "active": true, "timeZone": "Asia/Kolkata"}, "body": "Already an issue HIVE-26493 open for the same so its a duplicate issue.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=Dayakar", "name": "Dayakar", "key": "dayakar", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Dayakar M", "active": true, "timeZone": "Asia/Kolkata"}, "created": "2025-09-20T17:53:08.863+0000", "updated": "2025-09-20T17:53:08.863+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-09-20T17:53:08.000+0000", "created": "2025-09-18T13:24:40.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13629296", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13629296", "key": "HIVE-29213", "fields": {"summary": "HS2 report 'Failed to get primary keys' if using old beeline client", "description": "HIVE-19996 fixed a performance issue related to beeline. However, this fix causes a warning exception \"Failed to get primary keys\" to appear in the HS2 logs when using a beeline client that does not include this patch (such as Hive 4.1.0) to submit SQL queries to Hive 4.2.0.\r\n\r\n\u00a0\r\n\r\nHere is a simple test procedure:\r\n # Use the Hive 4.1.0 beeline client to connect to the latest version of Hive 4.2.0 (which includes HIVE-19996).\r\n\r\n// Create a test table\r\ncreate table testdb.test12(id int);\r\n\r\n{color:#de350b}*// Query this table, and you will notice the following exception: Failed to get primary keys ..*{color}\r\nselect * from testdb.test12;\r\n{code:java}\r\n2025-09-18T16:22:34,121 ERROR [HiveServer2-Handler-Pool: Thread-166] thrift.ThriftCLIService: Failed to get primary keys [request: TGetPrimaryKeysReq(sessionHandle:TSessionHandle(sessionId:THandleIdentifier(guid:56 22 37 10 00 2D 4B BB B6 FE 42 49 E2 25 75 1F, secret:C7 E6 BF 96 D1 82 4E F5 82 36 AC A7 A1 3E 7A A6)), catalogName:, tableName:test12)]\r\norg.apache.hive.service.cli.HiveSQLException: org.apache.thrift.protocol.TProtocolException: Required field 'db_name' is unset! Struct:PrimaryKeysRequest(db_name:null, tbl_name:test12, catName:hive)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.operation.GetPrimaryKeysOperation.runInternal(GetPrimaryKeysOperation.java:120) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.operation.Operation.run(Operation.java:286) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.session.HiveSessionImpl.getPrimaryKeys(HiveSessionImpl.java:998) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) ~[?:?]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.lang.reflect.Method.invoke(Method.java:580) ~[?:?]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.security.AccessController.doPrivileged(AccessController.java:714) ~[?:?]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/javax.security.auth.Subject.doAs(Subject.java:525) ~[?:?]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953) ~[hadoop-common-3.4.1.jar:?]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at jdk.proxy2/jdk.proxy2.$Proxy41.getPrimaryKeys(Unknown Source) ~[?:?]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.CLIService.getPrimaryKeys(CLIService.java:416) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.thrift.ThriftCLIService.GetPrimaryKeys(ThriftCLIService.java:919) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetPrimaryKeys.getResult(TCLIService.java:1870) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetPrimaryKeys.getResult(TCLIService.java:1850) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:250) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.lang.Thread.run(Thread.java:1583) [?:?]\r\nCaused by: org.apache.thrift.protocol.TProtocolException: Required field 'db_name' is unset! Struct:PrimaryKeysRequest(db_name:null, tbl_name:test12, catName:hive)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.api.PrimaryKeysRequest.validate(PrimaryKeysRequest.java:591) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_primary_keys_args.validate(ThriftHiveMetastore.java) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_primary_keys_args$get_primary_keys_argsStandardScheme.write(ThriftHiveMetastore.java) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_primary_keys_args$get_primary_keys_argsStandardScheme.write(ThriftHiveMetastore.java) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_primary_keys_args.write(ThriftHiveMetastore.java) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:71) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:62) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.send_get_primary_keys(ThriftHiveMetastore.java:4926) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_primary_keys(ThriftHiveMetastore.java:4918) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.client.ThriftHiveMetaStoreClient.getPrimaryKeys(ThriftHiveMetaStoreClient.java:2393) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.client.MetaStoreClientWrapper.getPrimaryKeys(MetaStoreClientWrapper.java:1081) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] {code}\r\n*{color:#de350b}// show\u00a0 tables\uff0c then you will find new exceptions\uff1aclient.ThriftHiveMetaStoreClient: Got error flushing the cache org.apache.thrift.TApplicationException: Unrecognized type -128{color}*\r\n\r\n\r\nshow tables;\r\n{code:java}\r\n2025-09-18T16:22:39,771 \u00a0INFO [56223710-002d-4bbb-b6fe-4249e225751f HiveServer2-Handler-Pool: Thread-166] ql.Driver: Compiling command(queryId=hive_20250918162239_69ae0c6e-309f-4b87-a2c4-8093ae4700e8): show tables\r\n2025-09-18T16:22:39,771 \u00a0WARN [56223710-002d-4bbb-b6fe-4249e225751f HiveServer2-Handler-Pool: Thread-166] client.ThriftHiveMetaStoreClient: Got error flushing the cache\r\norg.apache.thrift.TApplicationException: Unrecognized type -128\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:79) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_flushCache(ThriftHiveMetastore.java:7493) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.flushCache(ThriftHiveMetastore.java:7481) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.client.ThriftHiveMetaStoreClient.flushCache(ThriftHiveMetaStoreClient.java:2541) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.client.MetaStoreClientWrapper.flushCache(MetaStoreClientWrapper.java:1043) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.client.MetaStoreClientWrapper.flushCache(MetaStoreClientWrapper.java:1043) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.client.MetaStoreClientWrapper.flushCache(MetaStoreClientWrapper.java:1043) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) ~[?:?]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.lang.reflect.Method.invoke(Method.java:580) ~[?:?]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.client.SynchronizedMetaStoreClient$SynchronizedHandler.invoke(SynchronizedMetaStoreClient.java:69) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at jdk.proxy2/jdk.proxy2.$Proxy32.flushCache(Unknown Source) ~[?:?]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.client.MetaStoreClientWrapper.flushCache(MetaStoreClientWrapper.java:1043) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) ~[?:?]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.lang.reflect.Method.invoke(Method.java:580) ~[?:?]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:232) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at jdk.proxy2/jdk.proxy2.$Proxy32.flushCache(Unknown Source) ~[?:?]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Compiler.analyze(Compiler.java:198) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:109) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:498) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:450) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:414) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:408) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:205) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:268) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.operation.Operation.run(Operation.java:286) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:558) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:543) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) ~[?:?]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.lang.reflect.Method.invoke(Method.java:580) ~[?:?]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.security.AccessController.doPrivileged(AccessController.java:714) ~[?:?]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/javax.security.auth.Subject.doAs(Subject.java:525) ~[?:?]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953) ~[hadoop-common-3.4.1.jar:?]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at jdk.proxy2/jdk.proxy2.$Proxy41.executeStatementAsync(Unknown Source) ~[?:?]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:315) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:652) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1670) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1650) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:250) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.lang.Thread.run(Thread.java:1583) [?:?]\r\n2025-09-18T16:22:39,773 \u00a0WARN [56223710-002d-4bbb-b6fe-4249e225751f HiveServer2-Handler-Pool: Thread-166] metastore.RetryingMetaStoreClient: MetaStoreClient lost connection. Attempting to reconnect (1 of 1) after 1s. getDatabase\r\norg.apache.thrift.transport.TTransportException: Socket is closed by peer.\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:184) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.thrift.transport.TTransport.readAll(TTransport.java:109) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:464) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:362) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:245) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_database_req(ThriftHiveMetastore.java:1497) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_database_req(ThriftHiveMetastore.java:1484) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.client.ThriftHiveMetaStoreClient.getDatabase(ThriftHiveMetaStoreClient.java:1979) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.client.MetaStoreClientWrapper.getDatabase(MetaStoreClientWrapper.java:211) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getDatabase(SessionHiveMetaStoreClient.java:2281) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.metastore.client.MetaStoreClientWrapper.getDatabase(MetaStoreClientWrapper.java:211) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT]\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) ~[?:?]\r\n {code}\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n{color:#172b4d}HIVE-19996 may have broken the backward compatibility of beeline. Accessing Hive 4.2 through the Hive 4.1 beeline client should theoretically not present compatibility issues. We need to ensure backward compatibility as much as possible to provide users with a smooth experience.{color}", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13629296/comment/18032804", "id": "18032804", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "[~zhangbutao] is that a backward incompatibility issue? older beeline clients are unable to perform any SELECT operations?", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-24T15:36:00.846+0000", "updated": "2025-10-24T15:47:24.932+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13629296/comment/18032805", "id": "18032805", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "cc [~InvisibleProgrammer]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-24T15:39:01.558+0000", "updated": "2025-10-24T15:39:01.558+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13629296/comment/18032906", "id": "18032906", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "body": "{code:java}\r\nis that a backward incompatibility issue? older beeline clients are unable to perform any SELECT operations?{code}\r\n\u00a0\r\n\r\n[~dkuzmenko] I think this is a backward incompatibility issue. \u00a0For older beeline clients(4.1.0), while they can perform SELECT query operations, the query latency is significantly higher compared to new beeline clients(4.2.0). For example, when executing a query like `SELECT * FROM table` using `FETCH TASK` (with `set hive.fetch.task.conversion=more`), the old client takes over 1 second to return results, and error messages appear in the HS2 logs. In contrast, the new client only requires 0.1 seconds to return results. When using the old client, this creates a misconception for users that Hive is experiencing performance issues.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-25T02:45:42.521+0000", "updated": "2025-10-25T02:45:42.521+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13629296/comment/18033184", "id": "18033184", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "body": "Regarding the two exceptions reported above, is it confirmed that both are caused by HIVE-19996? Does reverting HIVE-19996 make all errors go away?\r\n\r\nGenerally mixing older vs newer binaries is not something really recommended and its not officially supported either. To exaggerate a bit someone may come and say that Beeline 0.11.0 cannot connect to HS2 4.2.0 but I don't think we would consider this a big deal. In a perfect world, everything should be backward and forward compatible but this is rarely the case in most software releases.\r\n\r\nAnyways, if somebody sees an easy way to fix the issue reported here then we can definitely do it and include it in the release but personally I wouldn't consider this a release blocker.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-10-27T11:09:27.822+0000", "updated": "2025-10-27T11:09:27.822+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13629296/comment/18033210", "id": "18033210", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "Introducing backward incompatibility in MINOR releases doesn\u2019t seem appropriate", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T12:19:30.395+0000", "updated": "2025-10-27T12:19:30.395+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13629296/comment/18033342", "id": "18033342", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=InvisibleProgrammer", "name": "InvisibleProgrammer", "key": "JIRAUSER290460", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Zsolt Miskolczi", "active": true, "timeZone": "Etc/UTC"}, "body": "Hi, I tried to reproduce the issue but couldn't:\u00a0\r\n * Picked up a 4.1.0 beeline client\r\n * Built the current master\r\n * ran minihs2 cluter\r\n * ran the scripts provided (as an extra, created the database testdb)\r\n\r\nGot no error at all. It worked fine, no errors.\u00a0\r\n\r\n\r\n{noformat}\r\nConnecting to jdbc:hive2://localhost:10000/\r\nConnected to: Apache Hive (version 4.2.0-SNAPSHOT)\r\nDriver: Hive JDBC (version 4.1.0)\r\nTransaction isolation: TRANSACTION_REPEATABLE_READ\r\nBeeline version 4.1.0 by Apache Hive{noformat}", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=InvisibleProgrammer", "name": "InvisibleProgrammer", "key": "JIRAUSER290460", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Zsolt Miskolczi", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T21:54:32.604+0000", "updated": "2025-10-27T21:54:32.604+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13629296/comment/18033397", "id": "18033397", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "body": "[~InvisibleProgrammer] I did not use the minihs2 cluster mode; I used the standard local deployment mode, with the backend metadata stored in MySQL. I'm not sure if the deployment differences are causing this. But I have tested many times and this issue persists.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T01:30:34.981+0000", "updated": "2025-10-28T01:30:34.981+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13629296/comment/18033398", "id": "18033398", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "body": "[~zabetak] You make a valid point. It seems we never explicitly promised that older beeline clients would be fully compatible with newer versions of the HS2 cluster. However, I believe that in practice, many users will still use older beeline clients to access newer HS2 versions, as users generally assume that beeline clients offer compatibility guarantees\u2014similar to how MySQL clients can typically connect to different versions of MySQL servers. Therefore, I think we should strive to ensure that beeline clients can access different versions of HS2 as compatibly as possible, in order to provide a better user experience.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T01:39:25.945+0000", "updated": "2025-10-28T01:39:25.945+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13629296/comment/18033412", "id": "18033412", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "body": "Sorry, this issue might be related to my mixed use of HMS metadata from versions 4.1.0 and 4.2.0. Since I assumed that no new metadata fields were added in the recent 4.2.0 release, I deployed both 4.1.0 and 4.2.0 clusters sharing a single metadata database (version 4.2.0). This might have caused some anomalies. Therefore, you folks can temporarily set this issue aside and i will continue investigating my local metadata configuration. If it\u2019s confirmed to be solely due to the mixed use of metadata, I will update and close this ticket.\r\n\r\nAdditionally, this ticket does not block the release of version 4.2.0, so you can skip this issue for now. Thank you.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T03:29:47.854+0000", "updated": "2025-10-28T03:29:47.854+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13629296/comment/18033761", "id": "18033761", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "body": "I have deployed Hive 4.0.1, 4.1.0, and 4.2.0 locally, each using its own HMS metadata to avoid issues caused by mixed metadata usage. After debugging the beeline/jdbc modules of these three versions, I finally identified the root cause of this issue.\r\n\r\nPrior to version 4.1.0, the *Rows::isPrimaryKey*\u00a0method would call `{*}HiveResultSetMetaData::getTableName{*}` to retrieve the table name: \u00a0\r\n[https://github.com/apache/hive/blob/2d1405e7feed176aeed337581292b8438cf13326/beeline/src/java/org/apache/hive/beeline/Rows.java#L86]\u00a0\r\n\r\nHowever, before version 4.1.0, the `HiveResultSetMetaData::getTableName` method was not implemented: \u00a0\r\n[https://github.com/apache/hive/blob/2d1405e7feed176aeed337581292b8438cf13326/jdbc/src/java/org/apache/hive/jdbc/HiveResultSetMetaData.java#L102]\u00a0\r\n\r\nAs a result, the `{*}Rows::isPrimaryKey{*}` method would catch an exception and terminate, without sending a Thrift RPC request to call `{*}HiveDatabaseMetaData::getPrimaryKeys(String catalog, String schema, String table)`{*} to retrieve primary key information related to the table. In other words, before version 4.1.0, the beeline client never actually executed the `HiveDatabaseMetaData::getPrimaryKeys` method. \u00a0\r\n{color:#de350b}It is worth noting that HIVE-19996\u00a0 mentions \"Beeline performance poor with drivers having slow DatabaseMetaData.getPrimaryKeys impl.\" I believe this issue should not have existed before version 4.1.0, as I mentioned above{*},{*} the beeline client never actually executed the `HiveDatabaseMetaData::getPrimaryKeys` method.{color}\r\n\r\nIn version 4.1.0, HIVE-27887 implemented `{*}HiveResultSetMetaData::getTableName{*}`: \u00a0\r\n[https://github.com/apache/hive/blob/75e40b7537c91a70ccaa31c397d21823c7528eeb/jdbc/src/java/org/apache/hive/jdbc/HiveResultSetMetaData.java#L113]\u00a0\r\n\r\nThus, the `{*}Rows::isPrimaryKey{*}` method executed normally and sent a Thrift RPC request to call `{*}HiveDatabaseMetaData::getPrimaryKeys(String catalog, String schema, String table){*}` to retrieve primary key information related to the table: \u00a0\r\n[https://github.com/apache/hive/blob/75e40b7537c91a70ccaa31c397d21823c7528eeb/beeline/src/java/org/apache/hive/beeline/Rows.java#L92]\u00a0\r\n\r\nHowever, in the Beeline module, there is currently no way to retrieve the catalog and schema. The catalog is an empty string, and the schema (i.e., db_name) is null. A null schema (db_name) causes the RPC method call to throw an exception because the Thrift definition prohibits db_name from being null: \u00a0\r\n[https://github.com/apache/hive/blob/75e40b7537c91a70ccaa31c397d21823c7528eeb/standalone-metastore/metastore-common/src/main/thrift/hive_metastore.thrift#L754]\u00a0\r\n\r\nThe error message is as follows: \u00a0\r\n{code:java}\r\n2025-09-18T16:22:34,121 ERROR [HiveServer2-Handler-Pool: Thread-166] thrift.ThriftCLIService: Failed to get primary keys [request: TGetPrimaryKeysReq(sessionHandle:TSessionHandle(sessionId:THandleIdentifier(guid:56 22 37 10 00 2D 4B BB B6 FE 42 49 E2 25 75 1F, secret:C7 E6 BF 96 D1 82 4E F5 82 36 AC A7 A1 3E 7A A6)), catalogName:, tableName:test12)]\r\norg.apache.hive.service.cli.HiveSQLException: org.apache.thrift.protocol.TProtocolException: Required field 'db_name' is unset! Struct:PrimaryKeysRequest(db_name:null, tbl_name:test12, catName:hive){code}\r\n*Therefore, in version 4.1.0, HIVE-27887 inadvertently triggered the execution of `HiveDatabaseMetaData::getPrimaryKeys`, leading to this exception.*\r\n\r\n\u00a0\r\n\r\nThen, in version 4.2.0, HIVE-29118 optimized the method call of `{*}Rows::isPrimaryKey{*}`, ensuring that `{*}Rows::isPrimaryKey{*}` and `{*}HiveDatabaseMetaData::getPrimaryKeys{*}` are only triggered when the `--color` option is specified, i.e.: \u00a0\r\n{code:java}\r\n./apache-hive-4.2.0-bin/bin/beeline -u jdbc:hive2://127.0.0.1:10000/default hive --color=true{code}\r\n{color:#de350b}*Thus, in version 4.2.0, if you use `beeline --color` to execute a SELECT query, the same exception will occur.*{color}\r\n\r\n\u00a0\r\n\r\nIn summary, before version 4.1.0 (HIVE-27887), we never actually retrieved the primary key of a table on the beeline side. After version 4.1.0 (HIVE-27887), since we cannot retrieve the actual db_name and catalog on the beeline side, the call to `{*}HiveDatabaseMetaData::getPrimaryKeys(String catalog, String schema, String table){*}` throws an exception. I believe retrieving the primary key of a table on the beeline side is meaningless (the original purpose was only for coloring the beeline output). {color:#de350b}*We do not need to investigate how to retrieve the correct db_name to ensure the method executes properly. I believe removing this code logic would suffice.*{color}", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-29T08:04:41.195+0000", "updated": "2025-10-29T08:04:41.195+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13629296/comment/18033798", "id": "18033798", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "body": "Let me continue with some additional points. I just noticed that it's not only Hive that uses the Beeline client; other components like Impala and HBase also utilize the Beeline component. Therefore, the relevant primary key code in Beeline\u2014specifically `DatabaseMetaData.getPrimaryKeys`\u2014though never used by Hive prior to Hive 4.1.0, might have been consistently used by HBase. This could very well be the source of the issue reported in HIVE-19996.\r\n\r\nThis makes it quite interesting: the Beeline codes related to primary key in Hive that has never been utilized by Hive itself but is actively used by other components.\r\n\r\nNevertheless, I still recommend removing the usage of `DatabaseMetaData.getPrimaryKeys` in Beeline. First, fixing this exception on the Hive side is neither easy nor meaningful;\u00a0 Second, as mentioned above, the purpose of retrieving primary keys in Beeline is solely for coloring the output results. Removing this code would not cause any compatibility issues and would actually improve Beeline's performance, as it avoids the heavyweight Thrift RPC call to the server for fetching primary keys.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-29T11:10:31.303+0000", "updated": "2025-10-29T11:10:31.303+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13629296/comment/18034078", "id": "18034078", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "body": "Created a new ticket HIVE-29296 to remove the getPrimaryKeys codes from beeline module", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zhangbutao", "name": "zhangbutao", "key": "zhangbutao", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10432", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"}, "displayName": "Butao Zhang", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-30T02:32:30.512+0000", "updated": "2025-10-30T02:32:30.512+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13629296/comment/18034368", "id": "18034368", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=InvisibleProgrammer", "name": "InvisibleProgrammer", "key": "JIRAUSER290460", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Zsolt Miskolczi", "active": true, "timeZone": "Etc/UTC"}, "body": "Hi,\u00a0\r\n\r\n\u00a0\r\n\r\nThank you for bringing up this issue.\u00a0\r\n\r\nFirstly, I totally agree with removing that functionality from Beeline entirely. I suggested the same when we met with the performance problem here: [https://lists.apache.org/thread/mk7b51bwcd53k5jcdsyyyvbrogsqm2bw]\r\n\r\nI hope https://issues.apache.org/jira/browse/HIVE-29296 will pass through the review.\u00a0\r\n\r\n\u00a0\r\n\r\nSecondly, I'm not arguing, just want to get a clear picture of the requirements of backward compatibility: If I understood the situation correctly, a change made just today, can break up the functionality with a version delivered yesterday. In software development, it is pretty standard. I know Beeline can be a special case, as it could be a standalone software, delivered independently from Hive. My question is: do we have some boundaries about how many releases Beeline should support backward?\u00a0\r\n\r\n\u00a0\r\n\r\nOther question: what is your opinion, does it make sense to detach Beeline on the repository level from Hive? If we don't only use it for Hive, and a given Beeline release has to support multiple versions of Hive, HBase, Impala, etc, maybe it is easier to do development and testing if it can be compiled without Hive.\u00a0", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=InvisibleProgrammer", "name": "InvisibleProgrammer", "key": "JIRAUSER290460", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Zsolt Miskolczi", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-31T10:24:59.066+0000", "updated": "2025-10-31T10:24:59.066+0000"}], "maxResults": 13, "total": 13, "startAt": 0}, "updated": "2025-10-31T10:24:59.000+0000", "created": "2025-09-18T08:58:24.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13629248", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13629248", "key": "HIVE-29211", "fields": {"summary": " Add LDAP group filtering support for Kerberos-authenticated users", "description": "Currently, HS2 and HMS support LDAP authentication with group filtering, but when users authenticate via Kerberos, LDAP group filters are not applied. This creates an inconsistency where authorization policies differ based on the authentication method used. We need to add the capability to optionally enforce LDAP group membership checks for Kerberos-authenticated users in both HS2 and HMS.", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13629248/comment/18023190", "id": "18023190", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=hemanth619", "name": "hemanth619", "key": "hemanth619", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=hemanth619&avatarId=48445", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hemanth619&avatarId=48445", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hemanth619&avatarId=48445", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hemanth619&avatarId=48445"}, "displayName": "Sai Hemanth Gantasala", "active": true, "timeZone": "America/Los_Angeles"}, "body": "[~hazeljiang] - Thanks for the contribution. The patch has been merged into the master branch.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=hemanth619", "name": "hemanth619", "key": "hemanth619", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=hemanth619&avatarId=48445", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hemanth619&avatarId=48445", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hemanth619&avatarId=48445", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hemanth619&avatarId=48445"}, "displayName": "Sai Hemanth Gantasala", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-09-26T19:15:34.791+0000", "updated": "2025-09-26T19:15:34.791+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-09-26T19:15:41.000+0000", "created": "2025-09-17T20:08:23.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13629232", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13629232", "key": "HIVE-29210", "fields": {"summary": "Minor compaction produces duplicates conditionally in case of HMS instance running initiator crash", "description": "In a case, with multiple HiveServer2 (HS2) instances, one of the HS2 instances may run on the same host as the Hive Metastore (HMS). In this setup, the initiator runs within HMS, while the compaction worker threads run within HS2.\r\n\r\nIf the HMS instance unexpectedly crashes, the method revokeFromLocalWorkers() is invoked. This method resets all compaction jobs back to the initiated state, provided they were running on the same host. We believe this behavior is by design: if both HMS and HS2(running workers) were to crash simultaneously, and jobs were not reset, those compactions could remain stalled until revokeTimedoutWorkers() eventually reclaims them.\r\n\r\nHowever, in the case where HMS crashes but the HS2 instance survives, the reset still occurs. As a result, the job is made available for reassignment even though the original HS2 worker is still actively processing it. This can lead to a scenario where another HS2 worker picks up the same compaction task, causing two workers to run the same minor compaction job concurrently.\r\n\r\nThis race condition can intermittently result in duplicate records being written to the table.", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13629232/comment/18028733", "id": "18028733", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=kuczoram", "name": "kuczoram", "key": "kuczoram", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Marta Kuczora", "active": true, "timeZone": "Etc/UTC"}, "body": "Thanks a lot [~tanishqchugh] for the fix.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=kuczoram", "name": "kuczoram", "key": "kuczoram", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Marta Kuczora", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-09T11:46:34.406+0000", "updated": "2025-10-09T11:46:34.406+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-10-09T11:46:34.000+0000", "created": "2025-09-17T17:13:26.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13629231", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13629231", "key": "HIVE-29209", "fields": {"summary": "Remove unnecessary usage of LoginException", "description": "{{LoginException}} is not necessary in many classes, we can clean it.", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13629231/comment/18023325", "id": "18023325", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn", "name": "ayushtkn", "key": "ayushtkn", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Ayush Saxena", "active": true, "timeZone": "Asia/Kolkata"}, "body": "Committed to master.\r\n\r\nThanx [~wechar] for the contribution & [~InvisibleProgrammer] for the review!!!", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn", "name": "ayushtkn", "key": "ayushtkn", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Ayush Saxena", "active": true, "timeZone": "Asia/Kolkata"}, "created": "2025-09-27T16:55:51.065+0000", "updated": "2025-09-27T16:57:48.990+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-09-27T16:57:48.000+0000", "created": "2025-09-17T17:10:33.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13629205", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13629205", "key": "HIVE-29208", "fields": {"summary": "Infinite loop while compiling query with filter predicate containing disjuncts on the same expression", "description": "Repro\r\n{code}\r\nCREATE TABLE t1 (Date_ STRING);\r\n\r\nSELECT * FROM t1\r\nWHERE ( (\r\n         MINUTE(`date_`) = 2 OR\r\n         MINUTE(`date_`) = 10\r\n        )\r\n        OR (MINUTE(`date_`) IS NULL)\r\n      );\r\n{code}\r\n\r\nThe expression {{MINUTE(`date_`)}} is compared in each disjunct hence search operator is not used\r\n\r\n\r\n", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13629205/comment/18022404", "id": "18022404", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=krisztiankasa", "name": "krisztiankasa", "key": "krisztiankasa", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Krisztian Kasa", "active": true, "timeZone": "Etc/UTC"}, "body": "Merged to master. Thanks [~zabetak], [~dkuzmenko], [~Dayakar] for the review.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=krisztiankasa", "name": "krisztiankasa", "key": "krisztiankasa", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Krisztian Kasa", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-24T11:36:17.078+0000", "updated": "2025-09-24T11:36:17.078+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-09-26T09:16:16.000+0000", "created": "2025-09-17T13:16:43.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13629133", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13629133", "key": "HIVE-29207", "fields": {"summary": "Remove check-spelling CI action", "description": "The check-spelling action reports many false positives that require additional work and extra commits to address. Addressing the errors requires additional work from contributors and extra resources from CI since all tests are triggered again on new commits. Occasionally it also detects valid typos but at this stage the negatives outweigh the positives.\r\n\r\n\u00a0", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13629133/comment/18020873", "id": "18020873", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "body": "The removal of the action is discussed on the mailing list:\r\n\r\n[https://lists.apache.org/thread/bb2ncb5ytk50j73fcwzw0wbdsblkw9x3]\r\n\r\n[https://lists.apache.org/thread/n7k808bkxtclww95fhgkznfsol32f0mn]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-09-17T08:26:43.331+0000", "updated": "2025-09-17T08:26:43.331+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13629133/comment/18021064", "id": "18021064", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "body": "Fixed in https://github.com/apache/hive/commit/f374f39ec7205d98ad5605be6890564fc58eea0c", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-09-18T07:16:31.172+0000", "updated": "2025-09-18T07:16:31.172+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-09-18T07:16:31.000+0000", "created": "2025-09-17T08:25:19.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13629104", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13629104", "key": "HIVE-29206", "fields": {"summary": "Remove commons-lang from hive-exec shaded jar", "description": "[HIVE-7145|https://issues.apache.org/jira/browse/HIVE-7145] and [HIVE-22653|https://issues.apache.org/jira/browse/HIVE-22653] removed commons-lang from the dependencies. However, it is being included in hive-exec shaded jar and needs to be removed.", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-09-22T23:57:37.000+0000", "created": "2025-09-17T05:08:26.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13629094", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13629094", "key": "HIVE-29205", "fields": {"summary": "Iceberg: Upgrade iceberg version to 1.10.1", "description": null, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-09-22T17:46:38.000+0000", "created": "2025-09-16T23:44:37.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13629028", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13629028", "key": "HIVE-29204", "fields": {"summary": "Hive-site: cleanup attachments and links to attachments", "description": "Some links to attachments lead to a 404 Not found, e.g. [attachments/40509928/42696874-txt|https://hive.apache.org/attachments/40509928/42696874-txt] in [SQL Standard Based Hive Authorization|https://hive.apache.org/docs/latest/language/sql-standard-based-hive-authorization/#hive-013].\r\n\r\nSome link texts replace the dot with a dash (e.g., content/community/resources/presentations.md). In general, it would be better to use the title of the document instead of numbers as file name and link text.\r\n{code:java}\r\n50:* [attachments/27362054/35193149-pptx](/attachments/27362054/35193149.pptx) (Ashutosh Chauhan){code}\r\nA few shell commands that might be helpful:\r\n{code:java}\r\nfind themes/hive/static/attachments -type f | sed 's#themes/hive/static/##' | sort -u > available-attachments.txt\r\nrg \"attachments/\" | sed 's#attachments/#\\nattachments/#g;' | grep '^attachments' | sed 's/\\([?\"<> )]\\|\\]\\).*//' | sort -u > needed-attachments.txt\r\n{code}\r\nThere are also some duplicate files:\r\n{code:java}\r\n$ cat available-attachments.txt| sed 's#^#themes/hive/static/#' | xargs md5sum | sort\r\n...\r\nf9f26fe37b0c5276d0b63f98e1188324  themes/hive/static/attachments/27362075/34177489.pdf\r\nf9f26fe37b0c5276d0b63f98e1188324  themes/hive/static/attachments/27362075/34177517.pdf\r\nf9f26fe37b0c5276d0b63f98e1188324  themes/hive/static/attachments/27362075/35193010.pdf\r\nf9f26fe37b0c5276d0b63f98e1188324  themes/hive/static/attachments/27362075/35193011.pdf\r\n...\r\n{code}", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-09-16T10:57:14.000+0000", "created": "2025-09-16T10:57:14.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13629016", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13629016", "key": "HIVE-29203", "fields": {"summary": "get_aggr_stats_for doesn't aggregate stats when direct sql batch retrieve is enabled", "description": "In case of metastore.direct.sql.batch.size > 0, and number of partition names or columns in get_aggr_stats_for is bigger than the metastore.direct.sql.batch.size, then the\r\nAggrStats from the call get_aggr_stats_for might have un-merged stats for the same column, so the aggregated stats is not correct, which may make CBO generate an outdated execution plan.", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13629016/comment/18020604", "id": "18020604", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dengzh", "name": "dengzh", "key": "dengzh", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Zhihua Deng", "active": true, "timeZone": "Etc/UTC"}, "body": "If we remove the batch processing from [https://github.com/apache/hive/blob/4bb08099d91acbefee73a449a36abb1ecd2b5925/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java#L1882-L1891,] perhaps we need to consider the memory usage when enableBitVector || enableKll, and the\u00a0\r\nrestrictions on IN list size in aggrStatsUseDB.", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dengzh", "name": "dengzh", "key": "dengzh", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Zhihua Deng", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-16T09:16:50.458+0000", "updated": "2025-09-16T09:16:50.458+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13629016/comment/18033347", "id": "18033347", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "body": "[~ramitg254] please set the affected version", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dkuzmenko", "name": "dkuzmenko", "key": "dkuzmenko", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Denys Kuzmenko", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T22:17:45.014+0000", "updated": "2025-10-27T22:17:45.014+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-10-28T04:47:37.000+0000", "created": "2025-09-16T08:28:59.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13628986", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13628986", "key": "HIVE-29202", "fields": {"summary": "Add HiveAuthzContext support to HiveMetaStoreAuthorizableEvent for enhanced authorization", "description": "Currently, HiveMetaStoreAuthorizableEvent and its implementations (like ReadDatabaseEvent) don't pass through the full HiveAuthzContext when creating HiveMetaStoreAuthzInfo. This limits the ability of custom authorization plugins to access contextual information during authorization decisions.", "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "updated": "2025-09-26T22:16:39.000+0000", "created": "2025-09-15T20:39:19.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13628948", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13628948", "key": "HIVE-29201", "fields": {"summary": "Fix flaky test query_iceberg_metadata_of_unpartitioned_table.q", "description": "The test in itself uses SORT_QUERY_RESULTS to keep a deterministic ordering of queries. But there is still scope of non determinism, as SORT_QUERY_RESULTS sorts the output of each query lexicographically, and if the present masked values change, then the output ordering changes as well. Hence, we need to add explicit order by on queries.", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13628948/comment/18020367", "id": "18020367", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=araika", "name": "araika", "key": "JIRAUSER301024", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=39931", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=39931", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=39931", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=39931"}, "displayName": "Araika Singh", "active": true, "timeZone": "Etc/UTC"}, "body": "PR: https://github.com/apache/hive/pull/6075", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=araika", "name": "araika", "key": "JIRAUSER301024", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=39931", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=39931", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=39931", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=39931"}, "displayName": "Araika Singh", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-15T13:55:35.090+0000", "updated": "2025-09-15T13:55:35.090+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13628948/comment/18023055", "id": "18023055", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "body": "The flakiness has been occurring for months affecting builds in master and PRs:\r\n* https://ci.hive.apache.org/job/hive-precommit/job/master/2565/ run on Jun 19, 2025\r\n* https://ci.hive.apache.org/job/hive-precommit/job/PR-6092/1/ run on Sep 22, 2025", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-09-26T09:41:05.043+0000", "updated": "2025-09-26T09:41:05.043+0000"}], "maxResults": 2, "total": 2, "startAt": 0}, "updated": "2025-10-01T20:39:21.000+0000", "created": "2025-09-15T13:51:08.000+0000"}}
{"expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields", "id": "13628945", "self": "https://issues.apache.org/jira/rest/api/latest/issue/13628945", "key": "HIVE-29200", "fields": {"summary": "CI Spell checking errors in ObjectInspectorUtils and TeradataBinarySerde", "description": "The CI spell checking action [currently fails|https://github.com/apache/hive/actions/runs/17731661136/job/50383947095] in master due to the following errors\r\n{noformat}\r\nWarning: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java: line 659, columns 70-83, Warning - `bucketedtables` is not a recognized word. (unrecognized-spelling)\r\nWarning: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java: line 659, columns 51-64, Warning - `languagemanual` is not a recognized word. (unrecognized-spelling)\r\nWarning: serde/src/java/org/apache/hadoop/hive/serde2/teradata/TeradataBinarySerde.java: line 80, columns 45-63, Warning - `teradatabinaryserde` is not a recognized word. (unrecognized-spelling)\r\n{noformat}\r\n", "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13628945/comment/18020615", "id": "18020615", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "body": "Fixed in https://github.com/apache/hive/commit/38236c704f8229b6a52bdbedd87f51638758d792", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=zabetak", "name": "zabetak", "key": "zabetak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zabetak&avatarId=36449", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zabetak&avatarId=36449", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zabetak&avatarId=36449", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zabetak&avatarId=36449"}, "displayName": "Stamatis Zampetakis", "active": true, "timeZone": "Europe/Paris"}, "created": "2025-09-16T09:36:26.533+0000", "updated": "2025-09-16T09:36:26.533+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "updated": "2025-09-16T09:36:26.000+0000", "created": "2025-09-15T13:17:10.000+0000"}}
