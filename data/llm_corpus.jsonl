{"id": "13632383", "key": "HADOOP-19736", "project": "HADOOP", "summary": "ABFS: Support for new auth type: User-bound SAS", "description": "Adding support for new authentication type: user bound SAS", "comments": "manika137 opened a new pull request, #8051: URL: https://github.com/apache/hadoop/pull/8051 ### Description of PR Adding support for new authentication type: user bound SAS ### How was this patch tested? Test suite will be run for the patch hadoop-yetus commented on PR #8051: URL: https://github.com/apache/hadoop/pull/8051#issuecomment-3442714485 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 20m 52s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 34m 38s | | trunk passed | | +1 :green_heart: | compile | 0m 44s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 45s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 50s | | trunk passed | | +1 :green_heart: | javadoc | 0m 45s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 37s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 1m 23s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) | hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 26m 16s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 34s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | compile | 0m 35s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 35s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 35s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 35s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04. | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/blanks-eol.txt) | The patch has 3 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 0m 22s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 36 new + 4 unchanged - 0 fixed = 40 total (was 4) | | -1 :x: | mvnsite | 0m 37s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | javadoc | 0m 32s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 2 new + 1472 unchanged - 0 fixed = 1474 total (was 1472) | | -1 :x: | javadoc | 0m 30s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 2 new + 1413 unchanged - 0 fixed = 1415 total (was 1413) | | -1 :x: | spotbugs | 0m 34s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | +1 :green_heart: | shadedclient | 29m 4s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 41s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | +1 :green_heart: | asflicense | 0m 34s | | The patch does not generate ASF License warnings. | | | | 119m 57s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8051 | | JIRA Issue | HADOOP-19736 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 88abe69cd96c 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 05b52e40f1bc99edc039fc0d2ab5f83d1ceb0da9 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/testReport/ | | Max. process+thread count | 779 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #8051: URL: https://github.com/apache/hadoop/pull/8051#issuecomment-3454395745 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 4m 52s | | Docker failed to build run-specific yetus/hadoop:tp-4947}. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/8051 | | JIRA Issue | HADOOP-19736 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/2/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #8051: URL: https://github.com/apache/hadoop/pull/8051#issuecomment-3454455082 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 6m 57s | | Docker failed to build run-specific yetus/hadoop:tp-29291}. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/8051 | | JIRA Issue | HADOOP-19736 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/3/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #8051: URL: https://github.com/apache/hadoop/pull/8051#issuecomment-3454654532 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 6m 5s | | Docker failed to build run-specific yetus/hadoop:tp-27223}. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/8051 | | JIRA Issue | HADOOP-19736 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/4/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. bhattmanish98 commented on code in PR #8051: URL: https://github.com/apache/hadoop/pull/8051#discussion_r2472593921 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientHandler.java: ########## @@ -80,6 +80,25 @@ public AbfsClientHandler(final URL baseUrl, abfsClientContext); } + public AbfsClientHandler(final URL baseUrl, Review Comment: Java doc missing for the constructor ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/AbfsHttpConstants.java: ########## @@ -187,7 +187,8 @@ public enum ApiVersion { DEC_12_2019(\"2019-12-12\"), APR_10_2021(\"2021-04-10\"), AUG_03_2023(\"2023-08-03\"), - NOV_04_2024(\"2024-11-04\"); + NOV_04_2024(\"2024-11-04\"), + JULY_05_2025(\"2025-07-05\"); Review Comment: We should follow the same format: JUL_05_2025, what do you think? ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java: ########## @@ -174,6 +174,17 @@ public AbfsDfsClient(final URL baseUrl, encryptionContextProvider, abfsClientContext, AbfsServiceType.DFS); } + public AbfsDfsClient(final URL baseUrl, Review Comment: Java doc missing ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -570,6 +570,11 @@ public void signRequest(final AbfsHttpOperation httpOperation, int bytesToSign) // do nothing; the SAS token should already be appended to the query string httpOperation.setMaskForSAS(); //mask sig/oid from url for logs break; + case UserboundSASWithOAuth: + httpOperation.setRequestProperty(HttpHeaderConfigurations.AUTHORIZATION, + client.getAccessToken()); + httpOperation.setMaskForSAS(); //mask sig/oid from url for logs Review Comment: Typo: *sign ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/constants/TestConfigurationKeys.java: ########## @@ -55,6 +55,9 @@ public final class TestConfigurationKeys { public static final String FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_OBJECT_ID = \"fs.azure.test.app.service.principal.object.id\"; + public static final String FS_AZURE_END_USER_TENANT_ID = \"fs.azure.test.end.user.tenant.id\"; Review Comment: Rename the variable to FS_AZURE_TEST_END_USER_TENANT_ID ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java: ########## @@ -1741,7 +1741,15 @@ private void initializeClient(URI uri, String fileSystemName, } else if (authType == AuthType.SAS) { LOG.trace(\"Fetching SAS Token Provider\"); sasTokenProvider = abfsConfiguration.getSASTokenProvider(); - } else { + } else if(authType == AuthType.UserboundSASWithOAuth){ + LOG.trace(\"Fetching SAS and OAuth Token Provider for user bound SAS\"); + AzureADAuthenticator.init(abfsConfiguration); + tokenProvider = abfsConfiguration.getTokenProvider(); + ExtensionHelper.bind(tokenProvider, uri, + abfsConfiguration.getRawConfiguration()); + sasTokenProvider = abfsConfiguration.getSASTokenProviderForUserBoundSAS(); + } Review Comment: else can be started in the same line after }, same format we are using at other places as well. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -363,6 +363,21 @@ public AbfsClient(final URL baseUrl, this.sasTokenProvider = sasTokenProvider; } + public AbfsClient(final URL baseUrl, Review Comment: Java doc missing ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/constants/TestConfigurationKeys.java: ########## @@ -55,6 +55,9 @@ public final class TestConfigurationKeys { public static final String FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_OBJECT_ID = \"fs.azure.test.app.service.principal.object.id\"; + public static final String FS_AZURE_END_USER_TENANT_ID = \"fs.azure.test.end.user.tenant.id\"; + public static final String FS_AZURE_END_USER_OBJECT_ID = \"fs.azure.test.end.user.object.id\"; Review Comment: same as above ########## hadoop-tools/hadoop-azure/src/site/markdown/index.md: ########## @@ -303,6 +303,7 @@ driven by them. 3. Deployed in-Azure with the Azure VMs providing OAuth 2.0 tokens to the application, \"Managed Instance\". 4. Using Shared Access Signature (SAS) tokens provided by a custom implementation of the SASTokenProvider interface. 5. By directly configuring a fixed Shared Access Signature (SAS) token in the account configuration settings files. +6. Using user-bound SAS auth type, which is requires OAuth 2.0 setup (point 2 above) and SAS setup (point 4 above) Review Comment: Grammatical mistake: which requires or which is required? ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AuthType.java: ########## @@ -24,5 +24,6 @@ public enum AuthType { SharedKey, OAuth, Custom, - SAS + SAS, + UserboundSASWithOAuth Review Comment: Format issue: There is an extra space before UserboundSASWithOAuth ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java: ########## @@ -1741,7 +1741,15 @@ private void initializeClient(URI uri, String fileSystemName, } else if (authType == AuthType.SAS) { LOG.trace(\"Fetching SAS Token Provider\"); sasTokenProvider = abfsConfiguration.getSASTokenProvider(); - } else { + } else if(authType == AuthType.UserboundSASWithOAuth){ Review Comment: Missing space between if and ( ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java: ########## @@ -1770,7 +1778,12 @@ private void initializeClient(URI uri, String fileSystemName, } LOG.trace(\"Initializing AbfsClient for {}\", baseUrl); - if (tokenProvider != null) { + if(tokenProvider != null && sasTokenProvider != null){ Review Comment: Space between if and ( ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java: ########## @@ -1770,7 +1778,12 @@ private void initializeClient(URI uri, String fileSystemName, } LOG.trace(\"Initializing AbfsClient for {}\", baseUrl); - if (tokenProvider != null) { + if(tokenProvider != null && sasTokenProvider != null){ + this.clientHandler = new AbfsClientHandler(baseUrl, creds, abfsConfiguration, + tokenProvider, sasTokenProvider, encryptionContextProvider, + populateAbfsClientContext()); + } + else if (tokenProvider != null) { Review Comment: same as above anmolanmol1234 commented on code in PR #8051: URL: https://github.com/apache/hadoop/pull/8051#discussion_r2481295321 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/utils/SASGenerator.java: ########## @@ -41,7 +41,8 @@ public abstract class SASGenerator { public enum AuthenticationVersion { Nov18(\"2018-11-09\"), Dec19(\"2019-12-12\"), - Feb20(\"2020-02-10\"); + Feb20(\"2020-02-10\"), + July5(\"2025-07-05\"); Review Comment: Same here should be JUL anmolanmol1234 commented on code in PR #8051: URL: https://github.com/apache/hadoop/pull/8051#discussion_r2481303390 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/utils/DelegationSASGenerator.java: ########## @@ -36,20 +38,26 @@ public class DelegationSASGenerator extends SASGenerator { private final String ske; private final String sks = \"b\"; private final String skv; + private final String skdutid; + private final String sduoid; - public DelegationSASGenerator(byte[] userDelegationKey, String skoid, String sktid, String skt, String ske, String skv) { + public DelegationSASGenerator(byte[] userDelegationKey, String skoid, String sktid, String skt, String ske, String skv, String skdutid, String sduoid) { Review Comment: add javadoc for what do all these params signify anmolanmol1234 commented on code in PR #8051: URL: https://github.com/apache/hadoop/pull/8051#discussion_r2481307275 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/utils/DelegationSASGenerator.java: ########## @@ -117,6 +125,15 @@ public String getDelegationSAS(String accountName, String containerName, String qb.addQuery(\"ske\", ske); qb.addQuery(\"sks\", sks); qb.addQuery(\"skv\", skv); + + //skdutid and sduoid are required for user bound SAS only + if(!Objects.equals(skdutid, EMPTY_STRING)){ Review Comment: spaces after if anmolanmol1234 commented on code in PR #8051: URL: https://github.com/apache/hadoop/pull/8051#discussion_r2481327412 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/utils/DelegationSASGenerator.java: ########## @@ -197,6 +228,7 @@ private String computeSignatureForSAS(String sp, String st, String se, String sv String stringToSign = sb.toString(); LOG.debug(\"Delegation SAS stringToSign: \" + stringToSign.replace(\"\\n\", \".\")); + System.out.println(\"Delegation SAS stringToSign: \" + stringToSign.replace(\"\\n\", \".\")); Review Comment: Remove this anmolanmol1234 commented on code in PR #8051: URL: https://github.com/apache/hadoop/pull/8051#discussion_r2481341233 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/extensions/MockUserBoundSASTokenProvider.java: ########## @@ -0,0 +1,166 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.extensions; + +import java.io.IOException; +import java.net.MalformedURLException; +import java.net.URL; +import java.nio.charset.StandardCharsets; +import java.time.Duration; +import java.time.Instant; +import java.util.ArrayList; +import java.util.List; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants; +import org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidUriException; +import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider; +import org.apache.hadoop.fs.azurebfs.services.AbfsHttpHeader; +import org.apache.hadoop.fs.azurebfs.services.AbfsJdkHttpOperation; +import org.apache.hadoop.fs.azurebfs.utils.Base64; +import org.apache.hadoop.fs.azurebfs.utils.DelegationSASGenerator; +import org.apache.hadoop.fs.azurebfs.utils.SASGenerator; +import org.apache.hadoop.security.AccessControlException; + +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_CONNECTION_TIMEOUT; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_READ_TIMEOUT; + +/** + * A mock user-bound SAS token provider implementation. + */ + +public class MockUserBoundSASTokenProvider implements SASTokenProvider { + + private DelegationSASGenerator generator; + + public static final String TEST_OWNER = \"325f1619-4205-432f-9fce-3fd594325ce5\"; + public static final String CORRELATION_ID = \"66ff4ffc-ff17-417e-a2a9-45db8c5b0b5c\"; + public static final String NO_AGENT_PATH = \"NoAgentPath\"; + + @Override + public void initialize(Configuration configuration, String accountName) throws IOException { + String appID = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_ID); + String appSecret = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SECRET); + String sktid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_TENANT_ID); + String skoid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_OBJECT_ID); + String skt = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().minus(SASGenerator.FIVE_MINUTES)); + String ske = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().plus(SASGenerator.ONE_DAY)); + String skv = SASGenerator.AuthenticationVersion.July5.toString(); + + String skdutid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_TENANT_ID); + String sduoid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_OBJECT_ID); + + byte[] key = getUserDelegationKey(accountName, appID, appSecret, sktid, skt, ske, skv, skdutid); + + generator = new DelegationSASGenerator(key, skoid, sktid, skt, ske, skv, skdutid, sduoid); + } + + // Invokes the AAD v2.0 authentication endpoint with a client credentials grant to get an + // access token. See https://docs.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-client-creds-grant-flow. + private String getAuthorizationHeader(String accountName, String appID, String appSecret, String sktid) throws IOException { Review Comment: include params in javadoc as well anmolanmol1234 commented on code in PR #8051: URL: https://github.com/apache/hadoop/pull/8051#discussion_r2481342577 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/extensions/MockUserBoundSASTokenProvider.java: ########## @@ -0,0 +1,166 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.extensions; + +import java.io.IOException; +import java.net.MalformedURLException; +import java.net.URL; +import java.nio.charset.StandardCharsets; +import java.time.Duration; +import java.time.Instant; +import java.util.ArrayList; +import java.util.List; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants; +import org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidUriException; +import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider; +import org.apache.hadoop.fs.azurebfs.services.AbfsHttpHeader; +import org.apache.hadoop.fs.azurebfs.services.AbfsJdkHttpOperation; +import org.apache.hadoop.fs.azurebfs.utils.Base64; +import org.apache.hadoop.fs.azurebfs.utils.DelegationSASGenerator; +import org.apache.hadoop.fs.azurebfs.utils.SASGenerator; +import org.apache.hadoop.security.AccessControlException; + +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_CONNECTION_TIMEOUT; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_READ_TIMEOUT; + +/** + * A mock user-bound SAS token provider implementation. + */ + +public class MockUserBoundSASTokenProvider implements SASTokenProvider { + + private DelegationSASGenerator generator; + + public static final String TEST_OWNER = \"325f1619-4205-432f-9fce-3fd594325ce5\"; + public static final String CORRELATION_ID = \"66ff4ffc-ff17-417e-a2a9-45db8c5b0b5c\"; + public static final String NO_AGENT_PATH = \"NoAgentPath\"; + + @Override + public void initialize(Configuration configuration, String accountName) throws IOException { + String appID = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_ID); + String appSecret = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SECRET); + String sktid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_TENANT_ID); + String skoid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_OBJECT_ID); + String skt = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().minus(SASGenerator.FIVE_MINUTES)); + String ske = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().plus(SASGenerator.ONE_DAY)); + String skv = SASGenerator.AuthenticationVersion.July5.toString(); + + String skdutid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_TENANT_ID); + String sduoid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_OBJECT_ID); + + byte[] key = getUserDelegationKey(accountName, appID, appSecret, sktid, skt, ske, skv, skdutid); + + generator = new DelegationSASGenerator(key, skoid, sktid, skt, ske, skv, skdutid, sduoid); + } + + // Invokes the AAD v2.0 authentication endpoint with a client credentials grant to get an + // access token. See https://docs.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-client-creds-grant-flow. + private String getAuthorizationHeader(String accountName, String appID, String appSecret, String sktid) throws IOException { + String authEndPoint = String.format(\"https://login.microsoftonline.com/%s/oauth2/v2.0/token\", sktid); Review Comment: Add a constant for the string anmolanmol1234 commented on code in PR #8051: URL: https://github.com/apache/hadoop/pull/8051#discussion_r2481343344 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/extensions/MockUserBoundSASTokenProvider.java: ########## @@ -0,0 +1,166 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.extensions; + +import java.io.IOException; +import java.net.MalformedURLException; +import java.net.URL; +import java.nio.charset.StandardCharsets; +import java.time.Duration; +import java.time.Instant; +import java.util.ArrayList; +import java.util.List; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants; +import org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidUriException; +import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider; +import org.apache.hadoop.fs.azurebfs.services.AbfsHttpHeader; +import org.apache.hadoop.fs.azurebfs.services.AbfsJdkHttpOperation; +import org.apache.hadoop.fs.azurebfs.utils.Base64; +import org.apache.hadoop.fs.azurebfs.utils.DelegationSASGenerator; +import org.apache.hadoop.fs.azurebfs.utils.SASGenerator; +import org.apache.hadoop.security.AccessControlException; + +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_CONNECTION_TIMEOUT; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_READ_TIMEOUT; + +/** + * A mock user-bound SAS token provider implementation. + */ + +public class MockUserBoundSASTokenProvider implements SASTokenProvider { + + private DelegationSASGenerator generator; + + public static final String TEST_OWNER = \"325f1619-4205-432f-9fce-3fd594325ce5\"; + public static final String CORRELATION_ID = \"66ff4ffc-ff17-417e-a2a9-45db8c5b0b5c\"; + public static final String NO_AGENT_PATH = \"NoAgentPath\"; + + @Override + public void initialize(Configuration configuration, String accountName) throws IOException { + String appID = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_ID); + String appSecret = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SECRET); + String sktid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_TENANT_ID); + String skoid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_OBJECT_ID); + String skt = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().minus(SASGenerator.FIVE_MINUTES)); + String ske = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().plus(SASGenerator.ONE_DAY)); + String skv = SASGenerator.AuthenticationVersion.July5.toString(); + + String skdutid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_TENANT_ID); + String sduoid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_OBJECT_ID); + + byte[] key = getUserDelegationKey(accountName, appID, appSecret, sktid, skt, ske, skv, skdutid); + + generator = new DelegationSASGenerator(key, skoid, sktid, skt, ske, skv, skdutid, sduoid); + } + + // Invokes the AAD v2.0 authentication endpoint with a client credentials grant to get an + // access token. See https://docs.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-client-creds-grant-flow. + private String getAuthorizationHeader(String accountName, String appID, String appSecret, String sktid) throws IOException { + String authEndPoint = String.format(\"https://login.microsoftonline.com/%s/oauth2/v2.0/token\", sktid); + ClientCredsTokenProvider provider = new ClientCredsTokenProvider(authEndPoint, appID, appSecret); + return \"Bearer \" + provider.getToken().getAccessToken(); + } + + private byte[] getUserDelegationKey(String accountName, String appID, String appSecret, Review Comment: javadoc anmolanmol1234 commented on code in PR #8051: URL: https://github.com/apache/hadoop/pull/8051#discussion_r2481344728 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/extensions/MockUserBoundSASTokenProvider.java: ########## @@ -0,0 +1,166 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.extensions; + +import java.io.IOException; +import java.net.MalformedURLException; +import java.net.URL; +import java.nio.charset.StandardCharsets; +import java.time.Duration; +import java.time.Instant; +import java.util.ArrayList; +import java.util.List; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants; +import org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidUriException; +import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider; +import org.apache.hadoop.fs.azurebfs.services.AbfsHttpHeader; +import org.apache.hadoop.fs.azurebfs.services.AbfsJdkHttpOperation; +import org.apache.hadoop.fs.azurebfs.utils.Base64; +import org.apache.hadoop.fs.azurebfs.utils.DelegationSASGenerator; +import org.apache.hadoop.fs.azurebfs.utils.SASGenerator; +import org.apache.hadoop.security.AccessControlException; + +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_CONNECTION_TIMEOUT; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_READ_TIMEOUT; + +/** + * A mock user-bound SAS token provider implementation. + */ + +public class MockUserBoundSASTokenProvider implements SASTokenProvider { + + private DelegationSASGenerator generator; + + public static final String TEST_OWNER = \"325f1619-4205-432f-9fce-3fd594325ce5\"; + public static final String CORRELATION_ID = \"66ff4ffc-ff17-417e-a2a9-45db8c5b0b5c\"; + public static final String NO_AGENT_PATH = \"NoAgentPath\"; + + @Override + public void initialize(Configuration configuration, String accountName) throws IOException { + String appID = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_ID); + String appSecret = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SECRET); + String sktid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_TENANT_ID); + String skoid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_OBJECT_ID); + String skt = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().minus(SASGenerator.FIVE_MINUTES)); + String ske = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().plus(SASGenerator.ONE_DAY)); + String skv = SASGenerator.AuthenticationVersion.July5.toString(); + + String skdutid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_TENANT_ID); + String sduoid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_OBJECT_ID); + + byte[] key = getUserDelegationKey(accountName, appID, appSecret, sktid, skt, ske, skv, skdutid); + + generator = new DelegationSASGenerator(key, skoid, sktid, skt, ske, skv, skdutid, sduoid); + } + + // Invokes the AAD v2.0 authentication endpoint with a client credentials grant to get an + // access token. See https://docs.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-client-creds-grant-flow. + private String getAuthorizationHeader(String accountName, String appID, String appSecret, String sktid) throws IOException { + String authEndPoint = String.format(\"https://login.microsoftonline.com/%s/oauth2/v2.0/token\", sktid); + ClientCredsTokenProvider provider = new ClientCredsTokenProvider(authEndPoint, appID, appSecret); + return \"Bearer \" + provider.getToken().getAccessToken(); + } + + private byte[] getUserDelegationKey(String accountName, String appID, String appSecret, + String sktid, String skt, String ske, String skv, String skdutid) throws IOException { + + String method = \"POST\"; Review Comment: we have constants for HTTP methods, can be used here anmolanmol1234 commented on code in PR #8051: URL: https://github.com/apache/hadoop/pull/8051#discussion_r2481347800 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/extensions/MockUserBoundSASTokenProvider.java: ########## @@ -0,0 +1,166 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.extensions; + +import java.io.IOException; +import java.net.MalformedURLException; +import java.net.URL; +import java.nio.charset.StandardCharsets; +import java.time.Duration; +import java.time.Instant; +import java.util.ArrayList; +import java.util.List; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants; +import org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidUriException; +import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider; +import org.apache.hadoop.fs.azurebfs.services.AbfsHttpHeader; +import org.apache.hadoop.fs.azurebfs.services.AbfsJdkHttpOperation; +import org.apache.hadoop.fs.azurebfs.utils.Base64; +import org.apache.hadoop.fs.azurebfs.utils.DelegationSASGenerator; +import org.apache.hadoop.fs.azurebfs.utils.SASGenerator; +import org.apache.hadoop.security.AccessControlException; + +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_CONNECTION_TIMEOUT; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_READ_TIMEOUT; + +/** + * A mock user-bound SAS token provider implementation. + */ + +public class MockUserBoundSASTokenProvider implements SASTokenProvider { + + private DelegationSASGenerator generator; + + public static final String TEST_OWNER = \"325f1619-4205-432f-9fce-3fd594325ce5\"; + public static final String CORRELATION_ID = \"66ff4ffc-ff17-417e-a2a9-45db8c5b0b5c\"; + public static final String NO_AGENT_PATH = \"NoAgentPath\"; + + @Override + public void initialize(Configuration configuration, String accountName) throws IOException { + String appID = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_ID); + String appSecret = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SECRET); + String sktid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_TENANT_ID); + String skoid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_OBJECT_ID); + String skt = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().minus(SASGenerator.FIVE_MINUTES)); + String ske = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().plus(SASGenerator.ONE_DAY)); + String skv = SASGenerator.AuthenticationVersion.July5.toString(); + + String skdutid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_TENANT_ID); + String sduoid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_OBJECT_ID); + + byte[] key = getUserDelegationKey(accountName, appID, appSecret, sktid, skt, ske, skv, skdutid); + + generator = new DelegationSASGenerator(key, skoid, sktid, skt, ske, skv, skdutid, sduoid); + } + + // Invokes the AAD v2.0 authentication endpoint with a client credentials grant to get an + // access token. See https://docs.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-client-creds-grant-flow. + private String getAuthorizationHeader(String accountName, String appID, String appSecret, String sktid) throws IOException { + String authEndPoint = String.format(\"https://login.microsoftonline.com/%s/oauth2/v2.0/token\", sktid); + ClientCredsTokenProvider provider = new ClientCredsTokenProvider(authEndPoint, appID, appSecret); + return \"Bearer \" + provider.getToken().getAccessToken(); + } + + private byte[] getUserDelegationKey(String accountName, String appID, String appSecret, + String sktid, String skt, String ske, String skv, String skdutid) throws IOException { + + String method = \"POST\"; + String account = accountName.substring(0, accountName.indexOf(AbfsHttpConstants.DOT)); + + final StringBuilder sb = new StringBuilder(128); + sb.append(\"https://\"); + sb.append(account); + sb.append(\".blob.core.windows.net/?restype=service&comp=userdelegationkey\"); Review Comment: Try to use constants as much as possible anmolanmol1234 commented on code in PR #8051: URL: https://github.com/apache/hadoop/pull/8051#discussion_r2481350011 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/extensions/MockUserBoundSASTokenProvider.java: ########## @@ -0,0 +1,166 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.extensions; + +import java.io.IOException; +import java.net.MalformedURLException; +import java.net.URL; +import java.nio.charset.StandardCharsets; +import java.time.Duration; +import java.time.Instant; +import java.util.ArrayList; +import java.util.List; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants; +import org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidUriException; +import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider; +import org.apache.hadoop.fs.azurebfs.services.AbfsHttpHeader; +import org.apache.hadoop.fs.azurebfs.services.AbfsJdkHttpOperation; +import org.apache.hadoop.fs.azurebfs.utils.Base64; +import org.apache.hadoop.fs.azurebfs.utils.DelegationSASGenerator; +import org.apache.hadoop.fs.azurebfs.utils.SASGenerator; +import org.apache.hadoop.security.AccessControlException; + +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_CONNECTION_TIMEOUT; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_HTTP_READ_TIMEOUT; + +/** + * A mock user-bound SAS token provider implementation. + */ + +public class MockUserBoundSASTokenProvider implements SASTokenProvider { + + private DelegationSASGenerator generator; + + public static final String TEST_OWNER = \"325f1619-4205-432f-9fce-3fd594325ce5\"; + public static final String CORRELATION_ID = \"66ff4ffc-ff17-417e-a2a9-45db8c5b0b5c\"; + public static final String NO_AGENT_PATH = \"NoAgentPath\"; + + @Override + public void initialize(Configuration configuration, String accountName) throws IOException { + String appID = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_ID); + String appSecret = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SECRET); + String sktid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_TENANT_ID); + String skoid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_OBJECT_ID); + String skt = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().minus(SASGenerator.FIVE_MINUTES)); + String ske = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().plus(SASGenerator.ONE_DAY)); + String skv = SASGenerator.AuthenticationVersion.July5.toString(); + + String skdutid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_TENANT_ID); + String sduoid = configuration.get(TestConfigurationKeys.FS_AZURE_END_USER_OBJECT_ID); + + byte[] key = getUserDelegationKey(accountName, appID, appSecret, sktid, skt, ske, skv, skdutid); + + generator = new DelegationSASGenerator(key, skoid, sktid, skt, ske, skv, skdutid, sduoid); + } + + // Invokes the AAD v2.0 authentication endpoint with a client credentials grant to get an + // access token. See https://docs.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-client-creds-grant-flow. + private String getAuthorizationHeader(String accountName, String appID, String appSecret, String sktid) throws IOException { + String authEndPoint = String.format(\"https://login.microsoftonline.com/%s/oauth2/v2.0/token\", sktid); + ClientCredsTokenProvider provider = new ClientCredsTokenProvider(authEndPoint, appID, appSecret); + return \"Bearer \" + provider.getToken().getAccessToken(); + } + + private byte[] getUserDelegationKey(String accountName, String appID, String appSecret, + String sktid, String skt, String ske, String skv, String skdutid) throws IOException { + + String method = \"POST\"; + String account = accountName.substring(0, accountName.indexOf(AbfsHttpConstants.DOT)); + + final StringBuilder sb = new StringBuilder(128); + sb.append(\"https://\"); + sb.append(account); + sb.append(\".blob.core.windows.net/?restype=service&comp=userdelegationkey\"); + + URL url; + try { + url = new URL(sb.toString()); + } catch (MalformedURLException ex) { + throw new InvalidUriException(sb.toString()); + } + + List<AbfsHttpHeader> requestHeaders = new ArrayList<AbfsHttpHeader>(); + requestHeaders.add(new AbfsHttpHeader(HttpHeaderConfigurations.X_MS_VERSION, skv)); + requestHeaders.add(new AbfsHttpHeader(HttpHeaderConfigurations.CONTENT_TYPE, \"application/x-www-form-urlencoded\")); + requestHeaders.add(new AbfsHttpHeader(HttpHeaderConfigurations.AUTHORIZATION, getAuthorizationHeader(account, appID, appSecret, sktid))); + + final StringBuilder requestBody = new StringBuilder(512); + requestBody.append(\"<?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?><KeyInfo><Start>\"); + requestBody.append(skt); + requestBody.append(\"</Start><Expiry>\"); + requestBody.append(ske); + requestBody.append(\"</Expiry><DelegatedUserTid>\"); + requestBody.append(skdutid); + requestBody.append(\"</DelegatedUserTid></KeyInfo>\"); + +// requestBody.append(\"<?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?><KeyInfo><Start>\"); Review Comment: Remove comments", "created": "2025-10-24T09:31:18.000+0000", "updated": "2025-10-31T13:05:18.000+0000", "derived": {"summary_task": "Summarize this issue: Adding support for new authentication type: user bound SAS", "classification_task": "Classify the issue priority and type: Adding support for new authentication type: user bound SAS", "qna_task": "Question: What is this issue about?\nAnswer: ABFS: Support for new auth type: User-bound SAS"}}
{"id": "13632361", "key": "HADOOP-19735", "project": "HADOOP", "summary": "ABFS: Adding request priority for prefetches", "description": "Adding low traffic request priority (behind a config flag) for prefetches to reduce load on server during throttling", "comments": "", "created": "2025-10-24T04:46:13.000+0000", "updated": "2025-10-24T05:00:42.000+0000", "derived": {"summary_task": "Summarize this issue: Adding low traffic request priority (behind a config flag) for prefetches to reduce load on server during throttling", "classification_task": "Classify the issue priority and type: Adding low traffic request priority (behind a config flag) for prefetches to reduce load on server during throttling", "qna_task": "Question: What is this issue about?\nAnswer: ABFS: Adding request priority for prefetches"}}
{"id": "13632319", "key": "HADOOP-19734", "project": "HADOOP", "summary": "S3A: retry on MPU completion failure \"One or more of the specified parts could not be found\"", "description": "Experienced transient failure in test run of https://github.com/apache/hadoop/pull/7882 : all MPU complete posts failed because the request or parts were not found...the tests started succeeding 60-90s later *and* a \"hadoop s3guards uploads\" call listed the outstanding uploads of the failing tests. Hypothesis: a transient failure meant the server receiving the POST calls to complete the uploads was mistakenly reporting no upload IDs. Outcome: all active write operations failed, without any retry attempts. This can lose data and fail jobs, even though the store may recover. Proposed. The multipart uploads, especially block output stream, retry on this error; treat it as a connectivity issue.", "comments": "{code} [ERROR] ITestS3AHugeMagicCommits.test_030_postCreationAssertions:192 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: JAEYPCZ4P3JYGMTD, Extended Request ID: O/135mw9Xd2aEuFUh0ICWYc8DLXSpBUWaVGkEgEFGf0xO8o+XlZXY0hI+mvennOGt+C/UI7mNrQ=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: JAEYPCZ4P3JYGMTD, Extended Request ID: O/135mw9Xd2aEuFUh0ICWYc8DLXSpBUWaVGkEgEFGf0xO8o+XlZXY0hI+mvennOGt+C/UI7mNrQ=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeMagicCommits>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin in s3a://stevel-london/job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/array/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/bytebuffer/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: K0K75V8AH7SVBHS3, Extended Request ID: kDosbp+Z2PLZn9tVtRF9QfOqh1MgLbIKYaYFn2JeIptXlBV4v1a/wFukoXnaF7fCp6zx3vR8feE0fScUJEw+WhNW9lzu9dBxssOA62UA2kg=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: K0K75V8AH7SVBHS3, Extended Request ID: kDosbp+Z2PLZn9tVtRF9QfOqh1MgLbIKYaYFn2JeIptXlBV4v1a/wFukoXnaF7fCp6zx3vR8feE0fScUJEw+WhNW9lzu9dBxssOA62UA2kg=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/disk/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 73T4YAYRWE63WAW5, Extended Request ID: 6ucEY2heh2NsxE8dBrlZp9AE4Tb+hbvnyxea1/yp5H85BEvkQdYsfNlRH5XZM1g4hHPDSoGMVtM=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 73T4YAYRWE63WAW5, Extended Request ID: 6ucEY2heh2NsxE8dBrlZp9AE4Tb+hbvnyxea1/yp5H85BEvkQdYsfNlRH5XZM1g4hHPDSoGMVtM=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/disk/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: ZSY181YB49GQFR83, Extended Request ID: FrPEfsXO3Gbhxi3m4ZmyYSiyfscQ1QSm/1lKjRPLHEbLWH5vtGked+fHvZl281Dm6u013/5VP6pj42h4XISftk7p9uEIDGw31E7Ymcoviq4=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: ZSY181YB49GQFR83, Extended Request ID: FrPEfsXO3Gbhxi3m4ZmyYSiyfscQ1QSm/1lKjRPLHEbLWH5vtGked+fHvZl281Dm6u013/5VP6pj42h4XISftk7p9uEIDGw31E7Ymcoviq4=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesStorageClass.test_010_CreateHugeFile:74->AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/array/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: APYCQNP1GY02DGDE, Extended Request ID: lE0hQJ67sSwCYSMmO7tDEAvEIOCcpwIbLdfqqrNTpWT0bHIaacaIEzZusajj79rnFQlWudxsMHBIUXdS9ELiKR0T923lcULZy4Essx1LoTs=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: APYCQNP1GY02DGDE, Extended Request ID: lE0hQJ67sSwCYSMmO7tDEAvEIOCcpwIbLdfqqrNTpWT0bHIaacaIEzZusajj79rnFQlWudxsMHBIUXdS9ELiKR0T923lcULZy4Essx1LoTs=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesStorageClass.test_030_postCreationAssertions:81->AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesStorageClass>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesStorageClass.test_100_renameHugeFile:108->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [INFO] [ERROR] Tests run: 124, Failures: 1, Errors: 30, Skipped: 13 [INFO] {code} This has to be some transient issue with my s3 london bucket, as if in progress upload parts were not being retained. Never seen this before; the expiry time is set to 24h When these uploads fail we do leave incomplete uploads in progress: {code} Listing uploads under path \"\" job-00-fork-0005/test/testCommitOperations 141OKG11JHhWF1GOnunHUd9ZzBJ8cUG9z0LsW_4wUGgCXCvDMQM3kRi5IOCUV8FdCHtg_w8SlipfubRtzCQoT5yEpOLv.cWOiOwjEaBzUjnuJORppfXuKy1piHpLnu98 job-00-fork-0005/test/testIfMatchTwoMultipartUploadsRaceConditionOneClosesFirst yBJpm3zh4DjNQIDtyWgEmWVCk5sehVz5Vzn3QGr_tQT2iOonRp5ErXsQy24yIvnzRxBCZqVapy5VepLeu2udZBT5EXLnKRA3bchvzjtKDlipywSzYlL2N_xLUDCT359I job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads AnspJPHUoPJqg61t28OvLfAogi6G9ocyx1Dm6XY2C.a_H_onklM0Nr0LIXaPiYlQjZIiH0fTsQ1e2KhEjS9pGxvSKOXq_4YibiGZmFC6rBolmfACMqIRpoeaqYDgzYW4 job-00-fork-0005/test/testMagicWriteRecovery/file.txt KpvoTuVh85Wzm9XuU1EuxbATjb6D.Zv8vEj3z2S6AvJBHCBssy4iphxNhTkLDs7ceEwak4IPtdXED1vRf3geXT7MRMJn8d6feafvHVEgzbD31odpzTLmOaPrU_mFQXGV job-00-fork-0005/test/testMagicWriteRecovery/file.txt CnrbWU3pzgEGvjRuDuaP43Xcv1eBF5aLknqYaZA1vwO3b1QUIu9QJSiZjuLMYKT9GKw1QXwqoKo4iuxTY1a18bARx4XMEiL98kZBv0TPMaAfXE.70Olh8Q2kTyDlUCSh job-00-fork-0005/test/testMagicWriteRecovery/file.txt dEVGPBRsuOAzL5pGA02ve9qJhAlNK8lb8khF6laKjo9U0j_aG1xLkHEfPLrmcrcsLxC3R755Yv_uKbzY_Vnoc.nXCprvutM1TZmLLN_7LHrQ0tY0IjYSS6hVzDVlHbvC job-00-fork-0006/test/restricted/testCommitEmptyFile/empty-commit.txt NOCjVJqycZhkalrvU26F5oIaJP51q055et2N6b74.2JVjiKL8KwrhOhdrtumOrZ2tZWNqaK4iKZ_iosqgehJOiPbWJwxvrfvA5V.dAUTLNqjtEf5tfWh0UXu.vahDy_S5SSgNLFXK.VB82i5MZtOcw-- job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin lsYNpdn_oiWLwEVvvM621hCvIwDVaL4y_bbwVpQouW1OBThA.P9cR8fZtxvBjGdMY41UH0dTjxGHtF3BXEY8WXqmcnO9QHs_Jy.os781pE3MGzqgzFyxmd0yN6LFcTbq test/restricted/testCommitEmptyFile/empty-commit.txt T3W9V56Bv_FMhKpgcBgJ1H2wOBkPKk23T0JomesBzZyqiIAu3NiROibAgoZUhWSdoTKSJoOgcn3UWYGOvGBbsHteS_N_c1QoTEp0GE7PNlzDfs1GheJ5SOpUgaEY6MaYdNe0mn0gY48FDXpVB2nqiA-- test/restricted/testCommitEmptyFile/empty-commit.txt .cr4b3xkfze4N24Bj3PAm_ACIyIVuTU4DueDktU1abNu2LJWXH2HKnUu1oOjfnnQwnUXp4VmXBVbZ5aq8E8gVCxN.Oyb7hmGVtESmRjpqIXSW80JrB_0_dqXe.uAT.JH7kEWywAlb4NIqJ5Xz99tvA-- Total 10 uploads found. {code} Most interesting here is `testIfNoneMatchTwoConcurrentMultipartUploads`, because this initiates then completes an MPU, so as to create a zero byte file. It doesn't upload any parts. The attempt to complete failed. {code} [ERROR] org.apache.hadoop.fs.s3a.impl.ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchTwoConcurrentMultipartUploads -- Time elapsed: 2.783 s <<< ERROR! org.apache.hadoop.fs.s3a.AWSBadRequestException: Completing multipart upload on job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 9JCJ6M5QRDGJNYYS, Extended Request ID: Z7Q7+LA0o/5B4xoIGhgo+tVppawZ0UBj7X4RNb+0m9RbOAOwD/Apv1o+KmnW0aypjwmfFlarxjo=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 9JCJ6M5QRDGJNYYS, Extended Request ID: Z7Q7+LA0o/5B4xoIGhgo+tVppawZ0UBj7X4RNb+0m9RbOAOwD/Apv1o+KmnW0aypjwmfFlarxjo=) (SDK Attempt Count: 1) at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:265) at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:124) at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$4(Invoker.java:376) at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468) at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:372) at org.apache.hadoop.fs.s3a.WriteOperationHelper.finalizeMultipartUpload(WriteOperationHelper.java:318) at org.apache.hadoop.fs.s3a.WriteOperationHelper.completeMPUwithRetries(WriteOperationHelper.java:370) at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.lambda$complete$3(S3ABlockOutputStream.java:1227) at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.measureDurationOfInvocation(IOStatisticsBinding.java:493) at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464) at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.complete(S3ABlockOutputStream.java:1225) at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.access$1500(S3ABlockOutputStream.java:876) at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.close(S3ABlockOutputStream.java:545) at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77) at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106) at org.apache.hadoop.fs.s3a.impl.ITestS3APutIfMatchAndIfNoneMatch.createFileWithFlags(ITestS3APutIfMatchAndIfNoneMatch.java:190) at org.apache.hadoop.fs.s3a.impl.ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchTwoConcurrentMultipartUploads(ITestS3APutIfMatchAndIfNoneMatch.java:380) at java.lang.reflect.Method.invoke(Method.java:498) at java.util.ArrayList.forEach(ArrayList.java:1259) at java.util.ArrayList.forEach(ArrayList.java:1259) Caused by: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 9JCJ6M5QRDGJNYYS, Extended Request ID: Z7Q7+LA0o/5B4xoIGhgo+tVppawZ0UBj7X4RNb+0m9RbOAOwD/Apv1o+KmnW0aypjwmfFlarxjo=) (SDK Attempt Count: 1) at software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:113) at software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:61) at software.amazon.awssdk.core.internal.http.pipeline.stages.utils.RetryableStageHelper.retryPolicyDisallowedRetryException(RetryableStageHelper.java:168) at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:73) at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36) at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206) at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:53) at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:35) at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:82) at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:62) at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:43) at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:50) at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:32) at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206) at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206) at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37) at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26) at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:210) at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103) at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:173) at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$1(BaseSyncClientHandler.java:80) at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:182) at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:74) at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:45) at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:53) at software.amazon.awssdk.services.s3.DefaultS3Client.completeMultipartUpload(DefaultS3Client.java:801) at software.amazon.awssdk.services.s3.DelegatingS3Client.lambda$completeMultipartUpload$1(DelegatingS3Client.java:611) at software.amazon.awssdk.services.s3.internal.crossregion.S3CrossRegionSyncClient.invokeOperation(S3CrossRegionSyncClient.java:67) at software.amazon.awssdk.services.s3.DelegatingS3Client.completeMultipartUpload(DelegatingS3Client.java:611) at org.apache.hadoop.fs.s3a.impl.S3AStoreImpl.completeMultipartUpload(S3AStoreImpl.java:906) at org.apache.hadoop.fs.s3a.S3AFileSystem$WriteOperationHelperCallbacksImpl.completeMultipartUpload(S3AFileSystem.java:1953) at org.apache.hadoop.fs.s3a.WriteOperationHelper.lambda$finalizeMultipartUpload$1(WriteOperationHelper.java:324) at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:122) ... 18 more {code} Yet the uploads list afterwards finds it {code} job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads AnspJPHUoPJqg61t28OvLfAogi6G9ocyx1Dm6XY2C.a_H_onklM0Nr0LIXaPiYlQjZIiH0fTsQ1e2KhEjS9pGxvSKOXq_4YibiGZmFC6rBolmfACMqIRpoeaqYDgzYW4 {code} And stack on a write failure. {code} [ERROR] org.apache.hadoop.fs.s3a.scale.ITestS3AHugeFilesArrayBlocks.test_010_CreateHugeFile -- Time elapsed: 2.870 s <<< ERROR! org.apache.hadoop.fs.s3a.AWSBadRequestException: Completing multipart upload on job-00/test/tests3ascale/array/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1) at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:265) at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:124) at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$4(Invoker.java:376) at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468) at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:372) at org.apache.hadoop.fs.s3a.WriteOperationHelper.finalizeMultipartUpload(WriteOperationHelper.java:318) at org.apache.hadoop.fs.s3a.WriteOperationHelper.completeMPUwithRetries(WriteOperationHelper.java:370) at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.lambda$complete$3(S3ABlockOutputStream.java:1227) at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.measureDurationOfInvocation(IOStatisticsBinding.java:493) at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464) at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.complete(S3ABlockOutputStream.java:1225) at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.access$1500(S3ABlockOutputStream.java:876) at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.close(S3ABlockOutputStream.java:545) at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77) at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106) at org.apache.hadoop.fs.s3a.scale.AbstractSTestS3AHugeFiles.test_010_CreateHugeFile(AbstractSTestS3AHugeFiles.java:276) at java.lang.reflect.Method.invoke(Method.java:498) at java.util.ArrayList.forEach(ArrayList.java:1259) at java.util.ArrayList.forEach(ArrayList.java:1259) Caused by: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1) at software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:113) at software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:61) at software.amazon.awssdk.core.internal.http.pipeline.stages.utils.RetryableStageHelper.retryPolicyDisallowedRetryException(RetryableStageHelper.java:168) at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:73) at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36) at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206) at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:53) at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:35) at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:82) at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:62) at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:43) at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:50) at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:32) at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206) at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206) at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37) at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26) at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:210) at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103) at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:173) at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$1(BaseSyncClientHandler.java:80) at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:182) at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:74) at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:45) at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:53) at software.amazon.awssdk.services.s3.DefaultS3Client.completeMultipartUpload(DefaultS3Client.java:801) at software.amazon.awssdk.services.s3.DelegatingS3Client.lambda$completeMultipartUpload$1(DelegatingS3Client.java:611) at software.amazon.awssdk.services.s3.internal.crossregion.S3CrossRegionSyncClient.invokeOperation(S3CrossRegionSyncClient.java:67) at software.amazon.awssdk.services.s3.DelegatingS3Client.completeMultipartUpload(DelegatingS3Client.java:611) at org.apache.hadoop.fs.s3a.impl.S3AStoreImpl.completeMultipartUpload(S3AStoreImpl.java:906) at org.apache.hadoop.fs.s3a.S3AFileSystem$WriteOperationHelperCallbacksImpl.completeMultipartUpload(S3AFileSystem.java:1953) at org.apache.hadoop.fs.s3a.WriteOperationHelper.lambda$finalizeMultipartUpload$1(WriteOperationHelper.java:324) at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:122) ... 17 more {code} we'd have to map 400 + the error text to a \"MultipartUploadCompleteFailed\" exception and add a policy for it, leaving other 400s as unrecoverable. + any tracking in block output stream should record when the POST to initiate the MPU was issued. That way if an error still surfaces but the output stream has been open for three days, we have a good cause \"stream open too long\" this is actually me making a mess of checksum config if the sdk checksum clalculation is set to \"always\" then the user MUST choose a checksum algorithm for s3 uploads (proposed: CRC32). I\"m going to leave checksum calculation off by default for performance and compatibility", "created": "2025-10-23T15:42:38.000+0000", "updated": "2025-10-24T13:48:33.000+0000", "derived": {"summary_task": "Summarize this issue: Experienced transient failure in test run of https://github.com/apache/hadoop/pull/7882 : all MPU complete posts failed because the request or parts were not found...the tests started succeeding 60-90s later *and* a \"hadoop s3guards uploads\" call listed the outstanding uploads of the failing tests. Hypothesis: a transient failure meant the server receiving the POST calls to complete the uploads wa", "classification_task": "Classify the issue priority and type: Experienced transient failure in test run of https://github.com/apache/hadoop/pull/7882 : all MPU complete posts failed because the request or parts were not found...the tests started succeeding 60-90s later *and* a \"hadoop s3guards uploads\" call listed the outstanding uploads of the failing tests. Hypothesis: a transient failure meant the server receiving the POST calls to complete the uploads wa", "qna_task": "Question: What is this issue about?\nAnswer: S3A: retry on MPU completion failure \"One or more of the specified parts could not be found\""}}
{"id": "13632245", "key": "HADOOP-19733", "project": "HADOOP", "summary": "S3A: Credentials provider classes not found despite setting `fs.s3a.classloader.isolation` to `false`", "description": "HADOOP-18993 added the option `fs.s3a.classloader.isolation` to support, for example, a Spark job using an AWS credentials provider class that is bundled into the Spark job JAR. In testing this, the AWS credentials provider classes are still not found. I think the cause is: * `fs.s3a.classloader.isolation` is implemented by setting (or not setting) a classloader on the `Configuration` * However, code paths to load AWS credential provider call `S3AUtils.getInstanceFromReflection`, which uses the classloader that loaded the S3AUtils class. That's likely to be the built-in application classloader, which won't be able to load classes in a Spark job JAR. And the fix seems small: * Change `S3AUtils.getInstanceFromReflection` to load classes using the `Configuration`'s classloader. Luckily we already have the Configuration in this method.", "comments": "I haven't contributed to Hadoop or other Apache projects before, but this approachable for a first contribution. I'll open a PR. brandonvin opened a new pull request, #8048: URL: https://github.com/apache/hadoop/pull/8048 \u2026lassloader <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR Follow-up to [HADOOP-18993](https://issues.apache.org/jira/browse/HADOOP-18993) and [HADOOP-19733](https://issues.apache.org/jira/browse/HADOOP-19733) before it. With `fs.s3a.classloader.isolation` set to `false` in a Spark application, it was still impossible to load a credentials provider class from the Spark application jar. `fs.s3a.classloader.isolation` works by saving a reference to the intended classloader in the `Configuration`. However, loading credentials providers goes through `S3AUtils#getInstanceFromReflection`, which always used the classloader that loaded `S3AUtils`. With this patch, credentials providers will be loaded using the `Configuration`'s classloader. ### How was this patch tested? Unit tests in `org.apache.hadoop.fs.s3a.ITestS3AFileSystemIsolatedClassloader`. Manual testing in a Spark application. ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? brandonvin commented on code in PR #8048: URL: https://github.com/apache/hadoop/pull/8048#discussion_r2453905622 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java: ########## @@ -77,19 +109,9 @@ private void assertInNewFilesystem(Map<String, String> confToSet, Consumer<FileS } } - private Map<String, String> mapOf() { - return new HashMap<>(); - } - - private Map<String, String> mapOf(String key, String value) { - HashMap<String, String> m = new HashMap<>(); - m.put(key, value); - return m; - } Review Comment: Since I added test cases that set 2 key-value pairs, I switched to `Map.of` instead of extending these. Not sure if there was a reason to avoid `Map.of` here. hadoop-yetus commented on PR #8048: URL: https://github.com/apache/hadoop/pull/8048#issuecomment-3435132657 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 24m 19s | | trunk passed | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 20s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 30s | | trunk passed | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 18s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 47s | [/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/1/artifact/out/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html) | hadoop-tools/hadoop-aws in trunk has 188 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 14m 51s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 11s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-aws.txt) | hadoop-tools/hadoop-aws: The patch generated 31 new + 4 unchanged - 0 fixed = 35 total (was 4) | | +1 :green_heart: | mvnsite | 0m 21s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 47s | [/new-spotbugs-hadoop-tools_hadoop-aws.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/1/artifact/out/new-spotbugs-hadoop-tools_hadoop-aws.html) | hadoop-tools/hadoop-aws generated 2 new + 188 unchanged - 0 fixed = 190 total (was 188) | | +1 :green_heart: | shadedclient | 15m 9s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 1m 57s | [/patch-unit-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/1/artifact/out/patch-unit-hadoop-tools_hadoop-aws.txt) | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 19s | | The patch does not generate ASF License warnings. | | | | 63m 28s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-aws | | | Nullcheck of conf at line 655 of value previously dereferenced in org.apache.hadoop.fs.s3a.S3AUtils.getInstanceFromReflection(String, Configuration, URI, Class, String, String) At S3AUtils.java:655 of value previously dereferenced in org.apache.hadoop.fs.s3a.S3AUtils.getInstanceFromReflection(String, Configuration, URI, Class, String, String) At S3AUtils.java:[line 645] | | | Non-virtual method call in org.apache.hadoop.fs.s3a.auth.SignerFactory.createSigner(String, String) passes null for non-null parameter of org.apache.hadoop.fs.s3a.S3AUtils.getInstanceFromReflection(String, Configuration, URI, Class, String, String) At SignerFactory.java:String) passes null for non-null parameter of org.apache.hadoop.fs.s3a.S3AUtils.getInstanceFromReflection(String, Configuration, URI, Class, String, String) At SignerFactory.java:[line 125] | | Failed junit tests | hadoop.fs.s3a.auth.TestSignerManager | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8048 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 573c49df2825 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 032c335082f24aef12ee3e002ae1cfd9c5f40507 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/1/testReport/ | | Max. process+thread count | 610 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/1/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #8048: URL: https://github.com/apache/hadoop/pull/8048#issuecomment-3435205771 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 25m 39s | | trunk passed | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 17s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 27s | | trunk passed | | +1 :green_heart: | javadoc | 0m 23s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 47s | [/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/2/artifact/out/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html) | hadoop-tools/hadoop-aws in trunk has 188 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 14m 50s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 23s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 23s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 10s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-aws.txt) | hadoop-tools/hadoop-aws: The patch generated 12 new + 4 unchanged - 0 fixed = 16 total (was 4) | | +1 :green_heart: | mvnsite | 0m 23s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 46s | [/new-spotbugs-hadoop-tools_hadoop-aws.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/2/artifact/out/new-spotbugs-hadoop-tools_hadoop-aws.html) | hadoop-tools/hadoop-aws generated 2 new + 188 unchanged - 0 fixed = 190 total (was 188) | | +1 :green_heart: | shadedclient | 14m 2s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 2m 0s | [/patch-unit-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/2/artifact/out/patch-unit-hadoop-tools_hadoop-aws.txt) | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 20s | | The patch does not generate ASF License warnings. | | | | 63m 47s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-aws | | | Nullcheck of conf at line 655 of value previously dereferenced in org.apache.hadoop.fs.s3a.S3AUtils.getInstanceFromReflection(String, Configuration, URI, Class, String, String) At S3AUtils.java:655 of value previously dereferenced in org.apache.hadoop.fs.s3a.S3AUtils.getInstanceFromReflection(String, Configuration, URI, Class, String, String) At S3AUtils.java:[line 645] | | | Non-virtual method call in org.apache.hadoop.fs.s3a.auth.SignerFactory.createSigner(String, String) passes null for non-null parameter of org.apache.hadoop.fs.s3a.S3AUtils.getInstanceFromReflection(String, Configuration, URI, Class, String, String) At SignerFactory.java:String) passes null for non-null parameter of org.apache.hadoop.fs.s3a.S3AUtils.getInstanceFromReflection(String, Configuration, URI, Class, String, String) At SignerFactory.java:[line 125] | | Failed junit tests | hadoop.fs.s3a.auth.TestSignerManager | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8048 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 55f7cbac0d20 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 249aef5213fa039d252e7f7ae03c060b6c87d94f | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/2/testReport/ | | Max. process+thread count | 616 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8048/2/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. steveloughran commented on code in PR #8048: URL: https://github.com/apache/hadoop/pull/8048#discussion_r2455036391 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java: ########## @@ -37,10 +46,33 @@ */ public class ITestS3AFileSystemIsolatedClassloader extends AbstractS3ATestBase { + private static String customClassName = \"custom.class.name\"; + + private static class CustomCredentialsProvider implements AwsCredentialsProvider { + + public CustomCredentialsProvider() { + } + + @Override + public AwsCredentials resolveCredentials() { + return null; + } + + } + private static class CustomClassLoader extends ClassLoader { } - private final ClassLoader customClassLoader = new CustomClassLoader(); + private final ClassLoader customClassLoader = spy(new CustomClassLoader()); + { + try { Review Comment: this is a nice way to simulate classloader pain. ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java: ########## @@ -28,6 +29,14 @@ import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.s3a.impl.InstantiationIOException; + +import software.amazon.awssdk.auth.credentials.AwsCredentials; Review Comment: nit: put the amazon imports in the same group as the junit ones ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java: ########## @@ -100,11 +122,26 @@ public void defaultIsolatedClassloader() throws IOException { .isEqualTo(fs.getClass().getClassLoader()) .describedAs(\"the classloader that loaded the fs\"); }); + + Throwable thrown = Assertions.catchThrowable(() -> { Review Comment: Use our `LambdaTestUtils.intercept()`; it's like the spark one and does the casting checks ``` InstantiationIOException ex = intercept(InstantiationIOException.class, () -> { assert...}) ``` we have a `assertExceptionContains` to look at the inner stuff, but the assert of L136 is fine. ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java: ########## @@ -115,11 +152,31 @@ public void isolatedClassloader() throws IOException { .isEqualTo(fs.getClass().getClassLoader()) .describedAs(\"the classloader that loaded the fs\"); }); + + Throwable thrown = Assertions.catchThrowable(() -> { Review Comment: again `intercept()` and cut the assert at L163 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java: ########## @@ -77,19 +109,9 @@ private void assertInNewFilesystem(Map<String, String> confToSet, Consumer<FileS } } - private Map<String, String> mapOf() { - return new HashMap<>(); - } - - private Map<String, String> mapOf(String key, String value) { - HashMap<String, String> m = new HashMap<>(); - m.put(key, value); - return m; - } Review Comment: It's because we only switched to java17 yesterday! And in trunk only. If you want to see this change in Hadoop 3.4.3 it'll still need to be java8 code, so this needs to be restored. Otherwise: trunk/3.5.0 only Ok, will also update the custom signer loading to use the configuration, for consistency. brandonvin commented on code in PR #8048: URL: https://github.com/apache/hadoop/pull/8048#discussion_r2456994430 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java: ########## @@ -77,19 +109,9 @@ private void assertInNewFilesystem(Map<String, String> confToSet, Consumer<FileS } } - private Map<String, String> mapOf() { - return new HashMap<>(); - } - - private Map<String, String> mapOf(String key, String value) { - HashMap<String, String> m = new HashMap<>(); - m.put(key, value); - return m; - } Review Comment: Thanks, makes sense!", "created": "2025-10-22T19:56:55.000+0000", "updated": "2025-10-23T19:44:03.000+0000", "derived": {"summary_task": "Summarize this issue: HADOOP-18993 added the option `fs.s3a.classloader.isolation` to support, for example, a Spark job using an AWS credentials provider class that is bundled into the Spark job JAR. In testing this, the AWS credentials provider classes are still not found. I think the cause is: * `fs.s3a.classloader.isolation` is implemented by setting (or not setting) a classloader on the `Configuration` * However, c", "classification_task": "Classify the issue priority and type: HADOOP-18993 added the option `fs.s3a.classloader.isolation` to support, for example, a Spark job using an AWS credentials provider class that is bundled into the Spark job JAR. In testing this, the AWS credentials provider classes are still not found. I think the cause is: * `fs.s3a.classloader.isolation` is implemented by setting (or not setting) a classloader on the `Configuration` * However, c", "qna_task": "Question: What is this issue about?\nAnswer: S3A: Credentials provider classes not found despite setting `fs.s3a.classloader.isolation` to `false`"}}
{"id": "13632141", "key": "HADOOP-19732", "project": "HADOOP", "summary": "Namenode Crash Due to Delegation Renewer Runtime Exit (NoMatchingRule)", "description": "The delegation token renewer enters runtime exit when a _NoMatchingRule_ error, which caused the entire namenode to crash. I think returning an error to the client should be fine but bringing down the namenode is not acceptable to anyone. After the AD change, the new realm was updated, but some jobs are still using the old realm as users are updating them gradually. This migration process will take time, and during this period, other jobs are still catching up with the new realm configuration. However, the namenode down disrupts all of them. {code:java} ERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover thread received unexpected exception java.lang.IllegalArgumentException: Illegal principal name hive/xxxxx@yyyy.COM org.apache.hadoop.security.authentication.util.KerberosName$NoMatchingRule: No rules applied to hive/xxxxx@yyyy.COM at org.apache.hadoop.security.User.<init>(User.java:51) at org.apache.hadoop.security.User.<init>(User.java:43) at org.apache.hadoop.security.UserGroupInformation.createRemoteUser(UserGroupInformation.java:1417) at org.apache.hadoop.security.UserGroupInformation.createRemoteUser(UserGroupInformation.java:1401) at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier.getUser(AbstractDelegationTokenIdentifier.java:80) at org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier.getUser(DelegationTokenIdentifier.java:81) at org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier.toString(DelegationTokenIdentifier.java:91) at java.lang.String.valueOf(String.java:2994) at java.lang.StringBuilder.append(StringBuilder.java:137) at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.formatTokenId(AbstractDelegationTokenSecretManager.java:58) at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.logExpireTokens(AbstractDelegationTokenSecretManager.java:642) at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.removeExpiredToken(AbstractDelegationTokenSecretManager.java:635) at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.access$400(AbstractDelegationTokenSecretManager.java:51) at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:694) at java.lang.Thread.run(Thread.java:750) Caused by: org.apache.hadoop.security.authentication.util.KerberosName$NoMatchingRule: No rules applied to hive/xxxxx@yyyy.COM at org.apache.hadoop.security.authentication.util.KerberosName.getShortName(KerberosName.java:429) at org.apache.hadoop.security.User.<init>(User.java:48) ... 14 more {code}", "comments": "looks a duplicate of HDFS-17138. [~kpalanisamy] please set the hadoop version you saw it with. If it is a version without HDFS-17138 -please upgrade. closing as a duplicate. If it surfaces on branches with HDFS-17138, re-open You\u2019re right [~stevel@apache.org]. My user version is 3.1.1, so it is missing this HDFS-17138 fix, which clearly addresses my user scenarios. Thanks for checking so quickly. I should have checked it, but unfortunately I taken your time :)", "created": "2025-10-21T18:43:38.000+0000", "updated": "2025-10-23T17:47:42.000+0000", "derived": {"summary_task": "Summarize this issue: The delegation token renewer enters runtime exit when a _NoMatchingRule_ error, which caused the entire namenode to crash. I think returning an error to the client should be fine but bringing down the namenode is not acceptable to anyone. After the AD change, the new realm was updated, but some jobs are still using the old realm as users are updating them gradually. This migration process will tak", "classification_task": "Classify the issue priority and type: The delegation token renewer enters runtime exit when a _NoMatchingRule_ error, which caused the entire namenode to crash. I think returning an error to the client should be fine but bringing down the namenode is not acceptable to anyone. After the AD change, the new realm was updated, but some jobs are still using the old realm as users are updating them gradually. This migration process will tak", "qna_task": "Question: What is this issue about?\nAnswer: Namenode Crash Due to Delegation Renewer Runtime Exit (NoMatchingRule)"}}
{"id": "13631920", "key": "HADOOP-19731", "project": "HADOOP", "summary": "Fix SpotBugs warnings introduced after SpotBugs version upgrade.", "description": "Following the upgrade to SpotBugs {*}4.9.7{*}, several new warnings have emerged. We plan to address these warnings to improve code safety and maintain compatibility with the updated analysis rules.", "comments": "Hi [~slfan1989] Thanks for tracking this. We do have a bunch of PRs open that are facing issue reported here. What are the expectations here? Do we need to address all warnings with the PR itself or they can be ignored and taken later as part of this Jira? I think later would be better. It will help keep the changes in PR limited to what is being done and will ease the review process. I agree with your point. We\u2019ll work on submitting a common PR that includes a SpotBugs rule to temporarily suppress the new static analysis warnings and restore the state back to the 4.2.0 level. Sounds awesome. Thanks for all the efforts. zhtttylz opened a new pull request, #8053: URL: https://github.com/apache/hadoop/pull/8053 ### Description of PR HADOOP-19731. Fix SpotBugs warnings introduced after SpotBugs version upgrade. ### How was this patch tested? Ran `mvn -Dspotbugs.skip=false spotbugs:spotbugs` on affected modules and verified the build no longer fails on SpotBugs warnings. No functional code changes, config-only. ### For code changes: - Add a project-wide baseline at dev-support/findbugs-exclude-global.xml. - Consolidate SpotBugs plugin config in affected module POMs to consistently include local excludes and the new global baseline. - Wire the global baseline from hadoop-project-dist/pom.xml; introduce a root path property to reference the repository root. slfan1989 commented on PR #8053: URL: https://github.com/apache/hadoop/pull/8053#issuecomment-3470749232 @zhtttylz Thank you for following up on this issue. hadoop-yetus commented on PR #8053: URL: https://github.com/apache/hadoop/pull/8053#issuecomment-3471069512 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 29s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 39s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 15m 41s | | trunk passed | | +1 :green_heart: | compile | 8m 19s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 8m 22s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | mvnsite | 6m 23s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8053/2/artifact/out/branch-mvnsite-root.txt) | root in trunk failed. | | +1 :green_heart: | javadoc | 5m 21s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 44s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | shadedclient | 71m 43s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 24s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 25m 51s | | the patch passed | | +1 :green_heart: | compile | 8m 1s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 1s | | the patch passed | | +1 :green_heart: | compile | 8m 20s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 20s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 3m 41s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8053/2/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | +1 :green_heart: | javadoc | 5m 15s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 38s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | shadedclient | 28m 29s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 589m 16s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8053/2/artifact/out/patch-unit-root.txt) | root in the patch passed. | | -1 :x: | asflicense | 0m 48s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8053/2/artifact/out/results-asflicense.txt) | The patch generated 1 ASF License warnings. | | | | 734m 52s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.router.subcluster.capacity.TestYarnFederationWithCapacityScheduler | | | hadoop.security.ssl.TestDelegatingSSLSocketFactory | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | | hadoop.hdfs.TestDecommission | | | hadoop.hdfs.tools.TestDFSAdmin | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8053/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8053 | | Optional Tests | dupname asflicense codespell detsecrets xmllint compile javac javadoc mvninstall mvnsite unit shadedclient | | uname | Linux 2197de4c49c9 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c46c6afc99a346474f5b255749c5a86ae4de90bc | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8053/2/testReport/ | | Max. process+thread count | 4391 (vs. ulimit of 5500) | | modules | C: hadoop-project-dist hadoop-common-project/hadoop-minikdc hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-nfs hadoop-common-project/hadoop-kms hadoop-common-project/hadoop-registry hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-yarn-project/hadoop-yarn hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-mapreduce-project/hadoop-mapreduce-examples hadoop-mapreduce-project hadoop-tools/hadoop-streaming hadoop-tools/hadoop-archive-logs hadoop-tools/hadoop-rumen hadoop-tools/hadoop-gridmix hadoop-tools/hadoop-datajoin hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure hadoop-tools/hadoop-aliyun hadoop-tools/hadoop-sls hadoop-tools/hadoop-fs2img hadoop-tools/hadoop-gcp hadoop-tools/hadoop-benchmark hadoop-cloud-storage-project/hadoop-cos hadoop-cloud-storage-project/hadoop-huaweicloud hadoop-cloud-storage-project/hadoop-tos . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8053/2/console | | versions | git=2.25.1 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-19T09:57:26.000+0000", "updated": "2025-10-31T02:29:09.000+0000", "derived": {"summary_task": "Summarize this issue: Following the upgrade to SpotBugs {*}4.9.7{*}, several new warnings have emerged. We plan to address these warnings to improve code safety and maintain compatibility with the updated analysis rules.", "classification_task": "Classify the issue priority and type: Following the upgrade to SpotBugs {*}4.9.7{*}, several new warnings have emerged. We plan to address these warnings to improve code safety and maintain compatibility with the updated analysis rules.", "qna_task": "Question: What is this issue about?\nAnswer: Fix SpotBugs warnings introduced after SpotBugs version upgrade."}}
{"id": "13631918", "key": "HADOOP-19730", "project": "HADOOP", "summary": "upgrade bouncycastle to 1.82 due to CVE-2025-8916", "description": "https://github.com/advisories/GHSA-4cx2-fc23-5wg6 Thought it was tidier to upgrade to latest version even if the fix was a while ago.", "comments": "pjfanning opened a new pull request, #8039: URL: https://github.com/apache/hadoop/pull/8039 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR HADOOP-19730 ### How was this patch tested? ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #8039: URL: https://github.com/apache/hadoop/pull/8039#issuecomment-3420366545 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 20m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 7m 51s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 28m 35s | | trunk passed | | +1 :green_heart: | compile | 15m 12s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 28s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | mvnsite | 8m 58s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/artifact/out/branch-mvnsite-root.txt) | root in trunk failed. | | +1 :green_heart: | javadoc | 9m 36s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 8m 36s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | shadedclient | 43m 37s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 33s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 27m 23s | | the patch passed | | +1 :green_heart: | compile | 14m 49s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 14m 49s | | the patch passed | | +1 :green_heart: | compile | 15m 37s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 37s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 7m 1s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 9m 42s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 8m 36s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | shadedclient | 45m 37s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 807m 40s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 36s | | The patch does not generate ASF License warnings. | | | | 1064m 10s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.server.datanode.fsdataset.impl.TestFsVolumeList | | | hadoop.hdfs.tools.TestDFSAdmin | | | hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints | | | hadoop.hdfs.server.balancer.TestBalancerWithHANameNodes | | | hadoop.hdfs.TestRollingUpgrade | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService | | | hadoop.yarn.service.TestYarnNativeServices | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8039 | | Optional Tests | dupname asflicense mvnsite codespell detsecrets markdownlint compile javac javadoc mvninstall unit shadedclient xmllint shellcheck shelldocs | | uname | Linux 66cf96c27f49 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 695a0a30232b143ec8837d6a6648344ffd4efec0 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/testReport/ | | Max. process+thread count | 4498 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-cloud-storage-project/hadoop-cos . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/console | | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 merged PR #8039: URL: https://github.com/apache/hadoop/pull/8039 slfan1989 commented on PR #8039: URL: https://github.com/apache/hadoop/pull/8039#issuecomment-3424343993 @pjfanning Thanks for the contribution! Merged into trunk. Could we also open a PR for branch-3.4? pjfanning opened a new pull request, #8047: URL: https://github.com/apache/hadoop/pull/8047 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR backport #6976 * HADOOP-19730. Upgrade Bouncycastle to 1.82 due to CVE-2025-8916 ### How was this patch tested? ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #8047: URL: https://github.com/apache/hadoop/pull/8047#issuecomment-3438232725 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 12m 50s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ branch-3.4 Compile Tests _ | | +0 :ok: | mvndep | 2m 54s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 39m 30s | | branch-3.4 passed | | +1 :green_heart: | compile | 18m 44s | | branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 17m 41s | | branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 22m 5s | | branch-3.4 passed | | +1 :green_heart: | javadoc | 9m 20s | | branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 36s | | branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 50m 39s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 34s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 29m 11s | | the patch passed | | +1 :green_heart: | compile | 17m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 17m 19s | | the patch passed | | +1 :green_heart: | compile | 15m 51s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 51s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 18m 13s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 9m 11s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 31s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 52m 50s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 720m 55s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8047/1/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 49s | | The patch does not generate ASF License warnings. | | | | 1024m 55s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.mapred.gridmix.TestGridmixSubmission | | | hadoop.mapred.gridmix.TestLoadJob | | | hadoop.security.ssl.TestDelegatingSSLSocketFactory | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8047/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8047 | | Optional Tests | dupname asflicense mvnsite codespell detsecrets markdownlint compile javac javadoc mvninstall unit shadedclient xmllint shellcheck shelldocs | | uname | Linux d69a46a8ee67 5.15.0-160-generic #170-Ubuntu SMP Wed Oct 1 10:06:56 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | branch-3.4 / c8b8fb4e82d33a470f10a447f4799cc872fb3c01 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8047/1/testReport/ | | Max. process+thread count | 3660 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-cloud-storage-project/hadoop-cos . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8047/1/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 merged PR #8047: URL: https://github.com/apache/hadoop/pull/8047 slfan1989 commented on PR #8047: URL: https://github.com/apache/hadoop/pull/8047#issuecomment-3449617778 @pjfanning Thanks for the contribution! Merged into trunk.", "created": "2025-10-19T09:09:36.000+0000", "updated": "2025-10-27T05:48:06.000+0000", "derived": {"summary_task": "Summarize this issue: https://github.com/advisories/GHSA-4cx2-fc23-5wg6 Thought it was tidier to upgrade to latest version even if the fix was a while ago.", "classification_task": "Classify the issue priority and type: https://github.com/advisories/GHSA-4cx2-fc23-5wg6 Thought it was tidier to upgrade to latest version even if the fix was a while ago.", "qna_task": "Question: What is this issue about?\nAnswer: upgrade bouncycastle to 1.82 due to CVE-2025-8916"}}
{"id": "13631778", "key": "HADOOP-19729", "project": "HADOOP", "summary": "ABFS: [Perf] Network Profiling of Tailing Requests and Killing Bad Connections Proactively", "description": "It has been observed that certain requests taking more time than expected to complete hinders the performance of whole workload. Such requests are known as tailing requests. They can be taking more time due to a number of reasons and the prominent among them is a bad network connection. In Abfs driver we cache network connections and keeping such bad connections in cache and reusing them can be bad for perf. In this effort we try to identify such connections and close them so that new good connetions can be established and perf can be improved. There are two parts of this effort. # Identifying Tailing Requests: This involves profiling all the network calls and getting percentiles value optimally. By default we consider p99 as the tail latency and all the future requests taking more than tail latency will be considere as Tailing requests. # Proactively Killing Socket Connections: With Apache client, we can now kill the socket connection and fail the tailing request. Such failures will not be thrown back to user and retried immediately without any sleep but from another socket connection.", "comments": "hadoop-yetus commented on PR #8043: URL: https://github.com/apache/hadoop/pull/8043#issuecomment-3448411577 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 11m 8s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 21m 31s | | trunk passed | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 18s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 26s | | trunk passed | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 22s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 43s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/2/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) | hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 13m 53s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 22s | | the patch passed | | +1 :green_heart: | compile | 0m 18s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 21s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 21s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 10s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 43 new + 3 unchanged - 0 fixed = 46 total (was 3) | | +1 :green_heart: | mvnsite | 0m 20s | | the patch passed | | -1 :x: | javadoc | 0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/2/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 34 new + 1472 unchanged - 0 fixed = 1506 total (was 1472) | | -1 :x: | javadoc | 0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/2/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 33 new + 1413 unchanged - 0 fixed = 1446 total (was 1413) | | -1 :x: | spotbugs | 0m 46s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/2/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) | hadoop-tools/hadoop-azure generated 4 new + 177 unchanged - 1 fixed = 181 total (was 178) | | +1 :green_heart: | shadedclient | 14m 12s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 8s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 19s | | The patch does not generate ASF License warnings. | | | | 70m 0s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient) At AbfsAHCHttpOperation.java:new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient) At AbfsAHCHttpOperation.java:[line 123] | | | Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext) At AbfsRestOperation.java:org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext) At AbfsRestOperation.java:[line 533] | | | new org.apache.hadoop.fs.azurebfs.services.AbfsTailLatencyTracker(AbfsConfiguration) may expose internal representation by storing an externally mutable object into AbfsTailLatencyTracker.configuration At AbfsTailLatencyTracker.java:representation by storing an externally mutable object into AbfsTailLatencyTracker.configuration At AbfsTailLatencyTracker.java:[line 55] | | | Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.SlidingWindowHdrHistogram(long, int, int, int, int, long, int, AbfsRestOperationType) At SlidingWindowHdrHistogram.java:new org.apache.hadoop.fs.azurebfs.services.SlidingWindowHdrHistogram(long, int, int, int, int, long, int, AbfsRestOperationType) At SlidingWindowHdrHistogram.java:[line 81] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8043 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux d98c9c1604f2 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7dcac93eec4dc5a48d643ae81372c581b6c3bebf | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/2/testReport/ | | Max. process+thread count | 614 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/2/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #8043: URL: https://github.com/apache/hadoop/pull/8043#issuecomment-3449602331 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 22m 11s | | trunk passed | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 17s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 25s | | trunk passed | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 23s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 46s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) | hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 14m 13s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 22s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 43 new + 3 unchanged - 0 fixed = 46 total (was 3) | | +1 :green_heart: | mvnsite | 0m 21s | | the patch passed | | -1 :x: | javadoc | 0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 35 new + 1472 unchanged - 0 fixed = 1507 total (was 1472) | | -1 :x: | javadoc | 0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 34 new + 1413 unchanged - 0 fixed = 1447 total (was 1413) | | -1 :x: | spotbugs | 0m 45s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) | hadoop-tools/hadoop-azure generated 4 new + 177 unchanged - 1 fixed = 181 total (was 178) | | +1 :green_heart: | shadedclient | 14m 10s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 8s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 18s | | The patch does not generate ASF License warnings. | | | | 60m 12s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient) At AbfsAHCHttpOperation.java:new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient) At AbfsAHCHttpOperation.java:[line 123] | | | Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext) At AbfsRestOperation.java:org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext) At AbfsRestOperation.java:[line 533] | | | new org.apache.hadoop.fs.azurebfs.services.AbfsTailLatencyTracker(AbfsConfiguration) may expose internal representation by storing an externally mutable object into AbfsTailLatencyTracker.configuration At AbfsTailLatencyTracker.java:representation by storing an externally mutable object into AbfsTailLatencyTracker.configuration At AbfsTailLatencyTracker.java:[line 55] | | | Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.SlidingWindowHdrHistogram(long, int, int, int, int, long, int, AbfsRestOperationType) At SlidingWindowHdrHistogram.java:new org.apache.hadoop.fs.azurebfs.services.SlidingWindowHdrHistogram(long, int, int, int, int, long, int, AbfsRestOperationType) At SlidingWindowHdrHistogram.java:[line 81] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8043 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 9b5c6baa74d5 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / de244d215362fca4d8ba16b3d01a9f39a3ff0e81 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/testReport/ | | Max. process+thread count | 637 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. anmolanmol1234 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2464810768 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -1824,4 +1860,41 @@ public int getBlobRenameDirConsumptionParallelism() { public int getBlobDeleteDirConsumptionParallelism() { return blobDeleteDirConsumptionParallelism; } + + public boolean isTailLatencyTrackerEnabled() { + return isTailLatencyTrackerEnabled; + } + + public boolean isTailLatencyRequestTimeoutEnabled() { + return isTailLatencyRequestTimeoutEnabled && isTailLatencyTrackerEnabled Review Comment: first check should be for isTailLatencyTrackerEnabled anmolanmol1234 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2464820798 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -1824,4 +1860,41 @@ public int getBlobRenameDirConsumptionParallelism() { public int getBlobDeleteDirConsumptionParallelism() { return blobDeleteDirConsumptionParallelism; } + + public boolean isTailLatencyTrackerEnabled() { + return isTailLatencyTrackerEnabled; + } + + public boolean isTailLatencyRequestTimeoutEnabled() { + return isTailLatencyRequestTimeoutEnabled && isTailLatencyTrackerEnabled + && getPreferredHttpOperationType().equals(HttpOperationType.APACHE_HTTP_CLIENT); + } + + public int getTailLatencyPercentile() { + return tailLatencyPercentile; + } + + public int getTailLatencyMinDeviation() { + return tailLatencyMinDeviation; + } + + public int getTailLatencyMinSampleSize() { + return tailLatencyMinSampleSize; + } + + public int getTailLatencyAnalysisWindowInMillis() { + return tailLatencyAnalysisWindowInMillis; + } + + public int getTailLatencyPercentileComputationIntervalInMillis() { Review Comment: Name should be shortened anmolanmol1234 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2465174540 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -266,5 +266,15 @@ public final class FileSystemConfigurations { public static final boolean DEFAULT_FS_AZURE_ENABLE_CREATE_BLOB_IDEMPOTENCY = true; + public static final boolean DEFAULT_FS_AZURE_ENABLE_TAIL_LATENCY_TRACKER = false; Review Comment: Do we not want this feature to be enabled by default ? anmolanmol1234 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2465238391 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsApacheHttpClient.java: ########## @@ -143,6 +173,51 @@ public HttpResponse execute(HttpRequestBase httpRequest, return httpClient.execute(httpRequest, abfsHttpClientContext); } + /** + * Executes the HTTP request with a deadline. If the request does not complete + * within the deadline, it is aborted and an IOException is thrown. + * + * @param httpRequest HTTP request to execute. + * @param abfsHttpClientContext HttpClient context. + * @param connectTimeout Connection timeout. + * @param readTimeout Read timeout. + * @param deadlineMillis Deadline in milliseconds. + * + * @return HTTP response. + * @throws IOException network error or deadline exceeded. + */ + public HttpResponse executeWithDeadline(HttpRequestBase httpRequest, + final AbfsManagedHttpClientContext abfsHttpClientContext, + final int connectTimeout, + final int readTimeout, + final long deadlineMillis) throws IOException { + RequestConfig.Builder requestConfigBuilder = RequestConfig + .custom() + .setConnectTimeout(connectTimeout) + .setSocketTimeout(readTimeout); + httpRequest.setConfig(requestConfigBuilder.build()); + ExecutorService executor = Executors.newSingleThreadExecutor(); + Future<HttpResponse> future = executor.submit(() -> + httpClient.execute(httpRequest, abfsHttpClientContext) + ); + + try { + return future.get(deadlineMillis, TimeUnit.MILLISECONDS); + } catch (TimeoutException e) { + /* Deadline exceeded, abort the request. + * This will also kill the underlying socket exception in the HttpClient. + * Connection will be marker stale and won't be returned back to KAC for reuse. Review Comment: nit: typo marked anmolanmol1234 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2465280967 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/RetryPolicyConstants.java: ########## @@ -32,4 +32,8 @@ private RetryPolicyConstants() { * Constant for Static Retry Policy Abbreviation. {@value} */ public static final String STATIC_RETRY_POLICY_ABBREVIATION = \"S\"; + /** + * Constant for Static Retry Policy Abbreviation. {@value} + */ + public static final String TAIL_LATENCY_TIMEOUT_RETRY_POLICY_ABBREVIATION = \"T\"; Review Comment: Can we make it TL ? anmolanmol1234 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2465316539 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java: ########## @@ -0,0 +1,249 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; +import org.HdrHistogram.Histogram; +import org.HdrHistogram.Recorder; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.util.concurrent.atomic.AtomicLong; +import java.util.concurrent.locks.ReentrantLock; +import java.util.concurrent.atomic.AtomicInteger; + +import org.apache.hadoop.classification.VisibleForTesting; + +public class SlidingWindowHdrHistogram { + private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class); + + // Configuration + private final long windowSizeMillis; // Total analysis window + private final long timeSegmentDurationMillis; // Subdivision on analysis window + private final int numSegments; + private final long highestTrackableValue; + private final int significantFigures; + + // Ring buffer of immutable snapshots for completed time segments + private final Histogram[] completedSegments; + private final AtomicInteger currentIndex = new AtomicInteger(0); + + // Active Time Segment + private volatile Recorder activeSegmentRecorder; + private Histogram currentSegmentAccumulation; + private volatile long currentSegmentStartMillis; + private final AtomicLong currentTotalCount = new AtomicLong(0L); + + // Synchronization + // Writers never take locks. Readers (queries) and rotation use this lock + // to mutate currentAccumulation and ring-buffer pointers safely. + private final ReentrantLock rotateLock = new ReentrantLock(); + + // Reusable temp histograms to minimize allocations + private Histogram tmpForDelta; + private Histogram tmpForMerge; + + private final AbfsRestOperationType operationType; + + private boolean isAnalysisWindowFilled = false; + private int minSampleSize; + private int tailLatencyPercentile; + private int tailLatencyMinDeviation; + + private double p50 = 0.0; + private double p90 = 0.0; + private double p99 = 0.0; + private double tailLatency = 0.0; + private int deviation = 0; + + public SlidingWindowHdrHistogram(long windowSizeMillis, + int numberOfSegments, + int minSampleSize, + int tailLatencyPercentile, + int tailLatencyMinDeviation, + long highestTrackableValue, + int significantFigures, + final AbfsRestOperationType operationType) { + if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\"); + if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\"); Review Comment: Should be numberOfSegments in exception anmolanmol1234 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2465371723 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java: ########## @@ -0,0 +1,249 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; +import org.HdrHistogram.Histogram; +import org.HdrHistogram.Recorder; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.util.concurrent.atomic.AtomicLong; +import java.util.concurrent.locks.ReentrantLock; +import java.util.concurrent.atomic.AtomicInteger; + +import org.apache.hadoop.classification.VisibleForTesting; + +public class SlidingWindowHdrHistogram { + private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class); + + // Configuration + private final long windowSizeMillis; // Total analysis window + private final long timeSegmentDurationMillis; // Subdivision on analysis window + private final int numSegments; + private final long highestTrackableValue; + private final int significantFigures; + + // Ring buffer of immutable snapshots for completed time segments + private final Histogram[] completedSegments; + private final AtomicInteger currentIndex = new AtomicInteger(0); + + // Active Time Segment + private volatile Recorder activeSegmentRecorder; + private Histogram currentSegmentAccumulation; + private volatile long currentSegmentStartMillis; + private final AtomicLong currentTotalCount = new AtomicLong(0L); + + // Synchronization + // Writers never take locks. Readers (queries) and rotation use this lock + // to mutate currentAccumulation and ring-buffer pointers safely. + private final ReentrantLock rotateLock = new ReentrantLock(); + + // Reusable temp histograms to minimize allocations + private Histogram tmpForDelta; + private Histogram tmpForMerge; + + private final AbfsRestOperationType operationType; + + private boolean isAnalysisWindowFilled = false; + private int minSampleSize; + private int tailLatencyPercentile; + private int tailLatencyMinDeviation; + + private double p50 = 0.0; + private double p90 = 0.0; + private double p99 = 0.0; + private double tailLatency = 0.0; + private int deviation = 0; + + public SlidingWindowHdrHistogram(long windowSizeMillis, + int numberOfSegments, + int minSampleSize, + int tailLatencyPercentile, + int tailLatencyMinDeviation, + long highestTrackableValue, + int significantFigures, + final AbfsRestOperationType operationType) { + if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\"); + if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\"); + if (highestTrackableValue <= 0) throw new IllegalArgumentException(\"highestTrackableValue > 0\"); + if (significantFigures < 1 || significantFigures > 5) throw new IllegalArgumentException(\"significantFigures in [1,5]\"); + + this.windowSizeMillis = windowSizeMillis; + this.numSegments = numberOfSegments; + this.timeSegmentDurationMillis = windowSizeMillis/numberOfSegments; + this.highestTrackableValue = highestTrackableValue; + this.significantFigures = significantFigures; + this.operationType = operationType; + this.minSampleSize = minSampleSize; + this.tailLatencyPercentile = tailLatencyPercentile; + this.tailLatencyMinDeviation = tailLatencyMinDeviation; // 5ms + + this.completedSegments = new Histogram[numSegments]; + long now = System.currentTimeMillis(); + this.currentSegmentStartMillis = alignToSegmentDuration(now); + currentIndex.set(0); + this.activeSegmentRecorder = new Recorder(highestTrackableValue, significantFigures); + this.currentSegmentAccumulation = new Histogram(highestTrackableValue, significantFigures); + this.tmpForDelta = new Histogram(highestTrackableValue, significantFigures); + this.tmpForMerge = new Histogram(highestTrackableValue, significantFigures); + + LOG.debug(\"[{}] Initialized SlidingWindowHdrHistogram with WindowSize {}, TimeSegmentDur: {}, NumOfSegments: {}\", operationType, windowSizeMillis, timeSegmentDurationMillis, numSegments); + } + + /** Record a single latency value (in your chosen time unit). Thread-safe and lock-free. */ + public void recordValue(long value) { + if (value < 0 || value > highestTrackableValue) { + LOG.warn(\"[{}] Value {} outside of range [0, {}]. Ignoring\", + operationType, value, highestTrackableValue); + return; + } + activeSegmentRecorder.recordValue(value); + currentTotalCount.incrementAndGet(); + LOG.debug(\"[{}] Recorded latency value: {}. Current total count: {}\", + operationType, value, currentTotalCount.get()); + } + + /** Get any percentile over the current sliding window. */ + public void computeLatency() { + if (getCurrentTotalCount() < minSampleSize) { + LOG.debug(\"[{}] Not enough data to report percentiles. Current total count: {}\", Review Comment: We can return here itself anmolanmol1234 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2465396005 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java: ########## @@ -0,0 +1,249 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; +import org.HdrHistogram.Histogram; +import org.HdrHistogram.Recorder; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.util.concurrent.atomic.AtomicLong; +import java.util.concurrent.locks.ReentrantLock; +import java.util.concurrent.atomic.AtomicInteger; + +import org.apache.hadoop.classification.VisibleForTesting; + +public class SlidingWindowHdrHistogram { + private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class); + + // Configuration + private final long windowSizeMillis; // Total analysis window + private final long timeSegmentDurationMillis; // Subdivision on analysis window + private final int numSegments; + private final long highestTrackableValue; + private final int significantFigures; + + // Ring buffer of immutable snapshots for completed time segments + private final Histogram[] completedSegments; + private final AtomicInteger currentIndex = new AtomicInteger(0); + + // Active Time Segment + private volatile Recorder activeSegmentRecorder; + private Histogram currentSegmentAccumulation; + private volatile long currentSegmentStartMillis; + private final AtomicLong currentTotalCount = new AtomicLong(0L); + + // Synchronization + // Writers never take locks. Readers (queries) and rotation use this lock + // to mutate currentAccumulation and ring-buffer pointers safely. + private final ReentrantLock rotateLock = new ReentrantLock(); + + // Reusable temp histograms to minimize allocations + private Histogram tmpForDelta; + private Histogram tmpForMerge; + + private final AbfsRestOperationType operationType; + + private boolean isAnalysisWindowFilled = false; + private int minSampleSize; + private int tailLatencyPercentile; + private int tailLatencyMinDeviation; + + private double p50 = 0.0; + private double p90 = 0.0; + private double p99 = 0.0; + private double tailLatency = 0.0; + private int deviation = 0; + + public SlidingWindowHdrHistogram(long windowSizeMillis, + int numberOfSegments, + int minSampleSize, + int tailLatencyPercentile, + int tailLatencyMinDeviation, + long highestTrackableValue, + int significantFigures, + final AbfsRestOperationType operationType) { + if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\"); + if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\"); + if (highestTrackableValue <= 0) throw new IllegalArgumentException(\"highestTrackableValue > 0\"); + if (significantFigures < 1 || significantFigures > 5) throw new IllegalArgumentException(\"significantFigures in [1,5]\"); + + this.windowSizeMillis = windowSizeMillis; + this.numSegments = numberOfSegments; + this.timeSegmentDurationMillis = windowSizeMillis/numberOfSegments; + this.highestTrackableValue = highestTrackableValue; + this.significantFigures = significantFigures; + this.operationType = operationType; + this.minSampleSize = minSampleSize; + this.tailLatencyPercentile = tailLatencyPercentile; + this.tailLatencyMinDeviation = tailLatencyMinDeviation; // 5ms + + this.completedSegments = new Histogram[numSegments]; + long now = System.currentTimeMillis(); + this.currentSegmentStartMillis = alignToSegmentDuration(now); + currentIndex.set(0); + this.activeSegmentRecorder = new Recorder(highestTrackableValue, significantFigures); + this.currentSegmentAccumulation = new Histogram(highestTrackableValue, significantFigures); + this.tmpForDelta = new Histogram(highestTrackableValue, significantFigures); + this.tmpForMerge = new Histogram(highestTrackableValue, significantFigures); + + LOG.debug(\"[{}] Initialized SlidingWindowHdrHistogram with WindowSize {}, TimeSegmentDur: {}, NumOfSegments: {}\", operationType, windowSizeMillis, timeSegmentDurationMillis, numSegments); + } + + /** Record a single latency value (in your chosen time unit). Thread-safe and lock-free. */ + public void recordValue(long value) { + if (value < 0 || value > highestTrackableValue) { + LOG.warn(\"[{}] Value {} outside of range [0, {}]. Ignoring\", + operationType, value, highestTrackableValue); + return; + } + activeSegmentRecorder.recordValue(value); + currentTotalCount.incrementAndGet(); + LOG.debug(\"[{}] Recorded latency value: {}. Current total count: {}\", + operationType, value, currentTotalCount.get()); + } + + /** Get any percentile over the current sliding window. */ + public void computeLatency() { + if (getCurrentTotalCount() < minSampleSize) { + LOG.debug(\"[{}] Not enough data to report percentiles. Current total count: {}\", + operationType, getCurrentTotalCount()); + } else { + rotateLock.lock(); + try { + tmpForMerge.reset(); + for (int i = 0; i < numSegments; i++) { + Histogram h = completedSegments[i]; + if (h != null && h.getTotalCount() > 0) { + tmpForMerge.add(h); + } + } + + if (tmpForMerge.getTotalCount() == 0) return; + + tailLatency = tmpForMerge.getValueAtPercentile(tailLatencyPercentile); + p50 = tmpForMerge.getValueAtPercentile(50); + p90 = tmpForMerge.getValueAtPercentile(90); + p99 = tmpForMerge.getValueAtPercentile(99); + deviation = (int) ((tailLatency - p50)/p50 * 100); Review Comment: Chances of division by zero error anmolanmol1234 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2465418474 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java: ########## @@ -0,0 +1,249 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; +import org.HdrHistogram.Histogram; +import org.HdrHistogram.Recorder; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.util.concurrent.atomic.AtomicLong; +import java.util.concurrent.locks.ReentrantLock; +import java.util.concurrent.atomic.AtomicInteger; + +import org.apache.hadoop.classification.VisibleForTesting; + +public class SlidingWindowHdrHistogram { + private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class); + + // Configuration + private final long windowSizeMillis; // Total analysis window + private final long timeSegmentDurationMillis; // Subdivision on analysis window + private final int numSegments; + private final long highestTrackableValue; + private final int significantFigures; + + // Ring buffer of immutable snapshots for completed time segments + private final Histogram[] completedSegments; + private final AtomicInteger currentIndex = new AtomicInteger(0); + + // Active Time Segment + private volatile Recorder activeSegmentRecorder; + private Histogram currentSegmentAccumulation; + private volatile long currentSegmentStartMillis; + private final AtomicLong currentTotalCount = new AtomicLong(0L); + + // Synchronization + // Writers never take locks. Readers (queries) and rotation use this lock + // to mutate currentAccumulation and ring-buffer pointers safely. + private final ReentrantLock rotateLock = new ReentrantLock(); + + // Reusable temp histograms to minimize allocations + private Histogram tmpForDelta; + private Histogram tmpForMerge; + + private final AbfsRestOperationType operationType; + + private boolean isAnalysisWindowFilled = false; + private int minSampleSize; + private int tailLatencyPercentile; + private int tailLatencyMinDeviation; + + private double p50 = 0.0; + private double p90 = 0.0; + private double p99 = 0.0; + private double tailLatency = 0.0; + private int deviation = 0; + + public SlidingWindowHdrHistogram(long windowSizeMillis, + int numberOfSegments, + int minSampleSize, + int tailLatencyPercentile, + int tailLatencyMinDeviation, + long highestTrackableValue, + int significantFigures, + final AbfsRestOperationType operationType) { + if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\"); + if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\"); + if (highestTrackableValue <= 0) throw new IllegalArgumentException(\"highestTrackableValue > 0\"); + if (significantFigures < 1 || significantFigures > 5) throw new IllegalArgumentException(\"significantFigures in [1,5]\"); + + this.windowSizeMillis = windowSizeMillis; + this.numSegments = numberOfSegments; + this.timeSegmentDurationMillis = windowSizeMillis/numberOfSegments; + this.highestTrackableValue = highestTrackableValue; + this.significantFigures = significantFigures; + this.operationType = operationType; + this.minSampleSize = minSampleSize; + this.tailLatencyPercentile = tailLatencyPercentile; + this.tailLatencyMinDeviation = tailLatencyMinDeviation; // 5ms + + this.completedSegments = new Histogram[numSegments]; + long now = System.currentTimeMillis(); + this.currentSegmentStartMillis = alignToSegmentDuration(now); + currentIndex.set(0); + this.activeSegmentRecorder = new Recorder(highestTrackableValue, significantFigures); + this.currentSegmentAccumulation = new Histogram(highestTrackableValue, significantFigures); + this.tmpForDelta = new Histogram(highestTrackableValue, significantFigures); + this.tmpForMerge = new Histogram(highestTrackableValue, significantFigures); + + LOG.debug(\"[{}] Initialized SlidingWindowHdrHistogram with WindowSize {}, TimeSegmentDur: {}, NumOfSegments: {}\", operationType, windowSizeMillis, timeSegmentDurationMillis, numSegments); + } + + /** Record a single latency value (in your chosen time unit). Thread-safe and lock-free. */ + public void recordValue(long value) { + if (value < 0 || value > highestTrackableValue) { + LOG.warn(\"[{}] Value {} outside of range [0, {}]. Ignoring\", + operationType, value, highestTrackableValue); + return; + } + activeSegmentRecorder.recordValue(value); + currentTotalCount.incrementAndGet(); + LOG.debug(\"[{}] Recorded latency value: {}. Current total count: {}\", + operationType, value, currentTotalCount.get()); + } + + /** Get any percentile over the current sliding window. */ + public void computeLatency() { + if (getCurrentTotalCount() < minSampleSize) { + LOG.debug(\"[{}] Not enough data to report percentiles. Current total count: {}\", + operationType, getCurrentTotalCount()); + } else { + rotateLock.lock(); + try { + tmpForMerge.reset(); + for (int i = 0; i < numSegments; i++) { + Histogram h = completedSegments[i]; + if (h != null && h.getTotalCount() > 0) { + tmpForMerge.add(h); + } + } + + if (tmpForMerge.getTotalCount() == 0) return; + + tailLatency = tmpForMerge.getValueAtPercentile(tailLatencyPercentile); + p50 = tmpForMerge.getValueAtPercentile(50); + p90 = tmpForMerge.getValueAtPercentile(90); + p99 = tmpForMerge.getValueAtPercentile(99); + deviation = (int) ((tailLatency - p50)/p50 * 100); + } finally { + rotateLock.unlock(); + } + } + LOG.debug(\"[{}] Computed Latencies. p50: {}, p90: {}, p99: {}, tailLatency: {}, deviation with p50: {} Current total count: {}\", + operationType, p50, p90, p99, tailLatency, deviation, getCurrentTotalCount()); + } + + private long alignToSegmentDuration(long timeMs) { + return timeMs - (timeMs % timeSegmentDurationMillis); + } + + /** Ensure active bucket is aligned to current time; rotate if we've crossed a boundary. */ + public void rotateIfNeeded() { + LOG.debug(\"[{}] Triggering Histogram Rotation\", operationType); + long now = System.currentTimeMillis(); + long expectedStart = alignToSegmentDuration(now); + if (expectedStart == currentSegmentStartMillis) { + LOG.debug(\"[{}] Current Time Segment Still Active at {}. Skipping Rotation\", operationType, expectedStart); + return; // still current + } + + rotateLock.lock(); + try { + // Re-check inside lock + now = System.currentTimeMillis(); + expectedStart = alignToSegmentDuration(now); + if (expectedStart == currentSegmentStartMillis) return; + + // Finalize the current bucket: + // Pull any remaining deltas from active recorder and add to currentAccumulation + tmpForDelta.reset(); + activeSegmentRecorder.getIntervalHistogramInto(tmpForDelta); + currentSegmentAccumulation.add(tmpForDelta); + + if (currentSegmentAccumulation.getTotalCount() <= 0) { Review Comment: Is less than 0 possible for total count ? It is increment always right anmolanmol1234 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2465418474 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java: ########## @@ -0,0 +1,249 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; +import org.HdrHistogram.Histogram; +import org.HdrHistogram.Recorder; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.util.concurrent.atomic.AtomicLong; +import java.util.concurrent.locks.ReentrantLock; +import java.util.concurrent.atomic.AtomicInteger; + +import org.apache.hadoop.classification.VisibleForTesting; + +public class SlidingWindowHdrHistogram { + private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class); + + // Configuration + private final long windowSizeMillis; // Total analysis window + private final long timeSegmentDurationMillis; // Subdivision on analysis window + private final int numSegments; + private final long highestTrackableValue; + private final int significantFigures; + + // Ring buffer of immutable snapshots for completed time segments + private final Histogram[] completedSegments; + private final AtomicInteger currentIndex = new AtomicInteger(0); + + // Active Time Segment + private volatile Recorder activeSegmentRecorder; + private Histogram currentSegmentAccumulation; + private volatile long currentSegmentStartMillis; + private final AtomicLong currentTotalCount = new AtomicLong(0L); + + // Synchronization + // Writers never take locks. Readers (queries) and rotation use this lock + // to mutate currentAccumulation and ring-buffer pointers safely. + private final ReentrantLock rotateLock = new ReentrantLock(); + + // Reusable temp histograms to minimize allocations + private Histogram tmpForDelta; + private Histogram tmpForMerge; + + private final AbfsRestOperationType operationType; + + private boolean isAnalysisWindowFilled = false; + private int minSampleSize; + private int tailLatencyPercentile; + private int tailLatencyMinDeviation; + + private double p50 = 0.0; + private double p90 = 0.0; + private double p99 = 0.0; + private double tailLatency = 0.0; + private int deviation = 0; + + public SlidingWindowHdrHistogram(long windowSizeMillis, + int numberOfSegments, + int minSampleSize, + int tailLatencyPercentile, + int tailLatencyMinDeviation, + long highestTrackableValue, + int significantFigures, + final AbfsRestOperationType operationType) { + if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\"); + if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\"); + if (highestTrackableValue <= 0) throw new IllegalArgumentException(\"highestTrackableValue > 0\"); + if (significantFigures < 1 || significantFigures > 5) throw new IllegalArgumentException(\"significantFigures in [1,5]\"); + + this.windowSizeMillis = windowSizeMillis; + this.numSegments = numberOfSegments; + this.timeSegmentDurationMillis = windowSizeMillis/numberOfSegments; + this.highestTrackableValue = highestTrackableValue; + this.significantFigures = significantFigures; + this.operationType = operationType; + this.minSampleSize = minSampleSize; + this.tailLatencyPercentile = tailLatencyPercentile; + this.tailLatencyMinDeviation = tailLatencyMinDeviation; // 5ms + + this.completedSegments = new Histogram[numSegments]; + long now = System.currentTimeMillis(); + this.currentSegmentStartMillis = alignToSegmentDuration(now); + currentIndex.set(0); + this.activeSegmentRecorder = new Recorder(highestTrackableValue, significantFigures); + this.currentSegmentAccumulation = new Histogram(highestTrackableValue, significantFigures); + this.tmpForDelta = new Histogram(highestTrackableValue, significantFigures); + this.tmpForMerge = new Histogram(highestTrackableValue, significantFigures); + + LOG.debug(\"[{}] Initialized SlidingWindowHdrHistogram with WindowSize {}, TimeSegmentDur: {}, NumOfSegments: {}\", operationType, windowSizeMillis, timeSegmentDurationMillis, numSegments); + } + + /** Record a single latency value (in your chosen time unit). Thread-safe and lock-free. */ + public void recordValue(long value) { + if (value < 0 || value > highestTrackableValue) { + LOG.warn(\"[{}] Value {} outside of range [0, {}]. Ignoring\", + operationType, value, highestTrackableValue); + return; + } + activeSegmentRecorder.recordValue(value); + currentTotalCount.incrementAndGet(); + LOG.debug(\"[{}] Recorded latency value: {}. Current total count: {}\", + operationType, value, currentTotalCount.get()); + } + + /** Get any percentile over the current sliding window. */ + public void computeLatency() { + if (getCurrentTotalCount() < minSampleSize) { + LOG.debug(\"[{}] Not enough data to report percentiles. Current total count: {}\", + operationType, getCurrentTotalCount()); + } else { + rotateLock.lock(); + try { + tmpForMerge.reset(); + for (int i = 0; i < numSegments; i++) { + Histogram h = completedSegments[i]; + if (h != null && h.getTotalCount() > 0) { + tmpForMerge.add(h); + } + } + + if (tmpForMerge.getTotalCount() == 0) return; + + tailLatency = tmpForMerge.getValueAtPercentile(tailLatencyPercentile); + p50 = tmpForMerge.getValueAtPercentile(50); + p90 = tmpForMerge.getValueAtPercentile(90); + p99 = tmpForMerge.getValueAtPercentile(99); + deviation = (int) ((tailLatency - p50)/p50 * 100); + } finally { + rotateLock.unlock(); + } + } + LOG.debug(\"[{}] Computed Latencies. p50: {}, p90: {}, p99: {}, tailLatency: {}, deviation with p50: {} Current total count: {}\", + operationType, p50, p90, p99, tailLatency, deviation, getCurrentTotalCount()); + } + + private long alignToSegmentDuration(long timeMs) { + return timeMs - (timeMs % timeSegmentDurationMillis); + } + + /** Ensure active bucket is aligned to current time; rotate if we've crossed a boundary. */ + public void rotateIfNeeded() { + LOG.debug(\"[{}] Triggering Histogram Rotation\", operationType); + long now = System.currentTimeMillis(); + long expectedStart = alignToSegmentDuration(now); + if (expectedStart == currentSegmentStartMillis) { + LOG.debug(\"[{}] Current Time Segment Still Active at {}. Skipping Rotation\", operationType, expectedStart); + return; // still current + } + + rotateLock.lock(); + try { + // Re-check inside lock + now = System.currentTimeMillis(); + expectedStart = alignToSegmentDuration(now); + if (expectedStart == currentSegmentStartMillis) return; + + // Finalize the current bucket: + // Pull any remaining deltas from active recorder and add to currentAccumulation + tmpForDelta.reset(); + activeSegmentRecorder.getIntervalHistogramInto(tmpForDelta); + currentSegmentAccumulation.add(tmpForDelta); + + if (currentSegmentAccumulation.getTotalCount() <= 0) { Review Comment: Is less than 0 possible for total count? It is incremented always right anmolanmol1234 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2465446936 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java: ########## @@ -0,0 +1,249 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; +import org.HdrHistogram.Histogram; +import org.HdrHistogram.Recorder; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.util.concurrent.atomic.AtomicLong; +import java.util.concurrent.locks.ReentrantLock; +import java.util.concurrent.atomic.AtomicInteger; + +import org.apache.hadoop.classification.VisibleForTesting; + +public class SlidingWindowHdrHistogram { + private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class); + + // Configuration + private final long windowSizeMillis; // Total analysis window + private final long timeSegmentDurationMillis; // Subdivision on analysis window + private final int numSegments; + private final long highestTrackableValue; + private final int significantFigures; + + // Ring buffer of immutable snapshots for completed time segments + private final Histogram[] completedSegments; + private final AtomicInteger currentIndex = new AtomicInteger(0); + + // Active Time Segment + private volatile Recorder activeSegmentRecorder; + private Histogram currentSegmentAccumulation; + private volatile long currentSegmentStartMillis; + private final AtomicLong currentTotalCount = new AtomicLong(0L); + + // Synchronization + // Writers never take locks. Readers (queries) and rotation use this lock + // to mutate currentAccumulation and ring-buffer pointers safely. + private final ReentrantLock rotateLock = new ReentrantLock(); + + // Reusable temp histograms to minimize allocations + private Histogram tmpForDelta; + private Histogram tmpForMerge; + + private final AbfsRestOperationType operationType; + + private boolean isAnalysisWindowFilled = false; + private int minSampleSize; + private int tailLatencyPercentile; + private int tailLatencyMinDeviation; + + private double p50 = 0.0; + private double p90 = 0.0; + private double p99 = 0.0; + private double tailLatency = 0.0; + private int deviation = 0; + + public SlidingWindowHdrHistogram(long windowSizeMillis, + int numberOfSegments, + int minSampleSize, + int tailLatencyPercentile, + int tailLatencyMinDeviation, + long highestTrackableValue, + int significantFigures, + final AbfsRestOperationType operationType) { + if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\"); + if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\"); + if (highestTrackableValue <= 0) throw new IllegalArgumentException(\"highestTrackableValue > 0\"); + if (significantFigures < 1 || significantFigures > 5) throw new IllegalArgumentException(\"significantFigures in [1,5]\"); + + this.windowSizeMillis = windowSizeMillis; + this.numSegments = numberOfSegments; + this.timeSegmentDurationMillis = windowSizeMillis/numberOfSegments; + this.highestTrackableValue = highestTrackableValue; + this.significantFigures = significantFigures; + this.operationType = operationType; + this.minSampleSize = minSampleSize; + this.tailLatencyPercentile = tailLatencyPercentile; + this.tailLatencyMinDeviation = tailLatencyMinDeviation; // 5ms + + this.completedSegments = new Histogram[numSegments]; + long now = System.currentTimeMillis(); + this.currentSegmentStartMillis = alignToSegmentDuration(now); + currentIndex.set(0); + this.activeSegmentRecorder = new Recorder(highestTrackableValue, significantFigures); + this.currentSegmentAccumulation = new Histogram(highestTrackableValue, significantFigures); + this.tmpForDelta = new Histogram(highestTrackableValue, significantFigures); + this.tmpForMerge = new Histogram(highestTrackableValue, significantFigures); + + LOG.debug(\"[{}] Initialized SlidingWindowHdrHistogram with WindowSize {}, TimeSegmentDur: {}, NumOfSegments: {}\", operationType, windowSizeMillis, timeSegmentDurationMillis, numSegments); + } + + /** Record a single latency value (in your chosen time unit). Thread-safe and lock-free. */ + public void recordValue(long value) { + if (value < 0 || value > highestTrackableValue) { + LOG.warn(\"[{}] Value {} outside of range [0, {}]. Ignoring\", + operationType, value, highestTrackableValue); + return; + } + activeSegmentRecorder.recordValue(value); + currentTotalCount.incrementAndGet(); + LOG.debug(\"[{}] Recorded latency value: {}. Current total count: {}\", + operationType, value, currentTotalCount.get()); + } + + /** Get any percentile over the current sliding window. */ + public void computeLatency() { + if (getCurrentTotalCount() < minSampleSize) { + LOG.debug(\"[{}] Not enough data to report percentiles. Current total count: {}\", + operationType, getCurrentTotalCount()); + } else { + rotateLock.lock(); + try { + tmpForMerge.reset(); + for (int i = 0; i < numSegments; i++) { + Histogram h = completedSegments[i]; + if (h != null && h.getTotalCount() > 0) { + tmpForMerge.add(h); + } + } + + if (tmpForMerge.getTotalCount() == 0) return; + + tailLatency = tmpForMerge.getValueAtPercentile(tailLatencyPercentile); + p50 = tmpForMerge.getValueAtPercentile(50); + p90 = tmpForMerge.getValueAtPercentile(90); + p99 = tmpForMerge.getValueAtPercentile(99); + deviation = (int) ((tailLatency - p50)/p50 * 100); + } finally { + rotateLock.unlock(); + } + } + LOG.debug(\"[{}] Computed Latencies. p50: {}, p90: {}, p99: {}, tailLatency: {}, deviation with p50: {} Current total count: {}\", + operationType, p50, p90, p99, tailLatency, deviation, getCurrentTotalCount()); + } + + private long alignToSegmentDuration(long timeMs) { + return timeMs - (timeMs % timeSegmentDurationMillis); + } + + /** Ensure active bucket is aligned to current time; rotate if we've crossed a boundary. */ + public void rotateIfNeeded() { + LOG.debug(\"[{}] Triggering Histogram Rotation\", operationType); + long now = System.currentTimeMillis(); + long expectedStart = alignToSegmentDuration(now); + if (expectedStart == currentSegmentStartMillis) { + LOG.debug(\"[{}] Current Time Segment Still Active at {}. Skipping Rotation\", operationType, expectedStart); + return; // still current + } + + rotateLock.lock(); + try { + // Re-check inside lock + now = System.currentTimeMillis(); + expectedStart = alignToSegmentDuration(now); + if (expectedStart == currentSegmentStartMillis) return; + + // Finalize the current bucket: + // Pull any remaining deltas from active recorder and add to currentAccumulation + tmpForDelta.reset(); + activeSegmentRecorder.getIntervalHistogramInto(tmpForDelta); + currentSegmentAccumulation.add(tmpForDelta); + + if (currentSegmentAccumulation.getTotalCount() <= 0) { + currentSegmentStartMillis = alignToSegmentDuration(System.currentTimeMillis()); + LOG.debug(\"[{}] No data recorded in current time segment at {}. Skipping Rotation. Current Index is {}.\", + operationType, currentSegmentStartMillis, currentIndex.get()); + return; + } + + LOG.debug(\"[{}] Rotating current segment with total count {} into slot {}\", + operationType, currentSegmentAccumulation.getTotalCount(), currentIndex.get()); + + // Place the finished currentAccumulation into the ring buffer slot ahead. + int currentIdx = (currentIndex.getAndIncrement()) % numSegments; + // Next slot is now going to be eradicated. Remove its count from total. + currentTotalCount.set(currentTotalCount.get() - (completedSegments[currentIdx] == null ? 0 : completedSegments[currentIdx].getTotalCount())); + // Store an immutable snapshot (make sure we don't mutate the instance after storing) + completedSegments[currentIdx] = currentSegmentAccumulation; Review Comment: how are we making sure that this is immutable after this point ? completedSegments[currentIdx] = currentSegmentAccumulation.copy(); ideally we should create a deep copy of the histogram data so that even if currentSegmentAccumulation is reused or reset for the next segment, the data in completedSegments remains unchanged. anmolanmol1234 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2465452348 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java: ########## @@ -0,0 +1,249 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; +import org.HdrHistogram.Histogram; +import org.HdrHistogram.Recorder; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.util.concurrent.atomic.AtomicLong; +import java.util.concurrent.locks.ReentrantLock; +import java.util.concurrent.atomic.AtomicInteger; + +import org.apache.hadoop.classification.VisibleForTesting; + +public class SlidingWindowHdrHistogram { + private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class); + + // Configuration + private final long windowSizeMillis; // Total analysis window + private final long timeSegmentDurationMillis; // Subdivision on analysis window + private final int numSegments; + private final long highestTrackableValue; + private final int significantFigures; + + // Ring buffer of immutable snapshots for completed time segments + private final Histogram[] completedSegments; + private final AtomicInteger currentIndex = new AtomicInteger(0); + + // Active Time Segment + private volatile Recorder activeSegmentRecorder; + private Histogram currentSegmentAccumulation; + private volatile long currentSegmentStartMillis; + private final AtomicLong currentTotalCount = new AtomicLong(0L); + + // Synchronization + // Writers never take locks. Readers (queries) and rotation use this lock + // to mutate currentAccumulation and ring-buffer pointers safely. + private final ReentrantLock rotateLock = new ReentrantLock(); + + // Reusable temp histograms to minimize allocations + private Histogram tmpForDelta; + private Histogram tmpForMerge; + + private final AbfsRestOperationType operationType; + + private boolean isAnalysisWindowFilled = false; + private int minSampleSize; + private int tailLatencyPercentile; + private int tailLatencyMinDeviation; + + private double p50 = 0.0; + private double p90 = 0.0; + private double p99 = 0.0; + private double tailLatency = 0.0; + private int deviation = 0; + + public SlidingWindowHdrHistogram(long windowSizeMillis, + int numberOfSegments, + int minSampleSize, + int tailLatencyPercentile, + int tailLatencyMinDeviation, + long highestTrackableValue, + int significantFigures, + final AbfsRestOperationType operationType) { + if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\"); + if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\"); + if (highestTrackableValue <= 0) throw new IllegalArgumentException(\"highestTrackableValue > 0\"); + if (significantFigures < 1 || significantFigures > 5) throw new IllegalArgumentException(\"significantFigures in [1,5]\"); + + this.windowSizeMillis = windowSizeMillis; + this.numSegments = numberOfSegments; + this.timeSegmentDurationMillis = windowSizeMillis/numberOfSegments; + this.highestTrackableValue = highestTrackableValue; + this.significantFigures = significantFigures; + this.operationType = operationType; + this.minSampleSize = minSampleSize; + this.tailLatencyPercentile = tailLatencyPercentile; + this.tailLatencyMinDeviation = tailLatencyMinDeviation; // 5ms + + this.completedSegments = new Histogram[numSegments]; + long now = System.currentTimeMillis(); + this.currentSegmentStartMillis = alignToSegmentDuration(now); + currentIndex.set(0); + this.activeSegmentRecorder = new Recorder(highestTrackableValue, significantFigures); + this.currentSegmentAccumulation = new Histogram(highestTrackableValue, significantFigures); + this.tmpForDelta = new Histogram(highestTrackableValue, significantFigures); + this.tmpForMerge = new Histogram(highestTrackableValue, significantFigures); + + LOG.debug(\"[{}] Initialized SlidingWindowHdrHistogram with WindowSize {}, TimeSegmentDur: {}, NumOfSegments: {}\", operationType, windowSizeMillis, timeSegmentDurationMillis, numSegments); + } + + /** Record a single latency value (in your chosen time unit). Thread-safe and lock-free. */ + public void recordValue(long value) { + if (value < 0 || value > highestTrackableValue) { + LOG.warn(\"[{}] Value {} outside of range [0, {}]. Ignoring\", + operationType, value, highestTrackableValue); + return; + } + activeSegmentRecorder.recordValue(value); + currentTotalCount.incrementAndGet(); + LOG.debug(\"[{}] Recorded latency value: {}. Current total count: {}\", + operationType, value, currentTotalCount.get()); + } + + /** Get any percentile over the current sliding window. */ + public void computeLatency() { + if (getCurrentTotalCount() < minSampleSize) { + LOG.debug(\"[{}] Not enough data to report percentiles. Current total count: {}\", + operationType, getCurrentTotalCount()); + } else { + rotateLock.lock(); + try { + tmpForMerge.reset(); + for (int i = 0; i < numSegments; i++) { + Histogram h = completedSegments[i]; + if (h != null && h.getTotalCount() > 0) { + tmpForMerge.add(h); + } + } + + if (tmpForMerge.getTotalCount() == 0) return; + + tailLatency = tmpForMerge.getValueAtPercentile(tailLatencyPercentile); + p50 = tmpForMerge.getValueAtPercentile(50); + p90 = tmpForMerge.getValueAtPercentile(90); + p99 = tmpForMerge.getValueAtPercentile(99); + deviation = (int) ((tailLatency - p50)/p50 * 100); + } finally { + rotateLock.unlock(); + } + } + LOG.debug(\"[{}] Computed Latencies. p50: {}, p90: {}, p99: {}, tailLatency: {}, deviation with p50: {} Current total count: {}\", + operationType, p50, p90, p99, tailLatency, deviation, getCurrentTotalCount()); + } + + private long alignToSegmentDuration(long timeMs) { + return timeMs - (timeMs % timeSegmentDurationMillis); + } + + /** Ensure active bucket is aligned to current time; rotate if we've crossed a boundary. */ + public void rotateIfNeeded() { + LOG.debug(\"[{}] Triggering Histogram Rotation\", operationType); + long now = System.currentTimeMillis(); Review Comment: what is the use of the variable now ? We can directly use System.currentTimeMillis(); in expectedStart as we are doing later in line 195 anmolanmol1234 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2465506034 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -114,6 +116,7 @@ public class AbfsRestOperation { */ private String failureReason; private AbfsRetryPolicy retryPolicy; + private boolean shouldTailLatencyTimeout = true; Review Comment: Can be renamed to enableTailLatencyTimeout anmolanmol1234 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2465510352 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -531,6 +544,12 @@ private boolean executeHttpOperation(final int retryCount, if (shouldUpdateCSTMetrics(statusCode) && !wasKnownExceptionThrown) { intercept.updateMetrics(operationType, httpOperation); } + + // Update Tail Latency Tracker only for successful requests. + if (tailLatencyTracker != null && statusCode < HttpURLConnection.HTTP_MULT_CHOICE) { Review Comment: Will get updated for -1 status code as well, should be checked between 200 to 300 anmolanmol1234 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2465561509 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestSlidingWindowHdrHistogram.java: ########## @@ -0,0 +1,162 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.junit.jupiter.api.Test; + +import static org.assertj.core.api.Assertions.assertThat; + +public class TestSlidingWindowHdrHistogram { + + @Test + public void testSlidingWindowHdrHistogram() throws Exception { + SlidingWindowHdrHistogram histogram = new SlidingWindowHdrHistogram( Review Comment: add comment for which value represents what anmolanmol1234 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2465576049 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestSlidingWindowHdrHistogram.java: ########## @@ -0,0 +1,162 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.junit.jupiter.api.Test; + +import static org.assertj.core.api.Assertions.assertThat; + +public class TestSlidingWindowHdrHistogram { + + @Test + public void testSlidingWindowHdrHistogram() throws Exception { + SlidingWindowHdrHistogram histogram = new SlidingWindowHdrHistogram( + 100, + 5, + 7, + 99, + 0, + 100, + 3, + AbfsRestOperationType.GetPathStatus); + + // Verify that the histogram is created successfully with default values and + // do not report any percentiles + assertThat(histogram).isNotNull(); + assertThat(histogram.getCurrentTotalCount()).isEqualTo(0); + assertThat(histogram.getCurrentIndex()).isEqualTo(0); + assertThat(histogram.getP50()).isEqualTo(0.0); + assertThat(histogram.getTailLatency()).isEqualTo(0.0); + + // Verify that recording values works as expected + addAndRotate(histogram, 10, 5); // Add 5 values of 10 + assertThat(histogram.getCurrentTotalCount()).isEqualTo(5); + + // Verify that percentiles are not computed with insufficient samples + assertThat(histogram.getP50()).isEqualTo(0.0); + assertThat(histogram.getTailLatency()).isEqualTo(0.0); + + // Record more values to exceed the minimum sample size + addAndRotate(histogram, 20, 5); // Add 5 values of 20 + + // Verify that percentiles are now computed but tail Latency is still not reported + assertThat(histogram.getP50()).isGreaterThan(0.0); + assertThat(histogram.getTailLatency()).isEqualTo(0.0); + + // Record more values and rotate histogram to fill whole analysis window + addAndRotate(histogram, 30, 5); // Add 5 values of 30 + assertThat(histogram.getCurrentTotalCount()).isEqualTo(15); + + // Verify that analysis window is not full until full rotation. + assertThat(histogram.isAnalysisWindowFilled()).isFalse(); + + addAndRotate(histogram, 60, 5); // Add 5 values of 60 + assertThat(histogram.getCurrentTotalCount()).isEqualTo(20); + + // Verify that analysis window is not full until full rotation. + assertThat(histogram.isAnalysisWindowFilled()).isFalse(); + + // Verify that rotation is skipped if nothing new recorded and hence window not filled + addAndRotate(histogram, 100, 0); // No new values added + assertThat(histogram.isAnalysisWindowFilled()).isFalse(); + + // Verify that rotation does not happen if analysis window is not filled + histogram.rotateIfNeeded(); + assertThat(histogram.isAnalysisWindowFilled()).isFalse(); + + addAndRotate(histogram, 80, 5); // Add 5 values of 80 + assertThat(histogram.getCurrentTotalCount()).isEqualTo(25); + + // Verify that analysis window is full after full rotation. + assertThat(histogram.isAnalysisWindowFilled()).isTrue(); + + // Verify that percentiles and tail latency are computed + assertThat(histogram.getP50()).isGreaterThan(0.0); + assertThat(histogram.getTailLatency()).isGreaterThan(0.0); + + // Verify that sliding window works. Old values should be evicted + addAndRotate(histogram, 90, 3); // Add 3 values of 90 + assertThat(histogram.getCurrentTotalCount()).isEqualTo(23); + assertThat(histogram.isAnalysisWindowFilled()).isTrue(); + } + + @Test + public void testMinDeviationRequirementNotMet() throws Exception { + SlidingWindowHdrHistogram histogram = new SlidingWindowHdrHistogram( + 100, + 5, + 7, + 99, + 100, + 100, + 3, + AbfsRestOperationType.GetPathStatus); + + // Add values with low deviation + addAndRotate(histogram, 50, 5); // Add 5 values of 50 + addAndRotate(histogram, 51, 5); // Add 5 values of 52 + addAndRotate(histogram, 52, 5); // Add 5 values of 51 + addAndRotate(histogram, 80, 5); // Add 5 values of 53 + addAndRotate(histogram, 90, 5); // Add 5 values of 50 + + // Verify that analysis window is full after full rotation. + assertThat(histogram.isAnalysisWindowFilled()).isTrue(); + + // Verify that percentiles are not computed due to low deviation + assertThat(histogram.getP50()).isGreaterThan(0.0); + assertThat(histogram.getTailLatency()).isEqualTo(0.0); + } + + @Test + public void testMinDeviationRequirementMet() throws Exception { + SlidingWindowHdrHistogram histogram = new SlidingWindowHdrHistogram( + 100, + 5, + 7, + 99, + 50, + 100, + 3, + AbfsRestOperationType.GetPathStatus); + + // Add values with low deviation + addAndRotate(histogram, 50, 5); // Add 5 values of 50 + addAndRotate(histogram, 51, 5); // Add 5 values of 52 + addAndRotate(histogram, 52, 5); // Add 5 values of 51 + addAndRotate(histogram, 80, 5); // Add 5 values of 53 + addAndRotate(histogram, 90, 5); // Add 5 values of 50 + + // Verify that analysis window is full after full rotation. + assertThat(histogram.isAnalysisWindowFilled()).isTrue(); + + // Verify that percentiles are not computed due to low deviation Review Comment: nit: should be computed ? bhattmanish98 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2469106859 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -266,5 +266,15 @@ public final class FileSystemConfigurations { public static final boolean DEFAULT_FS_AZURE_ENABLE_CREATE_BLOB_IDEMPOTENCY = true; + public static final boolean DEFAULT_FS_AZURE_ENABLE_TAIL_LATENCY_TRACKER = false; + public static final boolean DEFAULT_FS_AZURE_ENABLE_TAIL_LATENCY_REQUEST_TIMEOUT = false; + public static final int DEFAULT_FS_AZURE_TAIL_LATENCY_PERCENTILE = 99; Review Comment: shouldn't it be float/double instead of int? Tomorrow we can change default percentile to 99.9 or 99.99. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/exceptions/TailLatencyRequestTimeoutException.java: ########## @@ -0,0 +1,39 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * <p> + * http://www.apache.org/licenses/LICENSE-2.0 + * <p> + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.contracts.exceptions; + +import java.util.concurrent.TimeoutException; +import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_TAIL_LATENCY_REQUEST_TIMEOUT; + +/** + * Thrown when a request takes more time than the current reported tail latency. + */ +public class TailLatencyRequestTimeoutException extends AzureBlobFileSystemException { + + /** + * Constructs a TailLatencyRequestTimeoutException with TimeoutException as the cause. + */ + public TailLatencyRequestTimeoutException(TimeoutException innerException) { Review Comment: @param missing in the java doc ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsApacheHttpClient.java: ########## @@ -132,6 +138,30 @@ public void close() throws IOException { * @throws IOException network error. */ public HttpResponse execute(HttpRequestBase httpRequest, + final AbfsManagedHttpClientContext abfsHttpClientContext, + final int connectTimeout, + final int readTimeout, + final long tailLatencyTimeout) throws IOException { + if (tailLatencyTimeout <= 0) { + return executeWithoutDeadline(httpRequest, abfsHttpClientContext, + connectTimeout, readTimeout); + } + return executeWithDeadline(httpRequest, abfsHttpClientContext, + connectTimeout, readTimeout, tailLatencyTimeout); + } + + /** + * Executes the HTTP request. + * + * @param httpRequest HTTP request to execute. + * @param abfsHttpClientContext HttpClient context. + * @param connectTimeout Connection timeout. + * @param readTimeout Read timeout. + * + * @return HTTP response. + * @throws IOException network error. + */ + public HttpResponse executeWithoutDeadline(HttpRequestBase httpRequest, Review Comment: executeWithoutDeadline and executeWithDeadline can be private methods. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/exceptions/TailLatencyRequestTimeoutException.java: ########## @@ -0,0 +1,39 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * <p> + * http://www.apache.org/licenses/LICENSE-2.0 + * <p> + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.contracts.exceptions; + +import java.util.concurrent.TimeoutException; +import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_TAIL_LATENCY_REQUEST_TIMEOUT; + +/** + * Thrown when a request takes more time than the current reported tail latency. + */ +public class TailLatencyRequestTimeoutException extends AzureBlobFileSystemException { + + /** + * Constructs a TailLatencyRequestTimeoutException with TimeoutException as the cause. + */ + public TailLatencyRequestTimeoutException(TimeoutException innerException) { + super(ERR_TAIL_LATENCY_REQUEST_TIMEOUT, innerException); + } + + public TailLatencyRequestTimeoutException() { Review Comment: Java doc missing for this ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsTailLatencyTracker.java: ########## @@ -0,0 +1,159 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import java.util.HashMap; +import java.util.Map; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.locks.ReentrantLock; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; + +/** + * Account Specific Latency Tracker. + * This class tracks the latency of various operations like read, write etc for a single account. + * It maintains a sliding window histogram for each operation type to analyze latency patterns over time. + */ +public class AbfsTailLatencyTracker { + + private static final Logger LOG = LoggerFactory.getLogger( + AbfsTailLatencyTracker.class); + private static AbfsTailLatencyTracker singleton; Review Comment: We can rename this variable to something which is more relevant. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsTailLatencyTracker.java: ########## @@ -0,0 +1,159 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import java.util.HashMap; +import java.util.Map; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.locks.ReentrantLock; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; + +/** + * Account Specific Latency Tracker. + * This class tracks the latency of various operations like read, write etc for a single account. + * It maintains a sliding window histogram for each operation type to analyze latency patterns over time. + */ +public class AbfsTailLatencyTracker { + + private static final Logger LOG = LoggerFactory.getLogger( + AbfsTailLatencyTracker.class); + private static AbfsTailLatencyTracker singleton; + private static final ReentrantLock LOCK = new ReentrantLock(); + private static final int HISTOGRAM_MAX_VALUE = 60_000; + private static final int HISTOGRAM_SIGNIFICANT_FIGURES = 3; + private final Map<AbfsRestOperationType, SlidingWindowHdrHistogram> + operationLatencyMap = new HashMap<>(); + private final AbfsConfiguration configuration; + + /** + * Constructor to initialize the latency tracker with configuration. + * @param abfsConfiguration Configuration settings for latency tracking. + */ + public AbfsTailLatencyTracker(AbfsConfiguration abfsConfiguration) { + this.configuration = abfsConfiguration; + ScheduledExecutorService histogramRotatorThread = Executors.newSingleThreadScheduledExecutor( + r -> { + Thread t = new Thread(r, \"Histogram-Rotator-Thread\"); + t.setDaemon(true); + return t; + }); + long rotationInterval = configuration.getTailLatencyAnalysisWindowInMillis() + / configuration.getTailLatencyAnalysisWindowGranularity(); + histogramRotatorThread.scheduleAtFixedRate(this::rotateHistograms, + rotationInterval, rotationInterval, TimeUnit.MILLISECONDS); + + + ScheduledExecutorService tailLatencyComputationThread = Executors.newSingleThreadScheduledExecutor( Review Comment: We should close this thread pool and one below once the use is done or at least during filesystem close. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -611,10 +630,30 @@ AbfsJdkHttpOperation createAbfsHttpOperation() throws IOException { @VisibleForTesting AbfsAHCHttpOperation createAbfsAHCHttpOperation() throws IOException { + long tailLatency = getTailLatencyTimeoutIfEnabled(); return new AbfsAHCHttpOperation(url, method, requestHeaders, Duration.ofMillis(client.getAbfsConfiguration().getHttpConnectionTimeout()), Duration.ofMillis(client.getAbfsConfiguration().getHttpReadTimeout()), - client.getAbfsApacheHttpClient(), client); + tailLatency, client.getAbfsApacheHttpClient(), client); + } + + /** + * Get Tail Latency Timeout value if profiling is enabled, timeout is enabled + * and retries due to tail latency request timeout is allowed. + * @return tail latency timeout value else return zero. + */ + long getTailLatencyTimeoutIfEnabled() { + if (isTailLatencyTimeoutEnabled() && shouldTailLatencyTimeout) { + return (long) tailLatencyTracker.getTailLatency(this.operationType); + } + return ZERO; + } + + boolean isTailLatencyTimeoutEnabled() { Review Comment: Java doc missing anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470451730 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -1824,4 +1860,41 @@ public int getBlobRenameDirConsumptionParallelism() { public int getBlobDeleteDirConsumptionParallelism() { return blobDeleteDirConsumptionParallelism; } + + public boolean isTailLatencyTrackerEnabled() { + return isTailLatencyTrackerEnabled; + } + + public boolean isTailLatencyRequestTimeoutEnabled() { + return isTailLatencyRequestTimeoutEnabled && isTailLatencyTrackerEnabled Review Comment: Make sense. Taken anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470457723 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -1824,4 +1860,41 @@ public int getBlobRenameDirConsumptionParallelism() { public int getBlobDeleteDirConsumptionParallelism() { return blobDeleteDirConsumptionParallelism; } + + public boolean isTailLatencyTrackerEnabled() { + return isTailLatencyTrackerEnabled; + } + + public boolean isTailLatencyRequestTimeoutEnabled() { + return isTailLatencyRequestTimeoutEnabled && isTailLatencyTrackerEnabled + && getPreferredHttpOperationType().equals(HttpOperationType.APACHE_HTTP_CLIENT); + } + + public int getTailLatencyPercentile() { + return tailLatencyPercentile; + } + + public int getTailLatencyMinDeviation() { + return tailLatencyMinDeviation; + } + + public int getTailLatencyMinSampleSize() { + return tailLatencyMinSampleSize; + } + + public int getTailLatencyAnalysisWindowInMillis() { + return tailLatencyAnalysisWindowInMillis; + } + + public int getTailLatencyPercentileComputationIntervalInMillis() { Review Comment: Taken anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470459989 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -266,5 +266,15 @@ public final class FileSystemConfigurations { public static final boolean DEFAULT_FS_AZURE_ENABLE_CREATE_BLOB_IDEMPOTENCY = true; + public static final boolean DEFAULT_FS_AZURE_ENABLE_TAIL_LATENCY_TRACKER = false; Review Comment: There is no value add currently to just enable profling as we are not consuming it anywhere. anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470461231 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsApacheHttpClient.java: ########## @@ -143,6 +173,51 @@ public HttpResponse execute(HttpRequestBase httpRequest, return httpClient.execute(httpRequest, abfsHttpClientContext); } + /** + * Executes the HTTP request with a deadline. If the request does not complete + * within the deadline, it is aborted and an IOException is thrown. + * + * @param httpRequest HTTP request to execute. + * @param abfsHttpClientContext HttpClient context. + * @param connectTimeout Connection timeout. + * @param readTimeout Read timeout. + * @param deadlineMillis Deadline in milliseconds. + * + * @return HTTP response. + * @throws IOException network error or deadline exceeded. + */ + public HttpResponse executeWithDeadline(HttpRequestBase httpRequest, + final AbfsManagedHttpClientContext abfsHttpClientContext, + final int connectTimeout, + final int readTimeout, + final long deadlineMillis) throws IOException { + RequestConfig.Builder requestConfigBuilder = RequestConfig + .custom() + .setConnectTimeout(connectTimeout) + .setSocketTimeout(readTimeout); + httpRequest.setConfig(requestConfigBuilder.build()); + ExecutorService executor = Executors.newSingleThreadExecutor(); + Future<HttpResponse> future = executor.submit(() -> + httpClient.execute(httpRequest, abfsHttpClientContext) + ); + + try { + return future.get(deadlineMillis, TimeUnit.MILLISECONDS); + } catch (TimeoutException e) { + /* Deadline exceeded, abort the request. + * This will also kill the underlying socket exception in the HttpClient. + * Connection will be marker stale and won't be returned back to KAC for reuse. Review Comment: Taken anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470463895 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/RetryPolicyConstants.java: ########## @@ -32,4 +32,8 @@ private RetryPolicyConstants() { * Constant for Static Retry Policy Abbreviation. {@value} */ public static final String STATIC_RETRY_POLICY_ABBREVIATION = \"S\"; + /** + * Constant for Static Retry Policy Abbreviation. {@value} + */ + public static final String TAIL_LATENCY_TIMEOUT_RETRY_POLICY_ABBREVIATION = \"T\"; Review Comment: Other Retry policy abbreviations are already single character. Keeping it likewise anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470465366 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java: ########## @@ -0,0 +1,249 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; +import org.HdrHistogram.Histogram; +import org.HdrHistogram.Recorder; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.util.concurrent.atomic.AtomicLong; +import java.util.concurrent.locks.ReentrantLock; +import java.util.concurrent.atomic.AtomicInteger; + +import org.apache.hadoop.classification.VisibleForTesting; + +public class SlidingWindowHdrHistogram { + private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class); + + // Configuration + private final long windowSizeMillis; // Total analysis window + private final long timeSegmentDurationMillis; // Subdivision on analysis window + private final int numSegments; + private final long highestTrackableValue; + private final int significantFigures; + + // Ring buffer of immutable snapshots for completed time segments + private final Histogram[] completedSegments; + private final AtomicInteger currentIndex = new AtomicInteger(0); + + // Active Time Segment + private volatile Recorder activeSegmentRecorder; + private Histogram currentSegmentAccumulation; + private volatile long currentSegmentStartMillis; + private final AtomicLong currentTotalCount = new AtomicLong(0L); + + // Synchronization + // Writers never take locks. Readers (queries) and rotation use this lock + // to mutate currentAccumulation and ring-buffer pointers safely. + private final ReentrantLock rotateLock = new ReentrantLock(); + + // Reusable temp histograms to minimize allocations + private Histogram tmpForDelta; + private Histogram tmpForMerge; + + private final AbfsRestOperationType operationType; + + private boolean isAnalysisWindowFilled = false; + private int minSampleSize; + private int tailLatencyPercentile; + private int tailLatencyMinDeviation; + + private double p50 = 0.0; + private double p90 = 0.0; + private double p99 = 0.0; + private double tailLatency = 0.0; + private int deviation = 0; + + public SlidingWindowHdrHistogram(long windowSizeMillis, + int numberOfSegments, + int minSampleSize, + int tailLatencyPercentile, + int tailLatencyMinDeviation, + long highestTrackableValue, + int significantFigures, + final AbfsRestOperationType operationType) { + if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\"); + if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\"); Review Comment: Taken anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470469035 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java: ########## @@ -0,0 +1,249 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; +import org.HdrHistogram.Histogram; +import org.HdrHistogram.Recorder; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.util.concurrent.atomic.AtomicLong; +import java.util.concurrent.locks.ReentrantLock; +import java.util.concurrent.atomic.AtomicInteger; + +import org.apache.hadoop.classification.VisibleForTesting; + +public class SlidingWindowHdrHistogram { + private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class); + + // Configuration + private final long windowSizeMillis; // Total analysis window + private final long timeSegmentDurationMillis; // Subdivision on analysis window + private final int numSegments; + private final long highestTrackableValue; + private final int significantFigures; + + // Ring buffer of immutable snapshots for completed time segments + private final Histogram[] completedSegments; + private final AtomicInteger currentIndex = new AtomicInteger(0); + + // Active Time Segment + private volatile Recorder activeSegmentRecorder; + private Histogram currentSegmentAccumulation; + private volatile long currentSegmentStartMillis; + private final AtomicLong currentTotalCount = new AtomicLong(0L); + + // Synchronization + // Writers never take locks. Readers (queries) and rotation use this lock + // to mutate currentAccumulation and ring-buffer pointers safely. + private final ReentrantLock rotateLock = new ReentrantLock(); + + // Reusable temp histograms to minimize allocations + private Histogram tmpForDelta; + private Histogram tmpForMerge; + + private final AbfsRestOperationType operationType; + + private boolean isAnalysisWindowFilled = false; + private int minSampleSize; + private int tailLatencyPercentile; + private int tailLatencyMinDeviation; + + private double p50 = 0.0; + private double p90 = 0.0; + private double p99 = 0.0; + private double tailLatency = 0.0; + private int deviation = 0; + + public SlidingWindowHdrHistogram(long windowSizeMillis, + int numberOfSegments, + int minSampleSize, + int tailLatencyPercentile, + int tailLatencyMinDeviation, + long highestTrackableValue, + int significantFigures, + final AbfsRestOperationType operationType) { + if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\"); + if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\"); + if (highestTrackableValue <= 0) throw new IllegalArgumentException(\"highestTrackableValue > 0\"); + if (significantFigures < 1 || significantFigures > 5) throw new IllegalArgumentException(\"significantFigures in [1,5]\"); + + this.windowSizeMillis = windowSizeMillis; + this.numSegments = numberOfSegments; + this.timeSegmentDurationMillis = windowSizeMillis/numberOfSegments; + this.highestTrackableValue = highestTrackableValue; + this.significantFigures = significantFigures; + this.operationType = operationType; + this.minSampleSize = minSampleSize; + this.tailLatencyPercentile = tailLatencyPercentile; + this.tailLatencyMinDeviation = tailLatencyMinDeviation; // 5ms + + this.completedSegments = new Histogram[numSegments]; + long now = System.currentTimeMillis(); + this.currentSegmentStartMillis = alignToSegmentDuration(now); + currentIndex.set(0); + this.activeSegmentRecorder = new Recorder(highestTrackableValue, significantFigures); + this.currentSegmentAccumulation = new Histogram(highestTrackableValue, significantFigures); + this.tmpForDelta = new Histogram(highestTrackableValue, significantFigures); + this.tmpForMerge = new Histogram(highestTrackableValue, significantFigures); + + LOG.debug(\"[{}] Initialized SlidingWindowHdrHistogram with WindowSize {}, TimeSegmentDur: {}, NumOfSegments: {}\", operationType, windowSizeMillis, timeSegmentDurationMillis, numSegments); + } + + /** Record a single latency value (in your chosen time unit). Thread-safe and lock-free. */ + public void recordValue(long value) { + if (value < 0 || value > highestTrackableValue) { + LOG.warn(\"[{}] Value {} outside of range [0, {}]. Ignoring\", + operationType, value, highestTrackableValue); + return; + } + activeSegmentRecorder.recordValue(value); + currentTotalCount.incrementAndGet(); + LOG.debug(\"[{}] Recorded latency value: {}. Current total count: {}\", + operationType, value, currentTotalCount.get()); + } + + /** Get any percentile over the current sliding window. */ + public void computeLatency() { + if (getCurrentTotalCount() < minSampleSize) { + LOG.debug(\"[{}] Not enough data to report percentiles. Current total count: {}\", Review Comment: Taken anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470478310 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java: ########## @@ -0,0 +1,249 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; +import org.HdrHistogram.Histogram; +import org.HdrHistogram.Recorder; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.util.concurrent.atomic.AtomicLong; +import java.util.concurrent.locks.ReentrantLock; +import java.util.concurrent.atomic.AtomicInteger; + +import org.apache.hadoop.classification.VisibleForTesting; + +public class SlidingWindowHdrHistogram { + private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class); + + // Configuration + private final long windowSizeMillis; // Total analysis window + private final long timeSegmentDurationMillis; // Subdivision on analysis window + private final int numSegments; + private final long highestTrackableValue; + private final int significantFigures; + + // Ring buffer of immutable snapshots for completed time segments + private final Histogram[] completedSegments; + private final AtomicInteger currentIndex = new AtomicInteger(0); + + // Active Time Segment + private volatile Recorder activeSegmentRecorder; + private Histogram currentSegmentAccumulation; + private volatile long currentSegmentStartMillis; + private final AtomicLong currentTotalCount = new AtomicLong(0L); + + // Synchronization + // Writers never take locks. Readers (queries) and rotation use this lock + // to mutate currentAccumulation and ring-buffer pointers safely. + private final ReentrantLock rotateLock = new ReentrantLock(); + + // Reusable temp histograms to minimize allocations + private Histogram tmpForDelta; + private Histogram tmpForMerge; + + private final AbfsRestOperationType operationType; + + private boolean isAnalysisWindowFilled = false; + private int minSampleSize; + private int tailLatencyPercentile; + private int tailLatencyMinDeviation; + + private double p50 = 0.0; + private double p90 = 0.0; + private double p99 = 0.0; + private double tailLatency = 0.0; + private int deviation = 0; + + public SlidingWindowHdrHistogram(long windowSizeMillis, + int numberOfSegments, + int minSampleSize, + int tailLatencyPercentile, + int tailLatencyMinDeviation, + long highestTrackableValue, + int significantFigures, + final AbfsRestOperationType operationType) { + if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\"); + if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\"); + if (highestTrackableValue <= 0) throw new IllegalArgumentException(\"highestTrackableValue > 0\"); + if (significantFigures < 1 || significantFigures > 5) throw new IllegalArgumentException(\"significantFigures in [1,5]\"); + + this.windowSizeMillis = windowSizeMillis; + this.numSegments = numberOfSegments; + this.timeSegmentDurationMillis = windowSizeMillis/numberOfSegments; + this.highestTrackableValue = highestTrackableValue; + this.significantFigures = significantFigures; + this.operationType = operationType; + this.minSampleSize = minSampleSize; + this.tailLatencyPercentile = tailLatencyPercentile; + this.tailLatencyMinDeviation = tailLatencyMinDeviation; // 5ms + + this.completedSegments = new Histogram[numSegments]; + long now = System.currentTimeMillis(); + this.currentSegmentStartMillis = alignToSegmentDuration(now); + currentIndex.set(0); + this.activeSegmentRecorder = new Recorder(highestTrackableValue, significantFigures); + this.currentSegmentAccumulation = new Histogram(highestTrackableValue, significantFigures); + this.tmpForDelta = new Histogram(highestTrackableValue, significantFigures); + this.tmpForMerge = new Histogram(highestTrackableValue, significantFigures); + + LOG.debug(\"[{}] Initialized SlidingWindowHdrHistogram with WindowSize {}, TimeSegmentDur: {}, NumOfSegments: {}\", operationType, windowSizeMillis, timeSegmentDurationMillis, numSegments); + } + + /** Record a single latency value (in your chosen time unit). Thread-safe and lock-free. */ + public void recordValue(long value) { + if (value < 0 || value > highestTrackableValue) { + LOG.warn(\"[{}] Value {} outside of range [0, {}]. Ignoring\", + operationType, value, highestTrackableValue); + return; + } + activeSegmentRecorder.recordValue(value); + currentTotalCount.incrementAndGet(); + LOG.debug(\"[{}] Recorded latency value: {}. Current total count: {}\", + operationType, value, currentTotalCount.get()); + } + + /** Get any percentile over the current sliding window. */ + public void computeLatency() { + if (getCurrentTotalCount() < minSampleSize) { + LOG.debug(\"[{}] Not enough data to report percentiles. Current total count: {}\", + operationType, getCurrentTotalCount()); + } else { + rotateLock.lock(); + try { + tmpForMerge.reset(); + for (int i = 0; i < numSegments; i++) { + Histogram h = completedSegments[i]; + if (h != null && h.getTotalCount() > 0) { + tmpForMerge.add(h); + } + } + + if (tmpForMerge.getTotalCount() == 0) return; + + tailLatency = tmpForMerge.getValueAtPercentile(tailLatencyPercentile); + p50 = tmpForMerge.getValueAtPercentile(50); + p90 = tmpForMerge.getValueAtPercentile(90); + p99 = tmpForMerge.getValueAtPercentile(99); + deviation = (int) ((tailLatency - p50)/p50 * 100); Review Comment: Nice catch. Taken anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470481704 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java: ########## @@ -0,0 +1,249 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; +import org.HdrHistogram.Histogram; +import org.HdrHistogram.Recorder; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.util.concurrent.atomic.AtomicLong; +import java.util.concurrent.locks.ReentrantLock; +import java.util.concurrent.atomic.AtomicInteger; + +import org.apache.hadoop.classification.VisibleForTesting; + +public class SlidingWindowHdrHistogram { + private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class); + + // Configuration + private final long windowSizeMillis; // Total analysis window + private final long timeSegmentDurationMillis; // Subdivision on analysis window + private final int numSegments; + private final long highestTrackableValue; + private final int significantFigures; + + // Ring buffer of immutable snapshots for completed time segments + private final Histogram[] completedSegments; + private final AtomicInteger currentIndex = new AtomicInteger(0); + + // Active Time Segment + private volatile Recorder activeSegmentRecorder; + private Histogram currentSegmentAccumulation; + private volatile long currentSegmentStartMillis; + private final AtomicLong currentTotalCount = new AtomicLong(0L); + + // Synchronization + // Writers never take locks. Readers (queries) and rotation use this lock + // to mutate currentAccumulation and ring-buffer pointers safely. + private final ReentrantLock rotateLock = new ReentrantLock(); + + // Reusable temp histograms to minimize allocations + private Histogram tmpForDelta; + private Histogram tmpForMerge; + + private final AbfsRestOperationType operationType; + + private boolean isAnalysisWindowFilled = false; + private int minSampleSize; + private int tailLatencyPercentile; + private int tailLatencyMinDeviation; + + private double p50 = 0.0; + private double p90 = 0.0; + private double p99 = 0.0; + private double tailLatency = 0.0; + private int deviation = 0; + + public SlidingWindowHdrHistogram(long windowSizeMillis, + int numberOfSegments, + int minSampleSize, + int tailLatencyPercentile, + int tailLatencyMinDeviation, + long highestTrackableValue, + int significantFigures, + final AbfsRestOperationType operationType) { + if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\"); + if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\"); + if (highestTrackableValue <= 0) throw new IllegalArgumentException(\"highestTrackableValue > 0\"); + if (significantFigures < 1 || significantFigures > 5) throw new IllegalArgumentException(\"significantFigures in [1,5]\"); + + this.windowSizeMillis = windowSizeMillis; + this.numSegments = numberOfSegments; + this.timeSegmentDurationMillis = windowSizeMillis/numberOfSegments; + this.highestTrackableValue = highestTrackableValue; + this.significantFigures = significantFigures; + this.operationType = operationType; + this.minSampleSize = minSampleSize; + this.tailLatencyPercentile = tailLatencyPercentile; + this.tailLatencyMinDeviation = tailLatencyMinDeviation; // 5ms + + this.completedSegments = new Histogram[numSegments]; + long now = System.currentTimeMillis(); + this.currentSegmentStartMillis = alignToSegmentDuration(now); + currentIndex.set(0); + this.activeSegmentRecorder = new Recorder(highestTrackableValue, significantFigures); + this.currentSegmentAccumulation = new Histogram(highestTrackableValue, significantFigures); + this.tmpForDelta = new Histogram(highestTrackableValue, significantFigures); + this.tmpForMerge = new Histogram(highestTrackableValue, significantFigures); + + LOG.debug(\"[{}] Initialized SlidingWindowHdrHistogram with WindowSize {}, TimeSegmentDur: {}, NumOfSegments: {}\", operationType, windowSizeMillis, timeSegmentDurationMillis, numSegments); + } + + /** Record a single latency value (in your chosen time unit). Thread-safe and lock-free. */ + public void recordValue(long value) { + if (value < 0 || value > highestTrackableValue) { + LOG.warn(\"[{}] Value {} outside of range [0, {}]. Ignoring\", + operationType, value, highestTrackableValue); + return; + } + activeSegmentRecorder.recordValue(value); + currentTotalCount.incrementAndGet(); + LOG.debug(\"[{}] Recorded latency value: {}. Current total count: {}\", + operationType, value, currentTotalCount.get()); + } + + /** Get any percentile over the current sliding window. */ + public void computeLatency() { + if (getCurrentTotalCount() < minSampleSize) { + LOG.debug(\"[{}] Not enough data to report percentiles. Current total count: {}\", + operationType, getCurrentTotalCount()); + } else { + rotateLock.lock(); + try { + tmpForMerge.reset(); + for (int i = 0; i < numSegments; i++) { + Histogram h = completedSegments[i]; + if (h != null && h.getTotalCount() > 0) { + tmpForMerge.add(h); + } + } + + if (tmpForMerge.getTotalCount() == 0) return; + + tailLatency = tmpForMerge.getValueAtPercentile(tailLatencyPercentile); + p50 = tmpForMerge.getValueAtPercentile(50); + p90 = tmpForMerge.getValueAtPercentile(90); + p99 = tmpForMerge.getValueAtPercentile(99); + deviation = (int) ((tailLatency - p50)/p50 * 100); + } finally { + rotateLock.unlock(); + } + } + LOG.debug(\"[{}] Computed Latencies. p50: {}, p90: {}, p99: {}, tailLatency: {}, deviation with p50: {} Current total count: {}\", + operationType, p50, p90, p99, tailLatency, deviation, getCurrentTotalCount()); + } + + private long alignToSegmentDuration(long timeMs) { + return timeMs - (timeMs % timeSegmentDurationMillis); + } + + /** Ensure active bucket is aligned to current time; rotate if we've crossed a boundary. */ + public void rotateIfNeeded() { + LOG.debug(\"[{}] Triggering Histogram Rotation\", operationType); + long now = System.currentTimeMillis(); + long expectedStart = alignToSegmentDuration(now); + if (expectedStart == currentSegmentStartMillis) { + LOG.debug(\"[{}] Current Time Segment Still Active at {}. Skipping Rotation\", operationType, expectedStart); + return; // still current + } + + rotateLock.lock(); + try { + // Re-check inside lock + now = System.currentTimeMillis(); + expectedStart = alignToSegmentDuration(now); + if (expectedStart == currentSegmentStartMillis) return; + + // Finalize the current bucket: + // Pull any remaining deltas from active recorder and add to currentAccumulation + tmpForDelta.reset(); + activeSegmentRecorder.getIntervalHistogramInto(tmpForDelta); + currentSegmentAccumulation.add(tmpForDelta); + + if (currentSegmentAccumulation.getTotalCount() <= 0) { Review Comment: Yeah this is primarily for equal to 0 anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470498123 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java: ########## @@ -0,0 +1,249 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; +import org.HdrHistogram.Histogram; +import org.HdrHistogram.Recorder; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.util.concurrent.atomic.AtomicLong; +import java.util.concurrent.locks.ReentrantLock; +import java.util.concurrent.atomic.AtomicInteger; + +import org.apache.hadoop.classification.VisibleForTesting; + +public class SlidingWindowHdrHistogram { + private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class); + + // Configuration + private final long windowSizeMillis; // Total analysis window + private final long timeSegmentDurationMillis; // Subdivision on analysis window + private final int numSegments; + private final long highestTrackableValue; + private final int significantFigures; + + // Ring buffer of immutable snapshots for completed time segments + private final Histogram[] completedSegments; + private final AtomicInteger currentIndex = new AtomicInteger(0); + + // Active Time Segment + private volatile Recorder activeSegmentRecorder; + private Histogram currentSegmentAccumulation; + private volatile long currentSegmentStartMillis; + private final AtomicLong currentTotalCount = new AtomicLong(0L); + + // Synchronization + // Writers never take locks. Readers (queries) and rotation use this lock + // to mutate currentAccumulation and ring-buffer pointers safely. + private final ReentrantLock rotateLock = new ReentrantLock(); + + // Reusable temp histograms to minimize allocations + private Histogram tmpForDelta; + private Histogram tmpForMerge; + + private final AbfsRestOperationType operationType; + + private boolean isAnalysisWindowFilled = false; + private int minSampleSize; + private int tailLatencyPercentile; + private int tailLatencyMinDeviation; + + private double p50 = 0.0; + private double p90 = 0.0; + private double p99 = 0.0; + private double tailLatency = 0.0; + private int deviation = 0; + + public SlidingWindowHdrHistogram(long windowSizeMillis, + int numberOfSegments, + int minSampleSize, + int tailLatencyPercentile, + int tailLatencyMinDeviation, + long highestTrackableValue, + int significantFigures, + final AbfsRestOperationType operationType) { + if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\"); + if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\"); + if (highestTrackableValue <= 0) throw new IllegalArgumentException(\"highestTrackableValue > 0\"); + if (significantFigures < 1 || significantFigures > 5) throw new IllegalArgumentException(\"significantFigures in [1,5]\"); + + this.windowSizeMillis = windowSizeMillis; + this.numSegments = numberOfSegments; + this.timeSegmentDurationMillis = windowSizeMillis/numberOfSegments; + this.highestTrackableValue = highestTrackableValue; + this.significantFigures = significantFigures; + this.operationType = operationType; + this.minSampleSize = minSampleSize; + this.tailLatencyPercentile = tailLatencyPercentile; + this.tailLatencyMinDeviation = tailLatencyMinDeviation; // 5ms + + this.completedSegments = new Histogram[numSegments]; + long now = System.currentTimeMillis(); + this.currentSegmentStartMillis = alignToSegmentDuration(now); + currentIndex.set(0); + this.activeSegmentRecorder = new Recorder(highestTrackableValue, significantFigures); + this.currentSegmentAccumulation = new Histogram(highestTrackableValue, significantFigures); + this.tmpForDelta = new Histogram(highestTrackableValue, significantFigures); + this.tmpForMerge = new Histogram(highestTrackableValue, significantFigures); + + LOG.debug(\"[{}] Initialized SlidingWindowHdrHistogram with WindowSize {}, TimeSegmentDur: {}, NumOfSegments: {}\", operationType, windowSizeMillis, timeSegmentDurationMillis, numSegments); + } + + /** Record a single latency value (in your chosen time unit). Thread-safe and lock-free. */ + public void recordValue(long value) { + if (value < 0 || value > highestTrackableValue) { + LOG.warn(\"[{}] Value {} outside of range [0, {}]. Ignoring\", + operationType, value, highestTrackableValue); + return; + } + activeSegmentRecorder.recordValue(value); + currentTotalCount.incrementAndGet(); + LOG.debug(\"[{}] Recorded latency value: {}. Current total count: {}\", + operationType, value, currentTotalCount.get()); + } + + /** Get any percentile over the current sliding window. */ + public void computeLatency() { + if (getCurrentTotalCount() < minSampleSize) { + LOG.debug(\"[{}] Not enough data to report percentiles. Current total count: {}\", + operationType, getCurrentTotalCount()); + } else { + rotateLock.lock(); + try { + tmpForMerge.reset(); + for (int i = 0; i < numSegments; i++) { + Histogram h = completedSegments[i]; + if (h != null && h.getTotalCount() > 0) { + tmpForMerge.add(h); + } + } + + if (tmpForMerge.getTotalCount() == 0) return; + + tailLatency = tmpForMerge.getValueAtPercentile(tailLatencyPercentile); + p50 = tmpForMerge.getValueAtPercentile(50); + p90 = tmpForMerge.getValueAtPercentile(90); + p99 = tmpForMerge.getValueAtPercentile(99); + deviation = (int) ((tailLatency - p50)/p50 * 100); + } finally { + rotateLock.unlock(); + } + } + LOG.debug(\"[{}] Computed Latencies. p50: {}, p90: {}, p99: {}, tailLatency: {}, deviation with p50: {} Current total count: {}\", + operationType, p50, p90, p99, tailLatency, deviation, getCurrentTotalCount()); + } + + private long alignToSegmentDuration(long timeMs) { + return timeMs - (timeMs % timeSegmentDurationMillis); + } + + /** Ensure active bucket is aligned to current time; rotate if we've crossed a boundary. */ + public void rotateIfNeeded() { + LOG.debug(\"[{}] Triggering Histogram Rotation\", operationType); + long now = System.currentTimeMillis(); + long expectedStart = alignToSegmentDuration(now); + if (expectedStart == currentSegmentStartMillis) { + LOG.debug(\"[{}] Current Time Segment Still Active at {}. Skipping Rotation\", operationType, expectedStart); + return; // still current + } + + rotateLock.lock(); + try { + // Re-check inside lock + now = System.currentTimeMillis(); + expectedStart = alignToSegmentDuration(now); + if (expectedStart == currentSegmentStartMillis) return; + + // Finalize the current bucket: + // Pull any remaining deltas from active recorder and add to currentAccumulation + tmpForDelta.reset(); + activeSegmentRecorder.getIntervalHistogramInto(tmpForDelta); + currentSegmentAccumulation.add(tmpForDelta); + + if (currentSegmentAccumulation.getTotalCount() <= 0) { + currentSegmentStartMillis = alignToSegmentDuration(System.currentTimeMillis()); + LOG.debug(\"[{}] No data recorded in current time segment at {}. Skipping Rotation. Current Index is {}.\", + operationType, currentSegmentStartMillis, currentIndex.get()); + return; + } + + LOG.debug(\"[{}] Rotating current segment with total count {} into slot {}\", + operationType, currentSegmentAccumulation.getTotalCount(), currentIndex.get()); + + // Place the finished currentAccumulation into the ring buffer slot ahead. + int currentIdx = (currentIndex.getAndIncrement()) % numSegments; + // Next slot is now going to be eradicated. Remove its count from total. + currentTotalCount.set(currentTotalCount.get() - (completedSegments[currentIdx] == null ? 0 : completedSegments[currentIdx].getTotalCount())); + // Store an immutable snapshot (make sure we don't mutate the instance after storing) + completedSegments[currentIdx] = currentSegmentAccumulation; Review Comment: This is happening by reference. The reference earlier held by `currentSegmentAccumulation` is now saved into `completedSegments[currentIdx]`. And a new reference is created and saved into `currentSegmentAccumulation` anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470505964 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SlidingWindowHdrHistogram.java: ########## @@ -0,0 +1,249 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; +import org.HdrHistogram.Histogram; +import org.HdrHistogram.Recorder; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.util.concurrent.atomic.AtomicLong; +import java.util.concurrent.locks.ReentrantLock; +import java.util.concurrent.atomic.AtomicInteger; + +import org.apache.hadoop.classification.VisibleForTesting; + +public class SlidingWindowHdrHistogram { + private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowHdrHistogram.class); + + // Configuration + private final long windowSizeMillis; // Total analysis window + private final long timeSegmentDurationMillis; // Subdivision on analysis window + private final int numSegments; + private final long highestTrackableValue; + private final int significantFigures; + + // Ring buffer of immutable snapshots for completed time segments + private final Histogram[] completedSegments; + private final AtomicInteger currentIndex = new AtomicInteger(0); + + // Active Time Segment + private volatile Recorder activeSegmentRecorder; + private Histogram currentSegmentAccumulation; + private volatile long currentSegmentStartMillis; + private final AtomicLong currentTotalCount = new AtomicLong(0L); + + // Synchronization + // Writers never take locks. Readers (queries) and rotation use this lock + // to mutate currentAccumulation and ring-buffer pointers safely. + private final ReentrantLock rotateLock = new ReentrantLock(); + + // Reusable temp histograms to minimize allocations + private Histogram tmpForDelta; + private Histogram tmpForMerge; + + private final AbfsRestOperationType operationType; + + private boolean isAnalysisWindowFilled = false; + private int minSampleSize; + private int tailLatencyPercentile; + private int tailLatencyMinDeviation; + + private double p50 = 0.0; + private double p90 = 0.0; + private double p99 = 0.0; + private double tailLatency = 0.0; + private int deviation = 0; + + public SlidingWindowHdrHistogram(long windowSizeMillis, + int numberOfSegments, + int minSampleSize, + int tailLatencyPercentile, + int tailLatencyMinDeviation, + long highestTrackableValue, + int significantFigures, + final AbfsRestOperationType operationType) { + if (windowSizeMillis <= 0) throw new IllegalArgumentException(\"windowSizeMillis > 0\"); + if (numberOfSegments <= 0) throw new IllegalArgumentException(\"bucketDurationMillis > 0\"); + if (highestTrackableValue <= 0) throw new IllegalArgumentException(\"highestTrackableValue > 0\"); + if (significantFigures < 1 || significantFigures > 5) throw new IllegalArgumentException(\"significantFigures in [1,5]\"); + + this.windowSizeMillis = windowSizeMillis; + this.numSegments = numberOfSegments; + this.timeSegmentDurationMillis = windowSizeMillis/numberOfSegments; + this.highestTrackableValue = highestTrackableValue; + this.significantFigures = significantFigures; + this.operationType = operationType; + this.minSampleSize = minSampleSize; + this.tailLatencyPercentile = tailLatencyPercentile; + this.tailLatencyMinDeviation = tailLatencyMinDeviation; // 5ms + + this.completedSegments = new Histogram[numSegments]; + long now = System.currentTimeMillis(); + this.currentSegmentStartMillis = alignToSegmentDuration(now); + currentIndex.set(0); + this.activeSegmentRecorder = new Recorder(highestTrackableValue, significantFigures); + this.currentSegmentAccumulation = new Histogram(highestTrackableValue, significantFigures); + this.tmpForDelta = new Histogram(highestTrackableValue, significantFigures); + this.tmpForMerge = new Histogram(highestTrackableValue, significantFigures); + + LOG.debug(\"[{}] Initialized SlidingWindowHdrHistogram with WindowSize {}, TimeSegmentDur: {}, NumOfSegments: {}\", operationType, windowSizeMillis, timeSegmentDurationMillis, numSegments); + } + + /** Record a single latency value (in your chosen time unit). Thread-safe and lock-free. */ + public void recordValue(long value) { + if (value < 0 || value > highestTrackableValue) { + LOG.warn(\"[{}] Value {} outside of range [0, {}]. Ignoring\", + operationType, value, highestTrackableValue); + return; + } + activeSegmentRecorder.recordValue(value); + currentTotalCount.incrementAndGet(); + LOG.debug(\"[{}] Recorded latency value: {}. Current total count: {}\", + operationType, value, currentTotalCount.get()); + } + + /** Get any percentile over the current sliding window. */ + public void computeLatency() { + if (getCurrentTotalCount() < minSampleSize) { + LOG.debug(\"[{}] Not enough data to report percentiles. Current total count: {}\", + operationType, getCurrentTotalCount()); + } else { + rotateLock.lock(); + try { + tmpForMerge.reset(); + for (int i = 0; i < numSegments; i++) { + Histogram h = completedSegments[i]; + if (h != null && h.getTotalCount() > 0) { + tmpForMerge.add(h); + } + } + + if (tmpForMerge.getTotalCount() == 0) return; + + tailLatency = tmpForMerge.getValueAtPercentile(tailLatencyPercentile); + p50 = tmpForMerge.getValueAtPercentile(50); + p90 = tmpForMerge.getValueAtPercentile(90); + p99 = tmpForMerge.getValueAtPercentile(99); + deviation = (int) ((tailLatency - p50)/p50 * 100); + } finally { + rotateLock.unlock(); + } + } + LOG.debug(\"[{}] Computed Latencies. p50: {}, p90: {}, p99: {}, tailLatency: {}, deviation with p50: {} Current total count: {}\", + operationType, p50, p90, p99, tailLatency, deviation, getCurrentTotalCount()); + } + + private long alignToSegmentDuration(long timeMs) { + return timeMs - (timeMs % timeSegmentDurationMillis); + } + + /** Ensure active bucket is aligned to current time; rotate if we've crossed a boundary. */ + public void rotateIfNeeded() { + LOG.debug(\"[{}] Triggering Histogram Rotation\", operationType); + long now = System.currentTimeMillis(); Review Comment: Taken anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470514912 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -114,6 +116,7 @@ public class AbfsRestOperation { */ private String failureReason; private AbfsRetryPolicy retryPolicy; + private boolean shouldTailLatencyTimeout = true; Review Comment: That might be misleading. This variable is not a flag for this feature. Even when feature is enabled, we might have this as false. This is to indicate that all the retried due to TailLatencyTimeout are exhausted and even though the feature is still enabled, for the next retry we should not Timeout due to tail latency anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470519920 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -114,6 +116,7 @@ public class AbfsRestOperation { */ private String failureReason; private AbfsRetryPolicy retryPolicy; + private boolean shouldTailLatencyTimeout = true; Review Comment: Added javadoc anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470527273 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -531,6 +544,12 @@ private boolean executeHttpOperation(final int retryCount, if (shouldUpdateCSTMetrics(statusCode) && !wasKnownExceptionThrown) { intercept.updateMetrics(operationType, httpOperation); } + + // Update Tail Latency Tracker only for successful requests. + if (tailLatencyTracker != null && statusCode < HttpURLConnection.HTTP_MULT_CHOICE) { Review Comment: Taken anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470543064 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestSlidingWindowHdrHistogram.java: ########## @@ -0,0 +1,162 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.junit.jupiter.api.Test; + +import static org.assertj.core.api.Assertions.assertThat; + +public class TestSlidingWindowHdrHistogram { + + @Test + public void testSlidingWindowHdrHistogram() throws Exception { + SlidingWindowHdrHistogram histogram = new SlidingWindowHdrHistogram( Review Comment: Taken ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestSlidingWindowHdrHistogram.java: ########## @@ -0,0 +1,162 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.junit.jupiter.api.Test; + +import static org.assertj.core.api.Assertions.assertThat; + +public class TestSlidingWindowHdrHistogram { + + @Test + public void testSlidingWindowHdrHistogram() throws Exception { + SlidingWindowHdrHistogram histogram = new SlidingWindowHdrHistogram( + 100, + 5, + 7, + 99, + 0, + 100, + 3, + AbfsRestOperationType.GetPathStatus); + + // Verify that the histogram is created successfully with default values and + // do not report any percentiles + assertThat(histogram).isNotNull(); + assertThat(histogram.getCurrentTotalCount()).isEqualTo(0); + assertThat(histogram.getCurrentIndex()).isEqualTo(0); + assertThat(histogram.getP50()).isEqualTo(0.0); + assertThat(histogram.getTailLatency()).isEqualTo(0.0); + + // Verify that recording values works as expected + addAndRotate(histogram, 10, 5); // Add 5 values of 10 + assertThat(histogram.getCurrentTotalCount()).isEqualTo(5); + + // Verify that percentiles are not computed with insufficient samples + assertThat(histogram.getP50()).isEqualTo(0.0); + assertThat(histogram.getTailLatency()).isEqualTo(0.0); + + // Record more values to exceed the minimum sample size + addAndRotate(histogram, 20, 5); // Add 5 values of 20 + + // Verify that percentiles are now computed but tail Latency is still not reported + assertThat(histogram.getP50()).isGreaterThan(0.0); + assertThat(histogram.getTailLatency()).isEqualTo(0.0); + + // Record more values and rotate histogram to fill whole analysis window + addAndRotate(histogram, 30, 5); // Add 5 values of 30 + assertThat(histogram.getCurrentTotalCount()).isEqualTo(15); + + // Verify that analysis window is not full until full rotation. + assertThat(histogram.isAnalysisWindowFilled()).isFalse(); + + addAndRotate(histogram, 60, 5); // Add 5 values of 60 + assertThat(histogram.getCurrentTotalCount()).isEqualTo(20); + + // Verify that analysis window is not full until full rotation. + assertThat(histogram.isAnalysisWindowFilled()).isFalse(); + + // Verify that rotation is skipped if nothing new recorded and hence window not filled + addAndRotate(histogram, 100, 0); // No new values added + assertThat(histogram.isAnalysisWindowFilled()).isFalse(); + + // Verify that rotation does not happen if analysis window is not filled + histogram.rotateIfNeeded(); + assertThat(histogram.isAnalysisWindowFilled()).isFalse(); + + addAndRotate(histogram, 80, 5); // Add 5 values of 80 + assertThat(histogram.getCurrentTotalCount()).isEqualTo(25); + + // Verify that analysis window is full after full rotation. + assertThat(histogram.isAnalysisWindowFilled()).isTrue(); + + // Verify that percentiles and tail latency are computed + assertThat(histogram.getP50()).isGreaterThan(0.0); + assertThat(histogram.getTailLatency()).isGreaterThan(0.0); + + // Verify that sliding window works. Old values should be evicted + addAndRotate(histogram, 90, 3); // Add 3 values of 90 + assertThat(histogram.getCurrentTotalCount()).isEqualTo(23); + assertThat(histogram.isAnalysisWindowFilled()).isTrue(); + } + + @Test + public void testMinDeviationRequirementNotMet() throws Exception { + SlidingWindowHdrHistogram histogram = new SlidingWindowHdrHistogram( + 100, + 5, + 7, + 99, + 100, + 100, + 3, + AbfsRestOperationType.GetPathStatus); + + // Add values with low deviation + addAndRotate(histogram, 50, 5); // Add 5 values of 50 + addAndRotate(histogram, 51, 5); // Add 5 values of 52 + addAndRotate(histogram, 52, 5); // Add 5 values of 51 + addAndRotate(histogram, 80, 5); // Add 5 values of 53 + addAndRotate(histogram, 90, 5); // Add 5 values of 50 + + // Verify that analysis window is full after full rotation. + assertThat(histogram.isAnalysisWindowFilled()).isTrue(); + + // Verify that percentiles are not computed due to low deviation + assertThat(histogram.getP50()).isGreaterThan(0.0); + assertThat(histogram.getTailLatency()).isEqualTo(0.0); + } + + @Test + public void testMinDeviationRequirementMet() throws Exception { + SlidingWindowHdrHistogram histogram = new SlidingWindowHdrHistogram( + 100, + 5, + 7, + 99, + 50, + 100, + 3, + AbfsRestOperationType.GetPathStatus); + + // Add values with low deviation + addAndRotate(histogram, 50, 5); // Add 5 values of 50 + addAndRotate(histogram, 51, 5); // Add 5 values of 52 + addAndRotate(histogram, 52, 5); // Add 5 values of 51 + addAndRotate(histogram, 80, 5); // Add 5 values of 53 + addAndRotate(histogram, 90, 5); // Add 5 values of 50 + + // Verify that analysis window is full after full rotation. + assertThat(histogram.isAnalysisWindowFilled()).isTrue(); + + // Verify that percentiles are not computed due to low deviation Review Comment: Taken manika137 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470552760 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsTailLatencyTracker.java: ########## @@ -0,0 +1,159 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import java.util.HashMap; +import java.util.Map; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.locks.ReentrantLock; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; + +/** + * Account Specific Latency Tracker. + * This class tracks the latency of various operations like read, write etc for a single account. + * It maintains a sliding window histogram for each operation type to analyze latency patterns over time. + */ +public class AbfsTailLatencyTracker { + + private static final Logger LOG = LoggerFactory.getLogger( + AbfsTailLatencyTracker.class); + private static AbfsTailLatencyTracker singleton; + private static final ReentrantLock LOCK = new ReentrantLock(); + private static final int HISTOGRAM_MAX_VALUE = 60_000; + private static final int HISTOGRAM_SIGNIFICANT_FIGURES = 3; + private final Map<AbfsRestOperationType, SlidingWindowHdrHistogram> + operationLatencyMap = new HashMap<>(); + private final AbfsConfiguration configuration; + + /** + * Constructor to initialize the latency tracker with configuration. + * @param abfsConfiguration Configuration settings for latency tracking. + */ + public AbfsTailLatencyTracker(AbfsConfiguration abfsConfiguration) { + this.configuration = abfsConfiguration; + ScheduledExecutorService histogramRotatorThread = Executors.newSingleThreadScheduledExecutor( + r -> { + Thread t = new Thread(r, \"Histogram-Rotator-Thread\"); + t.setDaemon(true); + return t; + }); + long rotationInterval = configuration.getTailLatencyAnalysisWindowInMillis() Review Comment: the division could be by 0 if someone sets window granularity as 0 manika137 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470562303 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -519,6 +519,42 @@ public class AbfsConfiguration{ DefaultValue = DEFAULT_FS_AZURE_ENABLE_CREATE_BLOB_IDEMPOTENCY) private boolean enableCreateIdempotency; + @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_ENABLE_TAIL_LATENCY_TRACKER, + DefaultValue = DEFAULT_FS_AZURE_ENABLE_TAIL_LATENCY_TRACKER) + private boolean isTailLatencyTrackerEnabled; + + @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_ENABLE_TAIL_LATENCY_REQUEST_TIMEOUT, + DefaultValue = DEFAULT_FS_AZURE_ENABLE_TAIL_LATENCY_REQUEST_TIMEOUT) + private boolean isTailLatencyRequestTimeoutEnabled; + + @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_TAIL_LATENCY_PERCENTILE, + DefaultValue = DEFAULT_FS_AZURE_TAIL_LATENCY_PERCENTILE) + private int tailLatencyPercentile; + + @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_TAIL_LATENCY_MIN_DEVIATION, + DefaultValue = DEFAULT_FS_AZURE_TAIL_LATENCY_MIN_DEVIATION) + private int tailLatencyMinDeviation; + + @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_TAIL_LATENCY_MIN_SAMPLE_SIZE, + DefaultValue = DEFAULT_FS_AZURE_TAIL_LATENCY_MIN_SAMPLE_SIZE) + private int tailLatencyMinSampleSize; + + @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_TAIL_LATENCY_ANALYSIS_WINDOW_MILLIS, + DefaultValue = DEFAULT_FS_AZURE_TAIL_LATENCY_ANALYSIS_WINDOW_MILLIS) + private int tailLatencyAnalysisWindowInMillis; + + @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_TAIL_LATENCY_ANALYSIS_WINDOW_GRANULARITY, + DefaultValue = DEFAULT_FS_AZURE_TAIL_LATENCY_ANALYSIS_WINDOW_GRANULARITY) Review Comment: should we have a min, max value for window size above and window granularity? anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470579619 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -266,5 +266,15 @@ public final class FileSystemConfigurations { public static final boolean DEFAULT_FS_AZURE_ENABLE_CREATE_BLOB_IDEMPOTENCY = true; + public static final boolean DEFAULT_FS_AZURE_ENABLE_TAIL_LATENCY_TRACKER = false; + public static final boolean DEFAULT_FS_AZURE_ENABLE_TAIL_LATENCY_REQUEST_TIMEOUT = false; + public static final int DEFAULT_FS_AZURE_TAIL_LATENCY_PERCENTILE = 99; Review Comment: Nice suggestion. Will take it up. anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470580745 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/exceptions/TailLatencyRequestTimeoutException.java: ########## @@ -0,0 +1,39 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * <p> + * http://www.apache.org/licenses/LICENSE-2.0 + * <p> + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.contracts.exceptions; + +import java.util.concurrent.TimeoutException; +import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_TAIL_LATENCY_REQUEST_TIMEOUT; + +/** + * Thrown when a request takes more time than the current reported tail latency. + */ +public class TailLatencyRequestTimeoutException extends AzureBlobFileSystemException { + + /** + * Constructs a TailLatencyRequestTimeoutException with TimeoutException as the cause. + */ + public TailLatencyRequestTimeoutException(TimeoutException innerException) { Review Comment: Added ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/exceptions/TailLatencyRequestTimeoutException.java: ########## @@ -0,0 +1,39 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * <p> + * http://www.apache.org/licenses/LICENSE-2.0 + * <p> + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.contracts.exceptions; + +import java.util.concurrent.TimeoutException; +import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_TAIL_LATENCY_REQUEST_TIMEOUT; + +/** + * Thrown when a request takes more time than the current reported tail latency. + */ +public class TailLatencyRequestTimeoutException extends AzureBlobFileSystemException { + + /** + * Constructs a TailLatencyRequestTimeoutException with TimeoutException as the cause. + */ + public TailLatencyRequestTimeoutException(TimeoutException innerException) { + super(ERR_TAIL_LATENCY_REQUEST_TIMEOUT, innerException); + } + + public TailLatencyRequestTimeoutException() { Review Comment: Added manika137 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470581573 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsTailLatencyTracker.java: ########## @@ -0,0 +1,159 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import java.util.HashMap; +import java.util.Map; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.locks.ReentrantLock; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; + +/** + * Account Specific Latency Tracker. + * This class tracks the latency of various operations like read, write etc for a single account. + * It maintains a sliding window histogram for each operation type to analyze latency patterns over time. + */ +public class AbfsTailLatencyTracker { + + private static final Logger LOG = LoggerFactory.getLogger( + AbfsTailLatencyTracker.class); + private static AbfsTailLatencyTracker singleton; + private static final ReentrantLock LOCK = new ReentrantLock(); + private static final int HISTOGRAM_MAX_VALUE = 60_000; + private static final int HISTOGRAM_SIGNIFICANT_FIGURES = 3; + private final Map<AbfsRestOperationType, SlidingWindowHdrHistogram> + operationLatencyMap = new HashMap<>(); + private final AbfsConfiguration configuration; + + /** + * Constructor to initialize the latency tracker with configuration. + * @param abfsConfiguration Configuration settings for latency tracking. + */ + public AbfsTailLatencyTracker(AbfsConfiguration abfsConfiguration) { + this.configuration = abfsConfiguration; + ScheduledExecutorService histogramRotatorThread = Executors.newSingleThreadScheduledExecutor( + r -> { + Thread t = new Thread(r, \"Histogram-Rotator-Thread\"); + t.setDaemon(true); + return t; + }); + long rotationInterval = configuration.getTailLatencyAnalysisWindowInMillis() + / configuration.getTailLatencyAnalysisWindowGranularity(); + histogramRotatorThread.scheduleAtFixedRate(this::rotateHistograms, + rotationInterval, rotationInterval, TimeUnit.MILLISECONDS); + + + ScheduledExecutorService tailLatencyComputationThread = Executors.newSingleThreadScheduledExecutor( + r -> { + Thread t = new Thread(r, \"Tail-Latency-Computation-Thread\"); + t.setDaemon(true); + return t; + }); + + long computationalInterval = configuration.getTailLatencyPercentileComputationIntervalInMillis(); + tailLatencyComputationThread.scheduleAtFixedRate(this::computePercentiles, + computationalInterval, computationalInterval, TimeUnit.MILLISECONDS); + } + + /** + * Rotates all histograms to ensure they reflect the most recent latency data. + * This method is called periodically based on the configured rotation interval. + */ + private void rotateHistograms() { + for (SlidingWindowHdrHistogram histogram : operationLatencyMap.values()) { + histogram.rotateIfNeeded(); + } + } + + /** + * Computes the tail latency percentiles for all operation types. + * This method is called periodically based on the configured computation interval. + */ + private void computePercentiles() { + for (SlidingWindowHdrHistogram histogram : operationLatencyMap.values()) { + histogram.computeLatency(); + } + } + + /** + * Creates a singleton object of the {@link SlidingWindowHdrHistogram}. + * which is shared across all filesystem instances. + * @param abfsConfiguration configuration set. + * @return singleton object of intercept. + */ + static AbfsTailLatencyTracker initializeSingleton(AbfsConfiguration abfsConfiguration) { + if (singleton == null) { + LOCK.lock(); + try { + if (singleton == null) { + singleton = new AbfsTailLatencyTracker(abfsConfiguration); Review Comment: we could log the initialization with the granularity etc configs here anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470582171 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsApacheHttpClient.java: ########## @@ -132,6 +138,30 @@ public void close() throws IOException { * @throws IOException network error. */ public HttpResponse execute(HttpRequestBase httpRequest, + final AbfsManagedHttpClientContext abfsHttpClientContext, + final int connectTimeout, + final int readTimeout, + final long tailLatencyTimeout) throws IOException { + if (tailLatencyTimeout <= 0) { + return executeWithoutDeadline(httpRequest, abfsHttpClientContext, + connectTimeout, readTimeout); + } + return executeWithDeadline(httpRequest, abfsHttpClientContext, + connectTimeout, readTimeout, tailLatencyTimeout); + } + + /** + * Executes the HTTP request. + * + * @param httpRequest HTTP request to execute. + * @param abfsHttpClientContext HttpClient context. + * @param connectTimeout Connection timeout. + * @param readTimeout Read timeout. + * + * @return HTTP response. + * @throws IOException network error. + */ + public HttpResponse executeWithoutDeadline(HttpRequestBase httpRequest, Review Comment: Taken anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470583329 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -611,10 +630,30 @@ AbfsJdkHttpOperation createAbfsHttpOperation() throws IOException { @VisibleForTesting AbfsAHCHttpOperation createAbfsAHCHttpOperation() throws IOException { + long tailLatency = getTailLatencyTimeoutIfEnabled(); return new AbfsAHCHttpOperation(url, method, requestHeaders, Duration.ofMillis(client.getAbfsConfiguration().getHttpConnectionTimeout()), Duration.ofMillis(client.getAbfsConfiguration().getHttpReadTimeout()), - client.getAbfsApacheHttpClient(), client); + tailLatency, client.getAbfsApacheHttpClient(), client); + } + + /** + * Get Tail Latency Timeout value if profiling is enabled, timeout is enabled + * and retries due to tail latency request timeout is allowed. + * @return tail latency timeout value else return zero. + */ + long getTailLatencyTimeoutIfEnabled() { + if (isTailLatencyTimeoutEnabled() && shouldTailLatencyTimeout) { + return (long) tailLatencyTracker.getTailLatency(this.operationType); + } + return ZERO; + } + + boolean isTailLatencyTimeoutEnabled() { Review Comment: Added anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470586126 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsTailLatencyTracker.java: ########## @@ -0,0 +1,159 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import java.util.HashMap; +import java.util.Map; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.locks.ReentrantLock; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; + +/** + * Account Specific Latency Tracker. + * This class tracks the latency of various operations like read, write etc for a single account. + * It maintains a sliding window histogram for each operation type to analyze latency patterns over time. + */ +public class AbfsTailLatencyTracker { + + private static final Logger LOG = LoggerFactory.getLogger( + AbfsTailLatencyTracker.class); + private static AbfsTailLatencyTracker singleton; Review Comment: Taken anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470589733 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsTailLatencyTracker.java: ########## @@ -0,0 +1,159 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import java.util.HashMap; +import java.util.Map; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.locks.ReentrantLock; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; + +/** + * Account Specific Latency Tracker. + * This class tracks the latency of various operations like read, write etc for a single account. + * It maintains a sliding window histogram for each operation type to analyze latency patterns over time. + */ +public class AbfsTailLatencyTracker { + + private static final Logger LOG = LoggerFactory.getLogger( + AbfsTailLatencyTracker.class); + private static AbfsTailLatencyTracker singleton; + private static final ReentrantLock LOCK = new ReentrantLock(); + private static final int HISTOGRAM_MAX_VALUE = 60_000; + private static final int HISTOGRAM_SIGNIFICANT_FIGURES = 3; + private final Map<AbfsRestOperationType, SlidingWindowHdrHistogram> + operationLatencyMap = new HashMap<>(); + private final AbfsConfiguration configuration; + + /** + * Constructor to initialize the latency tracker with configuration. + * @param abfsConfiguration Configuration settings for latency tracking. + */ + public AbfsTailLatencyTracker(AbfsConfiguration abfsConfiguration) { + this.configuration = abfsConfiguration; + ScheduledExecutorService histogramRotatorThread = Executors.newSingleThreadScheduledExecutor( + r -> { + Thread t = new Thread(r, \"Histogram-Rotator-Thread\"); + t.setDaemon(true); + return t; + }); + long rotationInterval = configuration.getTailLatencyAnalysisWindowInMillis() + / configuration.getTailLatencyAnalysisWindowGranularity(); + histogramRotatorThread.scheduleAtFixedRate(this::rotateHistograms, + rotationInterval, rotationInterval, TimeUnit.MILLISECONDS); + + + ScheduledExecutorService tailLatencyComputationThread = Executors.newSingleThreadScheduledExecutor( Review Comment: Latency tracker are per account basis. They are shared across all filesystem in a single JVM. These are daemon threads and will be killed when JVM gets killed manika137 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470589786 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -531,6 +544,12 @@ private boolean executeHttpOperation(final int retryCount, if (shouldUpdateCSTMetrics(statusCode) && !wasKnownExceptionThrown) { intercept.updateMetrics(operationType, httpOperation); } + + // Update Tail Latency Tracker only for successful requests. + if (tailLatencyTracker != null && statusCode < HttpURLConnection.HTTP_MULT_CHOICE) { + tailLatencyTracker.updateLatency(operationType, Review Comment: can 2 threads call updateLatency() for the same operation type simultaneously and create histograms? anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470595674 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsTailLatencyTracker.java: ########## @@ -0,0 +1,159 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import java.util.HashMap; +import java.util.Map; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.locks.ReentrantLock; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; + +/** + * Account Specific Latency Tracker. + * This class tracks the latency of various operations like read, write etc for a single account. + * It maintains a sliding window histogram for each operation type to analyze latency patterns over time. + */ +public class AbfsTailLatencyTracker { + + private static final Logger LOG = LoggerFactory.getLogger( + AbfsTailLatencyTracker.class); + private static AbfsTailLatencyTracker singleton; + private static final ReentrantLock LOCK = new ReentrantLock(); + private static final int HISTOGRAM_MAX_VALUE = 60_000; + private static final int HISTOGRAM_SIGNIFICANT_FIGURES = 3; + private final Map<AbfsRestOperationType, SlidingWindowHdrHistogram> + operationLatencyMap = new HashMap<>(); + private final AbfsConfiguration configuration; + + /** + * Constructor to initialize the latency tracker with configuration. + * @param abfsConfiguration Configuration settings for latency tracking. + */ + public AbfsTailLatencyTracker(AbfsConfiguration abfsConfiguration) { + this.configuration = abfsConfiguration; + ScheduledExecutorService histogramRotatorThread = Executors.newSingleThreadScheduledExecutor( + r -> { + Thread t = new Thread(r, \"Histogram-Rotator-Thread\"); + t.setDaemon(true); + return t; + }); + long rotationInterval = configuration.getTailLatencyAnalysisWindowInMillis() Review Comment: Taken anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470598024 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -519,6 +519,42 @@ public class AbfsConfiguration{ DefaultValue = DEFAULT_FS_AZURE_ENABLE_CREATE_BLOB_IDEMPOTENCY) private boolean enableCreateIdempotency; + @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_ENABLE_TAIL_LATENCY_TRACKER, + DefaultValue = DEFAULT_FS_AZURE_ENABLE_TAIL_LATENCY_TRACKER) + private boolean isTailLatencyTrackerEnabled; + + @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_ENABLE_TAIL_LATENCY_REQUEST_TIMEOUT, + DefaultValue = DEFAULT_FS_AZURE_ENABLE_TAIL_LATENCY_REQUEST_TIMEOUT) + private boolean isTailLatencyRequestTimeoutEnabled; + + @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_TAIL_LATENCY_PERCENTILE, + DefaultValue = DEFAULT_FS_AZURE_TAIL_LATENCY_PERCENTILE) + private int tailLatencyPercentile; + + @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_TAIL_LATENCY_MIN_DEVIATION, + DefaultValue = DEFAULT_FS_AZURE_TAIL_LATENCY_MIN_DEVIATION) + private int tailLatencyMinDeviation; + + @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_TAIL_LATENCY_MIN_SAMPLE_SIZE, + DefaultValue = DEFAULT_FS_AZURE_TAIL_LATENCY_MIN_SAMPLE_SIZE) + private int tailLatencyMinSampleSize; + + @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_TAIL_LATENCY_ANALYSIS_WINDOW_MILLIS, + DefaultValue = DEFAULT_FS_AZURE_TAIL_LATENCY_ANALYSIS_WINDOW_MILLIS) + private int tailLatencyAnalysisWindowInMillis; + + @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_TAIL_LATENCY_ANALYSIS_WINDOW_GRANULARITY, + DefaultValue = DEFAULT_FS_AZURE_TAIL_LATENCY_ANALYSIS_WINDOW_GRANULARITY) Review Comment: Added min value for granularity. anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470599110 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsTailLatencyTracker.java: ########## @@ -0,0 +1,159 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import java.util.HashMap; +import java.util.Map; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.locks.ReentrantLock; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; + +/** + * Account Specific Latency Tracker. + * This class tracks the latency of various operations like read, write etc for a single account. + * It maintains a sliding window histogram for each operation type to analyze latency patterns over time. + */ +public class AbfsTailLatencyTracker { + + private static final Logger LOG = LoggerFactory.getLogger( + AbfsTailLatencyTracker.class); + private static AbfsTailLatencyTracker singleton; + private static final ReentrantLock LOCK = new ReentrantLock(); + private static final int HISTOGRAM_MAX_VALUE = 60_000; + private static final int HISTOGRAM_SIGNIFICANT_FIGURES = 3; + private final Map<AbfsRestOperationType, SlidingWindowHdrHistogram> + operationLatencyMap = new HashMap<>(); + private final AbfsConfiguration configuration; + + /** + * Constructor to initialize the latency tracker with configuration. + * @param abfsConfiguration Configuration settings for latency tracking. + */ + public AbfsTailLatencyTracker(AbfsConfiguration abfsConfiguration) { + this.configuration = abfsConfiguration; + ScheduledExecutorService histogramRotatorThread = Executors.newSingleThreadScheduledExecutor( + r -> { + Thread t = new Thread(r, \"Histogram-Rotator-Thread\"); + t.setDaemon(true); + return t; + }); + long rotationInterval = configuration.getTailLatencyAnalysisWindowInMillis() + / configuration.getTailLatencyAnalysisWindowGranularity(); + histogramRotatorThread.scheduleAtFixedRate(this::rotateHistograms, + rotationInterval, rotationInterval, TimeUnit.MILLISECONDS); + + + ScheduledExecutorService tailLatencyComputationThread = Executors.newSingleThreadScheduledExecutor( + r -> { + Thread t = new Thread(r, \"Tail-Latency-Computation-Thread\"); + t.setDaemon(true); + return t; + }); + + long computationalInterval = configuration.getTailLatencyPercentileComputationIntervalInMillis(); + tailLatencyComputationThread.scheduleAtFixedRate(this::computePercentiles, + computationalInterval, computationalInterval, TimeUnit.MILLISECONDS); + } + + /** + * Rotates all histograms to ensure they reflect the most recent latency data. + * This method is called periodically based on the configured rotation interval. + */ + private void rotateHistograms() { + for (SlidingWindowHdrHistogram histogram : operationLatencyMap.values()) { + histogram.rotateIfNeeded(); + } + } + + /** + * Computes the tail latency percentiles for all operation types. + * This method is called periodically based on the configured computation interval. + */ + private void computePercentiles() { + for (SlidingWindowHdrHistogram histogram : operationLatencyMap.values()) { + histogram.computeLatency(); + } + } + + /** + * Creates a singleton object of the {@link SlidingWindowHdrHistogram}. + * which is shared across all filesystem instances. + * @param abfsConfiguration configuration set. + * @return singleton object of intercept. + */ + static AbfsTailLatencyTracker initializeSingleton(AbfsConfiguration abfsConfiguration) { + if (singleton == null) { + LOCK.lock(); + try { + if (singleton == null) { + singleton = new AbfsTailLatencyTracker(abfsConfiguration); Review Comment: Already in SlidingWindowHdrHistogram class manika137 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470608205 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsTailLatencyTracker.java: ########## @@ -0,0 +1,159 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import java.util.HashMap; +import java.util.Map; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.locks.ReentrantLock; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; + +/** + * Account Specific Latency Tracker. + * This class tracks the latency of various operations like read, write etc for a single account. + * It maintains a sliding window histogram for each operation type to analyze latency patterns over time. + */ +public class AbfsTailLatencyTracker { + + private static final Logger LOG = LoggerFactory.getLogger( + AbfsTailLatencyTracker.class); + private static AbfsTailLatencyTracker singleton; + private static final ReentrantLock LOCK = new ReentrantLock(); + private static final int HISTOGRAM_MAX_VALUE = 60_000; + private static final int HISTOGRAM_SIGNIFICANT_FIGURES = 3; + private final Map<AbfsRestOperationType, SlidingWindowHdrHistogram> + operationLatencyMap = new HashMap<>(); + private final AbfsConfiguration configuration; + + /** + * Constructor to initialize the latency tracker with configuration. + * @param abfsConfiguration Configuration settings for latency tracking. + */ + public AbfsTailLatencyTracker(AbfsConfiguration abfsConfiguration) { + this.configuration = abfsConfiguration; + ScheduledExecutorService histogramRotatorThread = Executors.newSingleThreadScheduledExecutor( + r -> { + Thread t = new Thread(r, \"Histogram-Rotator-Thread\"); + t.setDaemon(true); + return t; + }); + long rotationInterval = configuration.getTailLatencyAnalysisWindowInMillis() + / configuration.getTailLatencyAnalysisWindowGranularity(); + histogramRotatorThread.scheduleAtFixedRate(this::rotateHistograms, + rotationInterval, rotationInterval, TimeUnit.MILLISECONDS); + + + ScheduledExecutorService tailLatencyComputationThread = Executors.newSingleThreadScheduledExecutor( + r -> { + Thread t = new Thread(r, \"Tail-Latency-Computation-Thread\"); + t.setDaemon(true); + return t; + }); + + long computationalInterval = configuration.getTailLatencyPercentileComputationIntervalInMillis(); + tailLatencyComputationThread.scheduleAtFixedRate(this::computePercentiles, + computationalInterval, computationalInterval, TimeUnit.MILLISECONDS); + } + + /** + * Rotates all histograms to ensure they reflect the most recent latency data. + * This method is called periodically based on the configured rotation interval. + */ + private void rotateHistograms() { + for (SlidingWindowHdrHistogram histogram : operationLatencyMap.values()) { + histogram.rotateIfNeeded(); + } + } + + /** + * Computes the tail latency percentiles for all operation types. + * This method is called periodically based on the configured computation interval. + */ + private void computePercentiles() { + for (SlidingWindowHdrHistogram histogram : operationLatencyMap.values()) { + histogram.computeLatency(); + } + } + + /** + * Creates a singleton object of the {@link SlidingWindowHdrHistogram}. + * which is shared across all filesystem instances. + * @param abfsConfiguration configuration set. + * @return singleton object of intercept. + */ + static AbfsTailLatencyTracker initializeSingleton(AbfsConfiguration abfsConfiguration) { + if (singleton == null) { + LOCK.lock(); + try { + if (singleton == null) { + singleton = new AbfsTailLatencyTracker(abfsConfiguration); + } + } finally { + LOCK.unlock(); + } + } + return singleton; + } + + /** + * Updates the latency for a specific operation type. + * @param latency Latency value to be recorded. + * @param operationType Only applicable for read and write operations. + */ + public void updateLatency(final AbfsRestOperationType operationType, + final long latency) { + SlidingWindowHdrHistogram histogram = operationLatencyMap.get(operationType); + if (histogram == null) { + LOG.debug(\"Creating new histogram for operation: {}\", operationType); + histogram = new SlidingWindowHdrHistogram( + configuration.getTailLatencyAnalysisWindowInMillis(), + configuration.getTailLatencyAnalysisWindowGranularity(), + configuration.getTailLatencyMinSampleSize(), + configuration.getTailLatencyPercentile(), + configuration.getTailLatencyMinDeviation(), + HISTOGRAM_MAX_VALUE, HISTOGRAM_SIGNIFICANT_FIGURES, operationType); + operationLatencyMap.put(operationType, histogram); + } else { + LOG.debug(\"Using existing histogram for operation: {}\", operationType); + } + histogram.recordValue(latency); + LOG.debug(\"Updated latency for operation: {} with latency: {}\", + operationType, latency); + } + + /** + * Gets the tail latency for a specific operation type. + * @param operationType Only applicable for read and write operations. Review Comment: why only for read, write operations? we are not making the operationType check inside the method anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470616013 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -531,6 +544,12 @@ private boolean executeHttpOperation(final int retryCount, if (shouldUpdateCSTMetrics(statusCode) && !wasKnownExceptionThrown) { intercept.updateMetrics(operationType, httpOperation); } + + // Update Tail Latency Tracker only for successful requests. + if (tailLatencyTracker != null && statusCode < HttpURLConnection.HTTP_MULT_CHOICE) { + tailLatencyTracker.updateLatency(operationType, Review Comment: Nice catch. Added Lock while creation. anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2470619056 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsTailLatencyTracker.java: ########## @@ -0,0 +1,159 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import java.util.HashMap; +import java.util.Map; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.locks.ReentrantLock; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; + +/** + * Account Specific Latency Tracker. + * This class tracks the latency of various operations like read, write etc for a single account. + * It maintains a sliding window histogram for each operation type to analyze latency patterns over time. + */ +public class AbfsTailLatencyTracker { + + private static final Logger LOG = LoggerFactory.getLogger( + AbfsTailLatencyTracker.class); + private static AbfsTailLatencyTracker singleton; + private static final ReentrantLock LOCK = new ReentrantLock(); + private static final int HISTOGRAM_MAX_VALUE = 60_000; + private static final int HISTOGRAM_SIGNIFICANT_FIGURES = 3; + private final Map<AbfsRestOperationType, SlidingWindowHdrHistogram> + operationLatencyMap = new HashMap<>(); + private final AbfsConfiguration configuration; + + /** + * Constructor to initialize the latency tracker with configuration. + * @param abfsConfiguration Configuration settings for latency tracking. + */ + public AbfsTailLatencyTracker(AbfsConfiguration abfsConfiguration) { + this.configuration = abfsConfiguration; + ScheduledExecutorService histogramRotatorThread = Executors.newSingleThreadScheduledExecutor( + r -> { + Thread t = new Thread(r, \"Histogram-Rotator-Thread\"); + t.setDaemon(true); + return t; + }); + long rotationInterval = configuration.getTailLatencyAnalysisWindowInMillis() + / configuration.getTailLatencyAnalysisWindowGranularity(); + histogramRotatorThread.scheduleAtFixedRate(this::rotateHistograms, + rotationInterval, rotationInterval, TimeUnit.MILLISECONDS); + + + ScheduledExecutorService tailLatencyComputationThread = Executors.newSingleThreadScheduledExecutor( + r -> { + Thread t = new Thread(r, \"Tail-Latency-Computation-Thread\"); + t.setDaemon(true); + return t; + }); + + long computationalInterval = configuration.getTailLatencyPercentileComputationIntervalInMillis(); + tailLatencyComputationThread.scheduleAtFixedRate(this::computePercentiles, + computationalInterval, computationalInterval, TimeUnit.MILLISECONDS); + } + + /** + * Rotates all histograms to ensure they reflect the most recent latency data. + * This method is called periodically based on the configured rotation interval. + */ + private void rotateHistograms() { + for (SlidingWindowHdrHistogram histogram : operationLatencyMap.values()) { + histogram.rotateIfNeeded(); + } + } + + /** + * Computes the tail latency percentiles for all operation types. + * This method is called periodically based on the configured computation interval. + */ + private void computePercentiles() { + for (SlidingWindowHdrHistogram histogram : operationLatencyMap.values()) { + histogram.computeLatency(); + } + } + + /** + * Creates a singleton object of the {@link SlidingWindowHdrHistogram}. + * which is shared across all filesystem instances. + * @param abfsConfiguration configuration set. + * @return singleton object of intercept. + */ + static AbfsTailLatencyTracker initializeSingleton(AbfsConfiguration abfsConfiguration) { + if (singleton == null) { + LOCK.lock(); + try { + if (singleton == null) { + singleton = new AbfsTailLatencyTracker(abfsConfiguration); + } + } finally { + LOCK.unlock(); + } + } + return singleton; + } + + /** + * Updates the latency for a specific operation type. + * @param latency Latency value to be recorded. + * @param operationType Only applicable for read and write operations. + */ + public void updateLatency(final AbfsRestOperationType operationType, + final long latency) { + SlidingWindowHdrHistogram histogram = operationLatencyMap.get(operationType); + if (histogram == null) { + LOG.debug(\"Creating new histogram for operation: {}\", operationType); + histogram = new SlidingWindowHdrHistogram( + configuration.getTailLatencyAnalysisWindowInMillis(), + configuration.getTailLatencyAnalysisWindowGranularity(), + configuration.getTailLatencyMinSampleSize(), + configuration.getTailLatencyPercentile(), + configuration.getTailLatencyMinDeviation(), + HISTOGRAM_MAX_VALUE, HISTOGRAM_SIGNIFICANT_FIGURES, operationType); + operationLatencyMap.put(operationType, histogram); + } else { + LOG.debug(\"Using existing histogram for operation: {}\", operationType); + } + histogram.recordValue(latency); + LOG.debug(\"Updated latency for operation: {} with latency: {}\", + operationType, latency); + } + + /** + * Gets the tail latency for a specific operation type. + * @param operationType Only applicable for read and write operations. Review Comment: Updated hadoop-yetus commented on PR #8043: URL: https://github.com/apache/hadoop/pull/8043#issuecomment-3458139061 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 22m 44s | | trunk passed | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 27s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 20s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 28s | | trunk passed | | +1 :green_heart: | javadoc | 0m 27s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 22s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 45s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/4/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) | hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 14m 14s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 18s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/4/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 44 new + 3 unchanged - 0 fixed = 47 total (was 3) | | +1 :green_heart: | mvnsite | 0m 20s | | the patch passed | | -1 :x: | javadoc | 0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/4/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 34 new + 1472 unchanged - 0 fixed = 1506 total (was 1472) | | -1 :x: | javadoc | 0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/4/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 33 new + 1413 unchanged - 0 fixed = 1446 total (was 1413) | | -1 :x: | spotbugs | 0m 44s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/4/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) | hadoop-tools/hadoop-azure generated 5 new + 177 unchanged - 1 fixed = 182 total (was 178) | | +1 :green_heart: | shadedclient | 14m 10s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 10s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 20s | | The patch does not generate ASF License warnings. | | | | 60m 40s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient) At AbfsAHCHttpOperation.java:new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient) At AbfsAHCHttpOperation.java:[line 123] | | | Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext) At AbfsRestOperation.java:org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext) At AbfsRestOperation.java:[line 539] | | | new org.apache.hadoop.fs.azurebfs.services.AbfsTailLatencyTracker(AbfsConfiguration) may expose internal representation by storing an externally mutable object into AbfsTailLatencyTracker.configuration At AbfsTailLatencyTracker.java:representation by storing an externally mutable object into AbfsTailLatencyTracker.configuration At AbfsTailLatencyTracker.java:[line 55] | | | Possible null pointer dereference of histogram in org.apache.hadoop.fs.azurebfs.services.AbfsTailLatencyTracker.updateLatency(AbfsRestOperationType, long) Dereferenced at AbfsTailLatencyTracker.java:histogram in org.apache.hadoop.fs.azurebfs.services.AbfsTailLatencyTracker.updateLatency(AbfsRestOperationType, long) Dereferenced at AbfsTailLatencyTracker.java:[line 149] | | | Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.SlidingWindowHdrHistogram(long, int, int, int, int, long, int, AbfsRestOperationType) At SlidingWindowHdrHistogram.java:new org.apache.hadoop.fs.azurebfs.services.SlidingWindowHdrHistogram(long, int, int, int, int, long, int, AbfsRestOperationType) At SlidingWindowHdrHistogram.java:[line 81] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8043 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux c4d12ec6b6d0 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 3ca8f94cbd461d8904d9c655d446c2927dac0cd5 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/4/testReport/ | | Max. process+thread count | 611 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/4/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. bhattmanish98 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2477231657 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -359,7 +369,7 @@ void completeExecute(TracingContext tracingContext) @VisibleForTesting void updateBackoffMetrics(int retryCount, int statusCode) { if (abfsBackoffMetrics != null) { - if (statusCode < HttpURLConnection.HTTP_OK Review Comment: This change can we reverted. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -453,7 +463,7 @@ private boolean executeHttpOperation(final int retryCount, } incrementCounter(AbfsStatistic.GET_RESPONSES, 1); //Only increment bytesReceived counter when the status code is 2XX. - if (httpOperation.getStatusCode() >= HttpURLConnection.HTTP_OK Review Comment: same as above ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsTailLatencyTracker.java: ########## @@ -0,0 +1,75 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.junit.jupiter.api.Test; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest; +import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem; + +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ENABLE_TAIL_LATENCY_REQUEST_TIMEOUT; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ENABLE_TAIL_LATENCY_TRACKER; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_NETWORKING_LIBRARY; +import static org.assertj.core.api.Assertions.assertThat; + +public class ITestAbfsTailLatencyTracker extends AbstractAbfsIntegrationTest { + + protected ITestAbfsTailLatencyTracker() throws Exception { + } + + @Test + public void testTailLatencyTimeoutEnabled() throws Exception { Review Comment: Java doc missing. Please add it to all newly added test cases anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2480181549 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -266,5 +266,15 @@ public final class FileSystemConfigurations { public static final boolean DEFAULT_FS_AZURE_ENABLE_CREATE_BLOB_IDEMPOTENCY = true; + public static final boolean DEFAULT_FS_AZURE_ENABLE_TAIL_LATENCY_TRACKER = false; + public static final boolean DEFAULT_FS_AZURE_ENABLE_TAIL_LATENCY_REQUEST_TIMEOUT = false; + public static final int DEFAULT_FS_AZURE_TAIL_LATENCY_PERCENTILE = 99; Review Comment: I am not finding an easy way to make this change. Will add a work item for this improvement and take it up in follow up items. hadoop-yetus commented on PR #8043: URL: https://github.com/apache/hadoop/pull/8043#issuecomment-3471759836 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 21m 55s | | trunk passed | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 18s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 27s | | trunk passed | | +1 :green_heart: | javadoc | 0m 26s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 22s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 44s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/5/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) | hadoop-tools/hadoop-azure in trunk has 177 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 13m 58s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 12s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/5/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 25 new + 3 unchanged - 0 fixed = 28 total (was 3) | | +1 :green_heart: | mvnsite | 0m 21s | | the patch passed | | -1 :x: | javadoc | 0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/5/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 32 new + 1518 unchanged - 0 fixed = 1550 total (was 1518) | | -1 :x: | javadoc | 0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/5/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 31 new + 1412 unchanged - 0 fixed = 1443 total (was 1412) | | -1 :x: | spotbugs | 0m 46s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/5/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) | hadoop-tools/hadoop-azure generated 4 new + 176 unchanged - 1 fixed = 180 total (was 177) | | +1 :green_heart: | shadedclient | 14m 3s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 10s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 20s | | The patch does not generate ASF License warnings. | | | | 59m 35s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient) At AbfsAHCHttpOperation.java:new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient) At AbfsAHCHttpOperation.java:[line 123] | | | Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext) At AbfsRestOperation.java:org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext) At AbfsRestOperation.java:[line 539] | | | new org.apache.hadoop.fs.azurebfs.services.AbfsTailLatencyTracker(AbfsConfiguration) may expose internal representation by storing an externally mutable object into AbfsTailLatencyTracker.configuration At AbfsTailLatencyTracker.java:representation by storing an externally mutable object into AbfsTailLatencyTracker.configuration At AbfsTailLatencyTracker.java:[line 55] | | | Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.SlidingWindowHdrHistogram(long, int, int, int, int, long, int, AbfsRestOperationType) At SlidingWindowHdrHistogram.java:new org.apache.hadoop.fs.azurebfs.services.SlidingWindowHdrHistogram(long, int, int, int, int, long, int, AbfsRestOperationType) At SlidingWindowHdrHistogram.java:[line 95] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8043 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 4e580718cb8c 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a57f2afea1b3be581f31a6595e498ec2b2a42e3a | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/5/testReport/ | | Max. process+thread count | 633 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/5/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. anujmodi2021 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2480654128 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -453,7 +463,7 @@ private boolean executeHttpOperation(final int retryCount, } incrementCounter(AbfsStatistic.GET_RESPONSES, 1); //Only increment bytesReceived counter when the status code is 2XX. - if (httpOperation.getStatusCode() >= HttpURLConnection.HTTP_OK Review Comment: Taken ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -359,7 +369,7 @@ void completeExecute(TracingContext tracingContext) @VisibleForTesting void updateBackoffMetrics(int retryCount, int statusCode) { if (abfsBackoffMetrics != null) { - if (statusCode < HttpURLConnection.HTTP_OK Review Comment: Taken ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsTailLatencyTracker.java: ########## @@ -0,0 +1,75 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.junit.jupiter.api.Test; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest; +import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem; + +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ENABLE_TAIL_LATENCY_REQUEST_TIMEOUT; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ENABLE_TAIL_LATENCY_TRACKER; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_NETWORKING_LIBRARY; +import static org.assertj.core.api.Assertions.assertThat; + +public class ITestAbfsTailLatencyTracker extends AbstractAbfsIntegrationTest { + + protected ITestAbfsTailLatencyTracker() throws Exception { + } + + @Test + public void testTailLatencyTimeoutEnabled() throws Exception { Review Comment: Taken anmolanmol1234 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2480943824 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -1949,7 +1950,7 @@ public boolean isTailLatencyTrackerEnabled() { } public boolean isTailLatencyRequestTimeoutEnabled() { - return isTailLatencyRequestTimeoutEnabled && isTailLatencyTrackerEnabled + return isTailLatencyTrackerEnabled && isTailLatencyRequestTimeoutEnabled Review Comment: javadocs for warnings can be added anmolanmol1234 commented on code in PR #8043: URL: https://github.com/apache/hadoop/pull/8043#discussion_r2481457271 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsTailLatencyTracker.java: ########## @@ -131,11 +139,11 @@ public void updateLatency(final AbfsRestOperationType operationType, if (operationLatencyMap.get(operationType) == null) { LOG.debug(\"Creating new histogram for operation: {}\", operationType); histogram = new SlidingWindowHdrHistogram( - configuration.getTailLatencyAnalysisWindowInMillis(), - configuration.getTailLatencyAnalysisWindowGranularity(), - configuration.getTailLatencyMinSampleSize(), - configuration.getTailLatencyPercentile(), - configuration.getTailLatencyMinDeviation(), + talLatencyAnalysisWindowInMillis, Review Comment: nit: spelling of tail hadoop-yetus commented on PR #8043: URL: https://github.com/apache/hadoop/pull/8043#issuecomment-3473362007 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 22m 32s | | trunk passed | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 18s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 27s | | trunk passed | | +1 :green_heart: | javadoc | 0m 26s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 45s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/6/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) | hadoop-tools/hadoop-azure in trunk has 177 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 14m 58s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 22s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 10s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 21s | | the patch passed | | -1 :x: | javadoc | 0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/6/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 21 new + 1517 unchanged - 1 fixed = 1538 total (was 1518) | | -1 :x: | javadoc | 0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/6/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 19 new + 1412 unchanged - 0 fixed = 1431 total (was 1412) | | -1 :x: | spotbugs | 0m 47s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/6/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) | hadoop-tools/hadoop-azure generated 2 new + 176 unchanged - 1 fixed = 178 total (was 177) | | +1 :green_heart: | shadedclient | 15m 45s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 10s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 20s | | The patch does not generate ASF License warnings. | | | | 63m 1s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient) At AbfsAHCHttpOperation.java:new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient) At AbfsAHCHttpOperation.java:[line 123] | | | Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext) At AbfsRestOperation.java:org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext) At AbfsRestOperation.java:[line 539] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/6/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8043 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 7c5c2cb63961 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 56129accd046c6ae237402d7f64894da6ba44046 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/6/testReport/ | | Max. process+thread count | 639 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/6/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #8043: URL: https://github.com/apache/hadoop/pull/8043#issuecomment-3473551869 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 32s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 33m 33s | | trunk passed | | +1 :green_heart: | compile | 0m 45s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 44s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 48s | | trunk passed | | +1 :green_heart: | javadoc | 0m 43s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 38s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 1m 25s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/7/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) | hadoop-tools/hadoop-azure in trunk has 177 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 28m 33s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 40s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 37s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 37s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 21s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 43s | | the patch passed | | -1 :x: | javadoc | 0m 31s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/7/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 21 new + 1517 unchanged - 1 fixed = 1538 total (was 1518) | | -1 :x: | javadoc | 0m 28s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/7/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 19 new + 1412 unchanged - 0 fixed = 1431 total (was 1412) | | -1 :x: | spotbugs | 1m 23s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/7/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) | hadoop-tools/hadoop-azure generated 2 new + 176 unchanged - 1 fixed = 178 total (was 177) | | +1 :green_heart: | shadedclient | 26m 57s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 4s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 105m 11s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient) At AbfsAHCHttpOperation.java:new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient) At AbfsAHCHttpOperation.java:[line 123] | | | Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext) At AbfsRestOperation.java:org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext) At AbfsRestOperation.java:[line 539] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/7/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8043 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 86ea63abc892 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 56129accd046c6ae237402d7f64894da6ba44046 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/7/testReport/ | | Max. process+thread count | 634 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/7/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-17T05:41:02.000+0000", "updated": "2025-10-31T15:17:41.000+0000", "derived": {"summary_task": "Summarize this issue: It has been observed that certain requests taking more time than expected to complete hinders the performance of whole workload. Such requests are known as tailing requests. They can be taking more time due to a number of reasons and the prominent among them is a bad network connection. In Abfs driver we cache network connections and keeping such bad connections in cache and reusing them can be ba", "classification_task": "Classify the issue priority and type: It has been observed that certain requests taking more time than expected to complete hinders the performance of whole workload. Such requests are known as tailing requests. They can be taking more time due to a number of reasons and the prominent among them is a bad network connection. In Abfs driver we cache network connections and keeping such bad connections in cache and reusing them can be ba", "qna_task": "Question: What is this issue about?\nAnswer: ABFS: [Perf] Network Profiling of Tailing Requests and Killing Bad Connections Proactively"}}
{"id": "13631619", "key": "HADOOP-19728", "project": "HADOOP", "summary": "S3A: add ipv6 support", "description": "Support IPv6 with a flag to enable/disable dual stack endpoints https://docs.aws.amazon.com/AmazonS3/latest/API/ipv6-access.html", "comments": "", "created": "2025-10-15T13:16:53.000+0000", "updated": "2025-10-15T13:16:53.000+0000", "derived": {"summary_task": "Summarize this issue: Support IPv6 with a flag to enable/disable dual stack endpoints https://docs.aws.amazon.com/AmazonS3/latest/API/ipv6-access.html", "classification_task": "Classify the issue priority and type: Support IPv6 with a flag to enable/disable dual stack endpoints https://docs.aws.amazon.com/AmazonS3/latest/API/ipv6-access.html", "qna_task": "Question: What is this issue about?\nAnswer: S3A: add ipv6 support"}}
{"id": "13631523", "key": "HADOOP-19727", "project": "HADOOP", "summary": "Release hadoop-thirdparty 1.5.0", "description": "Release hadoop-thirdparty 1.5.0", "comments": "", "created": "2025-10-14T14:34:32.000+0000", "updated": "2025-10-20T16:40:40.000+0000", "derived": {"summary_task": "Summarize this issue: Release hadoop-thirdparty 1.5.0", "classification_task": "Classify the issue priority and type: Release hadoop-thirdparty 1.5.0", "qna_task": "Question: What is this issue about?\nAnswer: Release hadoop-thirdparty 1.5.0"}}
{"id": "13631334", "key": "HADOOP-19726", "project": "HADOOP", "summary": "Add JDK 17 compile options for maven-surefire-plugin in hadoop-tos module", "description": "Currently, the {{hadoop-tos}} module does not have the JDK 17 compile options configured for the {{{}maven-surefire-plugin{}}}, which causes the following error during unit test execution: {code:java} java.lang.IllegalStateException: Failed to set environment variable at org.apache.hadoop.fs.tosfs.util.TestUtility.setSystemEnv(TestUtility.java:120) at org.apache.hadoop.fs.tosfs.object.ObjectStorageTestBase.setUp(ObjectStorageTestBase.java:59) at java.base/java.lang.reflect.Method.invoke(Method.java:569) at java.base/java.util.ArrayList.forEach(ArrayList.java:1511) at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make field private final java.util.Map java.util.Collections$UnmodifiableMap.m accessible: module java.base does not \"opens java.util\" to unnamed module @69eee410 at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354) at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297) at java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178) at java.base/java.lang.reflect.Field.setAccessible(Field.java:172) at org.apache.hadoop.fs.tosfs.util.TestUtility.setSystemEnv(TestUtility.java:116) ... 4 more {code} This error occurs due to the module system restrictions in JDK 17, where reflection cannot access private fields in the java.util.Collections$UnmodifiableMap class. To resolve this issue, JDK 17 compile options have been added to ensure the maven-surefire-plugin works correctly in a JDK 17 environment. This PR adds the necessary compile options for maven-surefire-plugin to support JDK 17, fixing the error and ensuring that unit tests can run smoothly.", "comments": "slfan1989 opened a new pull request, #8029: URL: https://github.com/apache/hadoop/pull/8029 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR JIRA: HADOOP-19726. [JDK17] Add JDK 17 compile options for maven-surefire-plugin in hadoop-tos module. ### How was this patch tested? CI ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #8029: URL: https://github.com/apache/hadoop/pull/8029#issuecomment-3395542255 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 8m 39s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 25m 39s | | trunk passed | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 18s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | mvnsite | 0m 21s | | trunk passed | | +1 :green_heart: | javadoc | 0m 20s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 17s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | shadedclient | 42m 23s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 14s | | the patch passed | | +1 :green_heart: | compile | 0m 14s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 14s | | the patch passed | | +1 :green_heart: | compile | 0m 13s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 13s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 13s | | the patch passed | | +1 :green_heart: | javadoc | 0m 13s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | javadoc | 0m 12s | [/patch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/1/artifact/out/patch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-tos in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | shadedclient | 1m 41s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 13s | [/patch-unit-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/1/artifact/out/patch-unit-hadoop-cloud-storage-project_hadoop-tos.txt) | hadoop-tos in the patch failed. | | +1 :green_heart: | asflicense | 0m 18s | | The patch does not generate ASF License warnings. | | | | 55m 3s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8029 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux dd7bc96b56b5 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f188198cdec1613ae955ea31795a8cb1ca496139 | | Default Java | Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/1/testReport/ | | Max. process+thread count | 576 (vs. ulimit of 5500) | | modules | C: hadoop-cloud-storage-project/hadoop-tos U: hadoop-cloud-storage-project/hadoop-tos | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/1/console | | versions | git=2.25.1 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #8029: URL: https://github.com/apache/hadoop/pull/8029#issuecomment-3411382684 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 24m 30s | | trunk passed | | +1 :green_heart: | compile | 0m 19s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 21s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | mvnsite | 0m 21s | | trunk passed | | +1 :green_heart: | javadoc | 0m 22s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 20s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | shadedclient | 40m 22s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 14s | | the patch passed | | +1 :green_heart: | compile | 0m 13s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 13s | | the patch passed | | +1 :green_heart: | compile | 0m 14s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 14s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 15s | | the patch passed | | +1 :green_heart: | javadoc | 0m 14s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 13s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | shadedclient | 14m 55s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 26s | [/patch-unit-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/2/artifact/out/patch-unit-hadoop-cloud-storage-project_hadoop-tos.txt) | hadoop-tos in the patch passed. | | +1 :green_heart: | asflicense | 0m 19s | | The patch does not generate ASF License warnings. | | | | 58m 21s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.tosfs.object.TestObjectMultiRangeInputStream | | | hadoop.fs.tosfs.object.TestObjectRangeInputStream | | | hadoop.fs.tosfs.object.tos.auth.TestEnvironmentCredentialsProvider | | | hadoop.fs.tosfs.object.tos.auth.TestDefaultCredentialsProviderChain | | | hadoop.fs.tosfs.object.TestObjectOutputStream | | | hadoop.fs.tosfs.commit.TestMagicOutputStream | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8029 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux 3f25ded462da 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 2bd00ae481cc8a6aeb977fef2700e684236375a4 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/2/testReport/ | | Max. process+thread count | 616 (vs. ulimit of 5500) | | modules | C: hadoop-cloud-storage-project/hadoop-tos U: hadoop-cloud-storage-project/hadoop-tos | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/2/console | | versions | git=2.25.1 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #8029: URL: https://github.com/apache/hadoop/pull/8029#issuecomment-3411690102 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 25m 6s | | trunk passed | | +1 :green_heart: | compile | 0m 19s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 18s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 16s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 24s | | trunk passed | | +1 :green_heart: | javadoc | 0m 21s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 35s | [/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/3/artifact/out/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos-warnings.html) | hadoop-cloud-storage-project/hadoop-tos in trunk has 56 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 14m 11s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 14s | | the patch passed | | +1 :green_heart: | compile | 0m 12s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 12s | | the patch passed | | +1 :green_heart: | compile | 0m 14s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 14s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 8s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 15s | | the patch passed | | +1 :green_heart: | javadoc | 0m 14s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 13s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | spotbugs | 0m 31s | | the patch passed | | +1 :green_heart: | shadedclient | 14m 0s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 27s | [/patch-unit-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/3/artifact/out/patch-unit-hadoop-cloud-storage-project_hadoop-tos.txt) | hadoop-tos in the patch passed. | | +1 :green_heart: | asflicense | 0m 18s | | The patch does not generate ASF License warnings. | | | | 60m 57s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.tosfs.object.TestObjectMultiRangeInputStream | | | hadoop.fs.tosfs.object.TestObjectRangeInputStream | | | hadoop.fs.tosfs.object.tos.auth.TestEnvironmentCredentialsProvider | | | hadoop.fs.tosfs.object.tos.auth.TestDefaultCredentialsProviderChain | | | hadoop.fs.tosfs.object.TestObjectOutputStream | | | hadoop.fs.tosfs.commit.TestMagicOutputStream | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8029 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 9e3548929018 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 9ba2bc5bf9cda429475a820bfcf689479e7fdc2d | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/3/testReport/ | | Max. process+thread count | 639 (vs. ulimit of 5500) | | modules | C: hadoop-cloud-storage-project/hadoop-tos U: hadoop-cloud-storage-project/hadoop-tos | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/3/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #8029: URL: https://github.com/apache/hadoop/pull/8029#issuecomment-3412145538 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 44s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 24m 9s | | trunk passed | | +1 :green_heart: | compile | 0m 19s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 18s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 16s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 20s | | trunk passed | | +1 :green_heart: | javadoc | 0m 23s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 34s | [/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/4/artifact/out/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos-warnings.html) | hadoop-cloud-storage-project/hadoop-tos in trunk has 56 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 14m 3s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 15s | | the patch passed | | +1 :green_heart: | compile | 0m 14s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 14s | | the patch passed | | +1 :green_heart: | compile | 0m 16s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 16s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 8s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 14s | | the patch passed | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 13s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | spotbugs | 0m 31s | | the patch passed | | +1 :green_heart: | shadedclient | 14m 12s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 58s | | hadoop-tos in the patch passed. | | +1 :green_heart: | asflicense | 0m 20s | | The patch does not generate ASF License warnings. | | | | 60m 20s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8029 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 6a680a7543dc 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1b29c9884dbd1895ea8116fe1c0cf495ce03d39f | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/4/testReport/ | | Max. process+thread count | 643 (vs. ulimit of 5500) | | modules | C: hadoop-cloud-storage-project/hadoop-tos U: hadoop-cloud-storage-project/hadoop-tos | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/4/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on PR #8029: URL: https://github.com/apache/hadoop/pull/8029#issuecomment-3413251437 @wojiaodoubao Could you please help review this PR again? Thanks a lot! slfan1989 commented on PR #8029: URL: https://github.com/apache/hadoop/pull/8029#issuecomment-3418122640 I plan to merge this PR, as the unit test errors in TOS have been resolved. If further optimization is needed later, we can submit a separate PR for improvements. cc: @steveloughran @wojiaodoubao slfan1989 merged PR #8029: URL: https://github.com/apache/hadoop/pull/8029", "created": "2025-10-12T23:47:15.000+0000", "updated": "2025-10-19T03:00:16.000+0000", "derived": {"summary_task": "Summarize this issue: Currently, the {{hadoop-tos}} module does not have the JDK 17 compile options configured for the {{{}maven-surefire-plugin{}}}, which causes the following error during unit test execution: {code:java} java.lang.IllegalStateException: Failed to set environment variable at org.apache.hadoop.fs.tosfs.util.TestUtility.setSystemEnv(TestUtility.java:120) at org.apache.hadoop.fs.tosfs.object.ObjectStorag", "classification_task": "Classify the issue priority and type: Currently, the {{hadoop-tos}} module does not have the JDK 17 compile options configured for the {{{}maven-surefire-plugin{}}}, which causes the following error during unit test execution: {code:java} java.lang.IllegalStateException: Failed to set environment variable at org.apache.hadoop.fs.tosfs.util.TestUtility.setSystemEnv(TestUtility.java:120) at org.apache.hadoop.fs.tosfs.object.ObjectStorag", "qna_task": "Question: What is this issue about?\nAnswer: Add JDK 17 compile options for maven-surefire-plugin in hadoop-tos module"}}
{"id": "13631333", "key": "HADOOP-19725", "project": "HADOOP", "summary": "Upgrade SpotBugs Version to Support JDK 17 Compilation", "description": "The current SpotBugs version used in the project is 4.2.2 (SpotBugs) and 4.2.0 (SpotBugs Maven Plugin), which is not fully compatible with JDK 17 compilation. To ensure proper functionality of the code quality check tool in a JDK 17 environment, SpotBugs needs to be upgraded to the latest version that supports JDK 17.", "comments": "slfan1989 opened a new pull request, #8028: URL: https://github.com/apache/hadoop/pull/8028 ### Description of PR JIRA: [JDK17] Upgrade SpotBugs Version to Support JDK 17 Compilation. ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? slfan1989 commented on PR #8028: URL: https://github.com/apache/hadoop/pull/8028#issuecomment-3397979702 @cnauroth @szetszwo After upgrading to JDK 17, I found that spotbug could not run properly because the current version does not support JDK 17. To resolve this issue, I upgraded the versions of the two related plugins. The changes have been tested locally, and the results are as expected. hadoop-yetus commented on PR #8028: URL: https://github.com/apache/hadoop/pull/8028#issuecomment-3398540465 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 14m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 36m 49s | | trunk passed | | +1 :green_heart: | compile | 15m 58s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 44s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | -1 :x: | mvnsite | 9m 55s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/1/artifact/out/branch-mvnsite-root.txt) | root in trunk failed. | | +1 :green_heart: | javadoc | 9m 22s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 8m 51s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | shadedclient | 124m 16s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 28m 30s | | the patch passed | | +1 :green_heart: | compile | 15m 17s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 17s | | the patch passed | | +1 :green_heart: | compile | 15m 43s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 43s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 7m 2s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/1/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | +1 :green_heart: | javadoc | 9m 7s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 8m 45s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | shadedclient | 53m 32s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 853m 28s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/1/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 44s | | The patch does not generate ASF License warnings. | | | | 1109m 31s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.tools.TestDFSAdmin | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8028 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux da08cd8994df 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 3b47bd160a2122ed84df1730e54cbfafa3ab60b8 | | Default Java | Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/1/testReport/ | | Max. process+thread count | 3529 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/1/console | | versions | git=2.25.1 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on PR #8028: URL: https://github.com/apache/hadoop/pull/8028#issuecomment-3399438331 I have completed the investigation of the mvnsite build issues and confirmed that the problem is related to the JDIFF module. We plan to submit a separate PR to fix and optimize this issue. slfan1989 commented on PR #8028: URL: https://github.com/apache/hadoop/pull/8028#issuecomment-3401292116 @steveloughran Could you please review this PR? Thank you very much! slfan1989 commented on PR #8028: URL: https://github.com/apache/hadoop/pull/8028#issuecomment-3405489643 @Hexiaoqiao Could you please review this PR? Thank you very much! szetszwo commented on code in PR #8028: URL: https://github.com/apache/hadoop/pull/8028#discussion_r2433007716 ########## pom.xml: ########## @@ -119,8 +119,8 @@ xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/x <maven-checkstyle-plugin.version>3.1.0</maven-checkstyle-plugin.version> <checkstyle.version>8.29</checkstyle.version> <dependency-check-maven.version>7.1.1</dependency-check-maven.version> - <spotbugs.version>4.2.2</spotbugs.version> - <spotbugs-maven-plugin.version>4.2.0</spotbugs-maven-plugin.version> + <spotbugs.version>4.8.3</spotbugs.version> + <spotbugs-maven-plugin.version>4.7.3.6</spotbugs-maven-plugin.version> Review Comment: Similarly, why not 4.9.7.0 (or 4.8.6.7)? ########## pom.xml: ########## @@ -119,8 +119,8 @@ xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/x <maven-checkstyle-plugin.version>3.1.0</maven-checkstyle-plugin.version> <checkstyle.version>8.29</checkstyle.version> <dependency-check-maven.version>7.1.1</dependency-check-maven.version> - <spotbugs.version>4.2.2</spotbugs.version> - <spotbugs-maven-plugin.version>4.2.0</spotbugs-maven-plugin.version> + <spotbugs.version>4.8.3</spotbugs.version> Review Comment: Why not 4.9.7 (or 4.8.6)? https://mvnrepository.com/artifact/com.github.spotbugs/spotbugs slfan1989 commented on code in PR #8028: URL: https://github.com/apache/hadoop/pull/8028#discussion_r2433103901 ########## pom.xml: ########## @@ -119,8 +119,8 @@ xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/x <maven-checkstyle-plugin.version>3.1.0</maven-checkstyle-plugin.version> <checkstyle.version>8.29</checkstyle.version> <dependency-check-maven.version>7.1.1</dependency-check-maven.version> - <spotbugs.version>4.2.2</spotbugs.version> - <spotbugs-maven-plugin.version>4.2.0</spotbugs-maven-plugin.version> + <spotbugs.version>4.8.3</spotbugs.version> Review Comment: Thank you for reviewing! You made a very good point \u2014 I\u2019ll update this PR accordingly. hadoop-yetus commented on PR #8028: URL: https://github.com/apache/hadoop/pull/8028#issuecomment-3409917424 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 32s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 37m 51s | | trunk passed | | +1 :green_heart: | compile | 15m 40s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | -1 :x: | mvnsite | 10m 13s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/2/artifact/out/branch-mvnsite-root.txt) | root in trunk failed. | | +1 :green_heart: | javadoc | 9m 11s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 8m 47s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | shadedclient | 125m 4s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 28m 20s | | the patch passed | | +1 :green_heart: | compile | 15m 11s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 11s | | the patch passed | | +1 :green_heart: | compile | 15m 58s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 58s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 7m 5s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/2/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | +1 :green_heart: | javadoc | 9m 8s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 8m 40s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | shadedclient | 53m 57s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 792m 24s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/2/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 46s | | The patch does not generate ASF License warnings. | | | | 1035m 29s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.tools.TestDFSAdmin | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8028 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux c50dc2503f2d 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c7ac33f5f774a890079fd002e27829cc7b38c67d | | Default Java | Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/2/testReport/ | | Max. process+thread count | 3553 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/2/console | | versions | git=2.25.1 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on PR #8028: URL: https://github.com/apache/hadoop/pull/8028#issuecomment-3411057566 > Thanks @slfan1989 . LGTM. +1. I think it is smooth after check spotbug release changes and other apache projects upgrade feedbacks. TBH, I am not check it with new JDK version carefully. @Hexiaoqiao Many thanks for reviewing the code! The new Sputbug plugin has been tested on JDK 17 and JDK 21 and works properly. slfan1989 merged PR #8028: URL: https://github.com/apache/hadoop/pull/8028 slfan1989 commented on PR #8028: URL: https://github.com/apache/hadoop/pull/8028#issuecomment-3411067429 @szetszwo @steveloughran @Hexiaoqiao @zhtttylz Thank you very much for reviewing the code!", "created": "2025-10-12T23:27:49.000+0000", "updated": "2025-10-16T14:06:32.000+0000", "derived": {"summary_task": "Summarize this issue: The current SpotBugs version used in the project is 4.2.2 (SpotBugs) and 4.2.0 (SpotBugs Maven Plugin), which is not fully compatible with JDK 17 compilation. To ensure proper functionality of the code quality check tool in a JDK 17 environment, SpotBugs needs to be upgraded to the latest version that supports JDK 17.", "classification_task": "Classify the issue priority and type: The current SpotBugs version used in the project is 4.2.2 (SpotBugs) and 4.2.0 (SpotBugs Maven Plugin), which is not fully compatible with JDK 17 compilation. To ensure proper functionality of the code quality check tool in a JDK 17 environment, SpotBugs needs to be upgraded to the latest version that supports JDK 17.", "qna_task": "Question: What is this issue about?\nAnswer: Upgrade SpotBugs Version to Support JDK 17 Compilation"}}
{"id": "13631320", "key": "HADOOP-19724", "project": "HADOOP", "summary": "[RISC-V] Add rv bulk CRC32 (non-CRC32C) optimized path", "description": "", "comments": "PeterPtroc opened a new pull request, #8031: URL: https://github.com/apache/hadoop/pull/8031 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR - Introduces a riscv64 native implementation path for CRC32 (CRC32C not optimized). - Adds runtime CPU feature detection on linux-riscv64 to enable hardware-accelerated CRC32 when available; falls back to the existing implementation if native is unavailable or disabled. Below are the performance changes observed using the built-in CRC32 benchmark. Although performance is poor when bpc <= 64, there are substantial improvements when bpc > 64. To keep the codebase simple and maintainable, I did not add bpc-size-specific handling. | bpc | #T | Native (origin) | Native (new) | \u0394 (MB/s) | \u0394% | |---:|---:|---:|---:|---:|---:| | 32 | 1 | 661.5 | 463.5 | -198.0 | -29.9% | | 32 | 2 | 642.6 | 491.4 | -151.2 | -23.5% | | 32 | 4 | 663.7 | 480.5 | -183.2 | -27.6% | | 32 | 8 | 653.0 | 472.0 | -181.0 | -27.7% | | 32 | 16 | 656.1 | 473.4 | -182.7 | -27.8% | | 64 | 1 | 793.9 | 318.0 | -475.9 | -59.9% | | 64 | 2 | 771.3 | 322.1 | -449.2 | -58.2% | | 64 | 4 | 787.3 | 315.0 | -472.3 | -60.0% | | 64 | 8 | 778.0 | 309.3 | -468.7 | -60.2% | | 64 | 16 | 773.5 | 308.1 | -465.4 | -60.2% | | 128 | 1 | 878.8 | 2398.8 | +1520.0 | +173.0% | | 128 | 2 | 846.8 | 1723.9 | +877.1 | +103.6% | | 128 | 4 | 861.2 | 1690.0 | +828.8 | +96.2% | | 128 | 8 | 857.8 | 1373.3 | +515.5 | +60.1% | | 128 | 16 | 853.8 | 1361.3 | +507.5 | +59.4% | | 256 | 1 | 783.9 | 2752.5 | +1968.6 | +251.1% | | 256 | 2 | 810.0 | 2053.3 | +1243.3 | +153.5% | | 256 | 4 | 835.2 | 1966.5 | +1131.3 | +135.5% | | 256 | 8 | 812.4 | 1756.3 | +943.9 | +116.2% | | 256 | 16 | 811.8 | 1524.7 | +712.9 | +87.8% | | 512 | 1 | 923.6 | 3328.9 | +2405.3 | +260.4% | | 512 | 2 | 886.5 | 3295.1 | +2408.6 | +271.7% | | 512 | 4 | 910.5 | 2359.9 | +1449.4 | +159.2% | | 512 | 8 | 888.1 | 1637.4 | +749.3 | +84.4% | | 512 | 16 | 897.0 | 1840.1 | +943.1 | +105.1% | | 1024 | 1 | 950.4 | 3045.0 | +2094.6 | +220.4% | | 1024 | 2 | 918.0 | 2202.9 | +1284.9 | +140.0% | | 1024 | 4 | 937.6 | 2040.4 | +1102.8 | +117.6% | | 1024 | 8 | 916.5 | 1961.5 | +1045.0 | +114.0% | | 1024 | 16 | 927.4 | 2003.9 | +1076.5 | +116.1% | | 2048 | 1 | 962.3 | 3189.1 | +2226.8 | +231.4% | | 2048 | 2 | 970.1 | 3192.3 | +2222.2 | +229.1% | | 2048 | 4 | 943.4 | 2411.2 | +1467.8 | +155.6% | | 2048 | 8 | 937.6 | 1837.7 | +900.1 | +96.0% | | 2048 | 16 | 933.1 | 1864.0 | +930.9 | +99.8% | | 4096 | 1 | 969.9 | 3654.5 | +2684.6 | +276.8% | | 4096 | 2 | 972.0 | 2798.0 | +1826.0 | +187.9% | | 4096 | 4 | 960.1 | 2307.0 | +1346.9 | +140.3% | | 4096 | 8 | 948.2 | 2753.1 | +1804.9 | +190.4% | | 4096 | 16 | 938.7 | 2170.5 | +1231.8 | +131.2% | | 8192 | 1 | 973.6 | 4008.1 | +3034.5 | +311.7% | | 8192 | 2 | 922.5 | 3018.2 | +2095.7 | +227.2% | | 8192 | 4 | 955.6 | 2968.7 | +2013.1 | +210.7% | | 8192 | 8 | 943.4 | 2077.9 | +1134.5 | +120.3% | | 8192 | 16 | 944.9 | 2191.7 | +1246.8 | +132.0% | | 16384 | 1 | 974.4 | 4090.3 | +3115.9 | +319.8% | | 16384 | 2 | 978.3 | 2999.6 | +2021.3 | +206.6% | | 16384 | 4 | 956.6 | 3248.9 | +2292.3 | +239.6% | | 16384 | 8 | 950.8 | 3228.0 | +2277.2 | +239.5% | | 16384 | 16 | 941.2 | 2832.1 | +1890.9 | +200.9% | | 32768 | 1 | 972.2 | 4205.7 | +3233.5 | +332.6% | | 32768 | 2 | 938.6 | 4115.2 | +3176.6 | +338.4% | | 32768 | 4 | 957.4 | 2508.9 | +1551.5 | +162.1% | | 32768 | 8 | 952.8 | 2319.8 | +1367.0 | +143.5% | | 32768 | 16 | 944.5 | 1657.7 | +713.2 | +75.5% | | 65536 | 1 | 976.3 | 4226.6 | +3250.3 | +332.9% | | 65536 | 2 | 940.0 | 3075.8 | +2135.8 | +227.2% | | 65536 | 4 | 958.5 | 1345.2 | +386.7 | +40.3% | | 65536 | 8 | 950.2 | 1954.7 | +1004.5 | +105.7% | | 65536 | 16 | 945.8 | 2414.0 | +1468.2 | +155.2% | ### How was this patch tested? Built hadoop-common with native profile on riscv64; verified it's function by TestNativeCrc32. Ran Hadoop\u2019s CRC32 benchmark on riscv64 (OpenEuler/EulixOS) with JDK 17. Here is the commands and results: Command\uff1a ``` mvn -Pnative \\ -Dtest=org.apache.hadoop.util.TestNativeCrc32 \\ -Djava.library.path=\"$HADOOP_COMMON_LIB_NATIVE_DIR\" \\ test ``` Results ``` [INFO] ------------------------------------------------------- [INFO] T E S T S [INFO] ------------------------------------------------------- [INFO] Running org.apache.hadoop.util.TestNativeCrc32 [INFO] Tests run: 22, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.017 s hadoop-yetus commented on PR #8031: URL: https://github.com/apache/hadoop/pull/8031#issuecomment-3398778305 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 23m 29s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 47m 25s | | trunk passed | | +1 :green_heart: | compile | 14m 16s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 32s | | trunk passed | | +1 :green_heart: | shadedclient | 101m 41s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 1m 10s | | the patch passed | | +1 :green_heart: | compile | 13m 15s | | the patch passed | | +1 :green_heart: | cc | 13m 15s | | the patch passed | | +1 :green_heart: | golang | 13m 15s | | the patch passed | | +1 :green_heart: | javac | 13m 15s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 2m 29s | | the patch passed | | +1 :green_heart: | shadedclient | 39m 47s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 23m 25s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 57s | | The patch does not generate ASF License warnings. | | | | 206m 55s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8031/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8031 | | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang | | uname | Linux cf2b3cead534 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0e62b049f710f680823489b1a77892ed49252fc4 | | Default Java | Red Hat, Inc.-1.8.0_462-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8031/1/testReport/ | | Max. process+thread count | 1376 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8031/1/console | | versions | git=2.43.7 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. PeterPtroc commented on PR #8031: URL: https://github.com/apache/hadoop/pull/8031#issuecomment-3401469367 @steveloughran could you please review this PR if you have time? thanks!", "created": "2025-10-12T12:49:32.000+0000", "updated": "2025-10-14T12:08:33.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: [RISC-V]  Add rv bulk CRC32 (non-CRC32C) optimized path"}}
{"id": "13631109", "key": "HADOOP-19723", "project": "HADOOP", "summary": "Build multi-arch hadoop image", "description": "Build {{apache/hadoop}} Docker image for both amd64 and arm64.", "comments": "adoroszlai opened a new pull request, #8023: URL: https://github.com/apache/hadoop/pull/8023 ## What changes were proposed in this pull request? - Update `Dockerfile` (on branch `docker-hadoop-3.4.2-lean`) to support building for `arm64`, too. - Use `ghcr.io/apache/hadoop-runner:jdk11-u2204` as base, because `apache/hadoop-runner:latest` only has `amd64` image available. - Use `TARGETPLATFORM` to decide which tarball to use. - Create args for version and flavor, replacing URL. - Update the `build-hadoop-image` workflow to create multi-arch images. - Add build-arg `BASE_URL` to allow using mirrors (for faster local build). - Replace deprecated `ENV HADOOP_CONF_DIR ` syntax. https://issues.apache.org/jira/browse/HADOOP-19723 ## How was this patch tested? Workflow [run](https://github.com/adoroszlai/hadoop/actions/runs/18377713437) in my fork created multi-arch [image](https://github.com/adoroszlai/hadoop/pkgs/container/hadoop/539671710?tag=HADOOP-19723). ``` #8 0.060 Building for linux/amd64 ... #8 0.060 + export HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2-lean.tar.gz ... #10 0.076 Building for linux/arm64 ... #10 0.077 + export HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2-aarch64-lean.tar.gz ``` Tested on both amd64 and arm64 platforms. ``` $ docker run -it --rm ghcr.io/adoroszlai/hadoop:HADOOP-19723 bash -c \"uname -a; hadoop version\" Linux cdb5cdd5ace9 6.8.0-65-generic #68~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Jul 15 18:06:34 UTC 2 x86_64 x86_64 x86_64 GNU/Linux Hadoop 3.4.2 Source code repository https://github.com/apache/hadoop.git -r 84e8b89ee2ebe6923691205b9e171badde7a495c Compiled by ahmarsu on 2025-08-20T10:30Z Compiled on platform linux-x86_64 Compiled with protoc 3.23.4 From source with checksum fa94c67d4b4be021b9e9515c9b0f7b6 This command was run using /opt/hadoop/share/hadoop/common/hadoop-common-3.4.2.jar ``` ``` $ docker run -it --rm ghcr.io/adoroszlai/hadoop:HADOOP-19723 bash -c \"uname -a; hadoop version\" Linux 9a1237ba8fbc 6.10.14-linuxkit #1 SMP Thu Oct 24 19:28:55 UTC 2024 aarch64 aarch64 aarch64 GNU/Linux Hadoop 3.4.2 Source code repository https://github.com/apache/hadoop.git -r e1c0dee881820a4d834ec4a4d2c70d0d953bb933 Compiled by ahmar on 2025-08-07T15:32Z Compiled on platform linux-aarch_64 Compiled with protoc 3.23.4 From source with checksum fa94c67d4b4be021b9e9515c9b0f7b6 This command was run using /opt/hadoop/share/hadoop/common/hadoop-common-3.4.2.jar ``` slfan1989 commented on PR #8023: URL: https://github.com/apache/hadoop/pull/8023#issuecomment-3385915162 LGTM. slfan1989 commented on PR #8023: URL: https://github.com/apache/hadoop/pull/8023#issuecomment-3385930578 @adoroszlai Thank you for the contribution. If there are no additional comments, I\u2019ll proceed to merge this PR shortly. adoroszlai commented on PR #8023: URL: https://github.com/apache/hadoop/pull/8023#issuecomment-3386892439 @smengcl would you like to take a look? slfan1989 commented on PR #8023: URL: https://github.com/apache/hadoop/pull/8023#issuecomment-3388049001 > @smengcl would you like to take a look? @smengcl I believe this PR is fine, and since @adoroszlai has extensive experience with this, any issues that may arise in the future can be quickly addressed. I will go ahead and merge this PR. slfan1989 merged PR #8023: URL: https://github.com/apache/hadoop/pull/8023 adoroszlai commented on PR #8023: URL: https://github.com/apache/hadoop/pull/8023#issuecomment-3388373138 Thanks @slfan1989 for reviewing and merging this. smengcl commented on PR #8023: URL: https://github.com/apache/hadoop/pull/8023#issuecomment-3388411324 Thanks @adoroszlai . The patch looks good. Thanks @slfan1989 for reviewing this as well.", "created": "2025-10-09T09:22:57.000+0000", "updated": "2025-10-10T06:07:00.000+0000", "derived": {"summary_task": "Summarize this issue: Build {{apache/hadoop}} Docker image for both amd64 and arm64.", "classification_task": "Classify the issue priority and type: Build {{apache/hadoop}} Docker image for both amd64 and arm64.", "qna_task": "Question: What is this issue about?\nAnswer: Build multi-arch hadoop image"}}
{"id": "13631098", "key": "HADOOP-19722", "project": "HADOOP", "summary": "Pin robotframework version", "description": "{{hadoop-runner}} installs {{robotframework}} without version definition. Re-building the image for unrelated changes may unexpectedly upgrade {{robotframework}}, too.", "comments": "adoroszlai opened a new pull request, #8025: URL: https://github.com/apache/hadoop/pull/8025 ## What changes were proposed in this pull request? `hadoop-runner` installs `robotframework` without version definition. Re-building the image for unrelated changes may unexpectedly upgrade `robotframework`, too. This PR proposes to pin `robotframework` to version 6.1.1. https://issues.apache.org/jira/browse/HADOOP-19722 ## How was this patch tested? ``` $ docker build -t hadoop-runner:dev . ... $ docker run -it --rm hadoop-runner:dev robot --version Robot Framework 6.1.1 (Python 3.10.12 on linux) ``` smengcl merged PR #8025: URL: https://github.com/apache/hadoop/pull/8025 adoroszlai commented on PR #8025: URL: https://github.com/apache/hadoop/pull/8025#issuecomment-3395090403 Thanks @slfan1989, @smengcl for the review.", "created": "2025-10-09T07:00:41.000+0000", "updated": "2025-10-12T17:54:24.000+0000", "derived": {"summary_task": "Summarize this issue: {{hadoop-runner}} installs {{robotframework}} without version definition. Re-building the image for unrelated changes may unexpectedly upgrade {{robotframework}}, too.", "classification_task": "Classify the issue priority and type: {{hadoop-runner}} installs {{robotframework}} without version definition. Re-building the image for unrelated changes may unexpectedly upgrade {{robotframework}}, too.", "qna_task": "Question: What is this issue about?\nAnswer: Pin robotframework version"}}
{"id": "13631091", "key": "HADOOP-19721", "project": "HADOOP", "summary": "Upgrade hadoop-runner to Ubuntu 24.04", "description": "Latest {{hadoop-runner}} images are based on Ubuntu 22.04. Upgrade to 24.04.", "comments": "", "created": "2025-10-09T06:23:41.000+0000", "updated": "2025-10-09T06:23:41.000+0000", "derived": {"summary_task": "Summarize this issue: Latest {{hadoop-runner}} images are based on Ubuntu 22.04. Upgrade to 24.04.", "classification_task": "Classify the issue priority and type: Latest {{hadoop-runner}} images are based on Ubuntu 22.04. Upgrade to 24.04.", "qna_task": "Question: What is this issue about?\nAnswer: Upgrade hadoop-runner to Ubuntu 24.04"}}
{"id": "13631035", "key": "HADOOP-19720", "project": "HADOOP", "summary": "Publish multi-arch hadoop-runner image to GitHub", "description": "Create GitHub Actions workflow to publish the apache/hadoop-runner Docker image to GitHub Container Registry. Build for both amd64 and arm64.", "comments": "adoroszlai opened a new pull request, #8021: URL: https://github.com/apache/hadoop/pull/8021 ## What changes were proposed in this pull request? Add workflow to publish `apache/hadoop-runner` (`jdk11-u2204` in this case) to GitHub Container Registry. https://issues.apache.org/jira/browse/HADOOP-19720 ## How was this patch tested? [Workflow run](https://github.com/adoroszlai/hadoop/actions/runs/18353000644) in my fork for push to branch `docker-hadoop-runner-HADOOP-19720-jdk11-u2204` built [image](https://github.com/adoroszlai/hadoop/pkgs/container/hadoop-runner/538747134?tag=HADOOP-19720-jdk11-u2204) `ghcr.io/adoroszlai/hadoop-runner:HADOOP-19720-jdk11-u2204`. It has both amd64 and arm64 arch. ```bash $ docker run -it --rm ghcr.io/adoroszlai/hadoop-runner:HADOOP-19720-jdk11-u2204 bash -c 'uname -a; cat /etc/lsb-release; java -version' Linux 8099f50d7322 6.8.0-65-generic #68~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Jul 15 18:06:34 UTC 2 x86_64 x86_64 x86_64 GNU/Linux DISTRIB_ID=Ubuntu DISTRIB_RELEASE=22.04 DISTRIB_CODENAME=jammy DISTRIB_DESCRIPTION=\"Ubuntu 22.04.5 LTS\" openjdk version \"11.0.28\" 2025-07-15 OpenJDK Runtime Environment Temurin-11.0.28+6 (build 11.0.28+6) OpenJDK 64-Bit Server VM Temurin-11.0.28+6 (build 11.0.28+6, mixed mode, sharing) ``` slfan1989 commented on PR #8021: URL: https://github.com/apache/hadoop/pull/8021#issuecomment-3383664657 @adoroszlai Many thanks for your contribution. Could we consider upgrading the image to ubuntu 24.04? smengcl merged PR #8021: URL: https://github.com/apache/hadoop/pull/8021 smengcl commented on PR #8021: URL: https://github.com/apache/hadoop/pull/8021#issuecomment-3383797788 > @adoroszlai Many thanks for your contribution. Could we consider upgrading the image to ubuntu 24.04? We could file a new jira for that task. slfan1989 commented on PR #8021: URL: https://github.com/apache/hadoop/pull/8021#issuecomment-3383809271 > > @adoroszlai Many thanks for your contribution. Could we consider upgrading the image to ubuntu 24.04? > > We could file a new jira for that task. You\u2019ve got a valid point. The branch name is indeed for 22.04, so we can address it in the next JIRA task. adoroszlai commented on PR #8021: URL: https://github.com/apache/hadoop/pull/8021#issuecomment-3384254098 Thanks @slfan1989, @smengcl for the review.", "created": "2025-10-08T17:21:10.000+0000", "updated": "2025-10-09T07:01:33.000+0000", "derived": {"summary_task": "Summarize this issue: Create GitHub Actions workflow to publish the apache/hadoop-runner Docker image to GitHub Container Registry. Build for both amd64 and arm64.", "classification_task": "Classify the issue priority and type: Create GitHub Actions workflow to publish the apache/hadoop-runner Docker image to GitHub Container Registry. Build for both amd64 and arm64.", "qna_task": "Question: What is this issue about?\nAnswer: Publish multi-arch hadoop-runner image to GitHub"}}
{"id": "13630984", "key": "HADOOP-19719", "project": "HADOOP", "summary": "Upgrade to wildfly version with support for openssl 3", "description": "Wildfly 2.1.4 * doesn't work with openssl 3 (that symbol change...why did they do that?) we need a version with https://github.com/wildfly-security/wildfly-openssl-natives/commit/6cecd42a254cd78585fefd9a0e41ad7954ece80d 2.2.5.Final does the openssl 3 support.", "comments": "steveloughran opened a new pull request, #8019: URL: https://github.com/apache/hadoop/pull/8019 ### How was this patch tested? Going to see if it works on a mac... ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? steveloughran commented on PR #8019: URL: https://github.com/apache/hadoop/pull/8019#issuecomment-3381330281 the test which is parameterized on ssl (and storediag when a store is forced to OpenSSL) ``` [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractSeek.testReadFullyZeroBytebufferPastEOF steveloughran commented on PR #8019: URL: https://github.com/apache/hadoop/pull/8019#issuecomment-3381381104 s3a tests all good, s3 london `-Dparallel-tests -DtestsThreadCount=8` steveloughran commented on PR #8019: URL: https://github.com/apache/hadoop/pull/8019#issuecomment-3405882774 OK, 2.2.5 doesn't include the arm linux binaries. It does in our private builds, which is why I was confused. steveloughran merged PR #8019: URL: https://github.com/apache/hadoop/pull/8019", "created": "2025-10-08T10:52:33.000+0000", "updated": "2025-10-20T12:47:50.000+0000", "derived": {"summary_task": "Summarize this issue: Wildfly 2.1.4 * doesn't work with openssl 3 (that symbol change...why did they do that?) we need a version with https://github.com/wildfly-security/wildfly-openssl-natives/commit/6cecd42a254cd78585fefd9a0e41ad7954ece80d 2.2.5.Final does the openssl 3 support.", "classification_task": "Classify the issue priority and type: Wildfly 2.1.4 * doesn't work with openssl 3 (that symbol change...why did they do that?) we need a version with https://github.com/wildfly-security/wildfly-openssl-natives/commit/6cecd42a254cd78585fefd9a0e41ad7954ece80d 2.2.5.Final does the openssl 3 support.", "qna_task": "Question: What is this issue about?\nAnswer: Upgrade to wildfly version with support for openssl 3"}}
{"id": "13630876", "key": "HADOOP-19718", "project": "HADOOP", "summary": "[ABFS]: Throw HTTPException when AAD token fetch fails", "description": "Reported by [~enigma25] : In [this code snippet](https://github.com/Azure/azure-data-lake-store-java/blob/26eca936da60286aa70b0dd7823ff5e627987bc4/src/main/java/com/microsoft/azure/datalake/store/oauth2/AzureADAuthenticator.java#L252-L293) from AzureADAuthenticator, the `conn` is being returned as null due to a (possibly) corrupted connection between Azure Data Lake Store Client and AAD while fetching AAD token. The code snippet linked, runs into an NPE. I wanted to know from the maintainers and community if this bug/symptom has been seen by them earlier and what might be the best way to handle this? Secondly, I wanted to know the right place for the fix too - whether it should be the application code or the SDK code itself should handle such NPEs and fail more gracefully? Open to thoughts and comments. Cheers, Nikhil ``` java.util.concurrent.ExecutionException: java.lang.NullPointerException: Cannot invoke \"java.io.InputStream.read(byte[], int, int)\" because \"inStream\" is null at org.apache.kafka.connect.util.ConvertingFutureCallback.result(ConvertingFutureCallback.java:135) at org.apache.kafka.connect.util.ConvertingFutureCallback.get(ConvertingFutureCallback.java:122) at org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource.validateConfigs(ConnectorPluginsResource.java:129) at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) at java.base/java.lang.reflect.Method.invoke(Method.java:580) at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52) at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:134) at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:177) at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$TypeOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:219) at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:81) at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:478) at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:400) at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81) at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:256) at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248) at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244) at org.glassfish.jersey.internal.Errors.process(Errors.java:292) at org.glassfish.jersey.internal.Errors.process(Errors.java:274) at org.glassfish.jersey.internal.Errors.process(Errors.java:244) at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265) at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:235) at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:684) at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394) at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346) at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:358) at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:311) at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205) at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799) at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554) at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233) at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624) at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233) at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440) at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188) at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505) at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594) at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186) at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234) at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:181) at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127) at org.eclipse.jetty.server.Server.handle(Server.java:516) at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487) at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732) at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479) at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277) at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311) at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105) at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104) at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338) at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315) at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173) at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131) at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409) at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883) at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034) at java.base/java.lang.Thread.run(Thread.java:1583) Caused by: java.lang.NullPointerException: Cannot invoke \"java.io.InputStream.read(byte[], int, int)\" because \"inStream\" is null at org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.consumeInputStream(AzureADAuthenticator.java:345) at org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenSingleCall(AzureADAuthenticator.java:275) at org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenCall(AzureADAuthenticator.java:216) at org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenUsingClientCreds(AzureADAuthenticator.java:95) at org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider.refreshToken(ClientCredsTokenProvider.java:58) at org.apache.hadoop.fs.azurebfs.oauth2.AccessTokenProvider.getToken(AccessTokenProvider.java:50) at org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAccessToken(AbfsClient.java:583) at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(AbfsRestOperation.java:162) at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:134) at org.apache.hadoop.fs.azurebfs.services.AbfsClient.getFilesystemProperties(AbfsClient.java:205) at io.confluent.connect.azure.datalake.gen2.validation.Validations.verifyClient(Validations.java:175) at io.confluent.connect.azure.datalake.gen2.validation.Validations.createAndValidateClient(Validations.java:395) at io.confluent.connect.azure.datalake.gen2.validation.Validations.validateAll(Validations.java:131) at io.confluent.connect.utils.validators.all.ConfigValidation.lambda$callValidators$0(ConfigValidation.java:222) at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:1024) at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762) at io.confluent.connect.utils.validators.all.ConfigValidation.callValidators(ConfigValidation.java:222) at io.confluent.connect.utils.validators.all.ConfigValidation.validate(ConfigValidation.java:182) at io.confluent.connect.azure.datalake.gen2.AzureDataLakeGen2SinkConnector.validate(AzureDataLakeGen2SinkConnector.java:97) at org.apache.kafka.connect.runtime.AbstractHerder.validateConnectorConfig(AbstractHerder.java:641) at org.apache.kafka.connect.runtime.AbstractHerder.lambda$validateConnectorConfig$7(AbstractHerder.java:493) at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ... 1 more ```", "comments": "", "created": "2025-10-07T12:56:26.000+0000", "updated": "2025-10-07T12:56:26.000+0000", "derived": {"summary_task": "Summarize this issue: Reported by [~enigma25] : In [this code snippet](https://github.com/Azure/azure-data-lake-store-java/blob/26eca936da60286aa70b0dd7823ff5e627987bc4/src/main/java/com/microsoft/azure/datalake/store/oauth2/AzureADAuthenticator.java#L252-L293) from AzureADAuthenticator, the `conn` is being returned as null due to a (possibly) corrupted connection between Azure Data Lake Store Client and AAD while fetc", "classification_task": "Classify the issue priority and type: Reported by [~enigma25] : In [this code snippet](https://github.com/Azure/azure-data-lake-store-java/blob/26eca936da60286aa70b0dd7823ff5e627987bc4/src/main/java/com/microsoft/azure/datalake/store/oauth2/AzureADAuthenticator.java#L252-L293) from AzureADAuthenticator, the `conn` is being returned as null due to a (possibly) corrupted connection between Azure Data Lake Store Client and AAD while fetc", "qna_task": "Question: What is this issue about?\nAnswer: [ABFS]: Throw HTTPException when AAD token fetch fails "}}
{"id": "13630839", "key": "HADOOP-19717", "project": "HADOOP", "summary": "Resolve build error caused by missing Checker Framework (NonNull not recognized)", "description": "In the recent build, we encountered the following issue: *org.checkerframework.checker.nullness.qual.NonNull* could not be recognized, and the following error was observed. {code:java} [ERROR] /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-8011/ubuntu-focal/src/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java:[216,50] package org.checkerframework.checker.nullness.qual does not exist [ERROR] /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-8011/ubuntu-focal/src/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java:[2509,30] cannot find symbol [ERROR] symbol: class NonNull [ERROR] location: class org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer.AsyncThreadFactory {code} I checked the usage in the related modules, and we should use *org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.NonNull* instead of directly using {*}org.checkerframework.checker.nullness.qual.NonNull{*}.", "comments": "slfan1989 opened a new pull request, #8015: URL: https://github.com/apache/hadoop/pull/8015 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR JIRA: HADOOP-19717. Resolve build error caused by missing Checker Framework (NonNull not recognized). ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? pan3793 commented on code in PR #8015: URL: https://github.com/apache/hadoop/pull/8015#discussion_r2409341008 ########## hadoop-cloud-storage-project/hadoop-tos/src/main/java/org/apache/hadoop/fs/tosfs/util/Iterables.java: ########## @@ -19,7 +19,7 @@ package org.apache.hadoop.fs.tosfs.util; import org.apache.hadoop.util.Preconditions; -import org.checkerframework.checker.nullness.qual.Nullable; +import org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.Nullable; Review Comment: I suspect this makes the `Nullable` useless, I don't think the static analyzer tools can recognize such a relocated annotation. hadoop-yetus commented on PR #8015: URL: https://github.com/apache/hadoop/pull/8015#issuecomment-3375421893 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 8m 31s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 35s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 22m 41s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | -1 :x: | compile | 5m 25s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 4m 39s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | checkstyle | 1m 18s | | trunk passed | | -1 :x: | mvnsite | 0m 24s | [/branch-mvnsite-hadoop-hdfs-project_hadoop-hdfs-rbf.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-mvnsite-hadoop-hdfs-project_hadoop-hdfs-rbf.txt) | hadoop-hdfs-rbf in trunk failed. | | -1 :x: | mvnsite | 0m 18s | [/branch-mvnsite-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-mvnsite-hadoop-cloud-storage-project_hadoop-tos.txt) | hadoop-tos in trunk failed. | | -1 :x: | javadoc | 0m 27s | [/branch-javadoc-hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-javadoc-hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-hdfs-rbf in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 16s | [/branch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-tos in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | +1 :green_heart: | javadoc | 0m 39s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 0m 25s | [/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-rbf.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-rbf.txt) | hadoop-hdfs-rbf in trunk failed. | | -1 :x: | spotbugs | 0m 16s | [/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt) | hadoop-tos in trunk failed. | | +1 :green_heart: | shadedclient | 24m 7s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 23s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 37s | | the patch passed | | -1 :x: | compile | 5m 41s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 5m 41s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 5m 11s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 5m 10s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 12s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 41s | | the patch passed | | +1 :green_heart: | javadoc | 0m 20s | | hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 0 new + 0 unchanged - 1 fixed = 0 total (was 1) | | +1 :green_heart: | javadoc | 0m 16s | | hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 0 new + 0 unchanged - 1 fixed = 0 total (was 1) | | +1 :green_heart: | javadoc | 0m 21s | | hadoop-hdfs-project_hadoop-hdfs-rbf-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 0 new + 0 unchanged - 2 fixed = 0 total (was 2) | | +1 :green_heart: | javadoc | 0m 19s | | hadoop-cloud-storage-project_hadoop-tos-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 0 new + 0 unchanged - 2 fixed = 0 total (was 2) | | +1 :green_heart: | spotbugs | 1m 33s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 26s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 38m 54s | | hadoop-hdfs-rbf in the patch passed. | | +1 :green_heart: | unit | 0m 54s | | hadoop-tos in the patch passed. | | +1 :green_heart: | asflicense | 0m 26s | | The patch does not generate ASF License warnings. | | | | 154m 4s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8015 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 1ad3180c6b2e 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f0c771fd1f48e1cc45617d4e2eb0afb552e5ba1f | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/testReport/ | | Max. process+thread count | 4610 (vs. ulimit of 5500) | | modules | C: hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-cloud-storage-project/hadoop-tos U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/console | | versions | git=2.25.1 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on code in PR #8015: URL: https://github.com/apache/hadoop/pull/8015#discussion_r2410285097 ########## hadoop-cloud-storage-project/hadoop-tos/src/main/java/org/apache/hadoop/fs/tosfs/util/Iterables.java: ########## @@ -19,7 +19,7 @@ package org.apache.hadoop.fs.tosfs.util; import org.apache.hadoop.util.Preconditions; -import org.checkerframework.checker.nullness.qual.Nullable; +import org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.Nullable; Review Comment: I think what you said makes some sense, but there are similar users in AzureBFS as well. https://github.com/apache/hadoop/blob/1566613c725979d0ccda45822dfa275cbd97467a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsLease.java#L38 slfan1989 commented on code in PR #8015: URL: https://github.com/apache/hadoop/pull/8015#discussion_r2410285097 ########## hadoop-cloud-storage-project/hadoop-tos/src/main/java/org/apache/hadoop/fs/tosfs/util/Iterables.java: ########## @@ -19,7 +19,7 @@ package org.apache.hadoop.fs.tosfs.util; import org.apache.hadoop.util.Preconditions; -import org.checkerframework.checker.nullness.qual.Nullable; +import org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.Nullable; Review Comment: I think what you said makes some sense, but there are similar users in AzureBFS as well. https://github.com/apache/hadoop/blob/1566613c725979d0ccda45822dfa275cbd97467a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsLease.java#L38 @steveloughran I\u2019d like to hear your thoughts \u2014 do you think we should reintroduce a new dependency to resolve the issue where org.checkerframework.checker.nullness.qual.Nullable cannot be found? cc: @szetszwo hadoop-yetus commented on PR #8015: URL: https://github.com/apache/hadoop/pull/8015#issuecomment-3376863181 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 12s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 22m 36s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | -1 :x: | compile | 5m 21s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 4m 43s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | checkstyle | 1m 24s | | trunk passed | | -1 :x: | mvnsite | 0m 24s | [/branch-mvnsite-hadoop-hdfs-project_hadoop-hdfs-rbf.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/branch-mvnsite-hadoop-hdfs-project_hadoop-hdfs-rbf.txt) | hadoop-hdfs-rbf in trunk failed. | | -1 :x: | mvnsite | 0m 18s | [/branch-mvnsite-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/branch-mvnsite-hadoop-cloud-storage-project_hadoop-tos.txt) | hadoop-tos in trunk failed. | | -1 :x: | javadoc | 0m 18s | [/branch-javadoc-hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/branch-javadoc-hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-hdfs-rbf in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 16s | [/branch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/branch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-tos in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | +1 :green_heart: | javadoc | 1m 4s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 0m 22s | [/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-rbf.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-rbf.txt) | hadoop-hdfs-rbf in trunk failed. | | -1 :x: | spotbugs | 0m 17s | [/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt) | hadoop-tos in trunk failed. | | +1 :green_heart: | shadedclient | 21m 20s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 22s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 4s | | the patch passed | | -1 :x: | compile | 5m 49s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 5m 49s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 5m 5s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 5m 5s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 12s | | the patch passed | | +1 :green_heart: | mvnsite | 1m 19s | | the patch passed | | +1 :green_heart: | javadoc | 0m 25s | | hadoop-yarn-server-resourcemanager in the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | +1 :green_heart: | javadoc | 0m 20s | | hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 0 new + 0 unchanged - 1 fixed = 0 total (was 1) | | +1 :green_heart: | javadoc | 0m 17s | | hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 0 new + 0 unchanged - 1 fixed = 0 total (was 1) | | +1 :green_heart: | javadoc | 0m 27s | | hadoop-yarn-server-resourcemanager in the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | javadoc | 0m 22s | | hadoop-hdfs-project_hadoop-hdfs-rbf-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 0 new + 0 unchanged - 2 fixed = 0 total (was 2) | | +1 :green_heart: | javadoc | 0m 19s | | hadoop-cloud-storage-project_hadoop-tos-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 0 new + 0 unchanged - 2 fixed = 0 total (was 2) | | +1 :green_heart: | spotbugs | 2m 41s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 13s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 90m 40s | | hadoop-yarn-server-resourcemanager in the patch passed. | | +1 :green_heart: | unit | 38m 2s | | hadoop-hdfs-rbf in the patch passed. | | +1 :green_heart: | unit | 0m 53s | | hadoop-tos in the patch passed. | | +1 :green_heart: | asflicense | 0m 24s | | The patch does not generate ASF License warnings. | | | | 241m 25s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8015 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux de756f6ac704 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / aca0e73a716b49f41b6eb3e0a57c876842a258a8 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/testReport/ | | Max. process+thread count | 4761 (vs. ulimit of 5500) | | modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-cloud-storage-project/hadoop-tos U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/2/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #8015: URL: https://github.com/apache/hadoop/pull/8015#issuecomment-3377007084 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 46s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 26m 13s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | -1 :x: | compile | 6m 12s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 5m 18s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | checkstyle | 1m 19s | | trunk passed | | -1 :x: | mvnsite | 0m 25s | [/branch-mvnsite-hadoop-hdfs-project_hadoop-hdfs-rbf.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/branch-mvnsite-hadoop-hdfs-project_hadoop-hdfs-rbf.txt) | hadoop-hdfs-rbf in trunk failed. | | -1 :x: | mvnsite | 0m 18s | [/branch-mvnsite-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/branch-mvnsite-hadoop-cloud-storage-project_hadoop-tos.txt) | hadoop-tos in trunk failed. | | -1 :x: | javadoc | 0m 23s | [/branch-javadoc-hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/branch-javadoc-hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-hdfs-rbf in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 17s | [/branch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/branch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-tos in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | +1 :green_heart: | javadoc | 0m 40s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 0m 25s | [/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-rbf.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-rbf.txt) | hadoop-hdfs-rbf in trunk failed. | | -1 :x: | spotbugs | 0m 16s | [/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt) | hadoop-tos in trunk failed. | | +1 :green_heart: | shadedclient | 26m 8s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 25s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 41s | | the patch passed | | -1 :x: | compile | 6m 23s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 6m 23s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 5m 50s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 5m 50s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 24s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 45s | | the patch passed | | +1 :green_heart: | javadoc | 0m 20s | | hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 0 new + 0 unchanged - 1 fixed = 0 total (was 1) | | +1 :green_heart: | javadoc | 0m 16s | | hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 0 new + 0 unchanged - 1 fixed = 0 total (was 1) | | +1 :green_heart: | javadoc | 0m 18s | | hadoop-hdfs-project_hadoop-hdfs-rbf-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 0 new + 0 unchanged - 2 fixed = 0 total (was 2) | | +1 :green_heart: | javadoc | 0m 17s | | hadoop-cloud-storage-project_hadoop-tos-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 0 new + 0 unchanged - 2 fixed = 0 total (was 2) | | +1 :green_heart: | spotbugs | 1m 28s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 54s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 39m 4s | | hadoop-hdfs-rbf in the patch passed. | | +1 :green_heart: | unit | 0m 54s | | hadoop-tos in the patch passed. | | +1 :green_heart: | asflicense | 0m 25s | | The patch does not generate ASF License warnings. | | | | 158m 17s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8015 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 30e4a84a468c 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f0c771fd1f48e1cc45617d4e2eb0afb552e5ba1f | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/testReport/ | | Max. process+thread count | 4202 (vs. ulimit of 5500) | | modules | C: hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-cloud-storage-project/hadoop-tos U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/3/console | | versions | git=2.25.1 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on PR #8015: URL: https://github.com/apache/hadoop/pull/8015#issuecomment-3377166843 @szetszwo @pan3793 My thought is that since `AzureBFS` already uses this approach, we should be able to apply the same solution in other places as well. For now, we can use`org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.Nullable` instead of `org.checkerframework.checker.nullness.qual.Nullable` to unblock the trunk build issue first. A follow-up PR can be submitted later to fully resolve this dependency problem in a cleaner way. Currently, the build result is as expected \u2014 before applying this patch, the trunk could not compile successfully, but after merging it, the build now passes under both JDK 8 and JDK 11. szetszwo commented on code in PR #8015: URL: https://github.com/apache/hadoop/pull/8015#discussion_r2411050812 ########## hadoop-cloud-storage-project/hadoop-tos/src/main/java/org/apache/hadoop/fs/tosfs/util/Iterables.java: ########## @@ -19,7 +19,7 @@ package org.apache.hadoop.fs.tosfs.util; import org.apache.hadoop.util.Preconditions; -import org.checkerframework.checker.nullness.qual.Nullable; +import org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.Nullable; Review Comment: > ... this makes the Nullable useless, ... Making it useless seems better than breaking the build. Unforturately, the the builds after this remain failing. szetszwo commented on PR #8015: URL: https://github.com/apache/hadoop/pull/8015#issuecomment-3377457340 @slfan1989 , if it can fix the build, then it is fine. But the builds after this remain failing. slfan1989 commented on PR #8015: URL: https://github.com/apache/hadoop/pull/8015#issuecomment-3377471954 > @slfan1989 , if it can fix the build, then it is fine. But the builds after this remain failing. @szetszwo A new issue occurred during the compilation of yarn-ui. The log output is as follows: ``` [INFO] [2/4] Fetching packages... [INFO] error color@5.0.2: The engine \"node\" is incompatible with this module. Expected version \">=18\". Got \"12.22.1\" [INFO] error Found incompatible module. ``` slfan1989 commented on PR #8015: URL: https://github.com/apache/hadoop/pull/8015#issuecomment-3377538969 > > @slfan1989 , if it can fix the build, then it is fine. But the builds after this remain failing. > > @szetszwo A new issue occurred during the compilation of yarn-ui. The log output is as follows: > > ``` > [INFO] [2/4] Fetching packages... > [INFO] error color@5.0.2: The engine \"node\" is incompatible with this module. Expected version \">=18\". Got \"12.22.1\" > [INFO] error Found incompatible module. > ``` > > I tried to apply a local fix for this issue. I manually specified `color@^3.1.3` in the package.json, and it took effect successfully. I will submit a PR to fix this issue. ``` [INFO] - slfan1989 merged PR #8015: URL: https://github.com/apache/hadoop/pull/8015 slfan1989 commented on PR #8015: URL: https://github.com/apache/hadoop/pull/8015#issuecomment-3377550294 @szetszwo Thank you very much for the review! szetszwo commented on PR #8015: URL: https://github.com/apache/hadoop/pull/8015#issuecomment-3377632827 @slfan1989 , thanks for fixing it! this is complicating the new thirdparty release FWIW. this should all be using the unshaded javax. Nullable/nonnull. And the hadoop-thirdparty release needs to address this stuff getting left out so 1.5.0 can be a drop-in replacement for 1.4.0 {code} [ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.10.1:compile (default-compile) on project hadoop-tos: Compilation failure: Compilation failure: [ERROR] /Users/stevel/Projects/hadoop-trunk/hadoop-cloud-storage-project/hadoop-tos/src/main/java/org/apache/hadoop/fs/tosfs/util/Iterables.java:[22,79] package org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual does not exist [ERROR] /Users/stevel/Projects/hadoop-trunk/hadoop-cloud-storage-project/hadoop-tos/src/main/java/org/apache/hadoop/fs/tosfs/util/Iterables.java:[89,29] cannot find symbol [ERROR] symbol: class Nullable [ERROR] location: class org.apache.hadoop.fs.tosfs.util.Iterables [ERROR] -> [Help 1] [ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.10.1:compile (default-compile) on project hadoop-azure: Compilation failure: Compilation failure: [ERROR] /Users/stevel/Projects/hadoop-trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsLease.java:[38,79] package org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual does not exist [ERROR] /Users/stevel/Projects/hadoop-trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsLease.java:[183,30] cannot find symbol [ERROR] symbol: class Nullable [ERROR] -> [Help 1] [ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.10.1:compile (default-compile) on project hadoop-hdfs-rbf: Compilation failure: Compilation failure: [ERROR] /Users/stevel/Projects/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java:[102,79] package org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual does not exist [ERROR] /Users/stevel/Projects/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java:[2509,30] cannot find symbol [ERROR] symbol: class NonNull [ERROR] location: class org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer.AsyncThreadFactory [ERROR] -> [Help 1] [ERROR] {code}", "created": "2025-10-07T03:49:55.000+0000", "updated": "2025-10-20T16:32:20.000+0000", "derived": {"summary_task": "Summarize this issue: In the recent build, we encountered the following issue: *org.checkerframework.checker.nullness.qual.NonNull* could not be recognized, and the following error was observed. {code:java} [ERROR] /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-8011/ubuntu-focal/src/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java:[216,5", "classification_task": "Classify the issue priority and type: In the recent build, we encountered the following issue: *org.checkerframework.checker.nullness.qual.NonNull* could not be recognized, and the following error was observed. {code:java} [ERROR] /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-8011/ubuntu-focal/src/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java:[216,5", "qna_task": "Question: What is this issue about?\nAnswer: Resolve build error caused by missing Checker Framework (NonNull not recognized)"}}
{"id": "13630760", "key": "HADOOP-19716", "project": "HADOOP", "summary": "Create lean docker image", "description": "Create a new docker image based on the lean tarball. hadoop-3.4.2-lean.tar.gz", "comments": "adoroszlai opened a new pull request, #8013: URL: https://github.com/apache/hadoop/pull/8013 ### Description of PR Create a new docker image based on `hadoop-3.4.2-lean.tar.gz`, which omits AWS `bundle-2.29.52.jar`. This PR should not be merged. I will push it from CLI as a new branch to publish the image with Docker tag `3.4.2-lean`, rather than overwrite the existing image `3.4.2`. ### How was this patch tested? [Workflow run](https://github.com/adoroszlai/hadoop/actions/runs/18277349554/job/52032416240) in my fork created the [image](https://github.com/adoroszlai/hadoop/pkgs/container/hadoop/535792302?tag=3.4.2-lean). steveloughran commented on PR #8013: URL: https://github.com/apache/hadoop/pull/8013#issuecomment-3371636931 I did check the url resolved, BTW. Note that in #7980 packaging will change where we move hadoop-aws and hadoop azure to common/lib, with all dependencies except bundle.jar; that'll come iff you do a \"-Paws-sdk\" build. And the other cloud modules will come in if you explicitly ask for them. Still a WiP; hope to be done ASAP with hadoop 3.4.3 like this. No more \"let's strip the build\" work, instead just choose the build options for a release. adoroszlai commented on PR #8013: URL: https://github.com/apache/hadoop/pull/8013#issuecomment-3371660323 Thanks @steveloughran for the review. Pushed 4cb319a9a98350bb0711029cf13c170f3c9ce043 to `docker-hadoop-3.4.2-lean`. adoroszlai closed pull request #8013: HADOOP-19716. Create lean docker image URL: https://github.com/apache/hadoop/pull/8013 slfan1989 commented on PR #8013: URL: https://github.com/apache/hadoop/pull/8013#issuecomment-3375033744 > Thanks @steveloughran for the review. Pushed [4cb319a](https://github.com/apache/hadoop/commit/4cb319a9a98350bb0711029cf13c170f3c9ce043) to `docker-hadoop-3.4.2-lean`. @adoroszlai Thanks for the contribution! LGTM.", "created": "2025-10-06T09:52:36.000+0000", "updated": "2025-10-09T10:04:45.000+0000", "derived": {"summary_task": "Summarize this issue: Create a new docker image based on the lean tarball. hadoop-3.4.2-lean.tar.gz", "classification_task": "Classify the issue priority and type: Create a new docker image based on the lean tarball. hadoop-3.4.2-lean.tar.gz", "qna_task": "Question: What is this issue about?\nAnswer: Create lean docker image"}}
{"id": "13630722", "key": "HADOOP-19715", "project": "HADOOP", "summary": "Update restrict-imports-enforcer-rule from 2.0.0 to 2.6.1", "description": "The project is currently using {{restrict-imports-enforcer-rule}} version {*}2.0.0{*}, which was released in *2021* and is now outdated. Since then, several new versions have been released with important bug fixes, performance improvements, and enhanced compatibility. To maintain a modern and stable build environment, it is necessary to upgrade to version {*}2.6.1{*}.", "comments": "slfan1989 opened a new pull request, #8012: URL: https://github.com/apache/hadoop/pull/8012 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR JIRA: HADOOP-19715. Update restrict-imports-enforcer-rule from 2.0.0 to 2.6.1. ### How was this patch tested? CI. ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #8012: URL: https://github.com/apache/hadoop/pull/8012#issuecomment-3379348657 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 43s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 0m 25s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | -1 :x: | compile | 0m 24s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 25s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | mvnsite | 0m 25s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/branch-mvnsite-root.txt) | root in trunk failed. | | -1 :x: | javadoc | 0m 25s | [/branch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/branch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 24s | [/branch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/branch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | shadedclient | 3m 37s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 24s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-mvninstall-root.txt) | root in the patch failed. | | -1 :x: | compile | 0m 24s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 24s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 22s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 0m 22s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 0m 25s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | -1 :x: | javadoc | 0m 24s | [/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 24s | [/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | shadedclient | 4m 41s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 25s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +0 :ok: | asflicense | 0m 28s | | ASF License check generated no output? | | | | 13m 6s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8012 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux bcae851086b8 5.15.0-152-generic #162-Ubuntu SMP Wed Jul 23 09:48:42 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f59839a3b410ba91d777948cc1b8683d10006e31 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/testReport/ | | Max. process+thread count | 55 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/console | | versions | git=2.25.1 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #8012: URL: https://github.com/apache/hadoop/pull/8012#issuecomment-3379885563 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 41s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 3m 52s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | -1 :x: | compile | 1m 29s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 3m 18s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | mvnsite | 1m 41s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/branch-mvnsite-root.txt) | root in trunk failed. | | -1 :x: | javadoc | 0m 25s | [/branch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/branch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 26s | [/branch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/branch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | shadedclient | 13m 13s | | branch has errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 1m 44s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-mvninstall-root.txt) | root in the patch failed. | | -1 :x: | compile | 0m 39s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 39s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 24s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 0m 24s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 0m 24s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | -1 :x: | javadoc | 0m 25s | [/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 24s | [/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | shadedclient | 4m 6s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 24s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 0m 27s | | The patch does not generate ASF License warnings. | | | | 23m 47s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8012 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux 679e127e6ac4 5.15.0-152-generic #162-Ubuntu SMP Wed Jul 23 09:48:42 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 67cae1f9eee8d0b5bb049e7b261a4e70c00d46a3 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/testReport/ | | Max. process+thread count | 106 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/console | | versions | git=2.25.1 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #8012: URL: https://github.com/apache/hadoop/pull/8012#issuecomment-3383838383 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 42s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 59m 35s | | trunk passed | | -1 :x: | compile | 19m 32s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 9m 12s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | mvnsite | 0m 39s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/branch-mvnsite-root.txt) | root in trunk failed. | | -1 :x: | javadoc | 0m 43s | [/branch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/branch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 45s | [/branch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/branch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | shadedclient | 93m 50s | | branch has errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 24s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/patch-mvninstall-root.txt) | root in the patch failed. | | -1 :x: | compile | 0m 22s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 22s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 21s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 0m 21s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 0m 9s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | -1 :x: | javadoc | 0m 27s | [/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 25s | [/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | shadedclient | 3m 9s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 25s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +0 :ok: | asflicense | 0m 13s | | ASF License check generated no output? | | | | 101m 41s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8012 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux cfb99cb39a9f 5.15.0-152-generic #162-Ubuntu SMP Wed Jul 23 09:48:42 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 96e6841274b4e99041a4d0bc12af671c07a5d62f | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/testReport/ | | Max. process+thread count | 258 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/4/console | | versions | git=2.25.1 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-06T01:54:13.000+0000", "updated": "2025-10-09T02:36:22.000+0000", "derived": {"summary_task": "Summarize this issue: The project is currently using {{restrict-imports-enforcer-rule}} version {*}2.0.0{*}, which was released in *2021* and is now outdated. Since then, several new versions have been released with important bug fixes, performance improvements, and enhanced compatibility. To maintain a modern and stable build environment, it is necessary to upgrade to version {*}2.6.1{*}.", "classification_task": "Classify the issue priority and type: The project is currently using {{restrict-imports-enforcer-rule}} version {*}2.0.0{*}, which was released in *2021* and is now outdated. Since then, several new versions have been released with important bug fixes, performance improvements, and enhanced compatibility. To maintain a modern and stable build environment, it is necessary to upgrade to version {*}2.6.1{*}.", "qna_task": "Question: What is this issue about?\nAnswer: Update restrict-imports-enforcer-rule from 2.0.0 to 2.6.1"}}
{"id": "13630581", "key": "HADOOP-19713", "project": "HADOOP", "summary": "make container build work on macOS Tahoe", "description": "macOS Tahoe includes a native container daemon and runtime and it is supposed to be near feature-compatible with docker. In principle, it should be possible to run the container build using the container command line on macOS Tahoe. It would be great if we can add this functionality to the build script so folks can build on macOS natively without creating another VM.", "comments": "This could be someone's good hack project. this means you can run linux in it? cool. puts it on a par with windows -though that has the advantage you can just dual boot the machine to linux or just replace windows entirely Correct. This is a container daemon/runtime that runs natively on Apple (Silicon), which does pretty much all the things that a Docker runtime would do without involving VMs. Also, I understand you can install this on macOS before Tahoe. Here's one article (among many out there): [https://www.infoq.com/news/2025/06/apple-container-linux/] after the NPM attack last month, I'm thinking I should do all builds which pull in remote artifacts in its own container, one with restricted access to the rest of the system lyes, complicates getting aws credentials, but that's part of what I want to lock down). ...", "created": "2025-10-02T23:15:57.000+0000", "updated": "2025-10-07T19:12:45.000+0000", "derived": {"summary_task": "Summarize this issue: macOS Tahoe includes a native container daemon and runtime and it is supposed to be near feature-compatible with docker. In principle, it should be possible to run the container build using the container command line on macOS Tahoe. It would be great if we can add this functionality to the build script so folks can build on macOS natively without creating another VM.", "classification_task": "Classify the issue priority and type: macOS Tahoe includes a native container daemon and runtime and it is supposed to be near feature-compatible with docker. In principle, it should be possible to run the container build using the container command line on macOS Tahoe. It would be great if we can add this functionality to the build script so folks can build on macOS natively without creating another VM.", "qna_task": "Question: What is this issue about?\nAnswer: make container build work on macOS Tahoe"}}
{"id": "13630457", "key": "HADOOP-19712", "project": "HADOOP", "summary": "S3A: Deadlock observed in IOStatistics EvaluatingStatisticsMap.entryset()", "description": "We have evidence that `IOStatisticsSupport.snapshotIOStatistics()` can hang, specifically on the statistics collected by an S3AInputStream, whose statistics are merged in to the FS stats in close(); {code} jdk.internal.misc.Unsafe.park(Native Method) java.util.concurrent.locks.LockSupport.park(LockSupport.java:341) java.util.concurrent.ForkJoinTask.awaitDone(ForkJoinTask.java:468) java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:687) java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:927) java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233) java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:682) org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap.entrySet(EvaluatingStatisticsMap.java:166) java.util.Collections$UnmodifiableMap.entrySet(Collections.java:1529) org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.copyMap(IOStatisticsBinding.java:172) org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.snapshotMap(IOStatisticsBinding.java:216) org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.snapshotMap(IOStatisticsBinding.java:199) org.apache.hadoop.fs.statistics.IOStatisticsSnapshot.snapshot(IOStatisticsSnapshot.java:165) org.apache.hadoop.fs.statistics.IOStatisticsSnapshot.<init>(IOStatisticsSnapshot.java:125) org.apache.hadoop.fs.statistics.IOStatisticsSupport.snapshotIOStatistics(IOStatisticsSupport.java:49) {code} the code in question is calling `parallelStream()`, which uses a fixed pool of threads shared by all uses of the API {code} Set<Entry<String, E>> r = evalEntries.parallelStream().map((e) -> new EntryImpl<>(e.getKey(), e.getValue().apply(e.getKey()))) .collect(Collectors.toSet()); {code} Proposed: * move off parallelStream() to stream() * review code to if there is any other way this iteration can lead to a deadlock, e.g. the apply() calls. * could we do the merge more efficiently?", "comments": "steveloughran opened a new pull request, #8006: URL: https://github.com/apache/hadoop/pull/8006 Reworked how entrySet() and values() work, using .forEach() iterators after reviewing what ConcurrentHashMap does internally; it does a (safe) traverse. Add EvaluatingStatisticsMap.forEach() implementation which maps the passed in BiConsumer down to the evaluators.forEach, evaluating each value as it goes. Use that in IOStatisticsBinding.snapshot() code. Tests for all this. ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? steveloughran commented on PR #8006: URL: https://github.com/apache/hadoop/pull/8006#issuecomment-3357467095 tested s3 london args ` -Dparallel-tests -DtestsThreadCount=8 -Dscale` ``` [ERROR] Failures: [ERROR] ITestS3APrefetchingInputStream.testReadLargeFileFully:130 [Maxiumum named action_executor_acquired.max] Expecting: <0L> to be greater than: <0L> ``` hadoop-yetus commented on PR #8006: URL: https://github.com/apache/hadoop/pull/8006#issuecomment-3358241968 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 51s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 55m 44s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | -1 :x: | compile | 12m 48s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 10m 53s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | checkstyle | 0m 47s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 29s | | trunk passed | | +1 :green_heart: | javadoc | 1m 7s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 24s | | trunk passed | | +1 :green_heart: | shadedclient | 42m 33s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 58s | | the patch passed | | -1 :x: | compile | 12m 27s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 12m 27s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 10m 39s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 10m 39s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 42s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) | hadoop-common-project/hadoop-common: The patch generated 3 new + 0 unchanged - 0 fixed = 3 total (was 0) | | +1 :green_heart: | mvnsite | 1m 24s | | the patch passed | | +1 :green_heart: | javadoc | 0m 57s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 39s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 29s | | the patch passed | | +1 :green_heart: | shadedclient | 41m 13s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 22m 15s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 0m 40s | | The patch does not generate ASF License warnings. | | | | 223m 40s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8006 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 48973a6e0048 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 40e7c252a5e39fb8e483afe5f900412bf17cd4a3 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/testReport/ | | Max. process+thread count | 3134 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. steveloughran commented on PR #8006: URL: https://github.com/apache/hadoop/pull/8006#issuecomment-3365176292 build failures are in the yarn-ui; it complains that node is too old ``` [INFO] - steveloughran commented on PR #8006: URL: https://github.com/apache/hadoop/pull/8006#issuecomment-3365209898 I see yarn-ui failure is already covered in a yarn jira. hadoop-yetus commented on PR #8006: URL: https://github.com/apache/hadoop/pull/8006#issuecomment-3365792744 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 50s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 52m 57s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/3/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | -1 :x: | compile | 11m 35s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/3/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 9m 54s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/3/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | checkstyle | 0m 45s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 29s | | trunk passed | | +1 :green_heart: | javadoc | 1m 4s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 21s | | trunk passed | | +1 :green_heart: | shadedclient | 41m 2s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 1m 0s | | the patch passed | | -1 :x: | compile | 11m 23s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/3/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 11m 23s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/3/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 9m 47s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/3/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 9m 47s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/3/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 42s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/3/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) | hadoop-common-project/hadoop-common: The patch generated 5 new + 0 unchanged - 0 fixed = 5 total (was 0) | | +1 :green_heart: | mvnsite | 1m 25s | | the patch passed | | +1 :green_heart: | javadoc | 0m 58s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 38s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 26s | | the patch passed | | +1 :green_heart: | shadedclient | 40m 56s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 22m 17s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 0m 40s | | The patch does not generate ASF License warnings. | | | | 214m 43s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8006 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux bac1487e44d1 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 78880e82a6bb33e328b07c3273a73289ba5e717d | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/3/testReport/ | | Max. process+thread count | 1438 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/3/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #8006: URL: https://github.com/apache/hadoop/pull/8006#issuecomment-3373592220 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 22m 3s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 54m 40s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/4/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | -1 :x: | compile | 11m 34s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/4/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 9m 42s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/4/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | checkstyle | 0m 47s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 28s | | trunk passed | | +1 :green_heart: | javadoc | 1m 5s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 40s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 21s | | trunk passed | | +1 :green_heart: | shadedclient | 41m 18s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 58s | | the patch passed | | -1 :x: | compile | 11m 25s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/4/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 11m 25s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/4/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 9m 42s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/4/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 9m 42s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/4/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 40s | | the patch passed | | +1 :green_heart: | mvnsite | 1m 24s | | the patch passed | | +1 :green_heart: | javadoc | 0m 57s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 37s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 27s | | the patch passed | | +1 :green_heart: | shadedclient | 40m 51s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 22m 32s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 0m 39s | | The patch does not generate ASF License warnings. | | | | 237m 33s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8006 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 92090e4cf01b 5.15.0-157-generic #167-Ubuntu SMP Wed Sep 17 21:35:53 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c3c15b84a4086d834b4fd9aa71b8078a2cb57b65 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/4/testReport/ | | Max. process+thread count | 1449 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/4/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #8006: URL: https://github.com/apache/hadoop/pull/8006#issuecomment-3403808610 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 21m 13s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 44m 40s | | trunk passed | | +1 :green_heart: | compile | 17m 35s | | trunk passed | | +1 :green_heart: | checkstyle | 1m 0s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 55s | | trunk passed | | +1 :green_heart: | javadoc | 1m 19s | | trunk passed | | -1 :x: | spotbugs | 1m 36s | [/branch-spotbugs-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/5/artifact/out/branch-spotbugs-hadoop-common-project_hadoop-common.txt) | hadoop-common in trunk failed. | | +1 :green_heart: | shadedclient | 37m 4s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 1m 10s | | the patch passed | | +1 :green_heart: | compile | 16m 48s | | the patch passed | | +1 :green_heart: | javac | 16m 48s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 1s | | the patch passed | | +1 :green_heart: | mvnsite | 1m 52s | | the patch passed | | +1 :green_heart: | javadoc | 1m 17s | | the patch passed | | -1 :x: | spotbugs | 1m 37s | [/patch-spotbugs-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/5/artifact/out/patch-spotbugs-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch failed. | | +1 :green_heart: | shadedclient | 39m 31s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 22m 47s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 3s | | The patch does not generate ASF License warnings. | | | | 204m 29s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8006 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux d65a60138612 5.15.0-157-generic #167-Ubuntu SMP Wed Sep 17 21:35:53 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 93fbc42bff57ec8dc97f2486fa6ba5ba82e31bdf | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/5/testReport/ | | Max. process+thread count | 1474 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/5/console | | versions | git=2.25.1 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. steveloughran merged PR #8006: URL: https://github.com/apache/hadoop/pull/8006", "created": "2025-10-01T15:29:06.000+0000", "updated": "2025-10-16T19:22:52.000+0000", "derived": {"summary_task": "Summarize this issue: We have evidence that `IOStatisticsSupport.snapshotIOStatistics()` can hang, specifically on the statistics collected by an S3AInputStream, whose statistics are merged in to the FS stats in close(); {code} jdk.internal.misc.Unsafe.park(Native Method) java.util.concurrent.locks.LockSupport.park(LockSupport.java:341) java.util.concurrent.ForkJoinTask.awaitDone(ForkJoinTask.java:468) java.util.concur", "classification_task": "Classify the issue priority and type: We have evidence that `IOStatisticsSupport.snapshotIOStatistics()` can hang, specifically on the statistics collected by an S3AInputStream, whose statistics are merged in to the FS stats in close(); {code} jdk.internal.misc.Unsafe.park(Native Method) java.util.concurrent.locks.LockSupport.park(LockSupport.java:341) java.util.concurrent.ForkJoinTask.awaitDone(ForkJoinTask.java:468) java.util.concur", "qna_task": "Question: What is this issue about?\nAnswer: S3A: Deadlock observed in IOStatistics EvaluatingStatisticsMap.entryset()"}}
{"id": "13630426", "key": "HADOOP-19711", "project": "HADOOP", "summary": "Upgrade hadoop3 docker scripts to 3.4.2", "description": "The Hadoop 3.4.2 version has been released, and we need to update the Hadoop Docker image to version 3.4.2.", "comments": "slfan1989 opened a new pull request, #8005: URL: https://github.com/apache/hadoop/pull/8005 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? slfan1989 commented on PR #8005: URL: https://github.com/apache/hadoop/pull/8005#issuecomment-3355515590 @jojochuang @adoroszlai @ayushtkn Hadoop 3.4.2 has been released, and we are preparing a corresponding Docker image for Hadoop 3.4.2. I have created this PR to complete the Docker image release. Could you please review this PR? Thank you very much! adoroszlai commented on code in PR #8005: URL: https://github.com/apache/hadoop/pull/8005#discussion_r2394132623 ########## Dockerfile: ########## @@ -14,7 +14,7 @@ # limitations under the License. FROM apache/hadoop-runner -ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz +ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar Review Comment: BTW do you know why the tarball is published without `.gz`? It still seems to be gzipped: ``` $ file hadoop-3.4.2.tar hadoop-3.4.2.tar: gzip compressed data, ... ``` slfan1989 commented on code in PR #8005: URL: https://github.com/apache/hadoop/pull/8005#discussion_r2396443322 ########## Dockerfile: ########## @@ -14,7 +14,7 @@ # limitations under the License. FROM apache/hadoop-runner -ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz +ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar Review Comment: Thank you very much for helping to review the code! I'm not sure why this package doesn't have the `.gz` @ahmarsuhail Could you please help take a look at this question? Thank you very much! slfan1989 commented on PR #8005: URL: https://github.com/apache/hadoop/pull/8005#issuecomment-3358754054 > Thanks @slfan1989 for the patch. > > ``` > $ docker run -it --rm ghcr.io/slfan1989/hadoop:3.4.2 hadoop version > ... > Hadoop 3.4.2 > Source code repository https://github.com/apache/hadoop.git -r 84e8b89ee2ebe6923691205b9e171badde7a495c > Compiled by ahmarsu on 2025-08-20T10:30Z > Compiled on platform linux-x86_64 > Compiled with protoc 3.23.4 > From source with checksum fa94c67d4b4be021b9e9515c9b0f7b6 > This command was run using /opt/hadoop/share/hadoop/common/hadoop-common-3.4.2.jar > ``` > > After this is merged, I suggest someone from Hadoop PMC upload the same image to Docker Hub, something like: > > ``` > docker pull ghcr.io/apache/hadoop:3.4.2 > docker tag ghcr.io/apache/hadoop:3.4.2 apache/hadoop:3.4.2 > docker push apache/hadoop:3.4.2 > ``` @adoroszlai Thank you very much for the detailed explanation. However, I have never published a Docker image before, and pushing to Docker Hub should require some additional authentication information. @jojochuang @ayushtkn , could you please take a look? Thank you very much! ahmarsuhail commented on code in PR #8005: URL: https://github.com/apache/hadoop/pull/8005#discussion_r2398213215 ########## Dockerfile: ########## @@ -14,7 +14,7 @@ # limitations under the License. FROM apache/hadoop-runner -ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz +ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar Review Comment: Hey, sorry I think I made a mistake while uploading the tar to [the staging repo](https://dist.apache.org/repos/dist/dev/hadoop/3.4.2-RC3/), and the it got copied incorrectly to the release directory. can someone from the PMC please update the file name in the release directory? it is gzipped, just missing the `.gz` . My apologies for the miss. slfan1989 commented on code in PR #8005: URL: https://github.com/apache/hadoop/pull/8005#discussion_r2400690548 ########## Dockerfile: ########## @@ -14,7 +14,7 @@ # limitations under the License. FROM apache/hadoop-runner -ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz +ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar Review Comment: @ahmarsuhail Thank you for the information\u2014no need to apologize. I\u2019ll try adding the `.gz` extension. Thanks again for your contribution to the hadoop-3.4.2 release. slfan1989 commented on code in PR #8005: URL: https://github.com/apache/hadoop/pull/8005#discussion_r2403881944 ########## Dockerfile: ########## @@ -14,7 +14,7 @@ # limitations under the License. FROM apache/hadoop-runner -ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz +ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar Review Comment: I\u2019ve already updated the [dist repo](https://dist.apache.org/repos/dist/release/hadoop/common/hadoop-3.4.2/ ), but the dlcdn hasn\u2019t synchronized yet. It may take a few more hours. slfan1989 commented on code in PR #8005: URL: https://github.com/apache/hadoop/pull/8005#discussion_r2403881944 ########## Dockerfile: ########## @@ -14,7 +14,7 @@ # limitations under the License. FROM apache/hadoop-runner -ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz +ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar Review Comment: I\u2019ve already updated the [dist repo](https://dist.apache.org/repos/dist/release/hadoop/common/hadoop-3.4.2/ ), but the dlcdn hasn\u2019t synchronized yet. It may take a few more hours. ``` .... [hadoop-3.4.2.tar.gz](https://dist.apache.org/repos/dist/release/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz) [hadoop-3.4.2.tar.gz.asc](https://dist.apache.org/repos/dist/release/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz.asc) [hadoop-3.4.2.tar.gz.sha512](https://dist.apache.org/repos/dist/release/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz.sha512) .... ``` slfan1989 merged PR #8005: URL: https://github.com/apache/hadoop/pull/8005 slfan1989 commented on PR #8005: URL: https://github.com/apache/hadoop/pull/8005#issuecomment-3369556514 @adoroszlai @ahmarsuhail Thank you very much for helping review the code!", "created": "2025-10-01T08:56:49.000+0000", "updated": "2025-10-09T10:05:02.000+0000", "derived": {"summary_task": "Summarize this issue: The Hadoop 3.4.2 version has been released, and we need to update the Hadoop Docker image to version 3.4.2.", "classification_task": "Classify the issue priority and type: The Hadoop 3.4.2 version has been released, and we need to update the Hadoop Docker image to version 3.4.2.", "qna_task": "Question: What is this issue about?\nAnswer: Upgrade hadoop3 docker scripts to 3.4.2"}}
{"id": "13630213", "key": "HADOOP-19710", "project": "HADOOP", "summary": "ABFS: Read Buffer Manager V2 should not be allowed untill implemented", "description": "Read Buffer Manager V2 is a Work in progress and not yet fully ready to be used. This is to stop any user explicitly enabling the config to enable RBMV2.", "comments": "anujmodi2021 opened a new pull request, #8002: URL: https://github.com/apache/hadoop/pull/8002 Read Buffer Manager V2 is a Work in progress and not yet fully ready to be used. This is to stop any user explicitly enabling the config to enable RBMV2. JIRA: https://issues.apache.org/jira/browse/HADOOP-19710 hadoop-yetus commented on PR #8002: URL: https://github.com/apache/hadoop/pull/8002#issuecomment-3346210665 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 9m 52s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 30m 30s | | trunk passed | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 17s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 26s | | trunk passed | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 20s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 40s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 40s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 17s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 16s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 16s | | the patch passed | | +1 :green_heart: | blanks | 0m 1s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 12s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 22s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 42s | | the patch passed | | +1 :green_heart: | shadedclient | 19m 49s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 20s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 25s | | The patch does not generate ASF License warnings. | | | | 90m 2s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8002/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8002 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 7ed8c85f9284 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / bc356262478fb82c786dc3bee7c312b2b1a29634 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8002/1/testReport/ | | Max. process+thread count | 566 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8002/1/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. anujmodi2021 merged PR #8002: URL: https://github.com/apache/hadoop/pull/8002", "created": "2025-09-29T08:59:21.000+0000", "updated": "2025-10-07T09:54:40.000+0000", "derived": {"summary_task": "Summarize this issue: Read Buffer Manager V2 is a Work in progress and not yet fully ready to be used. This is to stop any user explicitly enabling the config to enable RBMV2.", "classification_task": "Classify the issue priority and type: Read Buffer Manager V2 is a Work in progress and not yet fully ready to be used. This is to stop any user explicitly enabling the config to enable RBMV2.", "qna_task": "Question: What is this issue about?\nAnswer: ABFS: Read Buffer Manager V2 should not be allowed untill implemented"}}
{"id": "13630115", "key": "HADOOP-19709", "project": "HADOOP", "summary": "[JDK17] Add debian:12 and debian:13 as a build platform with JDK-17 as default", "description": "Add a new Dockerfiles to compile Hadoop on latest Debian:12 and Debian:13 with JDK17 as the default compiler.", "comments": "vinayakumarb opened a new pull request, #8001: URL: https://github.com/apache/hadoop/pull/8001 This commit introduces support for Debian 12 (Bookworm) and Debian 13 (Trixie) as build platforms, following the approach established for Ubuntu 24. Key changes include: - Creation of `Dockerfile_debian_12` and `Dockerfile_debian_13` based on `Dockerfile_ubuntu_24`, with appropriate base images and package resolver arguments. - Updates to `dev-support/docker/pkg-resolver/packages.json` to include package definitions for `debian:12` and `debian:13`. - Addition of `debian:12` and `debian:13` to `dev-support/docker/pkg-resolver/platforms.json`. - Modification of `BUILDING.txt` to list `debian_12` and `debian_13` as supported OS platforms. hadoop-yetus commented on PR #8001: URL: https://github.com/apache/hadoop/pull/8001#issuecomment-3341301544 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 23m 38s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | hadolint | 0m 0s | | hadolint was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | shadedclient | 37m 46s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/1/artifact/out/blanks-eol.txt) | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | +1 :green_heart: | shadedclient | 34m 39s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 53s | | The patch does not generate ASF License warnings. | | | | 98m 48s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8001 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint | | uname | Linux 01b5b1df2935 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7ccea846897ea6a8209a2238c06933afb4c489bc | | Max. process+thread count | 554 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/1/console | | versions | git=2.43.7 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. pan3793 commented on PR #8001: URL: https://github.com/apache/hadoop/pull/8001#issuecomment-3342170970 Previously, there were concerns about having many versions of Linux dist Dockerfiles, how about upgrading Debian 10 to 13 directly? slfan1989 commented on PR #8001: URL: https://github.com/apache/hadoop/pull/8001#issuecomment-3349642263 > Previously, there were concerns about having many versions of Linux dist Dockerfiles, how about upgrading Debian 10 to 13 directly? @vinayakumarb Thank you very much for your contribution. However, I still have some concerns. Expanding support to many operating systems could be a rather heavy undertaking, since it requires us to pay closer attention to their EOL and version lifecycles. I'm wondering if it might be more sustainable to maintain a smaller subset of supported systems instead. If users have other requirements, they could always try customizing the build themselves. cc: @pan3793 @ayushtkn @cnauroth vinayakumarb commented on PR #8001: URL: https://github.com/apache/hadoop/pull/8001#issuecomment-3376374939 > > Previously, there were concerns about having many versions of Linux dist Dockerfiles, how about upgrading Debian 10 to 13 directly? > > @vinayakumarb Thank you very much for your contribution. However, I still have some concerns. Expanding support to many operating systems could be a rather heavy undertaking, since it requires us to pay closer attention to their EOL and version lifecycles. I'm wondering if it might be more sustainable to maintain a smaller subset of supported systems instead. If users have other requirements, they could always try customizing the build themselves. > > cc: @pan3793 @ayushtkn @cnauroth I understand the concern. Directly upgrading the debian:10 to debian:13 may break existing pipelines. However, having a Dockerfiles for various platforms provides the developers to build an environment as per their choice. It not necessarily means Hadoop binaries (jars and tar) are compiled in these. if users are interested in building Hadoop in their own choice of environment, these Dockerfiles will be a good starting point. slfan1989 commented on PR #8001: URL: https://github.com/apache/hadoop/pull/8001#issuecomment-3376469046 @vinayakumarb Thank you for the clarification \u2014 I agree (+1). However, given the complexity of operating system EOL management, I would carefully evaluate the introduction of Docker support for new systems in the future, considering both maintenance costs and long-term sustainability. hadoop-yetus commented on PR #8001: URL: https://github.com/apache/hadoop/pull/8001#issuecomment-3376715593 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 31m 43s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | hadolint | 0m 0s | | hadolint was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 1s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | shadedclient | 40m 53s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/2/artifact/out/blanks-eol.txt) | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | +1 :green_heart: | shadedclient | 37m 52s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 59s | | The patch does not generate ASF License warnings. | | | | 113m 29s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8001 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint | | uname | Linux 9e7987a7030b 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 92bd7478449829a0e7b987157945cfd72199e4ac | | Max. process+thread count | 647 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/2/console | | versions | git=2.43.7 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. pan3793 commented on code in PR #8001: URL: https://github.com/apache/hadoop/pull/8001#discussion_r2412409276 ########## dev-support/docker/pkg-resolver/packages.json: ########## @@ -263,6 +352,14 @@ \"openjdk-11-jdk\", \"openjdk-17-jdk\" ], + \"debian:12\": [ + \"temurin-17-jdk\", + \"temurin-24-jdk\" + ], + \"debian:13\": [ + \"temurin-17-jdk\", + \"temurin-24-jdk\" Review Comment: temurin-25 is out BTW, I think we should prefer to use the JDK provided by official APT repo if possible, Debian 13 already has `openjdk-25-jdk` pan3793 commented on code in PR #8001: URL: https://github.com/apache/hadoop/pull/8001#discussion_r2412409614 ########## dev-support/docker/pkg-resolver/packages.json: ########## @@ -353,26 +472,34 @@ }, \"software-properties-common\": { \"debian:11\": \"software-properties-common\", + + Review Comment: ? pan3793 commented on code in PR #8001: URL: https://github.com/apache/hadoop/pull/8001#discussion_r2412425091 ########## dev-support/docker/Dockerfile_debian_13: ########## @@ -0,0 +1,110 @@ +# Licensed to the Apache Software Foundation (ASF) under one +# or more contributor license agreements. See the NOTICE file +# distributed with this work for additional information +# regarding copyright ownership. The ASF licenses this file +# to you under the Apache License, Version 2.0 (the +# \"License\"); you may not use this file except in compliance +# with the License. You may obtain a copy of the License at +# +# http://www.apache.org/licenses/LICENSE-2.0 +# +# Unless required by applicable law or agreed to in writing, software +# distributed under the License is distributed on an \"AS IS\" BASIS, +# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. +# See the License for the specific language governing permissions and +# limitations under the License. + +# Dockerfile for installing the necessary dependencies for building Hadoop. +# See BUILDING.txt. + +FROM debian:13 + +WORKDIR /root + +SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-c\"] + +##### +# Disable suggests/recommends +##### +RUN echo 'APT::Install-Recommends \"0\";' > /etc/apt/apt.conf.d/10disableextras +RUN echo 'APT::Install-Suggests \"0\";' >> /etc/apt/apt.conf.d/10disableextras + +ENV DEBIAN_FRONTEND=noninteractive +ENV DEBCONF_TERSE=true + +###### +# Platform package dependency resolver +###### +COPY pkg-resolver pkg-resolver +RUN chmod a+x pkg-resolver/*.sh pkg-resolver/*.py \\ + && chmod a+r pkg-resolver/*.json + +###### +# Install packages from apt +###### +# hadolint ignore=DL3008,SC2046 +RUN apt-get -q update +RUN apt-get -q install -y --no-install-recommends wget apt-transport-https gpg gpg-agent gawk ca-certificates +RUN apt-get -q install -y --no-install-recommends python3 +RUN echo \"deb https://packages.adoptium.net/artifactory/deb $(awk -F= '/^VERSION_CODENAME/{print$2}' /etc/os-release) main\" > /etc/apt/sources.list.d/adoptium.list +RUN wget -q -O - https://packages.adoptium.net/artifactory/api/gpg/key/public > /etc/apt/trusted.gpg.d/adoptium.asc +RUN apt-get -q update +RUN apt-get -q install -y --no-install-recommends $(pkg-resolver/resolve.py debian:13) +RUN apt-get clean +RUN update-java-alternatives -s temurin-17-jdk-amd64 +RUN rm -rf /var/lib/apt/lists/* Review Comment: each RUN produces one image layer, you should concat those shell commands by && instead pan3793 commented on PR #8001: URL: https://github.com/apache/hadoop/pull/8001#issuecomment-3379465889 @vinayakumarb, in addition to creating a dev container from the Dockerfile, have you verified that Hadoop can build successfully with native and frontend components in the created dev container? vinayakumarb commented on code in PR #8001: URL: https://github.com/apache/hadoop/pull/8001#discussion_r2422824903 ########## dev-support/docker/pkg-resolver/packages.json: ########## @@ -263,6 +352,14 @@ \"openjdk-11-jdk\", \"openjdk-17-jdk\" ], + \"debian:12\": [ + \"temurin-17-jdk\", + \"temurin-24-jdk\" + ], + \"debian:13\": [ + \"temurin-17-jdk\", + \"temurin-24-jdk\" Review Comment: Done. Using openjdk-25-jdk in debian-13 and temurin-25-jdk in debian 12 as openjdk is not available in debian repository for bookworm. vinayakumarb commented on code in PR #8001: URL: https://github.com/apache/hadoop/pull/8001#discussion_r2422825814 ########## dev-support/docker/Dockerfile_debian_13: ########## @@ -0,0 +1,110 @@ +# Licensed to the Apache Software Foundation (ASF) under one +# or more contributor license agreements. See the NOTICE file +# distributed with this work for additional information +# regarding copyright ownership. The ASF licenses this file +# to you under the Apache License, Version 2.0 (the +# \"License\"); you may not use this file except in compliance +# with the License. You may obtain a copy of the License at +# +# http://www.apache.org/licenses/LICENSE-2.0 +# +# Unless required by applicable law or agreed to in writing, software +# distributed under the License is distributed on an \"AS IS\" BASIS, +# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. +# See the License for the specific language governing permissions and +# limitations under the License. + +# Dockerfile for installing the necessary dependencies for building Hadoop. +# See BUILDING.txt. + +FROM debian:13 + +WORKDIR /root + +SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-c\"] + +##### +# Disable suggests/recommends +##### +RUN echo 'APT::Install-Recommends \"0\";' > /etc/apt/apt.conf.d/10disableextras +RUN echo 'APT::Install-Suggests \"0\";' >> /etc/apt/apt.conf.d/10disableextras + +ENV DEBIAN_FRONTEND=noninteractive +ENV DEBCONF_TERSE=true + +###### +# Platform package dependency resolver +###### +COPY pkg-resolver pkg-resolver +RUN chmod a+x pkg-resolver/*.sh pkg-resolver/*.py \\ + && chmod a+r pkg-resolver/*.json + +###### +# Install packages from apt +###### +# hadolint ignore=DL3008,SC2046 +RUN apt-get -q update +RUN apt-get -q install -y --no-install-recommends wget apt-transport-https gpg gpg-agent gawk ca-certificates +RUN apt-get -q install -y --no-install-recommends python3 +RUN echo \"deb https://packages.adoptium.net/artifactory/deb $(awk -F= '/^VERSION_CODENAME/{print$2}' /etc/os-release) main\" > /etc/apt/sources.list.d/adoptium.list +RUN wget -q -O - https://packages.adoptium.net/artifactory/api/gpg/key/public > /etc/apt/trusted.gpg.d/adoptium.asc +RUN apt-get -q update +RUN apt-get -q install -y --no-install-recommends $(pkg-resolver/resolve.py debian:13) +RUN apt-get clean +RUN update-java-alternatives -s temurin-17-jdk-amd64 +RUN rm -rf /var/lib/apt/lists/* Review Comment: Done. vinayakumarb commented on PR #8001: URL: https://github.com/apache/hadoop/pull/8001#issuecomment-3393325468 > @vinayakumarb, in addition to creating a dev container from the Dockerfile, have you verified that Hadoop can build successfully with native and frontend components in the created dev container? Yes. I have verified building both native and frontend. vinayakumarb commented on code in PR #8001: URL: https://github.com/apache/hadoop/pull/8001#discussion_r2422836979 ########## dev-support/docker/pkg-resolver/packages.json: ########## @@ -353,26 +472,34 @@ }, \"software-properties-common\": { \"debian:11\": \"software-properties-common\", + + Review Comment: Forgot to remove empty lines. `software-properties-common` not available for debian 12 and 13. Also does not look like it was needed. hadoop-yetus commented on PR #8001: URL: https://github.com/apache/hadoop/pull/8001#issuecomment-3393364407 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 13m 33s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | hadolint | 0m 0s | | hadolint was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 1s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | shadedclient | 24m 31s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 21m 24s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 41s | | The patch does not generate ASF License warnings. | | | | 61m 20s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8001 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint | | uname | Linux 5a71556ea849 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / e371f08075c145585c0d620978733fceb30c93b0 | | Max. process+thread count | 559 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/3/console | | versions | git=2.43.7 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #8001: URL: https://github.com/apache/hadoop/pull/8001#issuecomment-3393395506 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 11m 43s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 1s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | shadedclient | 14m 3s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | hadolint | 0m 1s | | No new issues. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | shadedclient | 13m 26s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 21s | | The patch does not generate ASF License warnings. | | | | 40m 45s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8001 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint | | uname | Linux 4e9b089f3372 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / e371f08075c145585c0d620978733fceb30c93b0 | | Max. process+thread count | 575 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/3/console | | versions | git=2.30.2 maven=3.9.11 hadolint=1.11.1-0-g0e692dd shellcheck=0.7.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #8001: URL: https://github.com/apache/hadoop/pull/8001#issuecomment-3393395990 (!) A patch to the testing environment has been detected. Re-executing against the patched versions to perform further tests. The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/3/console in case of problems. hadoop-yetus commented on PR #8001: URL: https://github.com/apache/hadoop/pull/8001#issuecomment-3393432943 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 8m 32s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 1s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | shadedclient | 19m 49s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | hadolint | 0m 2s | | No new issues. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | shadedclient | 19m 24s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 19s | | The patch does not generate ASF License warnings. | | | | 49m 15s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8001 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint | | uname | Linux 167189e0eac1 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / e371f08075c145585c0d620978733fceb30c93b0 | | Max. process+thread count | 568 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/3/console | | versions | git=2.25.1 maven=3.9.11 hadolint=1.11.1-0-g0e692dd shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. vinayakumarb merged PR #8001: URL: https://github.com/apache/hadoop/pull/8001", "created": "2025-09-27T04:38:55.000+0000", "updated": "2025-10-21T16:28:27.000+0000", "derived": {"summary_task": "Summarize this issue: Add a new Dockerfiles to compile Hadoop on latest Debian:12 and Debian:13 with JDK17 as the default compiler.", "classification_task": "Classify the issue priority and type: Add a new Dockerfiles to compile Hadoop on latest Debian:12 and Debian:13 with JDK17 as the default compiler.", "qna_task": "Question: What is this issue about?\nAnswer: [JDK17] Add debian:12 and debian:13 as a build platform with JDK-17 as default"}}
{"id": "13630052", "key": "HADOOP-19708", "project": "HADOOP", "summary": "volcano tos: disable shading when -DskipShade is set on a build", "description": "hadoop-tos is shaded, includes things like httpclient5, which leads to a big 4M artifact, with unknown content inside {code} 92K share/hadoop/common/lib/hadoop-aliyun-3.5.0-20250916.124028-685.jar 912K share/hadoop/common/lib/hadoop-aws-3.5.0-20250916.124028-686.jar 808K share/hadoop/common/lib/hadoop-azure-3.5.0-20250916.124028-685.jar 36K share/hadoop/common/lib/hadoop-azure-datalake-3.5.0-20250916.124028-685.jar 72K share/hadoop/common/lib/hadoop-cos-3.5.0-20250916.124028-683.jar 136K share/hadoop/common/lib/hadoop-gcp-3.5.0-SNAPSHOT.jar 140K share/hadoop/common/lib/hadoop-huaweicloud-3.5.0-SNAPSHOT.jar 3.8M share/hadoop/common/lib/hadoop-tos-3.5.0-20250916.124028-202.jar {code} One thing it includes is yet-another mozilla/public-suffix-list.txt. These are a recurrent PITA and I don't want to find them surfacing again. {code} 15. Required Resources ====================== resource: mozilla/public-suffix-list.txt jar:file:/Users/stevel/Projects/Releases/hadoop-3.5.0-SNAPSHOT/share/hadoop/common/lib/hadoop-tos-3.5.0-20250916.124028-202.jar!/mozilla/public-suffix-list.txt {code} Plan * Move the shade stage behind a profile; off for ASF releases. Exclude mozilla/public-suffix-list.txt * Explicitly declare and manage httpclient5 dependency * hadoop-cloud-storage pom to include hadoop-tos but not dependencies in build, unless asked. * LICENSE-binary to declare optional redist of ve-tos-java-sdk-hadoop and its license.", "comments": "doing this inside HADOOP-19696, as that's where I need the leaner artifacts (note the shading is troubled anyway {code} [INFO] Dependency-reduced POM written at: /Users/stevel/Projects/hadoop-trunk/hadoop-cloud-storage-project/hadoop-tos/dependency-reduced-pom.xml [WARNING] httpclient5-5.3.jar, httpcore5-5.2.4.jar, httpcore5-h2-5.2.4.jar define 3 overlapping resources: [WARNING] - META-INF/DEPENDENCIES [WARNING] - META-INF/LICENSE [WARNING] - META-INF/NOTICE [WARNING] hadoop-tos-3.5.0-SNAPSHOT.jar, httpclient5-5.3.jar, httpcore5-5.2.4.jar, httpcore5-h2-5.2.4.jar, nimbus-jose-jwt-10.4.jar, ve-tos-java-sdk-hadoop-2.8.9.jar define 1 overlapping resource: [WARNING] - META-INF/MANIFEST.MF [WARNING] maven-shade-plugin has detected that some files are [WARNING] present in two or more JARs. When this happens, only one [WARNING] single version of the file is copied to the uber jar. [WARNING] Usually this is not harmful and you can skip these warnings, [WARNING] otherwise try to manually exclude artifacts based on [WARNING] mvn dependency:tree -Ddetail=true and the above output. [WARNING] See https://maven.apache.org/plugins/maven-shade-plugin/ [INFO] Replacing original artifact with shaded artifact. [INFO] Replacing /Users/stevel/Projects/hadoop-trunk/hadoop-cloud-storage-project/hadoop-tos/target/hadoop-tos-3.5.0-SNAPSHOT.jar with /Users/stevel/Projects/hadoop-trunk/hadoop-cloud-storage-project/hadoop-tos/target/hadoop-tos-3.5.0-SNAPSHOT-shaded.jar [INFO] [INFO] --- cyclonedx:2.9.1:makeBom (default) @ hadoop-tos --- {code}", "created": "2025-09-26T09:30:31.000+0000", "updated": "2025-10-09T19:59:47.000+0000", "derived": {"summary_task": "Summarize this issue: hadoop-tos is shaded, includes things like httpclient5, which leads to a big 4M artifact, with unknown content inside {code} 92K share/hadoop/common/lib/hadoop-aliyun-3.5.0-20250916.124028-685.jar 912K share/hadoop/common/lib/hadoop-aws-3.5.0-20250916.124028-686.jar 808K share/hadoop/common/lib/hadoop-azure-3.5.0-20250916.124028-685.jar 36K share/hadoop/common/lib/hadoop-azure-datalake-3.5.0-20250", "classification_task": "Classify the issue priority and type: hadoop-tos is shaded, includes things like httpclient5, which leads to a big 4M artifact, with unknown content inside {code} 92K share/hadoop/common/lib/hadoop-aliyun-3.5.0-20250916.124028-685.jar 912K share/hadoop/common/lib/hadoop-aws-3.5.0-20250916.124028-686.jar 808K share/hadoop/common/lib/hadoop-azure-3.5.0-20250916.124028-685.jar 36K share/hadoop/common/lib/hadoop-azure-datalake-3.5.0-20250", "qna_task": "Question: What is this issue about?\nAnswer: volcano tos: disable shading when -DskipShade is set on a build"}}
{"id": "13630003", "key": "HADOOP-19707", "project": "HADOOP", "summary": "Surefire upgrade leads to increased report output, can cause Jenkins OOM", "description": "The Surefire upgrade to 3.3+ added enableOutErrElements which defaults to true and includes system-out and system-err in the xml artifacts. This significantly increases their size: ~45KB to ~1MB in many cases. In my test environment, that leads to {code} Recording test results ERROR: Step \u2018Publish JUnit test result report\u2019 aborted due to exception: java.lang.OutOfMemoryError: Java heap space {code} Capturing stdout seems useful when doing ad-hoc testing or smaller test runs. I'd propose adding a profile to set enableOutErrElements=false for CI environments where that extra output can cause problems.", "comments": "MikaelSmith opened a new pull request, #7998: URL: https://github.com/apache/hadoop/pull/7998 ### Description of PR Adds the `quiet-surefire` profile to set enableOutErrElements=false for maven-surefire-plugin. This restores the behavior prior to Surefire 3.3 that stdout/stderr are not included in the TEST-<package>.<class>.xml file for passing tests. The newer default behavior results in much larger TEST-*.xml files that can be a problem for CI tools processing them. ### How was this patch tested? Ran `mvn clean test -Dtest=TestHttpServer` and `mvn clean test -Dtest=TestHttpServer -Pquiet-surefire` and compared size and contents of hadoop-common-project/hadoop-common/target/surefire-reports/TEST-org.apache.hadoop.http.TestHttpServer.xml. ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #7998: URL: https://github.com/apache/hadoop/pull/7998#issuecomment-3336586704 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 21m 28s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 56m 17s | | trunk passed | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 0m 27s | | trunk passed | | +1 :green_heart: | javadoc | 0m 27s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 23s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 98m 4s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 15s | | the patch passed | | +1 :green_heart: | compile | 0m 14s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 14s | | the patch passed | | +1 :green_heart: | compile | 0m 14s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 14s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 17s | | the patch passed | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 14s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 40m 47s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 17s | | hadoop-project in the patch passed. | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 163m 42s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7998/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7998 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux 6c02dd609cd9 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 895391e2c52eadd071997eacaaf7bb2f2af8be30 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7998/1/testReport/ | | Max. process+thread count | 533 (vs. ulimit of 5500) | | modules | C: hadoop-project U: hadoop-project | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7998/1/console | | versions | git=2.25.1 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. steveloughran merged PR #7998: URL: https://github.com/apache/hadoop/pull/7998", "created": "2025-09-25T23:37:18.000+0000", "updated": "2025-10-02T19:02:11.000+0000", "derived": {"summary_task": "Summarize this issue: The Surefire upgrade to 3.3+ added enableOutErrElements which defaults to true and includes system-out and system-err in the xml artifacts. This significantly increases their size: ~45KB to ~1MB in many cases. In my test environment, that leads to {code} Recording test results ERROR: Step \u2018Publish JUnit test result report\u2019 aborted due to exception: java.lang.OutOfMemoryError: Java heap space {code", "classification_task": "Classify the issue priority and type: The Surefire upgrade to 3.3+ added enableOutErrElements which defaults to true and includes system-out and system-err in the xml artifacts. This significantly increases their size: ~45KB to ~1MB in many cases. In my test environment, that leads to {code} Recording test results ERROR: Step \u2018Publish JUnit test result report\u2019 aborted due to exception: java.lang.OutOfMemoryError: Java heap space {code", "qna_task": "Question: What is this issue about?\nAnswer: Surefire upgrade leads to increased report output, can cause Jenkins OOM"}}
{"id": "13629866", "key": "HADOOP-19706", "project": "HADOOP", "summary": "Support Java Modularity", "description": "This is an umbrella JIRA for supporting Java 9 Modularity.", "comments": "", "created": "2025-09-24T20:03:42.000+0000", "updated": "2025-09-24T20:04:21.000+0000", "derived": {"summary_task": "Summarize this issue: This is an umbrella JIRA for supporting Java 9 Modularity.", "classification_task": "Classify the issue priority and type: This is an umbrella JIRA for supporting Java 9 Modularity.", "qna_task": "Question: What is this issue about?\nAnswer: Support Java Modularity"}}
{"id": "13629860", "key": "HADOOP-19705", "project": "HADOOP", "summary": "[JDK17] Do not use Long(long) and similar constructors", "description": "'Long(long)' is deprecated since version 9 and marked for removal.", "comments": "", "created": "2025-09-24T19:00:19.000+0000", "updated": "2025-09-24T19:00:19.000+0000", "derived": {"summary_task": "Summarize this issue: 'Long(long)' is deprecated since version 9 and marked for removal.", "classification_task": "Classify the issue priority and type: 'Long(long)' is deprecated since version 9 and marked for removal.", "qna_task": "Question: What is this issue about?\nAnswer: [JDK17] Do not use Long(long) and similar constructors"}}
{"id": "13629826", "key": "HADOOP-19703", "project": "HADOOP", "summary": "UserGroupInformation.java is using a non-support operation in JDK25", "description": "Hello, I'm trying to upgrade my version of ParquetJava and I'm seeing the following error locally {code:java} java.lang.UnsupportedOperationException: getSubject is not supported at java.base/javax.security.auth.Subject.getSubject(Subject.java:277) at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:577) at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3852) at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3842) at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3630) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:290) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:541) at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365) at org.apache.parquet.hadoop.util.HadoopOutputFile.fromPath(HadoopOutputFile.java:58) at com.amazon.networkvalidator.parquet.ParquetWriter.write(ParquetWriter.kt:75) at com.amazon.networkvalidator.parquet.ParquetWriterTest$test writing to parquet$1.invokeSuspend(ParquetWriterTest.kt:88) at com.amazon.networkvalidator.parquet.ParquetWriterTest$test writing to parquet$1.invoke(ParquetWriterTest.kt) at com.amazon.networkvalidator.parquet.ParquetWriterTest$test writing to parquet$1.invoke(ParquetWriterTest.kt) at kotlinx.coroutines.test.TestBuildersKt__TestBuildersKt$runTest$2$1$1.invokeSuspend(TestBuilders.kt:318) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33) at kotlinx.coroutines.DispatchedTask.run(DispatchedTask.kt:101) at kotlinx.coroutines.test.TestDispatcher.processEvent$kotlinx_coroutines_test(TestDispatcher.kt:24) at kotlinx.coroutines.test.TestCoroutineScheduler.tryRunNextTaskUnless$kotlinx_coroutines_test(TestCoroutineScheduler.kt:99) at kotlinx.coroutines.test.TestBuildersKt__TestBuildersKt$runTest$2$1$workRunner$1.invokeSuspend(TestBuilders.kt:327) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33) at kotlinx.coroutines.DispatchedTask.run(DispatchedTask.kt:101) at kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:263) at kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:95) at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:69) at kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source) at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$default(Builders.kt:47) at kotlinx.coroutines.BuildersKt.runBlocking$default(Unknown Source) at kotlinx.coroutines.test.TestBuildersJvmKt.createTestResult(TestBuildersJvm.kt:10) at kotlinx.coroutines.test.TestBuildersKt__TestBuildersKt.runTest-8Mi8wO0(TestBuilders.kt:310) at kotlinx.coroutines.test.TestBuildersKt.runTest-8Mi8wO0(Unknown Source) at kotlinx.coroutines.test.TestBuildersKt__TestBuildersKt.runTest-8Mi8wO0(TestBuilders.kt:168) at kotlinx.coroutines.test.TestBuildersKt.runTest-8Mi8wO0(Unknown Source) at kotlinx.coroutines.test.TestBuildersKt__TestBuildersKt.runTest-8Mi8wO0$default(TestBuilders.kt:160) at kotlinx.coroutines.test.TestBuildersKt.runTest-8Mi8wO0$default(Unknown Source) at com.amazon.networkvalidator.parquet.ParquetWriterTest.test writing to parquet(ParquetWriterTest.kt:76) {code} The class making this unsupported call is UserGroupInformation, which is part of the common hadoop pkg - https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common/3.4.2", "comments": "Hadoop only supports Java 11. https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions There is work in progress to support Java 17. Java 21 and 25 will be worked on later. HADOOP-19486 Appreciate the link, thanks PJ, will keep a watch on that :)", "created": "2025-09-24T13:42:31.000+0000", "updated": "2025-09-24T15:26:11.000+0000", "derived": {"summary_task": "Summarize this issue: Hello, I'm trying to upgrade my version of ParquetJava and I'm seeing the following error locally {code:java} java.lang.UnsupportedOperationException: getSubject is not supported at java.base/javax.security.auth.Subject.getSubject(Subject.java:277) at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:577) at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(Fi", "classification_task": "Classify the issue priority and type: Hello, I'm trying to upgrade my version of ParquetJava and I'm seeing the following error locally {code:java} java.lang.UnsupportedOperationException: getSubject is not supported at java.base/javax.security.auth.Subject.getSubject(Subject.java:277) at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:577) at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(Fi", "qna_task": "Question: What is this issue about?\nAnswer: UserGroupInformation.java is using a non-support operation in JDK25"}}
{"id": "13629730", "key": "HADOOP-19702", "project": "HADOOP", "summary": "Update non-thirdparty Guava version to 33.4.8-jre", "description": "Keep in sync with recently upgraded thirdparty Guava", "comments": "stoty opened a new pull request, #7994: URL: https://github.com/apache/hadoop/pull/7994 ### Description of PR Update non-thirdparty Guava version to 33.4.8-jre The motivation is the same as for updating the thirdparty one. ### How was this patch tested? CI ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #7994: URL: https://github.com/apache/hadoop/pull/7994#issuecomment-3326537486 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 8m 28s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 31m 21s | | trunk passed | | +1 :green_heart: | compile | 0m 13s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 13s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 0m 20s | | trunk passed | | +1 :green_heart: | javadoc | 0m 16s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 14s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | shadedclient | 33m 33s | | branch has errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 9s | | the patch passed | | +1 :green_heart: | compile | 0m 9s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 9s | | the patch passed | | +1 :green_heart: | compile | 0m 9s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 9s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 10s | | the patch passed | | +1 :green_heart: | javadoc | 0m 8s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 10s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | shadedclient | 1m 22s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 10s | | hadoop-project in the patch passed. | | +1 :green_heart: | asflicense | 0m 19s | | The patch does not generate ASF License warnings. | | | | 45m 32s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7994/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7994 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux 9c1032fe6d33 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / bd6bdde979214be1291cda6a340e8e629a848d7a | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7994/1/testReport/ | | Max. process+thread count | 98 (vs. ulimit of 5500) | | modules | C: hadoop-project U: hadoop-project | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7994/1/console | | versions | git=2.25.1 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on PR #7994: URL: https://github.com/apache/hadoop/pull/7994#issuecomment-3332389576 @stoty In my opinion, there is no issue with this PR. I have one question, why not upgrade to 33.5.0-jre? stoty commented on PR #7994: URL: https://github.com/apache/hadoop/pull/7994#issuecomment-3332540273 > @stoty In my opinion, there is no issue with this PR. I have one question, why not upgrade to 33.5.0-jre? The hadoop-thirdparty guava is being updated to 33.4.8-jre, and I thought that it's easier to manage if we keep the unshaded version in sync with that, as long as we're able to. Also, we've already updated to 33.4.8-jre at my day job without issues, but I don't have experience yet with 33.5.0. slfan1989 commented on PR #7994: URL: https://github.com/apache/hadoop/pull/7994#issuecomment-3349644523 If there are no further comments, I will merge this PR today. slfan1989 merged PR #7994: URL: https://github.com/apache/hadoop/pull/7994 slfan1989 commented on PR #7994: URL: https://github.com/apache/hadoop/pull/7994#issuecomment-3364494335 @stoty Thanks for the contribution! Merged into trunk.", "created": "2025-09-23T17:28:42.000+0000", "updated": "2025-10-03T06:56:03.000+0000", "derived": {"summary_task": "Summarize this issue: Keep in sync with recently upgraded thirdparty Guava", "classification_task": "Classify the issue priority and type: Keep in sync with recently upgraded thirdparty Guava", "qna_task": "Question: What is this issue about?\nAnswer: Update non-thirdparty Guava version to  33.4.8-jre"}}
{"id": "13629664", "key": "HADOOP-19701", "project": "HADOOP", "summary": "Remove invalid `licenses` field of `hadoop-aliyun` SBOM by upgradding `cyclonedx` to 2.9.1", "description": "", "comments": "dongjoon-hyun opened a new pull request, #7990: URL: https://github.com/apache/hadoop/pull/7990 \u2026 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? dongjoon-hyun commented on PR #7990: URL: https://github.com/apache/hadoop/pull/7990#issuecomment-3322518878 Thank you, @slfan1989 . hadoop-yetus commented on PR #7990: URL: https://github.com/apache/hadoop/pull/7990#issuecomment-3325310193 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 52s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 60m 11s | | trunk passed | | +1 :green_heart: | compile | 20m 20s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 16m 2s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 23m 22s | | trunk passed | | +1 :green_heart: | javadoc | 11m 38s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 9m 20s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 185m 36s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 49m 43s | | the patch passed | | +1 :green_heart: | compile | 21m 13s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 21m 13s | | the patch passed | | +1 :green_heart: | compile | 18m 49s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 18m 49s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 21m 50s | | the patch passed | | +1 :green_heart: | javadoc | 11m 53s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 9m 0s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 88m 52s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 511m 54s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7990/1/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 18s | | The patch does not generate ASF License warnings. | | | | 880m 54s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.TestReconstructStripedFileWithRandomECPolicy | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7990/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7990 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux 3a691ff72698 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f944112d4216160cc394f6ea7bd013f7b92796a9 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7990/1/testReport/ | | Max. process+thread count | 2369 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7990/1/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. dongjoon-hyun commented on PR #7990: URL: https://github.com/apache/hadoop/pull/7990#issuecomment-3325322984 All tests passes except `test4tests` and `unit` tests which checks new or revised test cases. So, I believe this PR is ready. slfan1989 merged PR #7990: URL: https://github.com/apache/hadoop/pull/7990 slfan1989 commented on PR #7990: URL: https://github.com/apache/hadoop/pull/7990#issuecomment-3326078332 @dongjoon-hyun Thanks for the contribution! Merged into trunk. dongjoon-hyun commented on PR #7990: URL: https://github.com/apache/hadoop/pull/7990#issuecomment-3326405610 Thank you so much, @slfan1989 .", "created": "2025-09-23T04:55:31.000+0000", "updated": "2025-09-24T04:18:44.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Remove invalid `licenses` field of `hadoop-aliyun` SBOM by upgradding `cyclonedx` to 2.9.1"}}
{"id": "13629596", "key": "HADOOP-19700", "project": "HADOOP", "summary": "hadoop-thirdparty build to update maven plugin dependencies", "description": "github action builds of PRs for hadoopHthirdparty fail because of throttling NVE throttling of requests; needs an update to a later version with either retries or use of a github source cve list. dependency checker 11+ {code} Mandatory Upgrade Notice Upgrading to 10.0.2 or later is mandatory Older versions of dependency-check are causing numerous, duplicative requests that end in processing failures are causing unnecassary load on the NVD API. Dependency-check 10.0.2 uses an updated User-Agent header that will allow the NVD to block calls from the older client. {code} ---- The upgraded dependency checker now *requires* java11+, and *prefers* the provision of an API key for the national vulnerabilities database. It also skips the sonatype check as that no longer supports anonymous checks at all", "comments": "10.x cannot parse the current DB files, even if it manages to download them, [~stevel@apache.org]. latest pr does it, just needs java11 for that action. And I've turned off the sonatype checking", "created": "2025-09-22T13:05:13.000+0000", "updated": "2025-09-30T10:02:32.000+0000", "derived": {"summary_task": "Summarize this issue: github action builds of PRs for hadoopHthirdparty fail because of throttling NVE throttling of requests; needs an update to a later version with either retries or use of a github source cve list. dependency checker 11+ {code} Mandatory Upgrade Notice Upgrading to 10.0.2 or later is mandatory Older versions of dependency-check are causing numerous, duplicative requests that end in processing failur", "classification_task": "Classify the issue priority and type: github action builds of PRs for hadoopHthirdparty fail because of throttling NVE throttling of requests; needs an update to a later version with either retries or use of a github source cve list. dependency checker 11+ {code} Mandatory Upgrade Notice Upgrading to 10.0.2 or later is mandatory Older versions of dependency-check are causing numerous, duplicative requests that end in processing failur", "qna_task": "Question: What is this issue about?\nAnswer: hadoop-thirdparty build to update maven plugin dependencies"}}
{"id": "13629310", "key": "HADOOP-19698", "project": "HADOOP", "summary": "S3A Analytics-Accelerator: Update LICENSE-binary", "description": "update LICENSE-binary to include AAL dependency", "comments": "ahmarsuhail opened a new pull request, #7982: URL: https://github.com/apache/hadoop/pull/7982 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR Adds in AAL dependency to License-binary. ### How was this patch tested? not required. ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? ahmarsuhail commented on PR #7982: URL: https://github.com/apache/hadoop/pull/7982#issuecomment-3306787342 @steveloughran PR to add in license binary. hadoop-yetus commented on PR #7982: URL: https://github.com/apache/hadoop/pull/7982#issuecomment-3307163564 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 52s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | shadedclient | 46m 8s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | shadedclient | 40m 42s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 40s | | The patch does not generate ASF License warnings. | | | | 90m 19s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7982/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7982 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs | | uname | Linux 8e1ed683c4d3 5.15.0-151-generic #161-Ubuntu SMP Tue Jul 22 14:25:40 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / bf18e340d1dc826e0c031dc1e88e59a58fcb59d7 | | Max. process+thread count | 530 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7982/1/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. steveloughran merged PR #7982: URL: https://github.com/apache/hadoop/pull/7982", "created": "2025-09-18T10:46:22.000+0000", "updated": "2025-09-18T13:23:56.000+0000", "derived": {"summary_task": "Summarize this issue: update LICENSE-binary to include AAL dependency", "classification_task": "Classify the issue priority and type: update LICENSE-binary to include AAL dependency", "qna_task": "Question: What is this issue about?\nAnswer: S3A Analytics-Accelerator: Update LICENSE-binary"}}
{"id": "13629211", "key": "HADOOP-19697", "project": "HADOOP", "summary": "google gs connector registration failing", "description": "Surfaced during HADOOP-19696 and work with all the cloud connectors on the classpath. There's a missing dependency causing the gcs connector to fail to register *when the first filesystem is instantiated*, because the service registration process loads the gcs connector class, instantiates one and asks for its schema. As well as a sign of a problem, it's better to just add an entry in core-default.xml as this saves all classloading overhead. It's what the other asf bundled ones do.", "comments": "Trying to list local root If there are dependencies needed in the HADOOP-19696 let's make sure they get into common/lib, but this registration process mustn't fail this way, so let's just have a fs.gs.impl declararation in core-default.xml {code} bin/hadoop fs -ls file:/// 2025-09-17 15:03:47,688 [main] WARN fs.FileSystem (FileSystem.java:loadFileSystems(3539)) - Cannot load filesystem java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.gs.GoogleHadoopFileSystem Unable to get public no-arg constructor at java.base/java.util.ServiceLoader.fail(ServiceLoader.java:586) at java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:679) at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1240) at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273) at java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309) at java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393) at org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:3522) at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3562) at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612) at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716) at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557) at org.apache.hadoop.fs.Path.getFileSystem(Path.java:373) at org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:347) at org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:265) at org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:248) at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:105) at org.apache.hadoop.fs.shell.Command.run(Command.java:192) at org.apache.hadoop.fs.FsShell.run(FsShell.java:327) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:82) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97) at org.apache.hadoop.fs.FsShell.main(FsShell.java:390) Caused by: java.lang.NoClassDefFoundError: com/google/auth/Credentials at java.base/java.lang.Class.getDeclaredConstructors0(Native Method) at java.base/java.lang.Class.privateGetDeclaredConstructors(Class.java:3373) at java.base/java.lang.Class.getConstructor0(Class.java:3578) at java.base/java.lang.Class.getConstructor(Class.java:2271) at java.base/java.util.ServiceLoader$1.run(ServiceLoader.java:666) at java.base/java.util.ServiceLoader$1.run(ServiceLoader.java:663) at java.base/java.security.AccessController.doPrivileged(AccessController.java:569) at java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:674) ... 20 more Caused by: java.lang.ClassNotFoundException: com.google.auth.Credentials at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) ... 28 more Found 20 items ---------- 1 root admin 0 2025-08-16 19:44 file:///.file ... {code} Hi [~stevel@apache.org]. Not fully caught up on this one, but there is a {{fs.gs.impl}} entry in core-default.xml now. Was that all we needed? https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/resources/core-default.xml#L4508", "created": "2025-09-17T14:04:27.000+0000", "updated": "2025-10-08T04:44:25.000+0000", "derived": {"summary_task": "Summarize this issue: Surfaced during HADOOP-19696 and work with all the cloud connectors on the classpath. There's a missing dependency causing the gcs connector to fail to register *when the first filesystem is instantiated*, because the service registration process loads the gcs connector class, instantiates one and asks for its schema. As well as a sign of a problem, it's better to just add an entry in core-default", "classification_task": "Classify the issue priority and type: Surfaced during HADOOP-19696 and work with all the cloud connectors on the classpath. There's a missing dependency causing the gcs connector to fail to register *when the first filesystem is instantiated*, because the service registration process loads the gcs connector class, instantiates one and asks for its schema. As well as a sign of a problem, it's better to just add an entry in core-default", "qna_task": "Question: What is this issue about?\nAnswer: google gs connector registration failing"}}
{"id": "13629208", "key": "HADOOP-19696", "project": "HADOOP", "summary": "hadoop binary distribution to move cloud connectors to hadoop common/lib", "description": "Place all the cloud connector hadoop-* artifacts and dependencies into hadoop/common/lib so that the stores can be directly accessed. * filesystem operations against abfs, s3a, gcs, etc don't need any effort setting things up. * Releases without the aws bundle.jar can be trivially updated by adding any version of the sdk libraries to the common/lib dir. This adds a lot more stuff into the distribution, so I'm doing the following design * all hadoop-* modules in common/lib * minimal dependencies for hadoop-azure and hadoop-gcs (once we get those right!) * hadoop-aws: everything except bundle.jar * other connectors: only included with explicit profiles. ASF releases will support azure out the box, the others once you add the dependencies. And anyone can build their own release with everything One concern here, we make hadoop-cloud-storage artifact incomplete at pulling in things when depended on. We may need a separate module for the distro setup. Noticed during this that the hadoop-tos component is shaded and includes stuff (httpclient5) that we need under control. Filed HADOOP-19708 and incorporating here.", "comments": "steveloughran opened a new pull request, #7980: URL: https://github.com/apache/hadoop/pull/7980 * new assembly for hadoop cloud storage * hadoop-cloud-storage does the assembly on -Pdist * layout stitching to move into share/hadoop/common/lib * remove connectors from hadoop-tools-dist * cut old jackson version from huawaei cloud dependency -even though it was being upgraded by our own artifacts, it was a complication. ### How was this patch tested? Manual build, review, storediag, hadoop fs commands ### For code changes: - [=] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? steveloughran commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3303127263 * This puts the hadoop-azure, hadoop-aws &c binaries into common/lib and so on the classpath everywhere * some problem with gcs instantiation during enum (will file later, as while it surfaces here, I think it's unrelated) * my local builds end up (today) with some versioned jars as well as the -SNAPSHOT. I think this is from me tainting my maven repo, would like to see what others see ``` total 1401704 -rw-r--r--@ 1 stevel staff 106151 Sep 17 12:57 aliyun-java-core-0.2.11-beta.jar -rw-r--r--@ 1 stevel staff 194215 Sep 17 12:57 aliyun-java-sdk-core-4.5.10.jar -rw-r--r--@ 1 stevel staff 163698 Sep 17 12:57 aliyun-java-sdk-kms-2.11.0.jar -rw-r--r--@ 1 stevel staff 220800 Sep 17 12:57 aliyun-java-sdk-ram-3.1.0.jar -rw-r--r--@ 1 stevel staff 928456 Sep 17 12:57 aliyun-sdk-oss-3.18.1.jar -rw-r--r--@ 1 stevel staff 2470776 Sep 17 12:57 analyticsaccelerator-s3-1.3.0.jar -rw-r--r--@ 1 stevel staff 27006 Sep 17 13:11 aopalliance-repackaged-2.6.1.jar -rw-r--r--@ 1 stevel staff 20891 Sep 17 13:11 audience-annotations-0.12.0.jar -rw-r--r--@ 1 stevel staff 651391 Sep 17 13:11 avro-1.11.4.jar -rw-r--r--@ 1 stevel staff 113966 Sep 17 12:57 azure-data-lake-store-sdk-2.3.9.jar -rw-r--r--@ 1 stevel staff 10288 Sep 17 12:57 azure-keyvault-core-1.0.0.jar -rw-r--r--@ 1 stevel staff 815331 Sep 17 12:57 azure-storage-7.0.1.jar -rw-r--r--@ 1 stevel staff 8324412 Sep 17 13:11 bcprov-jdk18on-1.78.1.jar -rw-r--r--@ 1 stevel staff 641534749 Sep 17 12:57 bundle-2.29.52.jar -rw-r--r--@ 1 stevel staff 223979 Sep 17 13:11 checker-qual-3.33.0.jar -rw-r--r--@ 1 stevel staff 75479 Sep 17 13:11 commons-cli-1.9.0.jar -rw-r--r--@ 1 stevel staff 353793 Sep 17 13:11 commons-codec-1.15.jar -rw-r--r--@ 1 stevel staff 751914 Sep 17 13:11 commons-collections4-4.4.jar -rw-r--r--@ 1 stevel staff 1079377 Sep 17 13:11 commons-compress-1.26.1.jar -rw-r--r--@ 1 stevel staff 657516 Sep 17 13:11 commons-configuration2-2.10.1.jar -rw-r--r--@ 1 stevel staff 24239 Sep 17 13:11 commons-daemon-1.0.13.jar -rw-r--r--@ 1 stevel staff 508826 Sep 17 13:11 commons-io-2.16.1.jar -rw-r--r--@ 1 stevel staff 673587 Sep 17 13:11 commons-lang3-3.17.0.jar -rw-r--r--@ 1 stevel staff 70816 Sep 17 13:11 commons-logging-1.3.0.jar -rw-r--r--@ 1 stevel staff 2213560 Sep 17 13:11 commons-math3-3.6.1.jar -rw-r--r--@ 1 stevel staff 316431 Sep 17 13:11 commons-net-3.9.0.jar -rw-r--r--@ 1 stevel staff 238400 Sep 17 13:11 commons-text-1.10.0.jar -rw-r--r--@ 1 stevel staff 8661164 Sep 17 12:57 cos_api-bundle-5.6.19.jar -rw-r--r--@ 1 stevel staff 2983237 Sep 17 13:11 curator-client-5.2.0.jar -rw-r--r--@ 1 stevel staff 336384 Sep 17 13:11 curator-framework-5.2.0.jar -rw-r--r--@ 1 stevel staff 315569 Sep 17 13:11 curator-recipes-5.2.0.jar -rw-r--r--@ 1 stevel staff 583996 Sep 17 13:11 dnsjava-3.6.1.jar -rw-r--r--@ 1 stevel staff 324655 Sep 17 12:57 dom4j-2.1.4.jar -rw-r--r--@ 1 stevel staff 670059 Sep 17 12:57 esdk-obs-java-3.20.4.2.jar -rw-r--r--@ 1 stevel staff 4617 Sep 17 13:11 failureaccess-1.0.1.jar -rw-r--r--@ 1 stevel staff 249277 Sep 17 13:11 gson-2.9.0.jar -rw-r--r--@ 1 stevel staff 3037368 Sep 17 13:11 guava-32.0.1-jre.jar -rw-r--r--@ 1 stevel staff 94013 Sep 17 12:57 hadoop-aliyun-3.5.0-20250916.124028-685.jar -rw-r--r--@ 1 stevel staff 14456 Sep 17 13:11 hadoop-annotations-3.5.0-SNAPSHOT.jar -rw-r--r--@ 1 stevel staff 114335 Sep 17 13:11 hadoop-auth-3.5.0-SNAPSHOT.jar -rw-r--r--@ 1 stevel staff 930516 Sep 17 12:57 hadoop-aws-3.5.0-20250916.124028-686.jar -rw-r--r--@ 1 stevel staff 827349 Sep 17 12:57 hadoop-azure-3.5.0-20250916.124028-685.jar -rw-r--r--@ 1 stevel staff 33363 Sep 17 12:57 hadoop-azure-datalake-3.5.0-20250916.124028-685.jar -rw-r--r--@ 1 stevel staff 70007 Sep 17 12:57 hadoop-cos-3.5.0-20250916.124028-683.jar -rw-r--r--@ 1 stevel staff 138447 Sep 17 12:57 hadoop-gcp-3.5.0-SNAPSHOT.jar -rw-r--r--@ 1 stevel staff 142274 Sep 17 12:57 hadoop-huaweicloud-3.5.0-SNAPSHOT.jar -rw-r--r--@ 1 stevel staff 3519516 Sep 17 13:11 hadoop-shaded-guava-1.4.0.jar -rw-r--r--@ 1 stevel staff 1952967 Sep 17 13:11 hadoop-shaded-protobuf_3_25-1.4.0.jar -rw-r--r--@ 1 stevel staff 4019589 Sep 17 12:57 hadoop-tos-3.5.0-20250916.124028-202.jar -rw-r--r--@ 1 stevel staff 200223 Sep 17 13:11 hk2-api-2.6.1.jar -rw-r--r--@ 1 stevel staff 203358 Sep 17 13:11 hk2-locator-2.6.1.jar -rw-r--r--@ 1 stevel staff 131590 Sep 17 13:11 hk2-utils-2.6.1.jar -rw-r--r--@ 1 stevel staff 780321 Sep 17 13:11 httpclient-4.5.13.jar -rw-r--r--@ 1 stevel staff 328593 Sep 17 13:11 httpcore-4.4.13.jar -rw-r--r--@ 1 stevel staff 102220 Sep 17 12:57 ini4j-0.5.4.jar -rw-r--r--@ 1 stevel staff 29807 Sep 17 13:11 istack-commons-runtime-3.0.12.jar -rw-r--r--@ 1 stevel staff 9301 Sep 17 13:11 j2objc-annotations-2.8.jar -rw-r--r--@ 1 stevel staff 76636 Sep 17 13:11 jackson-annotations-2.14.3.jar -rw-r--r--@ 1 stevel staff 473081 Sep 17 13:11 jackson-core-2.14.3.jar -rw-r--r--@ 1 stevel staff 1617187 Sep 17 13:11 jackson-databind-2.14.3.jar -rw-r--r--@ 1 stevel staff 68453 Sep 17 13:11 jakarta.activation-1.2.2.jar -rw-r--r--@ 1 stevel staff 44399 Sep 17 13:11 jakarta.activation-api-1.2.1.jar -rw-r--r--@ 1 stevel staff 25058 Sep 17 13:11 jakarta.annotation-api-1.3.5.jar -rw-r--r--@ 1 stevel staff 18140 Sep 17 13:11 jakarta.inject-2.6.1.jar -rw-r--r--@ 1 stevel staff 82973 Sep 17 13:11 jakarta.servlet-api-4.0.4.jar -rw-r--r--@ 1 stevel staff 53683 Sep 17 13:11 jakarta.servlet.jsp-api-2.3.6.jar -rw-r--r--@ 1 stevel staff 91930 Sep 17 13:11 jakarta.validation-api-2.0.2.jar -rw-r--r--@ 1 stevel staff 140376 Sep 17 13:11 jakarta.ws.rs-api-2.1.6.jar -rw-r--r--@ 1 stevel staff 115638 Sep 17 13:11 jakarta.xml.bind-api-2.3.3.jar -rw-r--r--@ 1 stevel staff 7771 Sep 17 12:57 java-trace-api-0.2.11-beta.jar -rw-r--r--@ 1 stevel staff 18432 Sep 17 12:57 java-xmlbuilder-1.2.jar -rw-r--r--@ 1 stevel staff 794714 Sep 17 13:11 javassist-3.30.2-GA.jar -rw-r--r--@ 1 stevel staff 95806 Sep 17 13:11 javax.servlet-api-3.1.0.jar -rw-r--r--@ 1 stevel staff 1019097 Sep 17 13:11 jaxb-runtime-2.3.9.jar -rw-r--r--@ 1 stevel staff 4722 Sep 17 13:11 jcip-annotations-1.0-1.jar -rw-r--r--@ 1 stevel staff 327806 Sep 17 12:57 jdom2-2.0.6.1.jar -rw-r--r--@ 1 stevel staff 311826 Sep 17 13:11 jersey-client-2.46.jar -rw-r--r--@ 1 stevel staff 1267957 Sep 17 13:11 jersey-common-2.46.jar -rw-r--r--@ 1 stevel staff 32929 Sep 17 13:11 jersey-container-servlet-2.46.jar -rw-r--r--@ 1 stevel staff 75742 Sep 17 13:11 jersey-container-servlet-core-2.46.jar -rw-r--r--@ 1 stevel staff 80272 Sep 17 13:11 jersey-hk2-2.46.jar -rw-r--r--@ 1 stevel staff 964550 Sep 17 13:11 jersey-server-2.46.jar -rw-r--r--@ 1 stevel staff 90184 Sep 17 12:57 jettison-1.5.4.jar -rw-r--r--@ 1 stevel staff 249911 Sep 17 13:11 jetty-http-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 183011 Sep 17 13:11 jetty-io-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 118496 Sep 17 13:11 jetty-security-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 739348 Sep 17 13:11 jetty-server-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 146064 Sep 17 13:11 jetty-servlet-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 588962 Sep 17 13:11 jetty-util-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 66643 Sep 17 13:11 jetty-util-ajax-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 140308 Sep 17 13:11 jetty-webapp-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 68894 Sep 17 13:11 jetty-xml-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 282591 Sep 17 13:11 jsch-0.1.55.jar -rw-r--r--@ 1 stevel staff 19936 Sep 17 13:11 jsr305-3.0.2.jar -rw-r--r--@ 1 stevel staff 4519 Sep 17 13:11 jul-to-slf4j-1.7.36.jar -rw-r--r--@ 1 stevel staff 223129 Sep 17 13:11 kerb-core-2.0.3.jar -rw-r--r--@ 1 stevel staff 115065 Sep 17 13:11 kerb-crypto-2.0.3.jar -rw-r--r--@ 1 stevel staff 36361 Sep 17 13:11 kerb-util-2.0.3.jar -rw-r--r--@ 1 stevel staff 100095 Sep 17 13:11 kerby-asn1-2.0.3.jar -rw-r--r--@ 1 stevel staff 30190 Sep 17 13:11 kerby-config-2.0.3.jar -rw-r--r--@ 1 stevel staff 200581 Sep 17 13:11 kerby-pkix-2.0.3.jar -rw-r--r--@ 1 stevel staff 40787 Sep 17 13:11 kerby-util-2.0.3.jar -rw-r--r--@ 1 stevel staff 2199 Sep 17 13:11 listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar -rw-r--r--@ 1 stevel staff 136314 Sep 17 13:11 metrics-core-3.2.4.jar -rw-r--r--@ 1 stevel staff 4554 Sep 17 13:11 netty-all-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 339045 Sep 17 13:11 netty-buffer-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 355199 Sep 17 13:11 netty-codec-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 67192 Sep 17 13:11 netty-codec-dns-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 37789 Sep 17 13:11 netty-codec-haproxy-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 674362 Sep 17 13:11 netty-codec-http-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 490985 Sep 17 13:11 netty-codec-http2-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 44736 Sep 17 13:11 netty-codec-memcache-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 113699 Sep 17 13:11 netty-codec-mqtt-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 46015 Sep 17 13:11 netty-codec-redis-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 21344 Sep 17 13:11 netty-codec-smtp-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 121032 Sep 17 13:11 netty-codec-socks-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 34636 Sep 17 13:11 netty-codec-stomp-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 19823 Sep 17 13:11 netty-codec-xml-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 719225 Sep 17 13:11 netty-common-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 580162 Sep 17 13:11 netty-handler-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 25650 Sep 17 13:11 netty-handler-proxy-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 26833 Sep 17 13:11 netty-handler-ssl-ocsp-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 37842 Sep 17 13:11 netty-resolver-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 188360 Sep 17 13:11 netty-resolver-dns-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 9145 Sep 17 13:11 netty-resolver-dns-classes-macos-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 19825 Sep 17 13:11 netty-resolver-dns-native-macos-4.1.118.Final-osx-aarch_64.jar -rw-r--r--@ 1 stevel staff 19629 Sep 17 13:11 netty-resolver-dns-native-macos-4.1.118.Final-osx-x86_64.jar -rw-r--r--@ 1 stevel staff 521428 Sep 17 13:11 netty-transport-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 147621 Sep 17 13:11 netty-transport-classes-epoll-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 108558 Sep 17 13:11 netty-transport-classes-kqueue-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 42321 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-aarch_64.jar -rw-r--r--@ 1 stevel staff 36594 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-riscv64.jar -rw-r--r--@ 1 stevel staff 40644 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-x86_64.jar -rw-r--r--@ 1 stevel staff 6193 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 25741 Sep 17 13:11 netty-transport-native-kqueue-4.1.118.Final-osx-aarch_64.jar -rw-r--r--@ 1 stevel staff 25170 Sep 17 13:11 netty-transport-native-kqueue-4.1.118.Final-osx-x86_64.jar -rw-r--r--@ 1 stevel staff 44157 Sep 17 13:11 netty-transport-native-unix-common-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 18241 Sep 17 13:11 netty-transport-rxtx-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 50814 Sep 17 13:11 netty-transport-sctp-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 32189 Sep 17 13:11 netty-transport-udt-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 779369 Sep 17 13:11 nimbus-jose-jwt-9.37.2.jar -rw-r--r--@ 1 stevel staff 425763 Sep 17 12:57 okhttp-3.14.2.jar -rw-r--r--@ 1 stevel staff 91980 Sep 17 12:57 okio-1.17.2.jar -rw-r--r--@ 1 stevel staff 141734 Sep 17 12:57 opentelemetry-api-1.38.0.jar -rw-r--r--@ 1 stevel staff 47252 Sep 17 12:57 opentelemetry-context-1.38.0.jar -rw-r--r--@ 1 stevel staff 18189 Sep 17 12:57 opentracing-api-0.33.0.jar -rw-r--r--@ 1 stevel staff 10542 Sep 17 12:57 opentracing-noop-0.33.0.jar -rw-r--r--@ 1 stevel staff 7504 Sep 17 12:57 opentracing-util-0.33.0.jar -rw-r--r--@ 1 stevel staff 281989 Sep 17 12:57 org.jacoco.agent-0.8.5-runtime.jar -rw-r--r--@ 1 stevel staff 19479 Sep 17 13:11 osgi-resource-locator-1.0.3.jar -rw-r--r--@ 1 stevel staff 128414 Sep 17 13:11 re2j-1.1.jar -rw-r--r--@ 1 stevel staff 11369 Sep 17 12:57 reactive-streams-1.0.3.jar -rw-r--r--@ 1 stevel staff 332398 Sep 17 13:11 reload4j-1.2.22.jar -rw-r--r--@ 1 stevel staff 41125 Sep 17 13:11 slf4j-api-1.7.36.jar -rw-r--r--@ 1 stevel staff 9824 Sep 17 13:11 slf4j-reload4j-1.7.36.jar -rw-r--r--@ 1 stevel staff 2112099 Sep 17 13:11 snappy-java-1.1.10.4.jar -rw-r--r--@ 1 stevel staff 195909 Sep 17 13:11 stax2-api-4.2.1.jar -rw-r--r--@ 1 stevel staff 72007 Sep 17 13:11 txw2-2.3.9.jar -rw-r--r--@ 1 stevel staff 443788 Sep 17 12:57 wildfly-openssl-2.1.4.Final.jar -rw-r--r--@ 1 stevel staff 522679 Sep 17 13:11 woodstox-core-5.4.0.jar -rw-r--r--@ 1 stevel staff 1323991 Sep 17 13:11 zookeeper-3.8.4.jar -rw-r--r--@ 1 stevel staff 254932 Sep 17 13:11 zookeeper-jute-3.8.4.jar ``` steveloughran commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3303133677 for contrast, here's 3.4.2 ``` ../hadoop-3.4.2/: total 272 drwxr-xr-x@ 13 stevel staff 416 Aug 7 12:58 bin -rw-r--r--@ 1 stevel staff 824 Aug 27 16:58 binding.xml drwxr-xr-x@ 3 stevel staff 96 Aug 25 17:09 downloads drwxr-xr-x@ 3 stevel staff 96 Aug 7 12:16 etc drwxr-xr-x@ 7 stevel staff 224 Aug 7 12:58 include drwxr-xr-x@ 3 stevel staff 96 Aug 7 12:58 lib drwxr-xr-x@ 14 stevel staff 448 Aug 7 12:58 libexec -rw-r--r--@ 1 stevel staff 23682 Aug 7 10:40 LICENSE-binary -rw-r--r--@ 1 stevel staff 15791 Aug 7 10:39 LICENSE.txt drwxr-xr-x@ 45 stevel staff 1440 Aug 7 12:58 licenses-binary -rw-r--r--@ 1 stevel staff 45514 Aug 27 17:12 log.txt -rw-r--r--@ 1 stevel staff 27373 Aug 7 10:39 NOTICE-binary -rw-r--r--@ 1 stevel staff 1541 Aug 7 10:39 NOTICE.txt -rw-r--r--@ 1 stevel staff 175 Aug 7 10:39 README.txt drwxr-xr-x@ 29 stevel staff 928 Aug 7 12:16 sbin -rw-r--r--@ 1 stevel staff 438 Aug 25 16:59 secrets.bin drwxr-xr-x@ 4 stevel staff 128 Aug 7 13:23 share -rw-r--r--@ 1 stevel staff 275 Aug 27 17:12 system.properties share/hadoop/common/lib: total 1401704 -rw-r--r--@ 1 stevel staff 106151 Sep 17 12:57 aliyun-java-core-0.2.11-beta.jar -rw-r--r--@ 1 stevel staff 194215 Sep 17 12:57 aliyun-java-sdk-core-4.5.10.jar -rw-r--r--@ 1 stevel staff 163698 Sep 17 12:57 aliyun-java-sdk-kms-2.11.0.jar -rw-r--r--@ 1 stevel staff 220800 Sep 17 12:57 aliyun-java-sdk-ram-3.1.0.jar -rw-r--r--@ 1 stevel staff 928456 Sep 17 12:57 aliyun-sdk-oss-3.18.1.jar -rw-r--r--@ 1 stevel staff 2470776 Sep 17 12:57 analyticsaccelerator-s3-1.3.0.jar -rw-r--r--@ 1 stevel staff 27006 Sep 17 13:11 aopalliance-repackaged-2.6.1.jar -rw-r--r--@ 1 stevel staff 20891 Sep 17 13:11 audience-annotations-0.12.0.jar -rw-r--r--@ 1 stevel staff 651391 Sep 17 13:11 avro-1.11.4.jar -rw-r--r--@ 1 stevel staff 113966 Sep 17 12:57 azure-data-lake-store-sdk-2.3.9.jar -rw-r--r--@ 1 stevel staff 10288 Sep 17 12:57 azure-keyvault-core-1.0.0.jar -rw-r--r--@ 1 stevel staff 815331 Sep 17 12:57 azure-storage-7.0.1.jar -rw-r--r--@ 1 stevel staff 8324412 Sep 17 13:11 bcprov-jdk18on-1.78.1.jar -rw-r--r--@ 1 stevel staff 641534749 Sep 17 12:57 bundle-2.29.52.jar -rw-r--r--@ 1 stevel staff 223979 Sep 17 13:11 checker-qual-3.33.0.jar -rw-r--r--@ 1 stevel staff 75479 Sep 17 13:11 commons-cli-1.9.0.jar -rw-r--r--@ 1 stevel staff 353793 Sep 17 13:11 commons-codec-1.15.jar -rw-r--r--@ 1 stevel staff 751914 Sep 17 13:11 commons-collections4-4.4.jar -rw-r--r--@ 1 stevel staff 1079377 Sep 17 13:11 commons-compress-1.26.1.jar -rw-r--r--@ 1 stevel staff 657516 Sep 17 13:11 commons-configuration2-2.10.1.jar -rw-r--r--@ 1 stevel staff 24239 Sep 17 13:11 commons-daemon-1.0.13.jar -rw-r--r--@ 1 stevel staff 508826 Sep 17 13:11 commons-io-2.16.1.jar -rw-r--r--@ 1 stevel staff 673587 Sep 17 13:11 commons-lang3-3.17.0.jar -rw-r--r--@ 1 stevel staff 70816 Sep 17 13:11 commons-logging-1.3.0.jar -rw-r--r--@ 1 stevel staff 2213560 Sep 17 13:11 commons-math3-3.6.1.jar -rw-r--r--@ 1 stevel staff 316431 Sep 17 13:11 commons-net-3.9.0.jar -rw-r--r--@ 1 stevel staff 238400 Sep 17 13:11 commons-text-1.10.0.jar -rw-r--r--@ 1 stevel staff 8661164 Sep 17 12:57 cos_api-bundle-5.6.19.jar -rw-r--r--@ 1 stevel staff 2983237 Sep 17 13:11 curator-client-5.2.0.jar -rw-r--r--@ 1 stevel staff 336384 Sep 17 13:11 curator-framework-5.2.0.jar -rw-r--r--@ 1 stevel staff 315569 Sep 17 13:11 curator-recipes-5.2.0.jar -rw-r--r--@ 1 stevel staff 583996 Sep 17 13:11 dnsjava-3.6.1.jar -rw-r--r--@ 1 stevel staff 324655 Sep 17 12:57 dom4j-2.1.4.jar -rw-r--r--@ 1 stevel staff 670059 Sep 17 12:57 esdk-obs-java-3.20.4.2.jar -rw-r--r--@ 1 stevel staff 4617 Sep 17 13:11 failureaccess-1.0.1.jar -rw-r--r--@ 1 stevel staff 249277 Sep 17 13:11 gson-2.9.0.jar -rw-r--r--@ 1 stevel staff 3037368 Sep 17 13:11 guava-32.0.1-jre.jar -rw-r--r--@ 1 stevel staff 94013 Sep 17 12:57 hadoop-aliyun-3.5.0-20250916.124028-685.jar -rw-r--r--@ 1 stevel staff 14456 Sep 17 13:11 hadoop-annotations-3.5.0-SNAPSHOT.jar -rw-r--r--@ 1 stevel staff 114335 Sep 17 13:11 hadoop-auth-3.5.0-SNAPSHOT.jar -rw-r--r--@ 1 stevel staff 930516 Sep 17 12:57 hadoop-aws-3.5.0-20250916.124028-686.jar -rw-r--r--@ 1 stevel staff 827349 Sep 17 12:57 hadoop-azure-3.5.0-20250916.124028-685.jar -rw-r--r--@ 1 stevel staff 33363 Sep 17 12:57 hadoop-azure-datalake-3.5.0-20250916.124028-685.jar -rw-r--r--@ 1 stevel staff 70007 Sep 17 12:57 hadoop-cos-3.5.0-20250916.124028-683.jar -rw-r--r--@ 1 stevel staff 138447 Sep 17 12:57 hadoop-gcp-3.5.0-SNAPSHOT.jar -rw-r--r--@ 1 stevel staff 142274 Sep 17 12:57 hadoop-huaweicloud-3.5.0-SNAPSHOT.jar -rw-r--r--@ 1 stevel staff 3519516 Sep 17 13:11 hadoop-shaded-guava-1.4.0.jar -rw-r--r--@ 1 stevel staff 1952967 Sep 17 13:11 hadoop-shaded-protobuf_3_25-1.4.0.jar -rw-r--r--@ 1 stevel staff 4019589 Sep 17 12:57 hadoop-tos-3.5.0-20250916.124028-202.jar -rw-r--r--@ 1 stevel staff 200223 Sep 17 13:11 hk2-api-2.6.1.jar -rw-r--r--@ 1 stevel staff 203358 Sep 17 13:11 hk2-locator-2.6.1.jar -rw-r--r--@ 1 stevel staff 131590 Sep 17 13:11 hk2-utils-2.6.1.jar -rw-r--r--@ 1 stevel staff 780321 Sep 17 13:11 httpclient-4.5.13.jar -rw-r--r--@ 1 stevel staff 328593 Sep 17 13:11 httpcore-4.4.13.jar -rw-r--r--@ 1 stevel staff 102220 Sep 17 12:57 ini4j-0.5.4.jar -rw-r--r--@ 1 stevel staff 29807 Sep 17 13:11 istack-commons-runtime-3.0.12.jar -rw-r--r--@ 1 stevel staff 9301 Sep 17 13:11 j2objc-annotations-2.8.jar -rw-r--r--@ 1 stevel staff 76636 Sep 17 13:11 jackson-annotations-2.14.3.jar -rw-r--r--@ 1 stevel staff 473081 Sep 17 13:11 jackson-core-2.14.3.jar -rw-r--r--@ 1 stevel staff 1617187 Sep 17 13:11 jackson-databind-2.14.3.jar -rw-r--r--@ 1 stevel staff 68453 Sep 17 13:11 jakarta.activation-1.2.2.jar -rw-r--r--@ 1 stevel staff 44399 Sep 17 13:11 jakarta.activation-api-1.2.1.jar -rw-r--r--@ 1 stevel staff 25058 Sep 17 13:11 jakarta.annotation-api-1.3.5.jar -rw-r--r--@ 1 stevel staff 18140 Sep 17 13:11 jakarta.inject-2.6.1.jar -rw-r--r--@ 1 stevel staff 82973 Sep 17 13:11 jakarta.servlet-api-4.0.4.jar -rw-r--r--@ 1 stevel staff 53683 Sep 17 13:11 jakarta.servlet.jsp-api-2.3.6.jar -rw-r--r--@ 1 stevel staff 91930 Sep 17 13:11 jakarta.validation-api-2.0.2.jar -rw-r--r--@ 1 stevel staff 140376 Sep 17 13:11 jakarta.ws.rs-api-2.1.6.jar -rw-r--r--@ 1 stevel staff 115638 Sep 17 13:11 jakarta.xml.bind-api-2.3.3.jar -rw-r--r--@ 1 stevel staff 7771 Sep 17 12:57 java-trace-api-0.2.11-beta.jar -rw-r--r--@ 1 stevel staff 18432 Sep 17 12:57 java-xmlbuilder-1.2.jar -rw-r--r--@ 1 stevel staff 794714 Sep 17 13:11 javassist-3.30.2-GA.jar -rw-r--r--@ 1 stevel staff 95806 Sep 17 13:11 javax.servlet-api-3.1.0.jar -rw-r--r--@ 1 stevel staff 1019097 Sep 17 13:11 jaxb-runtime-2.3.9.jar -rw-r--r--@ 1 stevel staff 4722 Sep 17 13:11 jcip-annotations-1.0-1.jar -rw-r--r--@ 1 stevel staff 327806 Sep 17 12:57 jdom2-2.0.6.1.jar -rw-r--r--@ 1 stevel staff 311826 Sep 17 13:11 jersey-client-2.46.jar -rw-r--r--@ 1 stevel staff 1267957 Sep 17 13:11 jersey-common-2.46.jar -rw-r--r--@ 1 stevel staff 32929 Sep 17 13:11 jersey-container-servlet-2.46.jar -rw-r--r--@ 1 stevel staff 75742 Sep 17 13:11 jersey-container-servlet-core-2.46.jar -rw-r--r--@ 1 stevel staff 80272 Sep 17 13:11 jersey-hk2-2.46.jar -rw-r--r--@ 1 stevel staff 964550 Sep 17 13:11 jersey-server-2.46.jar -rw-r--r--@ 1 stevel staff 90184 Sep 17 12:57 jettison-1.5.4.jar -rw-r--r--@ 1 stevel staff 249911 Sep 17 13:11 jetty-http-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 183011 Sep 17 13:11 jetty-io-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 118496 Sep 17 13:11 jetty-security-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 739348 Sep 17 13:11 jetty-server-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 146064 Sep 17 13:11 jetty-servlet-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 588962 Sep 17 13:11 jetty-util-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 66643 Sep 17 13:11 jetty-util-ajax-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 140308 Sep 17 13:11 jetty-webapp-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 68894 Sep 17 13:11 jetty-xml-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 282591 Sep 17 13:11 jsch-0.1.55.jar -rw-r--r--@ 1 stevel staff 19936 Sep 17 13:11 jsr305-3.0.2.jar -rw-r--r--@ 1 stevel staff 4519 Sep 17 13:11 jul-to-slf4j-1.7.36.jar -rw-r--r--@ 1 stevel staff 223129 Sep 17 13:11 kerb-core-2.0.3.jar -rw-r--r--@ 1 stevel staff 115065 Sep 17 13:11 kerb-crypto-2.0.3.jar -rw-r--r--@ 1 stevel staff 36361 Sep 17 13:11 kerb-util-2.0.3.jar -rw-r--r--@ 1 stevel staff 100095 Sep 17 13:11 kerby-asn1-2.0.3.jar -rw-r--r--@ 1 stevel staff 30190 Sep 17 13:11 kerby-config-2.0.3.jar -rw-r--r--@ 1 stevel staff 200581 Sep 17 13:11 kerby-pkix-2.0.3.jar -rw-r--r--@ 1 stevel staff 40787 Sep 17 13:11 kerby-util-2.0.3.jar -rw-r--r--@ 1 stevel staff 2199 Sep 17 13:11 listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar -rw-r--r--@ 1 stevel staff 136314 Sep 17 13:11 metrics-core-3.2.4.jar -rw-r--r--@ 1 stevel staff 4554 Sep 17 13:11 netty-all-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 339045 Sep 17 13:11 netty-buffer-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 355199 Sep 17 13:11 netty-codec-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 67192 Sep 17 13:11 netty-codec-dns-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 37789 Sep 17 13:11 netty-codec-haproxy-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 674362 Sep 17 13:11 netty-codec-http-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 490985 Sep 17 13:11 netty-codec-http2-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 44736 Sep 17 13:11 netty-codec-memcache-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 113699 Sep 17 13:11 netty-codec-mqtt-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 46015 Sep 17 13:11 netty-codec-redis-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 21344 Sep 17 13:11 netty-codec-smtp-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 121032 Sep 17 13:11 netty-codec-socks-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 34636 Sep 17 13:11 netty-codec-stomp-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 19823 Sep 17 13:11 netty-codec-xml-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 719225 Sep 17 13:11 netty-common-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 580162 Sep 17 13:11 netty-handler-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 25650 Sep 17 13:11 netty-handler-proxy-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 26833 Sep 17 13:11 netty-handler-ssl-ocsp-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 37842 Sep 17 13:11 netty-resolver-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 188360 Sep 17 13:11 netty-resolver-dns-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 9145 Sep 17 13:11 netty-resolver-dns-classes-macos-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 19825 Sep 17 13:11 netty-resolver-dns-native-macos-4.1.118.Final-osx-aarch_64.jar -rw-r--r--@ 1 stevel staff 19629 Sep 17 13:11 netty-resolver-dns-native-macos-4.1.118.Final-osx-x86_64.jar -rw-r--r--@ 1 stevel staff 521428 Sep 17 13:11 netty-transport-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 147621 Sep 17 13:11 netty-transport-classes-epoll-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 108558 Sep 17 13:11 netty-transport-classes-kqueue-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 42321 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-aarch_64.jar -rw-r--r--@ 1 stevel staff 36594 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-riscv64.jar -rw-r--r--@ 1 stevel staff 40644 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-x86_64.jar -rw-r--r--@ 1 stevel staff 6193 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 25741 Sep 17 13:11 netty-transport-native-kqueue-4.1.118.Final-osx-aarch_64.jar -rw-r--r--@ 1 stevel staff 25170 Sep 17 13:11 netty-transport-native-kqueue-4.1.118.Final-osx-x86_64.jar -rw-r--r--@ 1 stevel staff 44157 Sep 17 13:11 netty-transport-native-unix-common-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 18241 Sep 17 13:11 netty-transport-rxtx-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 50814 Sep 17 13:11 netty-transport-sctp-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 32189 Sep 17 13:11 netty-transport-udt-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 779369 Sep 17 13:11 nimbus-jose-jwt-9.37.2.jar -rw-r--r--@ 1 stevel staff 425763 Sep 17 12:57 okhttp-3.14.2.jar -rw-r--r--@ 1 stevel staff 91980 Sep 17 12:57 okio-1.17.2.jar -rw-r--r--@ 1 stevel staff 141734 Sep 17 12:57 opentelemetry-api-1.38.0.jar -rw-r--r--@ 1 stevel staff 47252 Sep 17 12:57 opentelemetry-context-1.38.0.jar -rw-r--r--@ 1 stevel staff 18189 Sep 17 12:57 opentracing-api-0.33.0.jar -rw-r--r--@ 1 stevel staff 10542 Sep 17 12:57 opentracing-noop-0.33.0.jar -rw-r--r--@ 1 stevel staff 7504 Sep 17 12:57 opentracing-util-0.33.0.jar -rw-r--r--@ 1 stevel staff 281989 Sep 17 12:57 org.jacoco.agent-0.8.5-runtime.jar -rw-r--r--@ 1 stevel staff 19479 Sep 17 13:11 osgi-resource-locator-1.0.3.jar -rw-r--r--@ 1 stevel staff 128414 Sep 17 13:11 re2j-1.1.jar -rw-r--r--@ 1 stevel staff 11369 Sep 17 12:57 reactive-streams-1.0.3.jar -rw-r--r--@ 1 stevel staff 332398 Sep 17 13:11 reload4j-1.2.22.jar -rw-r--r--@ 1 stevel staff 41125 Sep 17 13:11 slf4j-api-1.7.36.jar -rw-r--r--@ 1 stevel staff 9824 Sep 17 13:11 slf4j-reload4j-1.7.36.jar -rw-r--r--@ 1 stevel staff 2112099 Sep 17 13:11 snappy-java-1.1.10.4.jar -rw-r--r--@ 1 stevel staff 195909 Sep 17 13:11 stax2-api-4.2.1.jar -rw-r--r--@ 1 stevel staff 72007 Sep 17 13:11 txw2-2.3.9.jar -rw-r--r--@ 1 stevel staff 443788 Sep 17 12:57 wildfly-openssl-2.1.4.Final.jar -rw-r--r--@ 1 stevel staff 522679 Sep 17 13:11 woodstox-core-5.4.0.jar -rw-r--r--@ 1 stevel staff 1323991 Sep 17 13:11 zookeeper-3.8.4.jar -rw-r--r--@ 1 stevel staff 254932 Sep 17 13:11 zookeeper-jute-3.8.4.jar ``` steveloughran commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3303139199 by contrast: 3.4.2 ``` total 98048 -rw-r--r--@ 1 stevel staff 3448 Aug 7 12:16 animal-sniffer-annotations-1.17.jar -rw-r--r--@ 1 stevel staff 20891 Aug 7 12:16 audience-annotations-0.12.0.jar -rw-r--r--@ 1 stevel staff 651391 Aug 7 12:16 avro-1.11.4.jar -rw-r--r--@ 1 stevel staff 8324412 Aug 7 12:15 bcprov-jdk18on-1.78.1.jar -rw-r--r--@ 1 stevel staff 193322 Aug 7 12:16 checker-qual-2.5.2.jar -rw-r--r--@ 1 stevel staff 75479 Aug 7 12:16 commons-cli-1.9.0.jar -rw-r--r--@ 1 stevel staff 353793 Aug 7 12:16 commons-codec-1.15.jar -rw-r--r--@ 1 stevel staff 751914 Aug 7 12:16 commons-collections4-4.4.jar -rw-r--r--@ 1 stevel staff 1079377 Aug 7 12:16 commons-compress-1.26.1.jar -rw-r--r--@ 1 stevel staff 657516 Aug 7 12:16 commons-configuration2-2.10.1.jar -rw-r--r--@ 1 stevel staff 24239 Aug 7 12:16 commons-daemon-1.0.13.jar -rw-r--r--@ 1 stevel staff 508826 Aug 7 12:16 commons-io-2.16.1.jar -rw-r--r--@ 1 stevel staff 673587 Aug 7 12:16 commons-lang3-3.17.0.jar -rw-r--r--@ 1 stevel staff 70816 Aug 7 12:16 commons-logging-1.3.0.jar -rw-r--r--@ 1 stevel staff 2213560 Aug 7 12:16 commons-math3-3.6.1.jar -rw-r--r--@ 1 stevel staff 316431 Aug 7 12:16 commons-net-3.9.0.jar -rw-r--r--@ 1 stevel staff 238400 Aug 7 12:16 commons-text-1.10.0.jar -rw-r--r--@ 1 stevel staff 2983237 Aug 7 12:16 curator-client-5.2.0.jar -rw-r--r--@ 1 stevel staff 336384 Aug 7 12:16 curator-framework-5.2.0.jar -rw-r--r--@ 1 stevel staff 315569 Aug 7 12:16 curator-recipes-5.2.0.jar -rw-r--r--@ 1 stevel staff 583996 Aug 7 12:16 dnsjava-3.6.1.jar -rw-r--r--@ 1 stevel staff 3727 Aug 7 12:16 failureaccess-1.0.jar -rw-r--r--@ 1 stevel staff 249277 Aug 7 12:16 gson-2.9.0.jar -rw-r--r--@ 1 stevel staff 2747878 Aug 7 12:16 guava-27.0-jre.jar -rw-r--r--@ 1 stevel staff 25517 Aug 7 12:16 hadoop-annotations-3.4.2.jar -rw-r--r--@ 1 stevel staff 110106 Aug 7 12:16 hadoop-auth-3.4.2.jar -rw-r--r--@ 1 stevel staff 810477 Aug 7 12:39 hadoop-azure-3.4.2.jar -rw-r--r--@ 1 stevel staff 3519516 Aug 7 12:16 hadoop-shaded-guava-1.4.0.jar -rw-r--r--@ 1 stevel staff 1952967 Aug 7 12:16 hadoop-shaded-protobuf_3_25-1.4.0.jar -rw-r--r--@ 1 stevel staff 780321 Aug 7 12:16 httpclient-4.5.13.jar -rw-r--r--@ 1 stevel staff 328593 Aug 7 12:16 httpcore-4.4.13.jar -rw-r--r--@ 1 stevel staff 8782 Aug 7 12:16 j2objc-annotations-1.1.jar -rw-r--r--@ 1 stevel staff 75705 Aug 7 12:16 jackson-annotations-2.12.7.jar -rw-r--r--@ 1 stevel staff 365538 Aug 7 12:16 jackson-core-2.12.7.jar -rw-r--r--@ 1 stevel staff 1512418 Aug 7 12:16 jackson-databind-2.12.7.1.jar -rw-r--r--@ 1 stevel staff 44399 Aug 7 12:16 jakarta.activation-api-1.2.1.jar -rw-r--r--@ 1 stevel staff 95806 Aug 7 12:16 javax.servlet-api-3.1.0.jar -rw-r--r--@ 1 stevel staff 102244 Aug 7 12:16 jaxb-api-2.2.11.jar -rw-r--r--@ 1 stevel staff 890168 Aug 7 12:16 jaxb-impl-2.2.3-1.jar -rw-r--r--@ 1 stevel staff 4722 Aug 7 12:16 jcip-annotations-1.0-1.jar -rw-r--r--@ 1 stevel staff 436731 Aug 7 12:16 jersey-core-1.19.4.jar -rw-r--r--@ 1 stevel staff 158890 Aug 7 12:16 jersey-json-1.22.0.jar -rw-r--r--@ 1 stevel staff 705276 Aug 7 12:16 jersey-server-1.19.4.jar -rw-r--r--@ 1 stevel staff 128990 Aug 7 12:16 jersey-servlet-1.19.4.jar -rw-r--r--@ 1 stevel staff 90184 Aug 7 12:16 jettison-1.5.4.jar -rw-r--r--@ 1 stevel staff 249911 Aug 7 12:16 jetty-http-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 183011 Aug 7 12:16 jetty-io-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 118496 Aug 7 12:16 jetty-security-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 739348 Aug 7 12:16 jetty-server-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 146064 Aug 7 12:16 jetty-servlet-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 588962 Aug 7 12:16 jetty-util-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 66643 Aug 7 12:16 jetty-util-ajax-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 140308 Aug 7 12:16 jetty-webapp-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 68894 Aug 7 12:16 jetty-xml-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 282591 Aug 7 12:16 jsch-0.1.55.jar -rw-r--r--@ 1 stevel staff 100636 Aug 7 12:15 jsp-api-2.1.jar -rw-r--r--@ 1 stevel staff 19936 Aug 7 12:16 jsr305-3.0.2.jar -rw-r--r--@ 1 stevel staff 46367 Aug 7 12:16 jsr311-api-1.1.1.jar -rw-r--r--@ 1 stevel staff 4519 Aug 7 12:16 jul-to-slf4j-1.7.36.jar -rw-r--r--@ 1 stevel staff 223129 Aug 7 12:16 kerb-core-2.0.3.jar -rw-r--r--@ 1 stevel staff 115065 Aug 7 12:16 kerb-crypto-2.0.3.jar -rw-r--r--@ 1 stevel staff 36361 Aug 7 12:16 kerb-util-2.0.3.jar -rw-r--r--@ 1 stevel staff 100095 Aug 7 12:16 kerby-asn1-2.0.3.jar -rw-r--r--@ 1 stevel staff 30190 Aug 7 12:16 kerby-config-2.0.3.jar -rw-r--r--@ 1 stevel staff 200581 Aug 7 12:16 kerby-pkix-2.0.3.jar -rw-r--r--@ 1 stevel staff 40787 Aug 7 12:16 kerby-util-2.0.3.jar -rw-r--r--@ 1 stevel staff 2199 Aug 7 12:16 listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar -rw-r--r--@ 1 stevel staff 136314 Aug 7 12:16 metrics-core-3.2.4.jar -rw-r--r--@ 1 stevel staff 4554 Aug 7 12:15 netty-all-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 339045 Aug 7 12:16 netty-buffer-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 355199 Aug 7 12:16 netty-codec-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 67192 Aug 7 12:15 netty-codec-dns-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 37789 Aug 7 12:15 netty-codec-haproxy-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 674362 Aug 7 12:15 netty-codec-http-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 490985 Aug 7 12:15 netty-codec-http2-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 44736 Aug 7 12:15 netty-codec-memcache-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 113699 Aug 7 12:15 netty-codec-mqtt-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 46015 Aug 7 12:15 netty-codec-redis-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 21344 Aug 7 12:15 netty-codec-smtp-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 121032 Aug 7 12:15 netty-codec-socks-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 34636 Aug 7 12:15 netty-codec-stomp-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 19823 Aug 7 12:15 netty-codec-xml-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 719225 Aug 7 12:16 netty-common-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 580162 Aug 7 12:16 netty-handler-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 25650 Aug 7 12:15 netty-handler-proxy-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 26833 Aug 7 12:15 netty-handler-ssl-ocsp-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 37842 Aug 7 12:16 netty-resolver-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 188360 Aug 7 12:15 netty-resolver-dns-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 9145 Aug 7 12:15 netty-resolver-dns-classes-macos-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 19825 Aug 7 12:15 netty-resolver-dns-native-macos-4.1.118.Final-osx-aarch_64.jar -rw-r--r--@ 1 stevel staff 19629 Aug 7 12:15 netty-resolver-dns-native-macos-4.1.118.Final-osx-x86_64.jar -rw-r--r--@ 1 stevel staff 521428 Aug 7 12:16 netty-transport-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 147621 Aug 7 12:16 netty-transport-classes-epoll-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 108558 Aug 7 12:15 netty-transport-classes-kqueue-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 42321 Aug 7 12:15 netty-transport-native-epoll-4.1.118.Final-linux-aarch_64.jar -rw-r--r--@ 1 stevel staff 36594 Aug 7 12:15 netty-transport-native-epoll-4.1.118.Final-linux-riscv64.jar -rw-r--r--@ 1 stevel staff 40644 Aug 7 12:15 netty-transport-native-epoll-4.1.118.Final-linux-x86_64.jar -rw-r--r--@ 1 stevel staff 6193 Aug 7 12:16 netty-transport-native-epoll-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 25741 Aug 7 12:15 netty-transport-native-kqueue-4.1.118.Final-osx-aarch_64.jar -rw-r--r--@ 1 stevel staff 25170 Aug 7 12:15 netty-transport-native-kqueue-4.1.118.Final-osx-x86_64.jar -rw-r--r--@ 1 stevel staff 44157 Aug 7 12:16 netty-transport-native-unix-common-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 18241 Aug 7 12:15 netty-transport-rxtx-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 50814 Aug 7 12:15 netty-transport-sctp-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 32189 Aug 7 12:15 netty-transport-udt-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 779369 Aug 7 12:16 nimbus-jose-jwt-9.37.2.jar -rw-r--r--@ 1 stevel staff 128414 Aug 7 12:16 re2j-1.1.jar -rw-r--r--@ 1 stevel staff 332398 Aug 7 12:16 reload4j-1.2.22.jar -rw-r--r--@ 1 stevel staff 41125 Aug 7 12:15 slf4j-api-1.7.36.jar -rw-r--r--@ 1 stevel staff 9824 Aug 7 12:15 slf4j-reload4j-1.7.36.jar -rw-r--r--@ 1 stevel staff 2112099 Aug 7 12:16 snappy-java-1.1.10.4.jar -rw-r--r--@ 1 stevel staff 195909 Aug 7 12:16 stax2-api-4.2.1.jar -rw-r--r--@ 1 stevel staff 522679 Aug 7 12:16 woodstox-core-5.4.0.jar -rw-r--r--@ 1 stevel staff 1323991 Aug 7 12:16 zookeeper-3.8.4.jar -rw-r--r--@ 1 stevel staff 254932 Aug 7 12:16 zookeeper-jute-3.8.4.jar ``` steveloughran commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3303435683 Having audited the files coming off the cloud connectors, we have about a dozen whose licenses aren't in the binary ``` analyticsaccelerator-s3-1.3.0.jar cos_api-bundle-5.6.19.jar dom4j-2.1.4.jar esdk-obs-java-3.20.4.2.jar java-trace-api-0.2.11-beta.jar java-xmlbuilder-1.2.jar opentracing-api-0.33.0.jar opentracing-noop-0.33.0.jar opentracing-util-0.33.0.jar reactive-streams-1.0.3.jar ve-tos-java-sdk-hadoop-2.8.9.jar ``` the analyticsaccelerator is @ahmarsuhail 's work to add to the license, not sure about the others. Proposed: identify which connector the unacknowledged artifacts are coming from, create homework for each team. hadoop-yetus commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3303936232 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 33s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 13m 4s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 43m 58s | | trunk passed | | +1 :green_heart: | compile | 16m 16s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 34s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 2m 57s | | trunk passed | | +1 :green_heart: | javadoc | 2m 53s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 2m 43s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 40m 19s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 35s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 10s | | the patch passed | | +1 :green_heart: | compile | 14m 58s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 14m 58s | | the patch passed | | +1 :green_heart: | compile | 13m 51s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 51s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 2m 55s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 2m 47s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 2m 41s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 42m 52s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 39s | | hadoop-assemblies in the patch passed. | | +1 :green_heart: | unit | 0m 41s | | hadoop-tools-dist in the patch passed. | | +1 :green_heart: | unit | 0m 44s | | hadoop-huaweicloud in the patch passed. | | +1 :green_heart: | unit | 0m 41s | | hadoop-cloud-storage in the patch passed. | | +1 :green_heart: | asflicense | 1m 6s | | The patch does not generate ASF License warnings. | | | | 214m 20s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7980 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs compile javac javadoc mvninstall mvnsite unit shadedclient xmllint | | uname | Linux 604f3bcd050e 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c59e35149ea17b7cea37be9203a265dcbff118fe | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/1/testReport/ | | Max. process+thread count | 548 (vs. ulimit of 5500) | | modules | C: hadoop-assemblies hadoop-tools/hadoop-tools-dist hadoop-cloud-storage-project/hadoop-huaweicloud hadoop-cloud-storage-project/hadoop-cloud-storage U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/1/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. ahmarsuhail commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3311933628 not very familiar with how the packaging stuff works, so finding this a bit difficult to review. How are testing the packaging, I just ran `mvn package -Pdist -DskipTests -Dmaven.javadoc.skip=true -DskipShade`, but the outputs in : `hadoop-cloud-storage-project/hadoop-cloud-storage/target/hadoop-cloud-storage-3.5.0-SNAPSHOT/share/hadoop/common/lib`, `hadoop-dist/target/hadoop-3.5.0-SNAPSHOT/share/hadoop/tools/lib` `hadoop/hadoop-dist/target/hadoop-3.5.0-SNAPSHOT/share/hadoop/common/lib` are all the same before and after your changes, so I must be doing something wrong. steveloughran commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3312222391 did you do a `mvn clean package`? `hadoop-cloud-storage-project/hadoop-cloud-storage/target/hadoop-cloud-storage-3.5.0-SNAPSHOT/share/hadoop/common/lib` -new, contains all cloud stuff we want in; should cut stuff already going to be there just to reduce copying `hadoop-dist/target/hadoop-3.5.0-SNAPSHOT/share/hadoop/tools/lib` should remove hadoop-azure, hadoop-aws, hadoop-gcs, bundle.jar... the big distro created under `hadoop-dist/target/hadoop-3.5.0-SNAPSHOT/` is what is shipped. hadoop-yetus commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3327784078 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 15m 12s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 10m 27s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 38m 56s | | trunk passed | | +1 :green_heart: | compile | 16m 7s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 50s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 20m 36s | | trunk passed | | +1 :green_heart: | javadoc | 9m 11s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 39s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | shadedclient | 18m 3s | | branch has errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 35s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 38m 59s | | the patch passed | | +1 :green_heart: | compile | 15m 25s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 25s | | the patch passed | | +1 :green_heart: | compile | 13m 56s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 56s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 18m 55s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | -1 :x: | javadoc | 9m 9s | [/results-javadoc-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/2/artifact/out/results-javadoc-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 10 new + 5526 unchanged - 10 fixed = 5536 total (was 5536) | | -1 :x: | javadoc | 7m 34s | [/results-javadoc-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/2/artifact/out/results-javadoc-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 10 new + 1418 unchanged - 10 fixed = 1428 total (was 1428) | | -1 :x: | shadedclient | 19m 38s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 800m 7s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/2/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 46s | | The patch does not generate ASF License warnings. | | | | 1046m 13s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.router.subcluster.capacity.TestYarnFederationWithCapacityScheduler | | | hadoop.mapreduce.v2.TestUberAM | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7980 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs compile javac javadoc mvninstall mvnsite unit shadedclient xmllint | | uname | Linux 271d5e8aaf6a 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0aaa6ce66e8a8fc7fc04fa4e2650badf956d531a | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/2/testReport/ | | Max. process+thread count | 4938 (vs. ulimit of 5500) | | modules | C: hadoop-assemblies hadoop-tools/hadoop-tools-dist hadoop-cloud-storage-project/hadoop-huaweicloud hadoop-cloud-storage-project/hadoop-cloud-storage . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/2/console | | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. I've just discovered quite how many things google-cloud-storage jar pulls in if you don't build a shaded release. Nowhere as big as the aws sdk, but it it is still significant. # I'm going to exclude hadoop-gcp dependencies by default in a build, so if you build hadoop distro with -DskipShade you don't get these in common-lib unless you have a -Dhadoop-gcp-package. # Most of these aren't in our published LICENSE-binary file. some are, but not all. # the opentelemetry/census artifacts are newer than those from one of the other projects; build both and you get conflict (joy!). # a protobuf 2.5 comes in from somewhere I think for now I'd say \"don't make issues 2-3 blockers on merging the PR\" because they're independent. But ideally the gcs imports should be tuned down and we should go for consistent opentelemetry/opencensus versions wherever imported. {code} 3.0K animal-sniffer-annotations-1.24.jar 3.0K annotations-4.1.1.4.jar 49K api-common-2.47.2.jar 7.3K auto-value-annotations-1.11.0.jar 232K checker-qual-3.49.0.jar 4.3M conscrypt-openjdk-uber-2.5.2.jar 18K detector-resources-support-0.33.0.jar 19K error_prone_annotations-2.36.0.jar 39K exporter-metrics-0.33.0.jar 4.6K failureaccess-1.0.2.jar 52K gapic-google-cloud-storage-v2-2.52.0.jar 424K gax-2.64.2.jar 154K gax-grpc-2.64.2.jar 162K gax-httpjson-2.64.2.jar 295K google-api-client-2.7.2.jar 252K google-api-services-storage-v1-rev20250420-2.0.0.jar 8.2K google-auth-library-credentials-1.33.1.jar 294K google-auth-library-oauth2-http-1.33.1.jar 137K google-cloud-core-2.54.2.jar 16K google-cloud-core-grpc-2.54.2.jar 15K google-cloud-core-http-2.54.2.jar 249K google-cloud-monitoring-3.52.0.jar 1.3M google-cloud-storage-2.52.0.jar 289K google-http-client-1.46.3.jar 11K google-http-client-apache-v2-1.46.3.jar 19K google-http-client-appengine-1.46.3.jar 13K google-http-client-gson-1.46.3.jar 9.4K google-http-client-jackson2-1.46.3.jar 80K google-oauth-client-1.37.0.jar 316K grpc-alts-1.70.0.jar 316K grpc-api-1.70.0.jar 14K grpc-auth-1.70.0.jar 293B grpc-context-1.70.0.jar 639K grpc-core-1.70.0.jar 30K grpc-google-cloud-storage-v2-2.52.0.jar 15K grpc-googleapis-1.70.0.jar 175K grpc-grpclb-1.70.0.jar 39K grpc-inprocess-1.70.0.jar 9.3M grpc-netty-shaded-1.70.0.jar 67K grpc-opentelemetry-1.70.0.jar 5.2K grpc-protobuf-1.70.0.jar 7.7K grpc-protobuf-lite-1.70.0.jar 248K grpc-rls-1.70.0.jar 928K grpc-services-1.70.0.jar 59K grpc-stub-1.70.0.jar 98K grpc-util-1.70.0.jar 9.4M grpc-xds-1.70.0.jar 243K gson-2.9.0.jar 2.9M guava-33.4.8-jre.jar 12K j2objc-annotations-3.0.0.jar 462K jackson-core-2.14.3.jar 26K javax.annotation-api-1.3.2.jar 3.7K jspecify-1.0.0.jar 19K jsr305-3.0.2.jar 2.1K listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar 347K opencensus-api-0.31.1.jar 23K opencensus-contrib-http-util-0.31.1.jar 155K opentelemetry-api-1.47.0.jar 48K opentelemetry-context-1.47.0.jar 8.1K opentelemetry-gcp-resources-1.37.0-alpha.jar 6.6K opentelemetry-sdk-1.47.0.jar 54K opentelemetry-sdk-common-1.47.0.jar 20K opentelemetry-sdk-extension-autoconfigure-spi-1.47.0.jar 53K opentelemetry-sdk-logs-1.47.0.jar 322K opentelemetry-sdk-metrics-1.47.0.jar 129K opentelemetry-sdk-trace-1.47.0.jar 73K opentelemetry-semconv-1.29.0-alpha.jar 6.8K perfmark-api-0.27.0.jar 1.9M proto-google-cloud-monitoring-v3-3.52.0.jar 980K proto-google-cloud-storage-v2-2.52.0.jar 2.6M proto-google-common-protos-2.55.2.jar 182K proto-google-iam-v1-1.50.2.jar 521K protobuf-java-2.5.0.jar 71K protobuf-java-util-3.25.5.jar 125K re2j-1.1.jar 91K shared-resourcemapping-0.33.0.jar 40K slf4j-api-1.7.36.jar 503K threetenbp-1.7.0.jar {code} steveloughran commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3387174385 Latest build generates stack traces from gcs and obs filesystem incomplete CP in service loader. Both need to move to core-default.xml *only* which is faster anyway. ``` 2025-10-09 20:06:44,452 [main] WARN fs.FileSystem (FileSystem.java:loadFileSystems(3539)) - Cannot load filesystem java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.fs.gs.GoogleHadoopFileSystem could not be instantiated at java.util.ServiceLoader.fail(ServiceLoader.java:232) at java.util.ServiceLoader.access$100(ServiceLoader.java:185) at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384) at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) at java.util.ServiceLoader$1.next(ServiceLoader.java:480) at org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:3525) at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3562) at org.apache.hadoop.fs.store.diag.StoreDiag.probeForFileSystemClass(StoreDiag.java:671) at org.apache.hadoop.fs.store.diag.StoreDiag.run(StoreDiag.java:223) at org.apache.hadoop.fs.store.diag.StoreDiag.run(StoreDiag.java:170) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:82) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97) at org.apache.hadoop.fs.store.diag.StoreDiag.exec(StoreDiag.java:1255) at org.apache.hadoop.fs.store.diag.StoreDiag.main(StoreDiag.java:1264) at storediag.main(storediag.java:25) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:333) at org.apache.hadoop.util.RunJar.main(RunJar.java:254) Caused by: java.lang.NoClassDefFoundError: com/google/auth/Credentials at java.lang.Class.getDeclaredConstructors0(Native Method) at java.lang.Class.privateGetDeclaredConstructors(Class.java:2671) at java.lang.Class.getConstructor0(Class.java:3075) at java.lang.Class.newInstance(Class.java:412) at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ... 18 more Caused by: java.lang.ClassNotFoundException: com.google.auth.Credentials at java.net.URLClassLoader.findClass(URLClassLoader.java:387) at java.lang.ClassLoader.loadClass(ClassLoader.java:419) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352) at java.lang.ClassLoader.loadClass(ClassLoader.java:352) ... 23 more 2025-10-09 20:06:44,456 [main] WARN fs.FileSystem (FileSystem.java:loadFileSystems(3539)) - Cannot load filesystem java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.fs.obs.OBSFileSystem could not be instantiated at java.util.ServiceLoader.fail(ServiceLoader.java:232) at java.util.ServiceLoader.access$100(ServiceLoader.java:185) at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384) at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) at java.util.ServiceLoader$1.next(ServiceLoader.java:480) at org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:3525) at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3562) at org.apache.hadoop.fs.store.diag.StoreDiag.probeForFileSystemClass(StoreDiag.java:671) at org.apache.hadoop.fs.store.diag.StoreDiag.run(StoreDiag.java:223) at org.apache.hadoop.fs.store.diag.StoreDiag.run(StoreDiag.java:170) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:82) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97) at org.apache.hadoop.fs.store.diag.StoreDiag.exec(StoreDiag.java:1255) at org.apache.hadoop.fs.store.diag.StoreDiag.main(StoreDiag.java:1264) at storediag.main(storediag.java:25) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:333) at org.apache.hadoop.util.RunJar.main(RunJar.java:254) Caused by: java.lang.NoClassDefFoundError: com/obs/services/exception/ObsException at java.lang.Class.getDeclaredConstructors0(Native Method) at java.lang.Class.privateGetDeclaredConstructors(Class.java:2671) at java.lang.Class.getConstructor0(Class.java:3075) at java.lang.Class.newInstance(Class.java:412) at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ... 18 more Caused by: java.lang.ClassNotFoundException: com.obs.services.exception.ObsException at java.net.URLClassLoader.findClass(URLClassLoader.java:387) at java.lang.ClassLoader.loadClass(ClassLoader.java:419) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352) at java.lang.ClassLoader.loadClass(ClassLoader.java:352) ... 23 more FileSystem for s3a:// is: org.apache.hadoop.fs.s3a.S3AFileSystem Loaded from: file:/Users/stevel/Projects/Releases/hadoop-3.5.0-SNAPSHOT/share/hadoop/common/lib/hadoop-aws-3.5.0-SNAPSHOT.jar via sun.misc.Launcher$AppClassLoader@41906a77 ``` hadoop-yetus commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3387715974 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 9s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 3s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 38m 50s | | trunk passed | | +1 :green_heart: | compile | 16m 3s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 14m 6s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 21m 2s | | trunk passed | | +1 :green_heart: | javadoc | 9m 0s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 29s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 49m 55s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 40s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 41m 14s | | the patch passed | | +1 :green_heart: | compile | 15m 33s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 33s | | the patch passed | | +1 :green_heart: | compile | 13m 29s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 18m 48s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | -1 :x: | javadoc | 9m 58s | [/results-javadoc-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/4/artifact/out/results-javadoc-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 10 new + 5526 unchanged - 10 fixed = 5536 total (was 5536) | | -1 :x: | javadoc | 8m 2s | [/results-javadoc-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/4/artifact/out/results-javadoc-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 10 new + 1418 unchanged - 10 fixed = 1428 total (was 1428) | | +1 :green_heart: | shadedclient | 63m 31s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 2m 22s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/4/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +0 :ok: | asflicense | 0m 32s | | ASF License check generated no output? | | | | 309m 40s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7980 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs compile javac javadoc mvninstall mvnsite unit shadedclient xmllint | | uname | Linux 01b725e22dc7 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 76b4eae78d738ee73cf68118a87c9204d120d752 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/4/testReport/ | | Max. process+thread count | 699 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-assemblies hadoop-tools/hadoop-tools-dist hadoop-cloud-storage-project/hadoop-huaweicloud hadoop-cloud-storage-project/hadoop-tos hadoop-cloud-storage-project/hadoop-cloud-storage hadoop-cloud-storage-project/hadoop-cloud-storage-dist hadoop-cloud-storage-project . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/4/console | | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. and full jars if you pull in everything. aws bundle.jar dominates; the rest adds up. {code} 104K aliyun-java-core-0.2.11-beta.jar 190K aliyun-java-sdk-core-4.5.10.jar 160K aliyun-java-sdk-kms-2.11.0.jar 216K aliyun-java-sdk-ram-3.1.0.jar 907K aliyun-sdk-oss-3.18.1.jar 2.4M analyticsaccelerator-s3-1.3.0.jar 3.0K animal-sniffer-annotations-1.24.jar 3.0K annotations-4.1.1.4.jar 49K api-common-2.47.2.jar 7.3K auto-value-annotations-1.11.0.jar 111K azure-data-lake-store-sdk-2.3.9.jar 10K azure-keyvault-core-1.0.0.jar 796K azure-storage-7.0.1.jar 612M bundle-2.29.52.jar 232K checker-qual-3.49.0.jar 346K commons-codec-1.15.jar 69K commons-logging-1.3.0.jar 4.3M conscrypt-openjdk-uber-2.5.2.jar 8.3M cos_api-bundle-5.6.19.jar 18K detector-resources-support-0.33.0.jar 317K dom4j-2.1.4.jar 19K error_prone_annotations-2.36.0.jar 39K exporter-metrics-0.33.0.jar 4.6K failureaccess-1.0.2.jar 52K gapic-google-cloud-storage-v2-2.52.0.jar 424K gax-2.64.2.jar 154K gax-grpc-2.64.2.jar 162K gax-httpjson-2.64.2.jar 295K google-api-client-2.7.2.jar 252K google-api-services-storage-v1-rev20250420-2.0.0.jar 8.2K google-auth-library-credentials-1.33.1.jar 294K google-auth-library-oauth2-http-1.33.1.jar 137K google-cloud-core-2.54.2.jar 16K google-cloud-core-grpc-2.54.2.jar 15K google-cloud-core-http-2.54.2.jar 249K google-cloud-monitoring-3.52.0.jar 1.3M google-cloud-storage-2.52.0.jar 289K google-http-client-1.46.3.jar 11K google-http-client-apache-v2-1.46.3.jar 19K google-http-client-appengine-1.46.3.jar 13K google-http-client-gson-1.46.3.jar 9.4K google-http-client-jackson2-1.46.3.jar 80K google-oauth-client-1.37.0.jar 316K grpc-alts-1.70.0.jar 316K grpc-api-1.70.0.jar 14K grpc-auth-1.70.0.jar 293B grpc-context-1.70.0.jar 639K grpc-core-1.70.0.jar 30K grpc-google-cloud-storage-v2-2.52.0.jar 15K grpc-googleapis-1.70.0.jar 175K grpc-grpclb-1.70.0.jar 39K grpc-inprocess-1.70.0.jar 9.3M grpc-netty-shaded-1.70.0.jar 67K grpc-opentelemetry-1.70.0.jar 5.2K grpc-protobuf-1.70.0.jar 7.7K grpc-protobuf-lite-1.70.0.jar 248K grpc-rls-1.70.0.jar 928K grpc-services-1.70.0.jar 59K grpc-stub-1.70.0.jar 98K grpc-util-1.70.0.jar 9.4M grpc-xds-1.70.0.jar 243K gson-2.9.0.jar 2.9M guava-33.4.8-jre.jar 92K hadoop-aliyun-3.5.0-SNAPSHOT.jar 910K hadoop-aws-3.5.0-SNAPSHOT.jar 810K hadoop-azure-3.5.0-SNAPSHOT.jar 33K hadoop-azure-datalake-3.5.0-SNAPSHOT.jar 68K hadoop-cos-3.5.0-SNAPSHOT.jar 135K hadoop-gcp-3.5.0-SNAPSHOT.jar 142K hadoop-huaweicloud-3.5.0-SNAPSHOT.jar 250K hadoop-tos-3.5.0-SNAPSHOT.jar 762K httpclient-4.5.13.jar 933K httpclient5-5.5.jar 321K httpcore-4.4.13.jar 888K httpcore5-5.3.6.jar 236K httpcore5-h2-5.3.4.jar 100K ini4j-0.5.4.jar 12K j2objc-annotations-3.0.0.jar 462K jackson-core-2.14.3.jar 7.6K java-trace-api-0.2.11-beta.jar 26K javax.annotation-api-1.3.2.jar 320K jdom2-2.0.6.1.jar 88K jettison-1.5.4.jar 575K jetty-util-9.4.57.v20241219.jar 65K jetty-util-ajax-9.4.57.v20241219.jar 3.7K jspecify-1.0.0.jar 19K jsr305-3.0.2.jar 2.1K listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar 347K opencensus-api-0.31.1.jar 23K opencensus-contrib-http-util-0.31.1.jar 155K opentelemetry-api-1.47.0.jar 48K opentelemetry-context-1.47.0.jar 8.1K opentelemetry-gcp-resources-1.37.0-alpha.jar 6.6K opentelemetry-sdk-1.47.0.jar 54K opentelemetry-sdk-common-1.47.0.jar 20K opentelemetry-sdk-extension-autoconfigure-spi-1.47.0.jar 53K opentelemetry-sdk-logs-1.47.0.jar 322K opentelemetry-sdk-metrics-1.47.0.jar 129K opentelemetry-sdk-trace-1.47.0.jar 73K opentelemetry-semconv-1.29.0-alpha.jar 275K org.jacoco.agent-0.8.5-runtime.jar 6.8K perfmark-api-0.27.0.jar 1.9M proto-google-cloud-monitoring-v3-3.52.0.jar 980K proto-google-cloud-storage-v2-2.52.0.jar 2.6M proto-google-common-protos-2.55.2.jar 182K proto-google-iam-v1-1.50.2.jar 521K protobuf-java-2.5.0.jar 71K protobuf-java-util-3.25.5.jar 125K re2j-1.1.jar 11K reactive-streams-1.0.3.jar 91K shared-resourcemapping-0.33.0.jar 40K slf4j-api-1.7.36.jar 503K threetenbp-1.7.0.jar 980K ve-tos-java-sdk-hadoop-2.8.9.jar 433K wildfly-openssl-2.1.4.Final.jar {code} The default settings will produce something a lot leaner {code} 2.4M analyticsaccelerator-s3-1.3.0.jar 10K azure-keyvault-core-1.0.0.jar 796K azure-storage-7.0.1.jar 346K commons-codec-1.15.jar 69K commons-logging-1.3.0.jar 910K hadoop-aws-3.5.0-SNAPSHOT.jar 810K hadoop-azure-3.5.0-SNAPSHOT.jar 33K hadoop-azure-datalake-3.5.0-SNAPSHOT.jar 68K hadoop-cos-3.5.0-SNAPSHOT.jar 135K hadoop-gcp-3.5.0-SNAPSHOT.jar 142K hadoop-huaweicloud-3.5.0-SNAPSHOT.jar 250K hadoop-tos-3.5.0-SNAPSHOT.jar 762K httpclient-4.5.13.jar 321K httpcore-4.4.13.jar 575K jetty-util-9.4.57.v20241219.jar 65K jetty-util-ajax-9.4.57.v20241219.jar 433K wildfly-openssl-2.1.4.Final.jar {code} steveloughran commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3402138222 shaded test failures * some in yarn which are presumably unrelated...let's see * lots of more skipped tests in hadoop-azure, hadoop-azuredatalake, such as `TestAbfsInputStreamStatistics` which is skipping .... Looks like maven is back to running these tests and skipping where they don't have the credentials hadoop-yetus commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3405523026 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 21m 30s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 15s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 35m 25s | | trunk passed | | +1 :green_heart: | compile | 17m 37s | | trunk passed | | +1 :green_heart: | checkstyle | 3m 40s | | trunk passed | | -1 :x: | mvnsite | 11m 2s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/branch-mvnsite-root.txt) | root in trunk failed. | | +1 :green_heart: | javadoc | 9m 44s | | trunk passed | | +0 :ok: | spotbugs | 0m 27s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +0 :ok: | spotbugs | 0m 22s | | branch/hadoop-assemblies no spotbugs output file (spotbugsXml.xml) | | -1 :x: | spotbugs | 1m 19s | [/branch-spotbugs-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/branch-spotbugs-hadoop-common-project_hadoop-common.txt) | hadoop-common in trunk failed. | | -1 :x: | spotbugs | 0m 32s | [/branch-spotbugs-hadoop-tools_hadoop-gcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/branch-spotbugs-hadoop-tools_hadoop-gcp.txt) | hadoop-gcp in trunk failed. | | +0 :ok: | spotbugs | 0m 23s | | branch/hadoop-tools/hadoop-tools-dist no spotbugs output file (spotbugsXml.xml) | | -1 :x: | spotbugs | 0m 32s | [/branch-spotbugs-hadoop-cloud-storage-project_hadoop-huaweicloud.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/branch-spotbugs-hadoop-cloud-storage-project_hadoop-huaweicloud.txt) | hadoop-huaweicloud in trunk failed. | | -1 :x: | spotbugs | 0m 37s | [/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt) | hadoop-tos in trunk failed. | | +0 :ok: | spotbugs | 0m 23s | | branch/hadoop-cloud-storage-project/hadoop-cloud-storage no spotbugs output file (spotbugsXml.xml) | | -1 :x: | spotbugs | 0m 31s | [/branch-spotbugs-hadoop-cloud-storage-project.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/branch-spotbugs-hadoop-cloud-storage-project.txt) | hadoop-cloud-storage-project in trunk failed. | | -1 :x: | spotbugs | 0m 30s | [/branch-spotbugs-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/branch-spotbugs-root.txt) | root in trunk failed. | | +1 :green_heart: | shadedclient | 30m 50s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 34s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 43m 52s | | the patch passed | | +1 :green_heart: | compile | 17m 5s | | the patch passed | | +1 :green_heart: | javac | 17m 5s | | the patch passed | | +1 :green_heart: | blanks | 0m 1s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 3m 39s | | the patch passed | | -1 :x: | mvnsite | 7m 36s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | -1 :x: | javadoc | 9m 41s | [/results-javadoc-javadoc-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/results-javadoc-javadoc-root.txt) | root generated 30 new + 42985 unchanged - 30 fixed = 43015 total (was 43015) | | +0 :ok: | spotbugs | 0m 22s | | hadoop-project has no data from spotbugs | | +0 :ok: | spotbugs | 0m 22s | | hadoop-assemblies has no data from spotbugs | | -1 :x: | spotbugs | 1m 19s | [/patch-spotbugs-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/patch-spotbugs-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch failed. | | +0 :ok: | spotbugs | 0m 24s | | hadoop-tools/hadoop-tools-dist has no data from spotbugs | | -1 :x: | spotbugs | 0m 33s | [/patch-spotbugs-hadoop-tools_hadoop-gcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/patch-spotbugs-hadoop-tools_hadoop-gcp.txt) | hadoop-gcp in the patch failed. | | -1 :x: | spotbugs | 0m 32s | [/patch-spotbugs-hadoop-cloud-storage-project_hadoop-huaweicloud.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/patch-spotbugs-hadoop-cloud-storage-project_hadoop-huaweicloud.txt) | hadoop-huaweicloud in the patch failed. | | -1 :x: | spotbugs | 0m 36s | [/patch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/patch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt) | hadoop-tos in the patch failed. | | +0 :ok: | spotbugs | 0m 22s | | hadoop-cloud-storage-project/hadoop-cloud-storage has no data from spotbugs | | +0 :ok: | spotbugs | 0m 23s | | hadoop-cloud-storage-project/hadoop-cloud-storage-dist has no data from spotbugs | | -1 :x: | spotbugs | 0m 31s | [/patch-spotbugs-hadoop-cloud-storage-project.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/patch-spotbugs-hadoop-cloud-storage-project.txt) | hadoop-cloud-storage-project in the patch failed. | | -1 :x: | spotbugs | 0m 30s | [/patch-spotbugs-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/patch-spotbugs-root.txt) | root in the patch failed. | | +1 :green_heart: | shadedclient | 31m 14s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 801m 10s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/patch-unit-root.txt) | root in the patch passed. | | -1 :x: | asflicense | 1m 45s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/results-asflicense.txt) | The patch generated 1 ASF License warnings. | | | | 1072m 30s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.sls.appmaster.TestAMSimulator | | | hadoop.hdfs.tools.TestDFSAdmin | | | hadoop.yarn.service.TestYarnNativeServices | | | hadoop.yarn.server.router.subcluster.capacity.TestYarnFederationWithCapacityScheduler | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.fs.tosfs.object.TestObjectOutputStream | | | hadoop.fs.tosfs.commit.TestMagicOutputStream | | | hadoop.fs.tosfs.object.tos.auth.TestEnvironmentCredentialsProvider | | | hadoop.fs.tosfs.object.tos.auth.TestDefaultCredentialsProviderChain | | | hadoop.fs.tosfs.object.TestObjectRangeInputStream | | | hadoop.fs.tosfs.object.TestObjectMultiRangeInputStream | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7980 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs compile javac javadoc mvninstall mvnsite unit shadedclient xmllint spotbugs checkstyle | | uname | Linux 209e4e5576fc 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0254bb430e1623793f30744d7b32a37202f2d586 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/testReport/ | | Max. process+thread count | 3665 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-assemblies hadoop-common-project/hadoop-common hadoop-tools/hadoop-tools-dist hadoop-tools/hadoop-gcp hadoop-cloud-storage-project/hadoop-huaweicloud hadoop-cloud-storage-project/hadoop-tos hadoop-cloud-storage-project/hadoop-cloud-storage hadoop-cloud-storage-project/hadoop-cloud-storage-dist hadoop-cloud-storage-project . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/7/console | | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. cnauroth commented on code in PR #7980: URL: https://github.com/apache/hadoop/pull/7980#discussion_r2446383325 ########## hadoop-cloud-storage-project/pom.xml: ########## @@ -34,6 +34,7 @@ <module>hadoop-cos</module> <module>hadoop-huaweicloud</module> <module>hadoop-tos</module> + <module>hadoop-cloud-storage-dist</module> Review Comment: It seems like we would never need to enter execution of `hadoop-cloud-storage-dist` unless we are building a distro (activating `-Pdist`). Should we also wrap inclusion of the sub-module here behind activation of the `dist` profile? ########## BUILDING.txt: ########## @@ -388,6 +388,58 @@ Create a local staging version of the website (in /tmp/hadoop-site) Note that the site needs to be built in a second pass after other artifacts. +---------------------------------------------------------------------------------- +Including Cloud Connector Dependencies in Distributions: + +Hadoop distributions include the hadoop modules need to work with data and services Review Comment: Nitpick: \"modules needed\". steveloughran commented on code in PR #7980: URL: https://github.com/apache/hadoop/pull/7980#discussion_r2447670994 ########## hadoop-cloud-storage-project/pom.xml: ########## @@ -34,6 +34,7 @@ <module>hadoop-cos</module> <module>hadoop-huaweicloud</module> <module>hadoop-tos</module> + <module>hadoop-cloud-storage-dist</module> Review Comment: valid point. Will do, as it'll save on disk space as well as time. hadoop-yetus commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3425948823 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 16s | | https://github.com/apache/hadoop/pull/7980 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7980 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7980/10/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. cnauroth commented on code in PR #7980: URL: https://github.com/apache/hadoop/pull/7980#discussion_r2449857037 ########## BUILDING.txt: ########## @@ -388,6 +388,57 @@ Create a local staging version of the website (in /tmp/hadoop-site) Note that the site needs to be built in a second pass after other artifacts. +---------------------------------------------------------------------------------- +Including Cloud Connector Dependencies in Distributions: + +Hadoop distributions include the hadoop modules needed to work with data and services +on cloud infrastructure + +However, dependencies are omitted for all cloud connectors except hadoop-azure +(abfs:// and wasb://) and possibly hadoop-gcp (gs://) and hadoop-tos (tos://). +For the latter two modules, it depends on shading options. + +For hadoop-aws the AWS SDK bundle.jar is omitted, but everything else is included. + +Excluding the extra binaries: +* Keeps release artifact size below the limit of the ASF distribution network. +* Reduces download and size overhead in docker usage. +* Reduces the CVE attack surface and audit-related complaints about those same ScVES. Review Comment: Nitpick: \"CVEs.\"", "created": "2025-09-17T13:36:12.000+0000", "updated": "2025-10-21T22:22:37.000+0000", "derived": {"summary_task": "Summarize this issue: Place all the cloud connector hadoop-* artifacts and dependencies into hadoop/common/lib so that the stores can be directly accessed. * filesystem operations against abfs, s3a, gcs, etc don't need any effort setting things up. * Releases without the aws bundle.jar can be trivially updated by adding any version of the sdk libraries to the common/lib dir. This adds a lot more stuff into the distribu", "classification_task": "Classify the issue priority and type: Place all the cloud connector hadoop-* artifacts and dependencies into hadoop/common/lib so that the stores can be directly accessed. * filesystem operations against abfs, s3a, gcs, etc don't need any effort setting things up. * Releases without the aws bundle.jar can be trivially updated by adding any version of the sdk libraries to the common/lib dir. This adds a lot more stuff into the distribu", "qna_task": "Question: What is this issue about?\nAnswer: hadoop binary distribution to move cloud connectors to hadoop common/lib"}}
{"id": "13629179", "key": "HADOOP-19695", "project": "HADOOP", "summary": "Add dual-stack/IPv6 Support to HttpServer2", "description": "To support clients connecting to JHS via IPv6, we need to equip the YARN WebApp class to bind to an IPv6 address. WebApp uses the HttpServer2, and adding the IPv6 connector to this class makes the solution more elegant. To enable dual-stack or IPv6 support, use InetAddress.getAllByName(hostname) to resolve the IP addresses of a host. When the system property java.net.preferIPv4Stack is set to true, only IPv4 addresses are returned, and any IPv6 addresses are ignored, so no extra check is needed to exclude IPv6. When java.net.preferIPv4Stack is false, both IPv4 and IPv6 addresses may be returned, and any IPv6 addresses will also be added as connectors. To disable IPv4, you need to configure the OS at the system level.", "comments": "ferdelyi opened a new pull request, #7979: URL: https://github.com/apache/hadoop/pull/7979 To enable dual-stack or IPv6 support, use InetAddress.getAllByName(hostname) to resolve the IP addresses of a host. When the system property java.net.preferIPv4Stack is set to true, only IPv4 addresses are returned, and any IPv6 addresses are ignored, so no extra check is needed to exclude IPv6. When java.net.preferIPv4Stack is false, both IPv4 and IPv6 addresses may be returned, and any IPv6 addresses will also be added as connectors. To disable IPv4, you need to configure the OS at the system level. <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR ### How was this patch tested? ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #7979: URL: https://github.com/apache/hadoop/pull/7979#issuecomment-3303626071 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 47s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 57m 18s | | trunk passed | | +1 :green_heart: | compile | 17m 55s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 34s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 18s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 41s | | trunk passed | | +1 :green_heart: | javadoc | 1m 18s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 52s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 33s | | trunk passed | | +1 :green_heart: | shadedclient | 42m 47s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 57s | | the patch passed | | +1 :green_heart: | compile | 17m 1s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 17m 1s | | the patch passed | | +1 :green_heart: | compile | 15m 40s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 40s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 14s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/1/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) | hadoop-common-project/hadoop-common: The patch generated 1 new + 68 unchanged - 0 fixed = 69 total (was 68) | | +1 :green_heart: | mvnsite | 1m 34s | | the patch passed | | +1 :green_heart: | javadoc | 1m 13s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 52s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 44s | | the patch passed | | -1 :x: | shadedclient | 42m 42s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 22m 33s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 5s | | The patch does not generate ASF License warnings. | | | | 251m 12s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7979 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 69abc2152550 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 05144e7770fd55d4ed400b5ee8d71ea02d4d37b6 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/1/testReport/ | | Max. process+thread count | 3098 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7979: URL: https://github.com/apache/hadoop/pull/7979#issuecomment-3304428637 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 48s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 57m 20s | | trunk passed | | +1 :green_heart: | compile | 18m 0s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 26s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 19s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 43s | | trunk passed | | +1 :green_heart: | javadoc | 1m 18s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 53s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 36s | | trunk passed | | +1 :green_heart: | shadedclient | 43m 16s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 56s | | the patch passed | | +1 :green_heart: | compile | 16m 57s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 16m 57s | | the patch passed | | +1 :green_heart: | compile | 15m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 17s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 14s | | the patch passed | | +1 :green_heart: | mvnsite | 1m 38s | | the patch passed | | +1 :green_heart: | javadoc | 1m 13s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 52s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 43s | | the patch passed | | -1 :x: | shadedclient | 42m 52s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 22m 37s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 5s | | The patch does not generate ASF License warnings. | | | | 251m 40s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7979 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 2f758f82d727 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 4e384cfeaf21c2674e992b6ec288d9403a8d1adb | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/2/testReport/ | | Max. process+thread count | 2679 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. brumi1024 commented on code in PR #7979: URL: https://github.com/apache/hadoop/pull/7979#discussion_r2359223851 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java: ########## @@ -549,25 +550,50 @@ public HttpServer2 build() throws IOException { } for (URI ep : endpoints) { - final ServerConnector connector; + // + // To enable dual-stack or IPv6 support, use InetAddress + // .getAllByName(hostname) to resolve the IP addresses of a host. + // When the system property java.net.preferIPv4Stack is set to true, + // only IPv4 addresses are returned, and any IPv6 addresses are + // ignored, so no extra check is needed to exclude IPv6. + // When java.net.preferIPv4Stack is false, both IPv4 and IPv6 + // addresses may be returned, and any IPv6 addresses will also be + // added as connectors. + // To disable IPv4, you need to configure the OS at the system level. + // + InetAddress[] addresses = InetAddress.getAllByName(ep.getHost()); + server = addConnectors( + ep, addresses, server, httpConfig, backlogSize, idleTimeout); + } + server.loadListeners(); + return server; + } + + @VisibleForTesting + HttpServer2 addConnectors( + URI ep, InetAddress[] addresses, HttpServer2 server, + HttpConfiguration httpConfig, int backlogSize, int idleTimeout){ + for (InetAddress addr : addresses) { + ServerConnector connector; String scheme = ep.getScheme(); if (HTTP_SCHEME.equals(scheme)) { - connector = createHttpChannelConnector(server.webServer, - httpConfig); + connector = createHttpChannelConnector( + server.webServer, httpConfig); } else if (HTTPS_SCHEME.equals(scheme)) { - connector = createHttpsChannelConnector(server.webServer, - httpConfig); + connector = createHttpsChannelConnector( + server.webServer, httpConfig); } else { throw new HadoopIllegalArgumentException( \"unknown scheme for endpoint:\" + ep); } - connector.setHost(ep.getHost()); + LOG.info(\"Adding connector to WebServer for address {}\", Review Comment: Do we need info level for this? Debug might be enough. ferdelyi commented on code in PR #7979: URL: https://github.com/apache/hadoop/pull/7979#discussion_r2359742305 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java: ########## @@ -549,25 +550,50 @@ public HttpServer2 build() throws IOException { } for (URI ep : endpoints) { - final ServerConnector connector; + // + // To enable dual-stack or IPv6 support, use InetAddress + // .getAllByName(hostname) to resolve the IP addresses of a host. + // When the system property java.net.preferIPv4Stack is set to true, + // only IPv4 addresses are returned, and any IPv6 addresses are + // ignored, so no extra check is needed to exclude IPv6. + // When java.net.preferIPv4Stack is false, both IPv4 and IPv6 + // addresses may be returned, and any IPv6 addresses will also be + // added as connectors. + // To disable IPv4, you need to configure the OS at the system level. + // + InetAddress[] addresses = InetAddress.getAllByName(ep.getHost()); + server = addConnectors( + ep, addresses, server, httpConfig, backlogSize, idleTimeout); + } + server.loadListeners(); + return server; + } + + @VisibleForTesting + HttpServer2 addConnectors( + URI ep, InetAddress[] addresses, HttpServer2 server, + HttpConfiguration httpConfig, int backlogSize, int idleTimeout){ + for (InetAddress addr : addresses) { + ServerConnector connector; String scheme = ep.getScheme(); if (HTTP_SCHEME.equals(scheme)) { - connector = createHttpChannelConnector(server.webServer, - httpConfig); + connector = createHttpChannelConnector( + server.webServer, httpConfig); } else if (HTTPS_SCHEME.equals(scheme)) { - connector = createHttpsChannelConnector(server.webServer, - httpConfig); + connector = createHttpsChannelConnector( + server.webServer, httpConfig); } else { throw new HadoopIllegalArgumentException( \"unknown scheme for endpoint:\" + ep); } - connector.setHost(ep.getHost()); + LOG.info(\"Adding connector to WebServer for address {}\", Review Comment: @brumi1024 thank you for your review! I've pushed a new commit with the suggested change. hadoop-yetus commented on PR #7979: URL: https://github.com/apache/hadoop/pull/7979#issuecomment-3309202536 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 52s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 57m 54s | | trunk passed | | +1 :green_heart: | compile | 17m 54s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 14s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 18s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 42s | | trunk passed | | +1 :green_heart: | javadoc | 1m 17s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 53s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 36s | | trunk passed | | +1 :green_heart: | shadedclient | 43m 42s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 58s | | the patch passed | | +1 :green_heart: | compile | 16m 55s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 16m 55s | | the patch passed | | +1 :green_heart: | compile | 15m 33s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 33s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 13s | | the patch passed | | +1 :green_heart: | mvnsite | 1m 39s | | the patch passed | | +1 :green_heart: | javadoc | 1m 13s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 52s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 45s | | the patch passed | | -1 :x: | shadedclient | 42m 49s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 22m 40s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 5s | | The patch does not generate ASF License warnings. | | | | 253m 33s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7979 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux d2fb039015c7 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 792d7c033b10af7f44ef638aa249d7594e5f3bcd | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/3/testReport/ | | Max. process+thread count | 1273 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/3/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7979: URL: https://github.com/apache/hadoop/pull/7979#issuecomment-3319347658 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 22m 12s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 54m 26s | | trunk passed | | +1 :green_heart: | compile | 17m 55s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 16m 29s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 21s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 42s | | trunk passed | | +1 :green_heart: | javadoc | 1m 17s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 53s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 35s | | trunk passed | | +1 :green_heart: | shadedclient | 44m 9s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 58s | | the patch passed | | +1 :green_heart: | compile | 17m 4s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 17m 4s | | the patch passed | | +1 :green_heart: | compile | 15m 33s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 33s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 15s | | the patch passed | | +1 :green_heart: | mvnsite | 1m 41s | | the patch passed | | +1 :green_heart: | javadoc | 1m 14s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 52s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 48s | | the patch passed | | -1 :x: | shadedclient | 43m 28s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 23m 7s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 6s | | The patch does not generate ASF License warnings. | | | | 273m 25s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7979 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux b753ac7f62a8 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7d394e5b895e03fa9e42057aa4ea2a74b3d0d034 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/4/testReport/ | | Max. process+thread count | 3098 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/4/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. ferdelyi commented on PR #7979: URL: https://github.com/apache/hadoop/pull/7979#issuecomment-3324196841 Earlier shadedclient failures: [ERROR] ITUseMiniCluster.clusterUp:78 \u00bb IO Problem starting http server Latest failure: ERROR: Failed to write github status. Token expired or missing repo:status write? hadoop-yetus commented on PR #7979: URL: https://github.com/apache/hadoop/pull/7979#issuecomment-3325156959 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 22m 16s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 57m 46s | | trunk passed | | +1 :green_heart: | compile | 18m 4s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 16m 24s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 19s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 43s | | trunk passed | | +1 :green_heart: | javadoc | 1m 19s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 54s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 35s | | trunk passed | | +1 :green_heart: | shadedclient | 42m 52s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 57s | | the patch passed | | +1 :green_heart: | compile | 16m 59s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 16m 59s | | the patch passed | | +1 :green_heart: | compile | 15m 23s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 23s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 13s | | the patch passed | | +1 :green_heart: | mvnsite | 1m 41s | | the patch passed | | +1 :green_heart: | javadoc | 1m 13s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 54s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 44s | | the patch passed | | -1 :x: | shadedclient | 42m 44s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 22m 31s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 7s | | The patch does not generate ASF License warnings. | | | | 273m 56s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7979 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux afe6e7544141 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 48a180410434866489d88bb6cfda181f2bcc602d | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/5/testReport/ | | Max. process+thread count | 3100 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/5/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7979: URL: https://github.com/apache/hadoop/pull/7979#issuecomment-3333947403 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 59s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 53m 11s | | trunk passed | | +1 :green_heart: | compile | 18m 17s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 17m 43s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 7s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 50s | | trunk passed | | +1 :green_heart: | javadoc | 1m 23s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 56s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 55s | | trunk passed | | +1 :green_heart: | shadedclient | 42m 38s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 58s | | the patch passed | | +1 :green_heart: | compile | 17m 14s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 17m 14s | | the patch passed | | +1 :green_heart: | compile | 15m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 19s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 56s | | the patch passed | | +1 :green_heart: | mvnsite | 1m 40s | | the patch passed | | +1 :green_heart: | javadoc | 1m 13s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 55s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 45s | | the patch passed | | -1 :x: | shadedclient | 42m 1s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 22m 35s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 4s | | The patch does not generate ASF License warnings. | | | | 248m 51s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/6/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7979 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 04cfc8fa6705 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 2b42d965a1cda228b2307bf26b0444c9b047d8c8 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/6/testReport/ | | Max. process+thread count | 1249 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/6/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. ferdelyi commented on PR #7979: URL: https://github.com/apache/hadoop/pull/7979#issuecomment-3335426490 shadedclient was failing on this: [INFO] Running org.apache.hadoop.example.ITUseMiniCluster [ERROR] Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 19.60 s <<< FAILURE! hadoop-yetus commented on PR #7979: URL: https://github.com/apache/hadoop/pull/7979#issuecomment-3336661768 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 4s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 10m 37s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 47m 15s | | trunk passed | | +1 :green_heart: | compile | 18m 8s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 47s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 3m 8s | | trunk passed | | +1 :green_heart: | mvnsite | 3m 18s | | trunk passed | | +1 :green_heart: | javadoc | 2m 40s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 2m 52s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 6m 3s | | trunk passed | | +1 :green_heart: | shadedclient | 43m 38s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 33s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 2m 12s | | the patch passed | | +1 :green_heart: | compile | 17m 17s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 17m 17s | | the patch passed | | +1 :green_heart: | compile | 15m 57s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 57s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 3m 6s | | the patch passed | | +1 :green_heart: | mvnsite | 3m 20s | | the patch passed | | +1 :green_heart: | javadoc | 2m 36s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 2m 54s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 6m 22s | | the patch passed | | -1 :x: | shadedclient | 42m 14s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 22m 39s | | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 276m 18s | | hadoop-hdfs in the patch passed. | | +1 :green_heart: | asflicense | 1m 20s | | The patch does not generate ASF License warnings. | | | | 552m 3s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/7/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7979 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux ed92b822cf7a 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 8aa88fc3e70c245050e33aab0a585dd5a9606340 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/7/testReport/ | | Max. process+thread count | 3098 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/7/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. ferdelyi commented on PR #7979: URL: https://github.com/apache/hadoop/pull/7979#issuecomment-3338485084 In the patch-shadedclient.txt the exception now is: Caused by: java.lang.IllegalStateException: Insufficient configured threads: required=5 < max=5 for QueuedThreadPool[qtp164052991]@9c73fff{STARTED,5<=5<=5,i=3,r=-1,q=0}[ReservedThreadExecutor@29be997f{reserved=0/1,pending=0}] I've increased HTTP_MAX_THREADS by one, and the required number of threads also increased by one. Just out of curiosity will puch e.g. 10 and see if it keeps increasing. hadoop-yetus commented on PR #7979: URL: https://github.com/apache/hadoop/pull/7979#issuecomment-3340611486 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 50s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 7m 44s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 47m 11s | | trunk passed | | +1 :green_heart: | compile | 18m 6s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 38s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 3m 14s | | trunk passed | | +1 :green_heart: | mvnsite | 3m 16s | | trunk passed | | +1 :green_heart: | javadoc | 2m 38s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 2m 50s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 6m 0s | | trunk passed | | +1 :green_heart: | shadedclient | 42m 47s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 35s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 2m 11s | | the patch passed | | +1 :green_heart: | compile | 17m 6s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 17m 6s | | the patch passed | | +1 :green_heart: | compile | 15m 23s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 23s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 3m 9s | | the patch passed | | +1 :green_heart: | mvnsite | 3m 16s | | the patch passed | | +1 :green_heart: | javadoc | 2m 35s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 2m 47s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 6m 26s | | the patch passed | | +1 :green_heart: | shadedclient | 43m 6s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 22m 42s | | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 278m 6s | | hadoop-hdfs in the patch passed. | | +1 :green_heart: | asflicense | 1m 21s | | The patch does not generate ASF License warnings. | | | | 549m 29s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/8/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7979 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 2ff86f039bb0 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / e14a395fa6450d1aeb2f0054197292b5af1540dc | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/8/testReport/ | | Max. process+thread count | 2198 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/8/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7979: URL: https://github.com/apache/hadoop/pull/7979#issuecomment-3375718964 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 30m 10s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 52s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 45m 43s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/9/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | -1 :x: | compile | 11m 43s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/9/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 10m 13s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/9/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | checkstyle | 2m 58s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 51s | | trunk passed | | +1 :green_heart: | javadoc | 2m 14s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 2m 29s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 5m 35s | | trunk passed | | +1 :green_heart: | shadedclient | 42m 2s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 34s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 2m 9s | | the patch passed | | -1 :x: | compile | 11m 28s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/9/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 11m 28s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/9/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 10m 9s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/9/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 10m 9s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/9/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 2m 49s | | the patch passed | | +1 :green_heart: | mvnsite | 2m 42s | | the patch passed | | +1 :green_heart: | javadoc | 2m 3s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 2m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 5m 51s | | the patch passed | | +1 :green_heart: | shadedclient | 42m 33s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 22m 22s | | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 470m 28s | | hadoop-hdfs in the patch passed. | | +1 :green_heart: | asflicense | 0m 57s | | The patch does not generate ASF License warnings. | | | | 741m 16s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/9/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7979 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 83d811e5f434 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 8e2cd7f590381ff77458ec6a44f0d3b83779eb53 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/9/testReport/ | | Max. process+thread count | 2216 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/9/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7979: URL: https://github.com/apache/hadoop/pull/7979#issuecomment-3377795853 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 50s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 32s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 46m 25s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/10/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | -1 :x: | compile | 11m 42s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/10/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 9m 59s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/10/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | checkstyle | 2m 58s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 52s | | trunk passed | | +1 :green_heart: | javadoc | 2m 12s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 2m 23s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 5m 32s | | trunk passed | | +1 :green_heart: | shadedclient | 43m 36s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 36s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 2m 29s | | the patch passed | | -1 :x: | compile | 11m 52s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/10/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 11m 52s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/10/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 9m 42s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/10/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 9m 42s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/10/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 2m 45s | | the patch passed | | +1 :green_heart: | mvnsite | 2m 43s | | the patch passed | | +1 :green_heart: | javadoc | 2m 3s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 2m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 5m 52s | | the patch passed | | +1 :green_heart: | shadedclient | 42m 51s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 22m 25s | | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 279m 19s | | hadoop-hdfs in the patch passed. | | +1 :green_heart: | asflicense | 0m 54s | | The patch does not generate ASF License warnings. | | | | 521m 50s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/10/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7979 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux d9d0c21591bb 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7adcb39cbe58f85adc3d03fa0ad751c64b87624e | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/10/testReport/ | | Max. process+thread count | 3098 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/10/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7979: URL: https://github.com/apache/hadoop/pull/7979#issuecomment-3386925666 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 21m 36s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 2s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 41m 3s | | trunk passed | | +1 :green_heart: | compile | 15m 50s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 55s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 2m 53s | | trunk passed | | +1 :green_heart: | mvnsite | 3m 21s | | trunk passed | | +1 :green_heart: | javadoc | 2m 33s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 2m 53s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 5m 55s | | trunk passed | | +1 :green_heart: | shadedclient | 37m 1s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 34s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 2m 9s | | the patch passed | | +1 :green_heart: | compile | 15m 24s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 24s | | the patch passed | | +1 :green_heart: | compile | 13m 45s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 45s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 2m 49s | | the patch passed | | +1 :green_heart: | mvnsite | 3m 17s | | the patch passed | | +1 :green_heart: | javadoc | 2m 34s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 2m 51s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 6m 16s | | the patch passed | | +1 :green_heart: | shadedclient | 38m 43s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 22m 50s | | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 289m 16s | | hadoop-hdfs in the patch passed. | | +1 :green_heart: | asflicense | 1m 19s | | The patch does not generate ASF License warnings. | | | | 557m 31s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/12/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7979 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 22968369ed69 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 87e9944f809d56e1c99a5ff5138c93377e99f464 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/12/testReport/ | | Max. process+thread count | 3152 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/12/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. brumi1024 merged PR #7979: URL: https://github.com/apache/hadoop/pull/7979", "created": "2025-09-17T11:21:23.000+0000", "updated": "2025-10-13T08:48:57.000+0000", "derived": {"summary_task": "Summarize this issue: To support clients connecting to JHS via IPv6, we need to equip the YARN WebApp class to bind to an IPv6 address. WebApp uses the HttpServer2, and adding the IPv6 connector to this class makes the solution more elegant. To enable dual-stack or IPv6 support, use InetAddress.getAllByName(hostname) to resolve the IP addresses of a host. When the system property java.net.preferIPv4Stack is set to true", "classification_task": "Classify the issue priority and type: To support clients connecting to JHS via IPv6, we need to equip the YARN WebApp class to bind to an IPv6 address. WebApp uses the HttpServer2, and adding the IPv6 connector to this class makes the solution more elegant. To enable dual-stack or IPv6 support, use InetAddress.getAllByName(hostname) to resolve the IP addresses of a host. When the system property java.net.preferIPv4Stack is set to true", "qna_task": "Question: What is this issue about?\nAnswer: Add dual-stack/IPv6 Support to HttpServer2"}}
{"id": "13629178", "key": "HADOOP-19694", "project": "HADOOP", "summary": "Bump guava to 33.4.8-jre due to EOL", "description": "We can use the latest 33.4.8-jre version as the current one is quite old.", "comments": "When I do a thirdparty build I now get a warning of duplicate module 9 info. {code} [INFO] No artifact matching filter org.checkerframework:checker-qual [WARNING] error_prone_annotations-2.36.0.jar, guava-33.4.8-jre.jar, failureaccess-1.0.3.jar, jspecify-1.0.0.jar, j2objc-annotations-3.0.0.jar define 1 overlapping classes: [WARNING] - META-INF.versions.9.module-info [WARNING] maven-shade-plugin has detected that some class files are [WARNING] present in two or more JARs. When this happens, only one [WARNING] single version of the class is copied to the uber jar. [WARNING] Usually this is not harmful and you can skip these warnings, [WARNING] otherwise try to manually exclude artifacts based on [WARNING] mvn dependency:tree -Ddetail=true and the above output. [WARNING] See http://maven.apache.org/plugins/maven-shade-plugin/ {code} This is new. given we want this to work on java17+ , we need to come up with a way of resolving the conflict, at the very least by making the guava one dominant. ahh, the later versions of guava exclude a dependency on checkerframework. So any references in our code (there are four) fail. Which means we have to manually add it if we want a drop in replacement. PITA.", "created": "2025-09-17T11:01:22.000+0000", "updated": "2025-10-20T16:41:38.000+0000", "derived": {"summary_task": "Summarize this issue: We can use the latest 33.4.8-jre version as the current one is quite old.", "classification_task": "Classify the issue priority and type: We can use the latest 33.4.8-jre version as the current one is quite old.", "qna_task": "Question: What is this issue about?\nAnswer: Bump guava to  33.4.8-jre due to EOL"}}
{"id": "13629129", "key": "HADOOP-19693", "project": "HADOOP", "summary": "Update Java 24 to 25 in docker images", "description": "Temurin JDK25 packages are expected to be available shortly, update JDK 24 to 25 in the docker images.", "comments": "The ubuntu packages have been released. stoty opened a new pull request, #7991: URL: https://github.com/apache/hadoop/pull/7991 ### Description of PR Update Java 24 to 25 in docker images ### How was this patch tested? Built ubuntu_20 and ubuntu_24 x64 images, and ran java 25 in them. ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? stoty commented on PR #7991: URL: https://github.com/apache/hadoop/pull/7991#issuecomment-3322708226 PTAL @slfan1989 hadoop-yetus commented on PR #7991: URL: https://github.com/apache/hadoop/pull/7991#issuecomment-3322950127 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 31m 53s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 10m 51s | | Maven dependency ordering for branch | | +1 :green_heart: | shadedclient | 53m 17s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 46s | | Maven dependency ordering for patch | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 41m 20s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 58s | | The patch does not generate ASF License warnings. | | | | 130m 13s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7991/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7991 | | Optional Tests | dupname asflicense codespell detsecrets jsonlint | | uname | Linux 7aa6647a89a6 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / edaa6e0885eeb0c0db357ef86e5defa0dfcc28d8 | | Max. process+thread count | 525 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7991/1/console | | versions | git=2.43.7 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7991: URL: https://github.com/apache/hadoop/pull/7991#issuecomment-3323181520 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 27m 28s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 0m 27s | | Maven dependency ordering for branch | | -1 :x: | shadedclient | 8m 53s | | branch has errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 24s | | Maven dependency ordering for patch | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 31m 4s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 32s | | The patch does not generate ASF License warnings. | | | | 70m 0s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7991/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7991 | | Optional Tests | dupname asflicense codespell detsecrets jsonlint | | uname | Linux 242b70b5bd76 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / edaa6e0885eeb0c0db357ef86e5defa0dfcc28d8 | | Max. process+thread count | 538 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7991/1/console | | versions | git=2.30.2 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on PR #7991: URL: https://github.com/apache/hadoop/pull/7991#issuecomment-3332430073 @pan3793 Could you please review this PR? Thank you very much! Pan has some experience with higher versions of JDK. stoty commented on PR #7991: URL: https://github.com/apache/hadoop/pull/7991#issuecomment-3332517143 Not much to review here @slfan1989 . This is just the first step to being able to test with JDK25. TBH I don't see much value in supporting or testing for JDK24, the real goal is JDK25, the stable release. Generally, I would test with the supported stable Java releases, plus the latest supported non-stable Java release. i.e when JDK 26 is released it, then keep Java 25 and add Java 26, then keep replacing Java 26 with 27,28,29,29... until the next stable Java is released (with the optimistic assumption that Hadoop is going to keep up with the non-stable Java releases, and we won't have to do any more big bang updates for 10+ java releases) slfan1989 merged PR #7991: URL: https://github.com/apache/hadoop/pull/7991 slfan1989 commented on PR #7991: URL: https://github.com/apache/hadoop/pull/7991#issuecomment-3369570388 @stoty Thanks for the contribution!", "created": "2025-09-17T07:57:36.000+0000", "updated": "2025-10-06T00:33:24.000+0000", "derived": {"summary_task": "Summarize this issue: Temurin JDK25 packages are expected to be available shortly, update JDK 24 to 25 in the docker images.", "classification_task": "Classify the issue priority and type: Temurin JDK25 packages are expected to be available shortly, update JDK 24 to 25 in the docker images.", "qna_task": "Question: What is this issue about?\nAnswer: Update Java 24 to 25 in docker images"}}
{"id": "13629073", "key": "HADOOP-19692", "project": "HADOOP", "summary": "Exclude junit 4 transitive dependency", "description": "HADOOP-19617 removed direct junit 4 dependency. However, junit 4 is still pulled transitively by other dependencies.", "comments": "{code} [INFO] ------------------< org.apache.hadoop:hadoop-common >------------------- [INFO] Building Apache Hadoop Common 3.5.0-SNAPSHOT [11/117] [INFO] from hadoop-common-project/hadoop-common/pom.xml ... [INFO] +- com.squareup.okhttp3:mockwebserver:jar:4.11.0:test [INFO] | +- com.squareup.okhttp3:okhttp:jar:4.11.0:test [INFO] | | \\- com.squareup.okio:okio:jar:3.2.0:test [INFO] | | \\- com.squareup.okio:okio-jvm:jar:3.2.0:test [INFO] | \\- junit:junit:jar:4.13:test [INFO] | \\- org.hamcrest:hamcrest-core:jar:1.3:test {code} {code} [INFO] ----------------< org.apache.hadoop:hadoop-hdfs-httpfs >---------------- [INFO] Building Apache Hadoop HttpFS 3.5.0-SNAPSHOT [19/117] [INFO] from hadoop-hdfs-project/hadoop-hdfs-httpfs/pom.xml ... [INFO] +- com.googlecode.json-simple:json-simple:jar:1.1.1:compile [INFO] | \\- junit:junit:jar:4.10:compile {code} {code} [INFO] -----< org.apache.hadoop:hadoop-yarn-applications-catalog-webapp >------ [INFO] Building Apache Hadoop YARN Application Catalog Webapp 3.5.0-SNAPSHOT [63/117] [INFO] from hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/pom.xml ... [INFO] +- org.apache.solr:solr-test-framework:jar:8.11.2:test [INFO] | +- org.apache.lucene:lucene-test-framework:jar:8.11.2:test [INFO] | +- com.carrotsearch.randomizedtesting:junit4-ant:jar:2.7.2:test [INFO] | +- com.carrotsearch.randomizedtesting:randomizedtesting-runner:jar:2.7.2:test [INFO] | +- io.opentracing:opentracing-mock:jar:0.33.0:test [INFO] | +- junit:junit:jar:4.13.1:test {code} {code} [INFO] --< org.apache.hadoop.applications.mawo:hadoop-yarn-applications-mawo >-- [INFO] Building Apache Hadoop YARN Application MaWo 3.5.0-SNAPSHOT [65/117] [INFO] from hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-mawo/pom.xml ... [INFO] +- com.googlecode.json-simple:json-simple:jar:1.1.1:compile [INFO] | \\- junit:junit:jar:4.10:compile [INFO] | \\- org.hamcrest:hamcrest-core:jar:1.1:compile {code} szetszwo opened a new pull request, #7978: URL: https://github.com/apache/hadoop/pull/7978 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR HADOOP-19692 The direct junit 4 dependency was removed by HADOOP-19617. However, junit 4 is still pulled transitively by other dependencies. ### How was this patch tested? By the pull request action. ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [NA] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [NA] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [NA] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #7978: URL: https://github.com/apache/hadoop/pull/7978#issuecomment-3300529165 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 49s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 12m 11s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 47m 40s | | trunk passed | | +1 :green_heart: | compile | 17m 49s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 15s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 4m 53s | | trunk passed | | +1 :green_heart: | javadoc | 3m 58s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 3m 26s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 146m 52s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 33s | | Maven dependency ordering for patch | | -1 :x: | mvninstall | 0m 53s | [/patch-mvninstall-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/2/artifact/out/patch-mvninstall-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch failed. | | -1 :x: | compile | 1m 23s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 1m 23s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 1m 13s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 1m 13s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 0m 59s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/2/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch failed. | | +1 :green_heart: | javadoc | 2m 9s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 42s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | shadedclient | 10m 7s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 14s | | hadoop-project in the patch passed. | | -1 :x: | unit | 0m 59s | [/patch-unit-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/2/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch failed. | | +1 :green_heart: | unit | 5m 32s | | hadoop-hdfs-httpfs in the patch passed. | | -1 :x: | unit | 0m 48s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/2/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) | hadoop-yarn-applications-catalog-webapp in the patch passed. | | +1 :green_heart: | unit | 0m 20s | | hadoop-yarn-applications-mawo-core in the patch passed. | | +1 :green_heart: | asflicense | 0m 32s | | The patch does not generate ASF License warnings. | | | | 173m 47s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.appcatalog.application.TestAppCatalogSolrClient | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7978 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux b2d417db0dc5 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 613024d63ccccb96c39f1fab428aa1356b68d2d4 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/2/testReport/ | | Max. process+thread count | 863 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-mawo/hadoop-yarn-applications-mawo-core U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/2/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. szetszwo commented on PR #7978: URL: https://github.com/apache/hadoop/pull/7978#issuecomment-3300605779 It turns out that we have not completely removed junit tests. ``` ERROR] /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-7978/ubuntu-focal/src/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/http/TestHttpFileSystem.java:[57,5] cannot access org.junit.rules.ExternalResource class file for org.junit.rules.ExternalResource not found ``` szetszwo commented on PR #7978: URL: https://github.com/apache/hadoop/pull/7978#issuecomment-3300697473 > ... we have not completely removed junit tests. The reason is that `mockwebserver` uses junit 4. We should replace it with `mockwebserver3-junit5`. hadoop-yetus commented on PR #7978: URL: https://github.com/apache/hadoop/pull/7978#issuecomment-3301302828 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 49s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 10m 57s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 49m 40s | | trunk passed | | +1 :green_heart: | compile | 17m 59s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 25s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 4m 44s | | trunk passed | | +1 :green_heart: | mvnsite | 4m 53s | | trunk passed | | +1 :green_heart: | javadoc | 3m 54s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 3m 27s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 41s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 41m 5s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 39s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 2m 31s | | the patch passed | | +1 :green_heart: | compile | 17m 9s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 17m 9s | | the patch passed | | +1 :green_heart: | compile | 15m 21s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 21s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 4m 32s | | the patch passed | | +1 :green_heart: | mvnsite | 4m 50s | | the patch passed | | +1 :green_heart: | javadoc | 3m 55s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 3m 27s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 35s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 41m 57s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 36s | | hadoop-project in the patch passed. | | +1 :green_heart: | unit | 22m 35s | | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 5m 55s | | hadoop-hdfs-httpfs in the patch passed. | | -1 :x: | unit | 1m 10s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/3/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) | hadoop-yarn-applications-catalog-webapp in the patch passed. | | +1 :green_heart: | unit | 0m 42s | | hadoop-yarn-applications-mawo-core in the patch passed. | | +1 :green_heart: | asflicense | 1m 7s | | The patch does not generate ASF License warnings. | | | | 298m 25s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.appcatalog.application.TestAppCatalogSolrClient | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7978 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 19a387ca4ff9 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 715044a0192f83d7661d00e9ede87d1067004e52 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/3/testReport/ | | Max. process+thread count | 1294 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-mawo/hadoop-yarn-applications-mawo-core U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/3/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. szetszwo commented on PR #7978: URL: https://github.com/apache/hadoop/pull/7978#issuecomment-3304127134 ``` [ERROR] org.apache.hadoop.yarn.appcatalog.application.TestAppCatalogSolrClient.testNotFoundSearch hadoop-yetus commented on PR #7978: URL: https://github.com/apache/hadoop/pull/7978#issuecomment-3304842636 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 48s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 11m 52s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 48m 8s | | trunk passed | | +1 :green_heart: | compile | 17m 56s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 19s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 4m 41s | | trunk passed | | +1 :green_heart: | mvnsite | 4m 49s | | trunk passed | | +1 :green_heart: | javadoc | 3m 58s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 3m 26s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 42s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 41m 30s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 37s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 2m 34s | | the patch passed | | +1 :green_heart: | compile | 16m 58s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 16m 58s | | the patch passed | | +1 :green_heart: | compile | 15m 14s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 14s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 4m 37s | | the patch passed | | +1 :green_heart: | mvnsite | 4m 51s | | the patch passed | | +1 :green_heart: | javadoc | 3m 56s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 3m 27s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 37s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 41m 8s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 36s | | hadoop-project in the patch passed. | | +1 :green_heart: | unit | 22m 38s | | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 5m 54s | | hadoop-hdfs-httpfs in the patch passed. | | +1 :green_heart: | unit | 1m 15s | | hadoop-yarn-applications-catalog-webapp in the patch passed. | | +1 :green_heart: | unit | 0m 42s | | hadoop-yarn-applications-mawo-core in the patch passed. | | +1 :green_heart: | asflicense | 1m 6s | | The patch does not generate ASF License warnings. | | | | 297m 9s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7978 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux ded193a6470d 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 90ed7ecdaeec392fff06e28edc555a5420794f66 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/4/testReport/ | | Max. process+thread count | 2987 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-mawo/hadoop-yarn-applications-mawo-core U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7978/4/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. szetszwo commented on PR #7978: URL: https://github.com/apache/hadoop/pull/7978#issuecomment-3304863270 > The junit 4 Assert is in LuceneTestCase. Not sure if updating the solr-test-framework version 8.11.2 could fix it. The current code in LuceneTestCase still use JUnit 4. https://github.com/apache/lucene/blob/13a7e1e53d0e69233e775f2fb241b86c3ac0e527/lucene/test-framework/src/java/org/apache/lucene/tests/util/LuceneTestCase.java#L202C1-L217C3 szetszwo commented on PR #7978: URL: https://github.com/apache/hadoop/pull/7978#issuecomment-3308469834 Filed HADOOP-19699 for TestAppCatalogSolrClient szetszwo merged PR #7978: URL: https://github.com/apache/hadoop/pull/7978 szetszwo commented on PR #7978: URL: https://github.com/apache/hadoop/pull/7978#issuecomment-3308483383 Thanks @cnauroth and @slfan1989 for reviewing this! The pull request was merged. slfan1989 commented on PR #7978: URL: https://github.com/apache/hadoop/pull/7978#issuecomment-3310127385 > Thanks @cnauroth and @slfan1989 for reviewing this! @szetszwo Thank you for the contribution!", "created": "2025-09-16T18:18:40.000+0000", "updated": "2025-09-19T17:06:35.000+0000", "derived": {"summary_task": "Summarize this issue: HADOOP-19617 removed direct junit 4 dependency. However, junit 4 is still pulled transitively by other dependencies.", "classification_task": "Classify the issue priority and type: HADOOP-19617 removed direct junit 4 dependency. However, junit 4 is still pulled transitively by other dependencies.", "qna_task": "Question: What is this issue about?\nAnswer: Exclude junit 4 transitive dependency"}}
{"id": "13628997", "key": "HADOOP-19691", "project": "HADOOP", "summary": "[JDK17] Disallow JUnit4 Imports After JUnit5 Migration", "description": "As our project has fully migrated to JUnit5, we should now enforce a rule that prevents the import and usage of JUnit4 classes (such as org.junit.Test, org.junit.Assert, etc.) to ensure consistency, avoid regressions, and allow safe removal of legacy dependencies. This task involves identifying and eliminating any remaining JUnit4 imports, and introducing static code analysis or linting rules to ban future usage.", "comments": "slfan1989 commented on PR #7976: URL: https://github.com/apache/hadoop/pull/7976#issuecomment-3297850797 @TaoYang526 We are currently working on upgrading the project to JUnit 5, and I have added a validation rule to prevent users from reintroducing JUnit 4 dependencies. During the review, I found that the testAsyncScheduleThreadExit method used JUnit 4 features, so I made some modifications. Could you please take a look and let me know if the changes are reasonable? hadoop-yetus commented on PR #7976: URL: https://github.com/apache/hadoop/pull/7976#issuecomment-3300118538 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 52s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 10m 33s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 47m 14s | | trunk passed | | +1 :green_heart: | compile | 17m 57s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 16s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 4m 40s | | trunk passed | | +1 :green_heart: | mvnsite | 23m 18s | | trunk passed | | +1 :green_heart: | javadoc | 10m 32s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 51s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 36m 56s | | trunk passed | | +1 :green_heart: | shadedclient | 73m 12s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 45s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 48m 15s | | the patch passed | | +1 :green_heart: | compile | 17m 20s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 17m 20s | | the patch passed | | +1 :green_heart: | compile | 15m 29s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 4m 42s | | the patch passed | | +1 :green_heart: | mvnsite | 19m 33s | | the patch passed | | +1 :green_heart: | javadoc | 10m 25s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 47s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 37m 40s | | the patch passed | | +1 :green_heart: | shadedclient | 73m 40s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 503m 15s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/1/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 2m 21s | | The patch does not generate ASF License warnings. | | | | 930m 38s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7976 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint | | uname | Linux 946810b75b42 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 78ed6e9e068b8c35d3eb372e09440128bf4fd0d8 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/1/testReport/ | | Max. process+thread count | 3137 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on PR #7976: URL: https://github.com/apache/hadoop/pull/7976#issuecomment-3300985791 > +1. Thanks @slfan1989 . > > I was going to suggest also banning `org.hamcrest`, but it looks like there is still a tiny amount of hamcrest remaining in YARN. Maybe this is a topic for a different PR. > > ``` > > grep -r --include '*.java' 'org.hamcrest' * > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/test/java/org/apache/hadoop/yarn/appcatalog/controller/AppListControllerTest.java:import static org.hamcrest.MatcherAssert.assertThat; > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/test/java/org/apache/hadoop/yarn/appcatalog/controller/AppListControllerTest.java:import static org.hamcrest.core.Is.is; > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/test/java/org/apache/hadoop/yarn/appcatalog/controller/AppDetailsControllerTest.java:import static org.hamcrest.MatcherAssert.assertThat; > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/test/java/org/apache/hadoop/yarn/appcatalog/controller/AppDetailsControllerTest.java:import static org.hamcrest.core.Is.is; > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/test/java/org/apache/hadoop/yarn/appcatalog/controller/AppStoreControllerTest.java:import static org.hamcrest.MatcherAssert.assertThat; > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/test/java/org/apache/hadoop/yarn/appcatalog/controller/AppStoreControllerTest.java:import static org.hamcrest.core.Is.is; > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestResourceCalculatorProcessTree.java:import static org.hamcrest.MatcherAssert.assertThat; > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestResourceCalculatorProcessTree.java:import static org.hamcrest.core.IsInstanceOf.instanceOf; > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestResourceCalculatorProcessTree.java:import static org.hamcrest.core.IsSame.sameInstance; > ``` @cnauroth Thank you for reviewing the code! I\u2019ll submit a separate PR to replace the usage of `org.hamcrest.`. hadoop-yetus commented on PR #7976: URL: https://github.com/apache/hadoop/pull/7976#issuecomment-3312878887 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 55s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 11m 55s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 47m 48s | | trunk passed | | +1 :green_heart: | compile | 18m 6s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 15s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 4m 40s | | trunk passed | | +1 :green_heart: | mvnsite | 23m 14s | | trunk passed | | +1 :green_heart: | javadoc | 10m 19s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 43s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 36m 53s | | trunk passed | | +1 :green_heart: | shadedclient | 72m 58s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 45s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 48m 31s | | the patch passed | | +1 :green_heart: | compile | 17m 26s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 17m 26s | | the patch passed | | +1 :green_heart: | compile | 15m 16s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 16s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 4m 46s | | the patch passed | | +1 :green_heart: | mvnsite | 20m 6s | | the patch passed | | +1 :green_heart: | javadoc | 10m 22s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 54s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 37m 38s | | the patch passed | | +1 :green_heart: | shadedclient | 73m 1s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 509m 47s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/2/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 26s | | The patch does not generate ASF License warnings. | | | | 937m 20s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7976 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint | | uname | Linux b7280a78b909 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / b0d0b609f7714de5a35a2e356b8eca12903f5d3b | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/2/testReport/ | | Max. process+thread count | 2676 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7976: URL: https://github.com/apache/hadoop/pull/7976#issuecomment-3328740044 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 22m 34s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 54m 10s | | trunk passed | | +1 :green_heart: | compile | 18m 17s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 16m 7s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 22m 48s | | trunk passed | | +1 :green_heart: | javadoc | 9m 45s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 25s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | shadedclient | 130m 22s | | branch has errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 45m 37s | | the patch passed | | +1 :green_heart: | compile | 17m 35s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 17m 35s | | the patch passed | | +1 :green_heart: | compile | 15m 28s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 28s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 19m 34s | | the patch passed | | +1 :green_heart: | javadoc | 9m 43s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 32s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | shadedclient | 39m 36s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 503m 30s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/4/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 16s | | The patch does not generate ASF License warnings. | | | | 778m 24s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7976 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux 0b5b9083f652 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c161c9b215d2498c3c9a2d060a0be1701444b768 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/4/testReport/ | | Max. process+thread count | 3135 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/4/console | | versions | git=2.25.1 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7976: URL: https://github.com/apache/hadoop/pull/7976#issuecomment-3328991107 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 21m 40s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 54m 49s | | trunk passed | | +1 :green_heart: | compile | 18m 9s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 13s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 23m 19s | | trunk passed | | +1 :green_heart: | javadoc | 9m 49s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 26s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | shadedclient | 130m 34s | | branch has errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 47m 25s | | the patch passed | | +1 :green_heart: | compile | 17m 24s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 17m 24s | | the patch passed | | +1 :green_heart: | compile | 15m 31s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 31s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 19m 40s | | the patch passed | | +1 :green_heart: | javadoc | 9m 46s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 28s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | shadedclient | 39m 49s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 538m 55s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/3/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 47s | | The patch does not generate ASF License warnings. | | | | 815m 59s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.TestRollingUpgrade | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7976 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux 8c48d2e66c3e 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c161c9b215d2498c3c9a2d060a0be1701444b768 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/3/testReport/ | | Max. process+thread count | 3137 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/3/console | | versions | git=2.25.1 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on PR #7976: URL: https://github.com/apache/hadoop/pull/7976#issuecomment-3330876058 This PR enforces import restrictions to prohibit the use of JUnit4. The previously reported shade error has already been resolved in #7995. Given the lengthy compilation time, I will not re-trigger the build for this PR. I will continue to investigate and address the YARN crash issue separately. @cnauroth Thanks for the review! slfan1989 merged PR #7976: URL: https://github.com/apache/hadoop/pull/7976", "created": "2025-09-16T03:05:49.000+0000", "updated": "2025-09-24T22:13:59.000+0000", "derived": {"summary_task": "Summarize this issue: As our project has fully migrated to JUnit5, we should now enforce a rule that prevents the import and usage of JUnit4 classes (such as org.junit.Test, org.junit.Assert, etc.) to ensure consistency, avoid regressions, and allow safe removal of legacy dependencies. This task involves identifying and eliminating any remaining JUnit4 imports, and introducing static code analysis or linting rules to b", "classification_task": "Classify the issue priority and type: As our project has fully migrated to JUnit5, we should now enforce a rule that prevents the import and usage of JUnit4 classes (such as org.junit.Test, org.junit.Assert, etc.) to ensure consistency, avoid regressions, and allow safe removal of legacy dependencies. This task involves identifying and eliminating any remaining JUnit4 imports, and introducing static code analysis or linting rules to b", "qna_task": "Question: What is this issue about?\nAnswer: [JDK17] Disallow JUnit4 Imports After JUnit5 Migration"}}
{"id": "13628970", "key": "HADOOP-19690", "project": "HADOOP", "summary": "Bump commons-lang3 to 3.18.0 due to CVE-2025-48924", "description": "https://www.cve.org/CVERecord?id=CVE-2025-48924 Will update commons-text to 1.14.0 which was released with commons-lang3 3.18.0. Due to https://issues.apache.org/jira/browse/HADOOP-19532 - seems best to upgrade them together.", "comments": "pjfanning opened a new pull request, #7970: URL: https://github.com/apache/hadoop/pull/7970 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR CVE-2025-48924 See https://issues.apache.org/jira/browse/HADOOP-19690 for reason to upgrade commons-text too. ### How was this patch tested? ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #7970: URL: https://github.com/apache/hadoop/pull/7970#issuecomment-3305413646 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 19s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 24m 23s | | trunk passed | | +1 :green_heart: | compile | 8m 23s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 7m 20s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 14m 9s | | trunk passed | | +1 :green_heart: | javadoc | 5m 34s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 58s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 31m 4s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 23s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 22m 56s | | the patch passed | | +1 :green_heart: | compile | 8m 11s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 11s | | the patch passed | | +1 :green_heart: | compile | 7m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 7m 26s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 11m 38s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 5m 29s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 5m 6s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 32m 0s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 963m 33s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7970/2/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 4s | | The patch does not generate ASF License warnings. | | | | 1144m 26s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.mapreduce.v2.TestUberAM | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7970/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7970 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux e242a07f343d 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 82dae1282632826d8977c56821441f5b32ee1ec8 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7970/2/testReport/ | | Max. process+thread count | 4334 (vs. ulimit of 5500) | | modules | C: hadoop-project . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7970/2/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 merged PR #7970: URL: https://github.com/apache/hadoop/pull/7970 slfan1989 commented on PR #7970: URL: https://github.com/apache/hadoop/pull/7970#issuecomment-3310116432 @pjfanning Thanks for the contribution! The unit test errors are unrelated to this PR. Could you please take a look at the branch-3.4? I think this PR should also be backported there. pjfanning opened a new pull request, #7985: URL: https://github.com/apache/hadoop/pull/7985 * relates to #7970 * HADOOP-19690. bump commons-lang3 to 3.18.0 due to CVE-2025-48924 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR ### How was this patch tested? ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #7985: URL: https://github.com/apache/hadoop/pull/7985#issuecomment-3311953658 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 7m 12s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ branch-3.4 Compile Tests _ | | +0 :ok: | mvndep | 2m 18s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 22m 13s | | branch-3.4 passed | | +1 :green_heart: | compile | 8m 32s | | branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 7m 43s | | branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 13m 58s | | branch-3.4 passed | | +1 :green_heart: | javadoc | 4m 40s | | branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 52s | | branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 28m 7s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 21s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 20m 8s | | the patch passed | | +1 :green_heart: | compile | 8m 42s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 42s | | the patch passed | | +1 :green_heart: | compile | 7m 47s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 7m 47s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 8m 21s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 4m 44s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 54s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 34m 4s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 25m 52s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7985/1/artifact/out/patch-unit-root.txt) | root in the patch failed. | | -1 :x: | asflicense | 0m 19s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7985/1/artifact/out/results-asflicense.txt) | The patch generated 296 ASF License warnings. | | | | 197m 56s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.metrics2.source.TestJvmMetrics | | | hadoop.ipc.TestCallQueueManager | | | hadoop.fs.shell.TestHdfsTextCommand | | | hadoop.hdfs.util.TestByteArrayManager | | | hadoop.hdfs.web.TestWebHDFSOAuth2 | | | hadoop.hdfs.web.TestWebHdfsContentLength | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7985/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7985 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux 71fdfc267120 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | branch-3.4 / fa8656b628557693f6cd66800a5d84bebd80501c | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7985/1/testReport/ | | Max. process+thread count | 685 (vs. ulimit of 5500) | | modules | C: hadoop-project . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7985/1/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 merged PR #7985: URL: https://github.com/apache/hadoop/pull/7985 slfan1989 commented on PR #7985: URL: https://github.com/apache/hadoop/pull/7985#issuecomment-3322481127 @pjfanning Thanks for the contribution! Merged into branch-3.4.", "created": "2025-09-15T17:53:25.000+0000", "updated": "2025-09-23T05:30:25.000+0000", "derived": {"summary_task": "Summarize this issue: https://www.cve.org/CVERecord?id=CVE-2025-48924 Will update commons-text to 1.14.0 which was released with commons-lang3 3.18.0. Due to https://issues.apache.org/jira/browse/HADOOP-19532 - seems best to upgrade them together.", "classification_task": "Classify the issue priority and type: https://www.cve.org/CVERecord?id=CVE-2025-48924 Will update commons-text to 1.14.0 which was released with commons-lang3 3.18.0. Due to https://issues.apache.org/jira/browse/HADOOP-19532 - seems best to upgrade them together.", "qna_task": "Question: What is this issue about?\nAnswer: Bump commons-lang3 to 3.18.0 due to CVE-2025-48924"}}
{"id": "13628968", "key": "HADOOP-19689", "project": "HADOOP", "summary": "Bump netty to 4.1.127 due to CVE-2025-58057", "description": "https://www.cve.org/CVERecord?id=CVE-2025-58057 fixed in 4.1.125 but no harm upgrading to latest", "comments": "pjfanning opened a new pull request, #7969: URL: https://github.com/apache/hadoop/pull/7969 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR Upgrade netty due to CVE-2025-58057 ### How was this patch tested? ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? slfan1989 merged PR #7969: URL: https://github.com/apache/hadoop/pull/7969 slfan1989 commented on PR #7969: URL: https://github.com/apache/hadoop/pull/7969#issuecomment-3310122835 @pjfanning Thanks for the contribution! Merged into trunk. The branch-3.4 should also be taken into consideration. pjfanning opened a new pull request, #7984: URL: https://github.com/apache/hadoop/pull/7984 * HADOOP-19689: bump netty to 4.1.127.Final due to CVE-2025-58057 Signed-off-by: Shilun Fan <slfan1989@apache.org> Update LICENSE-binary <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR ### How was this patch tested? ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #7984: URL: https://github.com/apache/hadoop/pull/7984#issuecomment-3314414435 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 13m 42s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ branch-3.4 Compile Tests _ | | +0 :ok: | mvndep | 2m 19s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 38m 3s | | branch-3.4 passed | | +1 :green_heart: | compile | 19m 9s | | branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 17m 20s | | branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 23m 6s | | branch-3.4 passed | | +1 :green_heart: | javadoc | 8m 58s | | branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 8m 19s | | branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 54m 54s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 38s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 32m 36s | | the patch passed | | +1 :green_heart: | compile | 17m 43s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 17m 43s | | the patch passed | | +1 :green_heart: | compile | 16m 20s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 16m 20s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 18m 39s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 8m 22s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 52s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 51m 4s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 738m 57s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7984/1/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 37s | | The patch does not generate ASF License warnings. | | | | 1050m 3s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.mapred.gridmix.TestGridmixSubmission | | | hadoop.mapred.gridmix.TestLoadJob | | | hadoop.yarn.server.timelineservice.security.TestTimelineAuthFilterForV2 | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7984/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7984 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux df829c2020d7 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | branch-3.4 / 80ba81ead32289cc5ee5aceb75950ca4e5a9c50e | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7984/1/testReport/ | | Max. process+thread count | 3744 (vs. ulimit of 5500) | | modules | C: hadoop-project . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7984/1/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 merged PR #7984: URL: https://github.com/apache/hadoop/pull/7984 slfan1989 commented on PR #7984: URL: https://github.com/apache/hadoop/pull/7984#issuecomment-3322482594 @pjfanning Thanks for the contribution! Merged into branch-3.4.", "created": "2025-09-15T17:45:00.000+0000", "updated": "2025-09-23T05:31:15.000+0000", "derived": {"summary_task": "Summarize this issue: https://www.cve.org/CVERecord?id=CVE-2025-58057 fixed in 4.1.125 but no harm upgrading to latest", "classification_task": "Classify the issue priority and type: https://www.cve.org/CVERecord?id=CVE-2025-58057 fixed in 4.1.125 but no harm upgrading to latest", "qna_task": "Question: What is this issue about?\nAnswer: Bump netty to 4.1.127 due to CVE-2025-58057"}}
{"id": "13628966", "key": "HADOOP-19688", "project": "HADOOP", "summary": "S3A: ITestS3ACommitterMRJob failing on Junit5", "description": "NPE in test200 of ITestS3ACommitterMRJob. Cause is * test dir setup and propagation calls Path.getRoot().toUri() which returns the /home dir * somehow the locatedFileStatus of that path being 1 so a codepath in FileInputFormat NPEs. Fix is in test code, though FileInputFormat has its NPE messages improved to help understand what is going wrong.", "comments": "steveloughran opened a new pull request, #7968: URL: https://github.com/apache/hadoop/pull/7968 Adds extra logging as to what is happening, passes in actual test dir set at class level. ### How was this patch tested? s3 london ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? steveloughran commented on PR #7968: URL: https://github.com/apache/hadoop/pull/7968#issuecomment-3293282933 ``` [INFO] Running org.apache.hadoop.fs.s3a.commit.integration.ITestS3ACommitterMRJob [INFO] Running org.apache.hadoop.fs.s3a.commit.integration.ITestS3ACommitterMRJob [INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 25.97 s steveloughran commented on PR #7968: URL: https://github.com/apache/hadoop/pull/7968#issuecomment-3293285487 fyi @slfan1989 @ahmarsuhail @mukund-thakur one of the final nits of junit5 migration hadoop-yetus commented on PR #7968: URL: https://github.com/apache/hadoop/pull/7968#issuecomment-3294044978 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 34s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 11m 17s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 41m 11s | | trunk passed | | +1 :green_heart: | compile | 15m 54s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 34s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 4m 12s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 0s | | trunk passed | | +1 :green_heart: | javadoc | 1m 44s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 38s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 3m 2s | | trunk passed | | +1 :green_heart: | shadedclient | 36m 28s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 35s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 7s | | the patch passed | | +1 :green_heart: | compile | 15m 15s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 15s | | the patch passed | | +1 :green_heart: | compile | 13m 37s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 37s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 4m 17s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7968/1/artifact/out/results-checkstyle-root.txt) | root: The patch generated 1 new + 45 unchanged - 1 fixed = 46 total (was 46) | | +1 :green_heart: | mvnsite | 1m 58s | | the patch passed | | +1 :green_heart: | javadoc | 1m 40s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 38s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 3m 21s | | the patch passed | | +1 :green_heart: | shadedclient | 36m 11s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 9m 58s | | hadoop-mapreduce-client-core in the patch passed. | | +1 :green_heart: | unit | 3m 47s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 1m 8s | | The patch does not generate ASF License warnings. | | | | 230m 49s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7968/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7968 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 0d73de768dd4 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a86c300286ffb8cd8884923b5d6a0deaf9095d63 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7968/1/testReport/ | | Max. process+thread count | 1567 (vs. ulimit of 5500) | | modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-tools/hadoop-aws U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7968/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on PR #7968: URL: https://github.com/apache/hadoop/pull/7968#issuecomment-3294374360 LGTM steveloughran merged PR #7968: URL: https://github.com/apache/hadoop/pull/7968", "created": "2025-09-15T17:38:39.000+0000", "updated": "2025-09-16T12:05:03.000+0000", "derived": {"summary_task": "Summarize this issue: NPE in test200 of ITestS3ACommitterMRJob. Cause is * test dir setup and propagation calls Path.getRoot().toUri() which returns the /home dir * somehow the locatedFileStatus of that path being 1 so a codepath in FileInputFormat NPEs. Fix is in test code, though FileInputFormat has its NPE messages improved to help understand what is going wrong.", "classification_task": "Classify the issue priority and type: NPE in test200 of ITestS3ACommitterMRJob. Cause is * test dir setup and propagation calls Path.getRoot().toUri() which returns the /home dir * somehow the locatedFileStatus of that path being 1 so a codepath in FileInputFormat NPEs. Fix is in test code, though FileInputFormat has its NPE messages improved to help understand what is going wrong.", "qna_task": "Question: What is this issue about?\nAnswer: S3A: ITestS3ACommitterMRJob failing on Junit5"}}
{"id": "13628928", "key": "HADOOP-19687", "project": "HADOOP", "summary": "Upgrade nimbus-jose-jwt to 10.0.2+ due to CVE-2025-53864", "description": "*CVE-2025-53864:* Connect2id Nimbus JOSE + JWT before 10.0.2 allows a remote attacker to cause a denial of service via a deeply nested JSON object supplied in a JWT claim set, because of uncontrolled recursion. NOTE: this is independent of the Gson 2.11.0 issue because the Connect2id product could have checked the JSON object nesting depth, regardless of what limits (if any) were imposed by Gson. Severity: 6.9 (medium)", "comments": "rohit-kb opened a new pull request, #7965: URL: https://github.com/apache/hadoop/pull/7965 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR 1. Bumping nimbus-jose-jwt to 10.4 due to CVEs 2. com.github.stephenc.jcip:jcip-annotations is being shaded and is no more a transitive dependency from nimbus starting from versions 9.38, so we can add it as an explicit dependency. ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? duplicate of HADOOP-19632 for which there are already PRs rohit-kb commented on PR #7965: URL: https://github.com/apache/hadoop/pull/7965#issuecomment-3302275652 Hi @pjfanning, can we use this patch instead of the original one as I don't see any progress on that? We need this upgrade in the downstream soon. Also, it seems like the original patch hasn't handled the shading of com.github.stephenc.jcip:jcip-annotations in later nimbus versions. Thanks pjfanning commented on PR #7965: URL: https://github.com/apache/hadoop/pull/7965#issuecomment-3302315128 Could you rebase this to force a new CI run? The tests crashed in the last run. pjfanning commented on PR #7965: URL: https://github.com/apache/hadoop/pull/7965#issuecomment-3302343034 Could you change the name of the PR and the git commit to use [HADOOP-19632](https://issues.apache.org/jira/browse/HADOOP-19632)?", "created": "2025-09-15T11:29:00.000+0000", "updated": "2025-09-17T10:25:28.000+0000", "derived": {"summary_task": "Summarize this issue: *CVE-2025-53864:* Connect2id Nimbus JOSE + JWT before 10.0.2 allows a remote attacker to cause a denial of service via a deeply nested JSON object supplied in a JWT claim set, because of uncontrolled recursion. NOTE: this is independent of the Gson 2.11.0 issue because the Connect2id product could have checked the JSON object nesting depth, regardless of what limits (if any) were imposed by Gson. ", "classification_task": "Classify the issue priority and type: *CVE-2025-53864:* Connect2id Nimbus JOSE + JWT before 10.0.2 allows a remote attacker to cause a denial of service via a deeply nested JSON object supplied in a JWT claim set, because of uncontrolled recursion. NOTE: this is independent of the Gson 2.11.0 issue because the Connect2id product could have checked the JSON object nesting depth, regardless of what limits (if any) were imposed by Gson. ", "qna_task": "Question: What is this issue about?\nAnswer: Upgrade nimbus-jose-jwt to 10.0.2+ due to CVE-2025-53864"}}
{"id": "13628797", "key": "HADOOP-19685", "project": "HADOOP", "summary": "Clover breaks on double semicolon", "description": "Building with {{-Pclover}} fails with {code} [INFO] Instrumentation error com.atlassian.clover.api.CloverException: /home/jenkins/hadoop/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/ITestS3APutIfMatchAndIfNoneMatch.java:43:43:unexpected token: ; ... Failed to execute goal org.openclover:clover-maven-plugin:4.4.1:setup (clover-setup) on project hadoop-aws: Clover has failed to instrument the source files in the [/home/jenkins/hadoop/hadoop-tools/hadoop-aws/target/clover/src-test-instrumented] directory -> [Help 1] org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.openclover:clover-maven-plugin:4.4.1:setup (clover-setup) on project hadoop-aws: Clover has failed to instrument the source files in the [/home/jenkins/hadoop/hadoop-tools/hadoop-aws/target/clover/src-test-instrumented] directory {code} It doesn't seem to like a double semicolon in ITestS3APutIfMatchAndIfNoneMatch.java that was added in HADOOP-19256.", "comments": "MikaelSmith opened a new pull request, #7956: URL: https://github.com/apache/hadoop/pull/7956 ### Description of PR Removes the extra semicolon after an import that causes `-Pclover` to fail with com.atlassian.clover.api.CloverException: hadoop/hadoop-tools/ hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/ ITestS3APutIfMatchAndIfNoneMatch.java:43:43:unexpected token: ;` ### How was this patch tested? ``` mvn -e -Pclover install -DskipTests -DskipShade --projects 'hadoop-tools/hadoop-aws' ``` ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #7956: URL: https://github.com/apache/hadoop/pull/7956#issuecomment-3287077606 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 15m 13s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 40s | | trunk passed | | +1 :green_heart: | compile | 0m 45s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | +1 :green_heart: | javadoc | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 36s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 10s | | trunk passed | | +1 :green_heart: | shadedclient | 35m 27s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 37s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 37s | | the patch passed | | +1 :green_heart: | compile | 0m 28s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 28s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 22s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 29s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 34m 38s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 21s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 39s | | The patch does not generate ASF License warnings. | | | | 140m 9s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7956/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7956 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux d1403c764b02 5.15.0-152-generic #162-Ubuntu SMP Wed Jul 23 09:48:42 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 9fc528eeb6112dd9b3209b648fb33da720214dff | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7956/1/testReport/ | | Max. process+thread count | 709 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7956/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. cnauroth closed pull request #7956: HADOOP-19685. Fix double semicolon breaking clover URL: https://github.com/apache/hadoop/pull/7956 cnauroth commented on PR #7956: URL: https://github.com/apache/hadoop/pull/7956#issuecomment-3287154332 I committed this to trunk and branch-3.4. Thank you for the patch @MikaelSmith !", "created": "2025-09-12T20:38:19.000+0000", "updated": "2025-09-12T23:48:23.000+0000", "derived": {"summary_task": "Summarize this issue: Building with {{-Pclover}} fails with {code} [INFO] Instrumentation error com.atlassian.clover.api.CloverException: /home/jenkins/hadoop/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/ITestS3APutIfMatchAndIfNoneMatch.java:43:43:unexpected token: ; ... Failed to execute goal org.openclover:clover-maven-plugin:4.4.1:setup (clover-setup) on project hadoop-aws: Clover has failed t", "classification_task": "Classify the issue priority and type: Building with {{-Pclover}} fails with {code} [INFO] Instrumentation error com.atlassian.clover.api.CloverException: /home/jenkins/hadoop/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/ITestS3APutIfMatchAndIfNoneMatch.java:43:43:unexpected token: ; ... Failed to execute goal org.openclover:clover-maven-plugin:4.4.1:setup (clover-setup) on project hadoop-aws: Clover has failed t", "qna_task": "Question: What is this issue about?\nAnswer: Clover breaks on double semicolon"}}
{"id": "13628581", "key": "HADOOP-19684", "project": "HADOOP", "summary": "Add JDK 21 to Ubuntu 20.04 docker development images", "description": "We want to support JDK21, we better have it available in the development image for testing.", "comments": "stoty opened a new pull request, #7947: URL: https://github.com/apache/hadoop/pull/7947 ### Description of PR Add JDK 21 to Ubuntu 20.04 and 24.04 docker development images ### How was this patch tested? Built the default image locally and started JDK21. ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #7947: URL: https://github.com/apache/hadoop/pull/7947#issuecomment-3275227945 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 1m 32s | | Docker failed to build run-specific yetus/hadoop:tp-5188}. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7947 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7947/1/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. pan3793 commented on code in PR #7947: URL: https://github.com/apache/hadoop/pull/7947#discussion_r2340621524 ########## dev-support/docker/pkg-resolver/packages.json: ########## @@ -268,17 +268,20 @@ ], \"ubuntu:focal\": [ \"temurin-24-jdk\", + \"temurin-21-jdk\", Review Comment: the ubuntu official apt repo provides `openjdk-21-jdk`, it's unnecessary to install from 3rd party. let's use the official one and put it at the end of the list. pan3793 commented on PR #7947: URL: https://github.com/apache/hadoop/pull/7947#issuecomment-3280408907 The Jenkins failure should be fixed by https://github.com/apache/hadoop/pull/7938, @slfan1989 can you help merging that to unblock this patch? stoty commented on code in PR #7947: URL: https://github.com/apache/hadoop/pull/7947#discussion_r2340811930 ########## dev-support/docker/pkg-resolver/packages.json: ########## @@ -268,17 +268,20 @@ ], \"ubuntu:focal\": [ \"temurin-24-jdk\", + \"temurin-21-jdk\", Review Comment: Thanks. Done @pan3793 . hadoop-yetus commented on PR #7947: URL: https://github.com/apache/hadoop/pull/7947#issuecomment-3280638248 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 1m 21s | | Docker failed to build run-specific yetus/hadoop:tp-1035}. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7947 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7947/2/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on PR #7947: URL: https://github.com/apache/hadoop/pull/7947#issuecomment-3282893605 > The Jenkins failure should be fixed by #7938, @slfan1989 can you help merge that to unblock this patch? @stoty I\u2019ve already merged #7938 into the trunk branch, so we can move forward with this PR. stoty commented on PR #7947: URL: https://github.com/apache/hadoop/pull/7947#issuecomment-3283557198 restarted CI hadoop-yetus commented on PR #7947: URL: https://github.com/apache/hadoop/pull/7947#issuecomment-3283756503 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 25m 28s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 48s | | Maven dependency ordering for branch | | +1 :green_heart: | shadedclient | 46m 30s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 45s | | Maven dependency ordering for patch | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 36m 2s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 56s | | The patch does not generate ASF License warnings. | | | | 111m 23s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7947/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7947 | | Optional Tests | dupname asflicense codespell detsecrets jsonlint | | uname | Linux 68e70612efec 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 4ed7fc2da756cc4332d3f9ecc8cce291d094b50d | | Max. process+thread count | 708 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7947/3/console | | versions | git=2.43.7 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on PR #7947: URL: https://github.com/apache/hadoop/pull/7947#issuecomment-3283820061 The CI results meet expectations. I will proceed with merging the change shortly. hadoop-yetus commented on PR #7947: URL: https://github.com/apache/hadoop/pull/7947#issuecomment-3284002090 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 20m 27s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 0m 35s | | Maven dependency ordering for branch | | +1 :green_heart: | shadedclient | 26m 20s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 30s | | Maven dependency ordering for patch | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 25m 38s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 74m 57s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7947/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7947 | | Optional Tests | dupname asflicense codespell detsecrets jsonlint | | uname | Linux 26f820c8794f 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 4ed7fc2da756cc4332d3f9ecc8cce291d094b50d | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7947/3/console | | versions | git=2.30.2 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7947: URL: https://github.com/apache/hadoop/pull/7947#issuecomment-3284223003 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 33s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 0m 34s | | Maven dependency ordering for branch | | +1 :green_heart: | shadedclient | 34m 46s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 32s | | Maven dependency ordering for patch | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 33m 55s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 71m 51s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7947/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7947 | | Optional Tests | dupname asflicense codespell detsecrets jsonlint | | uname | Linux cedf7863bb64 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 4ed7fc2da756cc4332d3f9ecc8cce291d094b50d | | Max. process+thread count | 567 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7947/3/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 merged PR #7947: URL: https://github.com/apache/hadoop/pull/7947 slfan1989 commented on PR #7947: URL: https://github.com/apache/hadoop/pull/7947#issuecomment-3289299477 @stoty Thanks for the contribution! @pan3793 Thanks for the review!", "created": "2025-09-10T14:00:14.000+0000", "updated": "2025-09-14T07:14:28.000+0000", "derived": {"summary_task": "Summarize this issue: We want to support JDK21, we better have it available in the development image for testing.", "classification_task": "Classify the issue priority and type: We want to support JDK21, we better have it available in the development image for testing.", "qna_task": "Question: What is this issue about?\nAnswer: Add JDK 21 to Ubuntu 20.04 docker development images"}}
{"id": "13628430", "key": "HADOOP-19682", "project": "HADOOP", "summary": "Fix incorrect link from current3 of hadoop-site", "description": "Fix hadoop site incorrect link which reported by https://lists.apache.org/thread/ozvkh0x7qdr8d3vr0tbm5g7jx1jprbct.", "comments": "", "created": "2025-09-09T03:15:33.000+0000", "updated": "2025-09-09T03:57:52.000+0000", "derived": {"summary_task": "Summarize this issue: Fix hadoop site incorrect link which reported by https://lists.apache.org/thread/ozvkh0x7qdr8d3vr0tbm5g7jx1jprbct.", "classification_task": "Classify the issue priority and type: Fix hadoop site incorrect link which reported by https://lists.apache.org/thread/ozvkh0x7qdr8d3vr0tbm5g7jx1jprbct.", "qna_task": "Question: What is this issue about?\nAnswer: Fix incorrect link from current3 of hadoop-site"}}
{"id": "13628348", "key": "HADOOP-19681", "project": "HADOOP", "summary": "Fix S3A failing to initialize S3 buckets having namespace with dot followed by number", "description": "S3A fails to initialize when S3 bucket namespace is having dot followed by a number. {*}Specific Problem{*}: URI parsing fails when S3 bucket names contain a dot followed by a number (like {{{}bucket-v1.1-us-east-1{}}}). Java's URI.getHost() method incorrectly interprets the dot-number pattern as a port specification, causing it to return null. {{}} {code:java} hadoop dfs -ls s3a://bucket-v1.1-us-east-1/ WARNING: Use of this script to execute dfs is deprecated. WARNING: Attempting to execute replacement \"hdfs dfs\" instead. 2025-09-08 06:13:06,670 WARN fs.FileSystem: Failed to initialize filesystem s3://bucket-v1.1-us-east-1/: java.lang.IllegalArgumentException: bucket is null/empty -ls: bucket is null/empty{code} {*}Please Note{*}: Although there has been discussion on not allowing S3 buckets with such a namespace ([https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/]) , Amazon S3 still allows you to create a bucket with such a namespace.", "comments": "shameersss1 opened a new pull request, #7942: URL: https://github.com/apache/hadoop/pull/7942 ### Description of PR S3A fails to initialize when S3 bucket namespace is having dot followed by a number. Specific Problem: URI parsing fails when S3 bucket names contain a dot followed by a number (like bucket-v1.1-us-east-1). Java's URI.getHost() method incorrectly interprets the dot-number pattern as a port specification, causing it to return null. ### How was this patch tested? Tested in us-east-1 with bucket having namespace with dot followed by a number. ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? shameersss1 commented on PR #7942: URL: https://github.com/apache/hadoop/pull/7942#issuecomment-3267057573 Test `ITestBucketTool,ITestS3ACommitterMRJob` are failing even without the change. shameersss1 commented on PR #7942: URL: https://github.com/apache/hadoop/pull/7942#issuecomment-3267074576 @steveloughran : Could you please review the changes. hadoop-yetus commented on PR #7942: URL: https://github.com/apache/hadoop/pull/7942#issuecomment-3267937541 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 15m 0s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 25s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 32m 58s | | trunk passed | | +1 :green_heart: | compile | 15m 48s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 57s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 4m 16s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 38s | | trunk passed | | +1 :green_heart: | javadoc | 2m 10s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 41s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 3m 58s | | trunk passed | | +1 :green_heart: | shadedclient | 36m 15s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 34s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 30s | | the patch passed | | +1 :green_heart: | compile | 15m 6s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 6s | | the patch passed | | +1 :green_heart: | compile | 13m 48s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 48s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 4m 12s | | the patch passed | | +1 :green_heart: | mvnsite | 2m 38s | | the patch passed | | +1 :green_heart: | javadoc | 2m 10s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 46s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 4m 16s | | the patch passed | | +1 :green_heart: | shadedclient | 36m 59s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 22m 55s | [/patch-unit-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/1/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 3m 41s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 1m 9s | | The patch does not generate ASF License warnings. | | | | 253m 43s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.viewfs.TestViewFsWithAuthorityLocalFs | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7942 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 3434362c830f 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 82914829acf34ecd0b05c57af673c1e4095acb30 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/1/testReport/ | | Max. process+thread count | 3056 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7942: URL: https://github.com/apache/hadoop/pull/7942#issuecomment-3269845775 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 40s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 52s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 32m 34s | | trunk passed | | +1 :green_heart: | compile | 15m 48s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 51s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 4m 13s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 43s | | trunk passed | | +1 :green_heart: | javadoc | 2m 14s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 50s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 3m 56s | | trunk passed | | +1 :green_heart: | shadedclient | 36m 5s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 33s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 30s | | the patch passed | | +1 :green_heart: | compile | 15m 13s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 13s | | the patch passed | | +1 :green_heart: | compile | 13m 48s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 48s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 4m 6s | | the patch passed | | +1 :green_heart: | mvnsite | 2m 38s | | the patch passed | | +1 :green_heart: | javadoc | 2m 8s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 47s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 4m 21s | | the patch passed | | +1 :green_heart: | shadedclient | 36m 26s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 22m 53s | | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 3m 49s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 1m 9s | | The patch does not generate ASF License warnings. | | | | 237m 39s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7942 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux b5e9b0d84482 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / ea1308a4c1fdf443562e752e09d43ae0a7f5cd8b | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/2/testReport/ | | Max. process+thread count | 1271 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. steveloughran commented on code in PR #7942: URL: https://github.com/apache/hadoop/pull/7942#discussion_r2348701684 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AConfiguration.java: ########## @@ -594,13 +595,28 @@ private static <T> T getField(Object target, Class<T> fieldType, public void testConfOptionPropagationToFS() throws Exception { Configuration config = new Configuration(); String testFSName = config.getTrimmed(TEST_FS_S3A_NAME, \"\"); - String bucket = new URI(testFSName).getHost(); + URI uri = new URI(testFSName); + String bucket = uri.getHost(); + if (bucket == null) { + bucket = uri.getAuthority(); + } setBucketOption(config, bucket, \"propagation\", \"propagated\"); fs = S3ATestUtils.createTestFileSystem(config); Configuration updated = fs.getConf(); assertOptionEquals(updated, \"fs.s3a.propagation\", \"propagated\"); } + @Test + public void testBucketNameWithDotAndNumber() throws Exception { + Configuration config = new Configuration(); + Path path = new Path(\"s3a://test-bucket-v1.1\"); + try (FileSystem fs = path.getFileSystem(config)) { + assertThat(fs instanceof S3AFileSystem) Review Comment: use whatever assertj assertion is about instanceof, so you get a better error. raphaelazzolini commented on PR #7942: URL: https://github.com/apache/hadoop/pull/7942#issuecomment-3293303278 > Oh, this complicates things. I've been happily treating all issues related to buckets with . in them as WONTFIX. Storediag tells people off too. @steveloughran, in HADOOP-17241, you referenced this announcement as one of the reasons to not support this buckets with dot in the name: https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/ However, AWS have since revised their stance. AWS has confirmed they will continue supporting buckets with dots in their names through virtual hosted-style URLs due to customer feedback and compatibility requirements. > We have also heard feedback from customers that virtual hosted-style URLs should support buckets that have dots in their names for compatibility reasons, so we\u2019re working on developing that support. So I guess it makes sense to add support for it in S3A. hadoop-yetus commented on PR #7942: URL: https://github.com/apache/hadoop/pull/7942#issuecomment-3298148674 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 59s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 40m 44s | | trunk passed | | +1 :green_heart: | compile | 16m 1s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 53s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 4m 23s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 45s | | trunk passed | | +1 :green_heart: | javadoc | 2m 15s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 47s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 4m 0s | | trunk passed | | +1 :green_heart: | shadedclient | 42m 20s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 35s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 32s | | the patch passed | | +1 :green_heart: | compile | 15m 39s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 39s | | the patch passed | | +1 :green_heart: | compile | 14m 49s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 14m 49s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/3/artifact/out/blanks-eol.txt) | The patch has 3 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 4m 12s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/3/artifact/out/results-checkstyle-root.txt) | root: The patch generated 1 new + 63 unchanged - 0 fixed = 64 total (was 63) | | +1 :green_heart: | mvnsite | 2m 38s | | the patch passed | | +1 :green_heart: | javadoc | 2m 29s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 46s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 4m 17s | | the patch passed | | +1 :green_heart: | shadedclient | 41m 35s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 23m 39s | | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 3m 44s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 1m 8s | | The patch does not generate ASF License warnings. | | | | 261m 30s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7942 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 8e602b48d6d4 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 5014c326331ebcc9e8cb30cf2632774733c6ce56 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/3/testReport/ | | Max. process+thread count | 1302 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/3/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7942: URL: https://github.com/apache/hadoop/pull/7942#issuecomment-3299506183 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 11m 7s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 42m 17s | | trunk passed | | +1 :green_heart: | compile | 16m 8s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 59s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 4m 15s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 42s | | trunk passed | | +1 :green_heart: | javadoc | 2m 13s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 48s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 4m 3s | | trunk passed | | +1 :green_heart: | shadedclient | 36m 43s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 34s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 31s | | the patch passed | | +1 :green_heart: | compile | 15m 6s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 6s | | the patch passed | | +1 :green_heart: | compile | 13m 49s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 49s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/4/artifact/out/blanks-eol.txt) | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | +1 :green_heart: | checkstyle | 4m 14s | | the patch passed | | +1 :green_heart: | mvnsite | 2m 38s | | the patch passed | | +1 :green_heart: | javadoc | 2m 8s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 47s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 4m 17s | | the patch passed | | +1 :green_heart: | shadedclient | 36m 33s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 23m 4s | | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 3m 45s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 1m 9s | | The patch does not generate ASF License warnings. | | | | 251m 11s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7942 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 2d2f97096065 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f86ae95b09e52b9c71c1d9d937b060aa9409c096 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/4/testReport/ | | Max. process+thread count | 1270 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7942/4/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. shameersss1 commented on PR #7942: URL: https://github.com/apache/hadoop/pull/7942#issuecomment-3303302768 Thanks @steveloughran for the review. I have addressed your comments. steveloughran commented on PR #7942: URL: https://github.com/apache/hadoop/pull/7942#issuecomment-3329432114 @raphaelazzolini > AWS has confirmed they will continue supporting buckets with dots in their names through virtual hosted-style URLs due to customer feedback and compatibility requirements. OK. main issue is that there may be existing code which cares about hostname and may not look at FQDN for differencing names. I can't think of any right now -bucket names which don't map to valid hostnames are more a pain point steveloughran commented on code in PR #7942: URL: https://github.com/apache/hadoop/pull/7942#discussion_r2376222445 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -573,8 +573,11 @@ private static void addDeprecatedKeys() { */ public void initialize(URI name, Configuration originalConf) throws IOException { - // get the host; this is guaranteed to be non-null, non-empty + // get the host; fallback to authority if getHost() returns null bucket = name.getHost(); + if (bucket == null) { Review Comment: pull this out, stick it in `S3AUtils`, add unit tests that now try to break things. Use everywhere ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/AbstractFileSystem.java: ########## @@ -335,7 +335,7 @@ private URI getUri(URI uri, String supportedScheme, int port = uri.getPort(); port = (port == -1 ? defaultPort : port); if (port == -1) { // no port supplied and default port is not specified - return new URI(supportedScheme, authority, \"/\", null); + return URI.create(supportedScheme + \"://\" + authority + \"/\"); Review Comment: why this change? ########## hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/index.md: ########## @@ -31,6 +31,12 @@ before 2021. Consult [S3A and Directory Markers](directory_markers.html) for full details. +### <a name=\"bucket-name-compatibility\"></a> S3 Bucket Name Compatibility + +This release adds support for S3 bucket names containing dots followed by numbers +(e.g., `my-bucket-v1.1`, `data-store.v2.3`). Previous versions of the Hadoop S3A +client failed to initialize such buckets due to URI parsing limitations. + Review Comment: * highlight that per-bucket settings do not work for dotted buckets (they don't, do they?), so the ability to use them is still very much downgraded. * Explain that AWS do not recommend dotted buckets for anything other than web site serving * highlight that path style access is needed to access (correct? never tried)", "created": "2025-09-08T08:51:08.000+0000", "updated": "2025-09-24T15:46:51.000+0000", "derived": {"summary_task": "Summarize this issue: S3A fails to initialize when S3 bucket namespace is having dot followed by a number. {*}Specific Problem{*}: URI parsing fails when S3 bucket names contain a dot followed by a number (like {{{}bucket-v1.1-us-east-1{}}}). Java's URI.getHost() method incorrectly interprets the dot-number pattern as a port specification, causing it to return null. {{}} {code:java} hadoop dfs -ls s3a://bucket-v1.1-us-", "classification_task": "Classify the issue priority and type: S3A fails to initialize when S3 bucket namespace is having dot followed by a number. {*}Specific Problem{*}: URI parsing fails when S3 bucket names contain a dot followed by a number (like {{{}bucket-v1.1-us-east-1{}}}). Java's URI.getHost() method incorrectly interprets the dot-number pattern as a port specification, causing it to return null. {{}} {code:java} hadoop dfs -ls s3a://bucket-v1.1-us-", "qna_task": "Question: What is this issue about?\nAnswer: Fix S3A failing to initialize S3 buckets having namespace with dot followed by number"}}
{"id": "13628326", "key": "HADOOP-19680", "project": "HADOOP", "summary": "Update non-thirdparty Guava version to 32.0.1", "description": "Guava has been already updated to 32.0.1 in hadoop-thirdparty. However, Hadoop also dependency manages the non-thirdparty guava version (coming from dependencies), which is still at 27.0-jre , showing up on static scanners. Sync the non-thirdparty Guava version to the thirdparty one.", "comments": "stoty opened a new pull request, #7940: URL: https://github.com/apache/hadoop/pull/7940 same as the current thirdparty Guava version ### Description of PR Guava has been already updated to 32.0.1 in hadoop-thirdparty. However, Hadoop also dependency manages the non-thirdparty guava version (coming from dependencies), which is still at 27.0-jre , showing up on static scanners. Sync the non-thirdparty Guava version to the thirdparty one. ### How was this patch tested? Test suite in CI (on this PR) ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #7940: URL: https://github.com/apache/hadoop/pull/7940#issuecomment-3264930118 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 57s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 45m 8s | | trunk passed | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 0m 26s | | trunk passed | | +1 :green_heart: | javadoc | 0m 28s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 22s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 87m 32s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 14s | | the patch passed | | +1 :green_heart: | compile | 0m 13s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 13s | | the patch passed | | +1 :green_heart: | compile | 0m 13s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 13s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 15s | | the patch passed | | +1 :green_heart: | javadoc | 0m 14s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 14s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 42m 13s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 17s | | hadoop-project in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 134m 9s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7940/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7940 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux fe8aa1f6455e 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 8794a7e5b6cf955551d93f3a15effbd83d5174b7 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7940/1/testReport/ | | Max. process+thread count | 527 (vs. ulimit of 5500) | | modules | C: hadoop-project U: hadoop-project | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7940/1/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. pan3793 commented on PR #7940: URL: https://github.com/apache/hadoop/pull/7940#issuecomment-3280459780 cc @cnauroth slfan1989 commented on PR #7940: URL: https://github.com/apache/hadoop/pull/7940#issuecomment-3289303390 > +1 Thank you @stoty . I'm good with this change. Ideally, I would like to see at least one more committer +1, just in case there are some downstream impacts I haven't thought of. > > CC: @steveloughran , @ayushtkn , @mukund-thakur @cnauroth Thanks for the review! I think we still need to carefully consider this change and should wait for Steve's confirmation before making a decision. cc: @steveloughran stoty commented on PR #7940: URL: https://github.com/apache/hadoop/pull/7940#issuecomment-3289335562 For what it's worth: I've fought a LOT with guava versions in the last six years, but I haven't seen any issue (apart from Google adding new annotation libraries which throw off some shading tests) when upgrading from 27 to a newer version. steveloughran commented on PR #7940: URL: https://github.com/apache/hadoop/pull/7940#issuecomment-3291722519 I've been away. let's do it an update in release notes that you can change the version without breaking any hadoop code. stoty commented on PR #7940: URL: https://github.com/apache/hadoop/pull/7940#issuecomment-3291915610 > I've been away. let's do it an update in release notes that you can change the version without breaking any hadoop code. Thanks @steveloughran . I'm not sure I understand your comment. I have added a release note that explains the change to the JIRA. steveloughran merged PR #7940: URL: https://github.com/apache/hadoop/pull/7940 steveloughran commented on PR #7940: URL: https://github.com/apache/hadoop/pull/7940#issuecomment-3293320945 thanks...your release note is good, added something in the commit too. can you do a backport PR to branch-3.4; I think we should be looking at a \"dependency update\" release there with minimal actual code changes slfan1989 merged PR #7977: URL: https://github.com/apache/hadoop/pull/7977 slfan1989 commented on PR #7977: URL: https://github.com/apache/hadoop/pull/7977#issuecomment-3310107461 @stoty Sorry for the late reply, thanks for the contribution!", "created": "2025-09-08T04:59:06.000+0000", "updated": "2025-09-23T17:30:25.000+0000", "derived": {"summary_task": "Summarize this issue: Guava has been already updated to 32.0.1 in hadoop-thirdparty. However, Hadoop also dependency manages the non-thirdparty guava version (coming from dependencies), which is still at 27.0-jre , showing up on static scanners. Sync the non-thirdparty Guava version to the thirdparty one.", "classification_task": "Classify the issue priority and type: Guava has been already updated to 32.0.1 in hadoop-thirdparty. However, Hadoop also dependency manages the non-thirdparty guava version (coming from dependencies), which is still at 27.0-jre , showing up on static scanners. Sync the non-thirdparty Guava version to the thirdparty one.", "qna_task": "Question: What is this issue about?\nAnswer: Update non-thirdparty Guava version to 32.0.1"}}
{"id": "13628261", "key": "HADOOP-19679", "project": "HADOOP", "summary": "Maven site task fails with Java 17", "description": "If I try to build site with Java 17, I get an IllegalArgumentException from analyze-report {code} $ mvn -e -Dmaven.javadoc.skip=true -Dhbase.profile=2.0 -DskipTests -DskipITs clean install site:site ... [ERROR] Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.9.1:site (default-cli) on project hadoop-annotations: Error generating maven-dependency-plugin:3.0.2:analyze-report report: IllegalArgumentException -> [Help 1] org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.9.1:site (default-cli) on project hadoop-annotations: Error generating maven-dependency-plugin:3.0.2:analyze-report report at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:347) at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175) at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76) at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163) at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73) at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53) at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118) at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261) at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173) at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101) at org.apache.maven.cli.MavenCli.execute (MavenCli.java:910) at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283) at org.apache.maven.cli.MavenCli.main (MavenCli.java:206) at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method) at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77) at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke (Method.java:569) at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283) at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226) at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407) at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348) Caused by: org.apache.maven.plugin.MojoExecutionException: Error generating maven-dependency-plugin:3.0.2:analyze-report report at org.apache.maven.plugins.site.render.SiteMojo.execute (SiteMojo.java:153) at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126) at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:342) at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175) at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76) at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163) at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73) at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53) at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118) at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261) at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173) at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101) at org.apache.maven.cli.MavenCli.execute (MavenCli.java:910) at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283) at org.apache.maven.cli.MavenCli.main (MavenCli.java:206) at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method) at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77) at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke (Method.java:569) at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283) at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226) at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407) at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348) Caused by: org.apache.maven.doxia.siterenderer.RendererException: Error generating maven-dependency-plugin:3.0.2:analyze-report report at org.apache.maven.plugins.site.render.ReportDocumentRenderer.renderDocument (ReportDocumentRenderer.java:247) at org.apache.maven.doxia.siterenderer.DefaultSiteRenderer.render (DefaultSiteRenderer.java:349) at org.apache.maven.plugins.site.render.SiteMojo.renderLocale (SiteMojo.java:194) at org.apache.maven.plugins.site.render.SiteMojo.execute (SiteMojo.java:143) at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126) at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:342) at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175) at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76) at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163) at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73) at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53) at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118) at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261) at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173) at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101) at org.apache.maven.cli.MavenCli.execute (MavenCli.java:910) at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283) at org.apache.maven.cli.MavenCli.main (MavenCli.java:206) at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method) at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77) at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke (Method.java:569) at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283) at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226) at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407) at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348) Caused by: java.lang.IllegalArgumentException at org.objectweb.asm.ClassReader.<init> (Unknown Source) at org.objectweb.asm.ClassReader.<init> (Unknown Source) at org.objectweb.asm.ClassReader.<init> (Unknown Source) at org.apache.maven.shared.dependency.analyzer.asm.DependencyClassFileVisitor.visitClass (DependencyClassFileVisitor.java:65) at org.apache.maven.shared.dependency.analyzer.ClassFileVisitorUtils.visitClass (ClassFileVisitorUtils.java:163) at org.apache.maven.shared.dependency.analyzer.ClassFileVisitorUtils.acceptDirectory (ClassFileVisitorUtils.java:143) at org.apache.maven.shared.dependency.analyzer.ClassFileVisitorUtils.accept (ClassFileVisitorUtils.java:71) at org.apache.maven.shared.dependency.analyzer.asm.ASMDependencyAnalyzer.analyze (ASMDependencyAnalyzer.java:50) at org.apache.maven.shared.dependency.analyzer.DefaultProjectDependencyAnalyzer.buildDependencyClasses (DefaultProjectDependencyAnalyzer.java:211) at org.apache.maven.shared.dependency.analyzer.DefaultProjectDependencyAnalyzer.buildDependencyClasses (DefaultProjectDependencyAnalyzer.java:198) at org.apache.maven.shared.dependency.analyzer.DefaultProjectDependencyAnalyzer.analyze (DefaultProjectDependencyAnalyzer.java:74) at org.apache.maven.plugins.dependency.analyze.AnalyzeReportMojo.executeReport (AnalyzeReportMojo.java:138) at org.apache.maven.reporting.AbstractMavenReport.generate (AbstractMavenReport.java:255) at org.apache.maven.plugins.site.render.ReportDocumentRenderer.renderDocument (ReportDocumentRenderer.java:226) at org.apache.maven.doxia.siterenderer.DefaultSiteRenderer.render (DefaultSiteRenderer.java:349) at org.apache.maven.plugins.site.render.SiteMojo.renderLocale (SiteMojo.java:194) at org.apache.maven.plugins.site.render.SiteMojo.execute (SiteMojo.java:143) at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126) at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:342) at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175) at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76) at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163) at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73) at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53) at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118) at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261) at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173) at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101) at org.apache.maven.cli.MavenCli.execute (MavenCli.java:910) at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283) at org.apache.maven.cli.MavenCli.main (MavenCli.java:206) at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method) at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77) at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke (Method.java:569) at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283) at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226) at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407) at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348) [ERROR] [ERROR] Re-run Maven using the -X switch to enable full debug logging. [ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles: [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException [ERROR] [ERROR] After correcting the problems, you can resume the build with the command [ERROR] mvn <args> -rf :hadoop-annotations {code} Updating to the latest maven-dependency-plugin version (3.8.1) fixes it for me.", "comments": "MikaelSmith opened a new pull request, #7936: URL: https://github.com/apache/hadoop/pull/7936 ### Description of PR Updates maven-dependency-plugin to 3.8.1 to fix an IllegalArgumentException when running the `site:site` Maven task with Java 17. ### How was this patch tested? Ran `mvn -e -Dmaven.javadoc.skip=true -Dhbase.profile=2.0 -DskipTests -DskipITs clean install site:site` locally with openjdk-17 and it succeeded. ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #7936: URL: https://github.com/apache/hadoop/pull/7936#issuecomment-3262110832 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 26m 53s | | trunk passed | | +1 :green_heart: | compile | 9m 34s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 8m 19s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 15m 46s | | trunk passed | | +1 :green_heart: | javadoc | 6m 7s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 5m 24s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 94m 20s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 19m 24s | | the patch passed | | +1 :green_heart: | compile | 9m 3s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 9m 3s | | the patch passed | | +1 :green_heart: | compile | 8m 20s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 8m 20s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 9m 59s | | the patch passed | | +1 :green_heart: | javadoc | 5m 5s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 5m 21s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 42m 54s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 663m 19s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7936/1/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 6s | | The patch does not generate ASF License warnings. | | | | 840m 24s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.mapreduce.v2.TestUberAM | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | | hadoop.yarn.server.router.subcluster.capacity.TestYarnFederationWithCapacityScheduler | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7936/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7936 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux 33f1385abb00 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / e89a211da8c19aed6f68dbd9ea4418f6e657d615 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7936/1/testReport/ | | Max. process+thread count | 3913 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7936/1/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. szetszwo merged PR #7936: URL: https://github.com/apache/hadoop/pull/7936 The pull request is now merged. Thanks, [~MikaelSmith]! Thanks! PR was merged against trunk, so I think current Fix Version would be 3.5.0. You are right -- updated it to 3.5.0.", "created": "2025-09-05T22:47:46.000+0000", "updated": "2025-09-08T16:27:15.000+0000", "derived": {"summary_task": "Summarize this issue: If I try to build site with Java 17, I get an IllegalArgumentException from analyze-report {code} $ mvn -e -Dmaven.javadoc.skip=true -Dhbase.profile=2.0 -DskipTests -DskipITs clean install site:site ... [ERROR] Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.9.1:site (default-cli) on project hadoop-annotations: Error generating maven-dependency-plugin:3.0.2:analyze-report repor", "classification_task": "Classify the issue priority and type: If I try to build site with Java 17, I get an IllegalArgumentException from analyze-report {code} $ mvn -e -Dmaven.javadoc.skip=true -Dhbase.profile=2.0 -DskipTests -DskipITs clean install site:site ... [ERROR] Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.9.1:site (default-cli) on project hadoop-annotations: Error generating maven-dependency-plugin:3.0.2:analyze-report repor", "qna_task": "Question: What is this issue about?\nAnswer: Maven site task fails with Java 17"}}
{"id": "13628243", "key": "HADOOP-19678", "project": "HADOOP", "summary": "[JDK17] Remove powermock dependency", "description": "The powermock dependency is specified in - hadoop-cloud-storage-project/hadoop-huaweicloud/pom.xml - hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/pom.xml but not used anywhere. We should remove it.", "comments": "Will include this in HADOOP-19677. Reopen for removing unused powermock dependencies. szetszwo opened a new pull request, #7939: URL: https://github.com/apache/hadoop/pull/7939 ### Description of PR HADOOP-19678 The powermock dependency is specified in - hadoop-cloud-storage-project/hadoop-huaweicloud/pom.xml - hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/pom.xml but not used anywhere. We should remove it. ### How was this patch tested? By existing tests. ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [NA] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [NA ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [NA ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #7939: URL: https://github.com/apache/hadoop/pull/7939#issuecomment-3264013057 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 14m 3s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 24s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 32m 13s | | trunk passed | | +1 :green_heart: | compile | 15m 39s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 43s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 1m 31s | | trunk passed | | +1 :green_heart: | javadoc | 1m 33s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 26s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 111m 0s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 33s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 40s | | the patch passed | | +1 :green_heart: | compile | 15m 3s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 3s | | the patch passed | | +1 :green_heart: | compile | 13m 40s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 40s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 1m 30s | | the patch passed | | +1 :green_heart: | javadoc | 1m 28s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 41m 41s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 1m 4s | | hadoop-yarn-server-timelineservice-documentstore in the patch passed. | | +1 :green_heart: | unit | 0m 44s | | hadoop-huaweicloud in the patch passed. | | +1 :green_heart: | asflicense | 1m 6s | | The patch does not generate ASF License warnings. | | | | 201m 38s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7939/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7939 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux 9f779bd63474 5.15.0-152-generic #162-Ubuntu SMP Wed Jul 23 09:48:42 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / dd72444841a0ec35affd2cf1e058030ef5fe99d2 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7939/1/testReport/ | | Max. process+thread count | 708 (vs. ulimit of 5500) | | modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore hadoop-cloud-storage-project/hadoop-huaweicloud U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7939/1/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 merged PR #7939: URL: https://github.com/apache/hadoop/pull/7939 szetszwo commented on PR #7939: URL: https://github.com/apache/hadoop/pull/7939#issuecomment-3267073155 @slfan1989 , @pan3793 , thanks a lot for reviewing this!", "created": "2025-09-05T18:22:53.000+0000", "updated": "2025-09-08T16:28:18.000+0000", "derived": {"summary_task": "Summarize this issue: The powermock dependency is specified in - hadoop-cloud-storage-project/hadoop-huaweicloud/pom.xml - hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/pom.xml but not used anywhere. We should remove it.", "classification_task": "Classify the issue priority and type: The powermock dependency is specified in - hadoop-cloud-storage-project/hadoop-huaweicloud/pom.xml - hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/pom.xml but not used anywhere. We should remove it.", "qna_task": "Question: What is this issue about?\nAnswer: [JDK17] Remove powermock dependency"}}
{"id": "13628240", "key": "HADOOP-19677", "project": "HADOOP", "summary": "[JDK17] Remove mockito-all 1.10.19 and powermock", "description": "- The mockito-all 1.10.19 dependency should be replaced by mockito-core 4.11. - The powermock dependency is specified in the pom below. We should replace it with mockito since it is known to be incompatible with JDK17. -* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/pom.xml", "comments": "szetszwo opened a new pull request, #7935: URL: https://github.com/apache/hadoop/pull/7935 ### Description of PR The mockito-all 1.10.19 dependency should be replaced by mockito-core 4.11. ### How was this patch tested? By updating existing tests ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [NA] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [NA] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [NA] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #7935: URL: https://github.com/apache/hadoop/pull/7935#issuecomment-3260267569 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 23m 29s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 33s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 41m 48s | | trunk passed | | +1 :green_heart: | compile | 19m 26s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 17m 26s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 4m 56s | | trunk passed | | +1 :green_heart: | mvnsite | 7m 35s | | trunk passed | | +1 :green_heart: | javadoc | 5m 46s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 56s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 28s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +0 :ok: | spotbugs | 0m 24s | | branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 42m 10s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 32s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 4m 40s | | the patch passed | | -1 :x: | compile | 0m 15s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 15s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 14s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 0m 14s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 4m 35s | | the patch passed | | +1 :green_heart: | mvnsite | 5m 5s | | the patch passed | | +1 :green_heart: | javadoc | 4m 41s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 3m 59s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 17s | | hadoop-project has no data from spotbugs | | +0 :ok: | spotbugs | 0m 18s | | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui has no data from spotbugs | | +1 :green_heart: | shadedclient | 42m 25s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 18s | | hadoop-project in the patch passed. | | +1 :green_heart: | unit | 3m 33s | | hadoop-auth in the patch passed. | | -1 :x: | unit | 118m 19s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt) | hadoop-yarn-server-resourcemanager in the patch failed. | | -1 :x: | unit | 141m 25s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt) | hadoop-mapreduce-client in the patch passed. | | +1 :green_heart: | unit | 0m 50s | | hadoop-yarn-server-timelineservice-documentstore in the patch passed. | | +1 :green_heart: | unit | 1m 23s | | hadoop-yarn-server-globalpolicygenerator in the patch passed. | | +1 :green_heart: | unit | 1m 5s | | hadoop-yarn-applications-catalog-webapp in the patch passed. | | -1 :x: | unit | 0m 20s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-ui.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-ui.txt) | hadoop-yarn-ui in the patch failed. | | +1 :green_heart: | unit | 3m 12s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 47s | | The patch does not generate ASF License warnings. | | | | 541m 38s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.resourcemanager.reservation.TestCapacityOverTimePolicy | | | hadoop.mapreduce.v2.TestUberAM | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7935 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 21c4e19a37e1 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7dadba11fdeaf702d291642f8b2d59c197ec9ecb | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/testReport/ | | Max. process+thread count | 1139 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-auth hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-globalpolicygenerator hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui hadoop-tools/hadoop-azure U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7935: URL: https://github.com/apache/hadoop/pull/7935#issuecomment-3260284208 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 0s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 4s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 46m 0s | | trunk passed | | +1 :green_heart: | compile | 20m 8s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 16m 16s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 4m 57s | | trunk passed | | +1 :green_heart: | mvnsite | 7m 48s | | trunk passed | | +1 :green_heart: | javadoc | 5m 46s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 5m 9s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 28s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +0 :ok: | spotbugs | 0m 24s | | branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 41m 19s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 31s | | Maven dependency ordering for patch | | -1 :x: | mvninstall | 0m 38s | [/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) | hadoop-yarn-applications-catalog-webapp in the patch failed. | | -1 :x: | compile | 0m 16s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 16s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 15s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 0m 15s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 4m 46s | | the patch passed | | -1 :x: | mvnsite | 0m 39s | [/patch-mvnsite-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-mvnsite-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) | hadoop-yarn-applications-catalog-webapp in the patch failed. | | +1 :green_heart: | javadoc | 4m 50s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 11s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 15s | | hadoop-project has no data from spotbugs | | -1 :x: | spotbugs | 0m 34s | [/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) | hadoop-yarn-applications-catalog-webapp in the patch failed. | | +0 :ok: | spotbugs | 0m 17s | | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui has no data from spotbugs | | +1 :green_heart: | shadedclient | 41m 10s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 20s | | hadoop-project in the patch passed. | | +1 :green_heart: | unit | 3m 29s | | hadoop-auth in the patch passed. | | -1 :x: | unit | 118m 15s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt) | hadoop-yarn-server-resourcemanager in the patch failed. | | -1 :x: | unit | 139m 46s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt) | hadoop-mapreduce-client in the patch passed. | | +1 :green_heart: | unit | 0m 51s | | hadoop-yarn-server-timelineservice-documentstore in the patch passed. | | +1 :green_heart: | unit | 1m 15s | | hadoop-yarn-server-globalpolicygenerator in the patch passed. | | -1 :x: | unit | 0m 48s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) | hadoop-yarn-applications-catalog-webapp in the patch failed. | | -1 :x: | unit | 0m 21s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-ui.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-ui.txt) | hadoop-yarn-ui in the patch failed. | | +1 :green_heart: | unit | 3m 4s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 50s | | The patch does not generate ASF License warnings. | | | | 517m 52s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.resourcemanager.reservation.TestCapacityOverTimePolicy | | | hadoop.mapreduce.v2.TestUberAM | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7935 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 1742a3955488 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1d5fb3ced9d16f8c847312dfd39d90d69a3f608e | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/testReport/ | | Max. process+thread count | 1134 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-auth hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-globalpolicygenerator hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui hadoop-tools/hadoop-azure U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7935: URL: https://github.com/apache/hadoop/pull/7935#issuecomment-3262843831 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 52s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 7m 42s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 38m 24s | | trunk passed | | +1 :green_heart: | compile | 18m 5s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 16m 54s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 4m 47s | | trunk passed | | +1 :green_heart: | mvnsite | 7m 54s | | trunk passed | | +1 :green_heart: | javadoc | 6m 15s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 56s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 28s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +0 :ok: | spotbugs | 0m 23s | | branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 43m 34s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 32s | | Maven dependency ordering for patch | | -1 :x: | mvninstall | 0m 34s | [/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/artifact/out/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) | hadoop-yarn-applications-catalog-webapp in the patch failed. | | -1 :x: | compile | 12m 54s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 12m 54s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 11m 40s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 11m 40s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 5m 19s | | the patch passed | | -1 :x: | mvnsite | 0m 40s | [/patch-mvnsite-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/artifact/out/patch-mvnsite-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) | hadoop-yarn-applications-catalog-webapp in the patch failed. | | +1 :green_heart: | javadoc | 5m 24s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 5m 9s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 35s | | hadoop-project has no data from spotbugs | | -1 :x: | spotbugs | 0m 39s | [/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/artifact/out/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) | hadoop-yarn-applications-catalog-webapp in the patch failed. | | +0 :ok: | spotbugs | 0m 27s | | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui has no data from spotbugs | | +1 :green_heart: | shadedclient | 43m 31s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 22s | | hadoop-project in the patch passed. | | +1 :green_heart: | unit | 3m 28s | | hadoop-auth in the patch passed. | | -1 :x: | unit | 117m 33s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt) | hadoop-yarn-server-resourcemanager in the patch failed. | | -1 :x: | unit | 141m 22s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/artifact/out/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt) | hadoop-mapreduce-client in the patch passed. | | +1 :green_heart: | unit | 0m 55s | | hadoop-yarn-server-timelineservice-documentstore in the patch passed. | | +1 :green_heart: | unit | 1m 14s | | hadoop-yarn-server-globalpolicygenerator in the patch passed. | | -1 :x: | unit | 0m 47s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) | hadoop-yarn-applications-catalog-webapp in the patch failed. | | +1 :green_heart: | unit | 2m 10s | | hadoop-yarn-ui in the patch passed. | | +1 :green_heart: | unit | 3m 19s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 1m 9s | | The patch does not generate ASF License warnings. | | | | 542m 48s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.mapreduce.v2.TestUberAM | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7935 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 2cc4b9991723 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 5eec2aa52aaafe54638d427ab3943557b7660e21 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/testReport/ | | Max. process+thread count | 1143 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-auth hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-globalpolicygenerator hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui hadoop-tools/hadoop-azure U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/3/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7935: URL: https://github.com/apache/hadoop/pull/7935#issuecomment-3262979489 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 51s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 7m 37s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 40m 58s | | trunk passed | | +1 :green_heart: | compile | 19m 1s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 16m 31s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 5m 16s | | trunk passed | | +1 :green_heart: | mvnsite | 7m 48s | | trunk passed | | +1 :green_heart: | javadoc | 5m 29s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 40s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 27s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +0 :ok: | spotbugs | 0m 23s | | branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 43m 4s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 32s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 4m 32s | | the patch passed | | +1 :green_heart: | compile | 18m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 18m 19s | | the patch passed | | +1 :green_heart: | compile | 16m 58s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 16m 58s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 4m 48s | | the patch passed | | +1 :green_heart: | mvnsite | 7m 16s | | the patch passed | | +1 :green_heart: | javadoc | 5m 20s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 43s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 21s | | hadoop-project has no data from spotbugs | | +0 :ok: | spotbugs | 0m 22s | | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui has no data from spotbugs | | +1 :green_heart: | shadedclient | 41m 52s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 24s | | hadoop-project in the patch passed. | | +1 :green_heart: | unit | 3m 39s | | hadoop-auth in the patch passed. | | -1 :x: | unit | 118m 36s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/4/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt) | hadoop-yarn-server-resourcemanager in the patch failed. | | -1 :x: | unit | 142m 36s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/4/artifact/out/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt) | hadoop-mapreduce-client in the patch passed. | | +1 :green_heart: | unit | 0m 55s | | hadoop-yarn-server-timelineservice-documentstore in the patch passed. | | +1 :green_heart: | unit | 1m 16s | | hadoop-yarn-server-globalpolicygenerator in the patch passed. | | +1 :green_heart: | unit | 1m 10s | | hadoop-yarn-applications-catalog-webapp in the patch passed. | | +1 :green_heart: | unit | 2m 2s | | hadoop-yarn-ui in the patch passed. | | +1 :green_heart: | unit | 3m 18s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 1m 17s | | The patch does not generate ASF License warnings. | | | | 557m 11s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.mapreduce.v2.TestUberAM | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7935 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux fd48f302fc58 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 2d02b1f3029f4fa6964b28470c69a5d603dd995a | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/4/testReport/ | | Max. process+thread count | 1130 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-auth hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-globalpolicygenerator hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui hadoop-tools/hadoop-azure U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/4/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. szetszwo commented on PR #7935: URL: https://github.com/apache/hadoop/pull/7935#issuecomment-3263174197 The failed tests seem not related. slfan1989 commented on PR #7935: URL: https://github.com/apache/hadoop/pull/7935#issuecomment-3263559296 @szetszwo Thank you for your contribution! LGTM. szetszwo merged PR #7935: URL: https://github.com/apache/hadoop/pull/7935 szetszwo commented on PR #7935: URL: https://github.com/apache/hadoop/pull/7935#issuecomment-3263885757 @slfan1989 , thanks a lot for reviewing this! The pull request was merged. Resolving ...", "created": "2025-09-05T17:51:30.000+0000", "updated": "2025-09-08T16:27:38.000+0000", "derived": {"summary_task": "Summarize this issue: - The mockito-all 1.10.19 dependency should be replaced by mockito-core 4.11. - The powermock dependency is specified in the pom below. We should replace it with mockito since it is known to be incompatible with JDK17. -* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/pom.xml", "classification_task": "Classify the issue priority and type: - The mockito-all 1.10.19 dependency should be replaced by mockito-core 4.11. - The powermock dependency is specified in the pom below. We should replace it with mockito since it is known to be incompatible with JDK17. -* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/pom.xml", "qna_task": "Question: What is this issue about?\nAnswer: [JDK17] Remove mockito-all 1.10.19 and powermock"}}
{"id": "13628135", "key": "HADOOP-19676", "project": "HADOOP", "summary": "ABFS: Enhancing ABFS Driver Metrics for Analytical Usability", "description": "", "comments": "", "created": "2025-09-04T16:09:30.000+0000", "updated": "2025-09-04T16:09:46.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: ABFS: Enhancing ABFS Driver Metrics for Analytical Usability"}}
{"id": "13628082", "key": "HADOOP-19675", "project": "HADOOP", "summary": "Close stale PRs updated over 100 days ago.", "description": "Close stale PRs on GitHub which updated over 100 days ago, base on the voting thread: https://lists.apache.org/thread/7x6c029fp9k62bxt2995dz6q6j6sg0bh", "comments": "Hexiaoqiao opened a new pull request, #7930: URL: https://github.com/apache/hadoop/pull/7930 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR This PR adds a GitHub workflow to automatically close stale PRs which have no activity over 100 days. ### How was this patch tested? No. ### For code changes: - [Y] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [N] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [N] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [Y] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #7930: URL: https://github.com/apache/hadoop/pull/7930#issuecomment-3252923998 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 55s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | yamllint | 0m 0s | | yamllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | shadedclient | 44m 42s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7930/1/artifact/out/blanks-eol.txt) | The patch has 3 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | +1 :green_heart: | shadedclient | 40m 49s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 40s | | The patch does not generate ASF License warnings. | | | | 88m 49s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7930/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7930 | | Optional Tests | dupname asflicense codespell detsecrets yamllint | | uname | Linux 8be7cf7a0c05 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 5e03454c04c43efa24691ef3d9245d9333a46e70 | | Max. process+thread count | 536 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7930/1/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7930: URL: https://github.com/apache/hadoop/pull/7930#issuecomment-3253473455 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 55s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | yamllint | 0m 0s | | yamllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | shadedclient | 44m 27s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 39m 50s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 39s | | The patch does not generate ASF License warnings. | | | | 87m 32s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7930/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7930 | | Optional Tests | dupname asflicense codespell detsecrets yamllint | | uname | Linux 013fcfcc6e1b 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / bae392d8dfb7f2edb9de956122a07626d8802b20 | | Max. process+thread count | 531 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7930/2/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. Hexiaoqiao commented on PR #7930: URL: https://github.com/apache/hadoop/pull/7930#issuecomment-3257436978 Thanks @slfan1989 . ping @ayushtkn @KeeProMise @ahmarsuhail and other guys, any more suggestions here? Thanks. ahmarsuhail commented on code in PR #7930: URL: https://github.com/apache/hadoop/pull/7930#discussion_r2324404204 ########## .github/workflows/stale.yml: ########## @@ -0,0 +1,44 @@ +# +# Licensed to the Apache Software Foundation (ASF) under one +# or more contributor license agreements. See the NOTICE file +# distributed with this work for additional information +# regarding copyright ownership. The ASF licenses this file +# to you under the Apache License, Version 2.0 (the +# \"License\"); you may not use this file except in compliance +# with the License. You may obtain a copy of the License at +# +# http://www.apache.org/licenses/LICENSE-2.0 +# +# Unless required by applicable law or agreed to in writing, +# software distributed under the License is distributed on an +# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY +# KIND, either express or implied. See the License for the +# specific language governing permissions and limitations +# under the License. +# + +name: Close stale PRs +on: + schedule: + - cron: \"0 0 * * *\" + +jobs: + stale: + if: github.repository == 'apache/hadoop' + runs-on: ubuntu-latest + steps: + - uses: actions/stale@v1 + with: + stale-pr-message: > + We're closing this stale PR because it has been open 100 days with Review Comment: nit: open for 100 days hadoop-yetus commented on PR #7930: URL: https://github.com/apache/hadoop/pull/7930#issuecomment-3264681771 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 59s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | yamllint | 0m 0s | | yamllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | shadedclient | 46m 43s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 39m 46s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 39s | | The patch does not generate ASF License warnings. | | | | 89m 40s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7930/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7930 | | Optional Tests | dupname asflicense codespell detsecrets yamllint | | uname | Linux c705d2f588b2 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / fbadafe3b09613b1566c16d9c0b7518ad1ec53e0 | | Max. process+thread count | 525 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7930/3/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. Hexiaoqiao commented on code in PR #7930: URL: https://github.com/apache/hadoop/pull/7930#discussion_r2329238358 ########## .github/workflows/stale.yml: ########## @@ -0,0 +1,44 @@ +# +# Licensed to the Apache Software Foundation (ASF) under one +# or more contributor license agreements. See the NOTICE file +# distributed with this work for additional information +# regarding copyright ownership. The ASF licenses this file +# to you under the Apache License, Version 2.0 (the +# \"License\"); you may not use this file except in compliance +# with the License. You may obtain a copy of the License at +# +# http://www.apache.org/licenses/LICENSE-2.0 +# +# Unless required by applicable law or agreed to in writing, +# software distributed under the License is distributed on an +# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY +# KIND, either express or implied. See the License for the +# specific language governing permissions and limitations +# under the License. +# + +name: Close stale PRs +on: + schedule: + - cron: \"0 0 * * *\" + +jobs: + stale: + if: github.repository == 'apache/hadoop' + runs-on: ubuntu-latest + steps: + - uses: actions/stale@v1 + with: + stale-pr-message: > + We're closing this stale PR because it has been open 100 days with Review Comment: Thanks. Fixed. Hexiaoqiao merged PR #7930: URL: https://github.com/apache/hadoop/pull/7930 Hexiaoqiao commented on PR #7930: URL: https://github.com/apache/hadoop/pull/7930#issuecomment-3264769501 Committed. Thanks all. Hexiaoqiao opened a new pull request, #7943: URL: https://github.com/apache/hadoop/pull/7943 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR This PR adds a GitHub workflow to automatically close stale PRs which have no activity over 100 days, linked https://github.com/apache/hadoop/pull/7930. Addendum. add github token. ### How was this patch tested? No. ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? slfan1989 commented on PR #7943: URL: https://github.com/apache/hadoop/pull/7943#issuecomment-3268690888 LGTM KeeProMise commented on PR #7943: URL: https://github.com/apache/hadoop/pull/7943#issuecomment-3268702731 LGTM. hadoop-yetus commented on PR #7943: URL: https://github.com/apache/hadoop/pull/7943#issuecomment-3268837255 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 50s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | yamllint | 0m 0s | | yamllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | shadedclient | 45m 25s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 39m 53s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 39s | | The patch does not generate ASF License warnings. | | | | 88m 29s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7943/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7943 | | Optional Tests | dupname asflicense codespell detsecrets yamllint | | uname | Linux 5223952659a9 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 612de4cfeddb2b582bed25abb524eec88f366245 | | Max. process+thread count | 527 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7943/1/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. Hexiaoqiao merged PR #7943: URL: https://github.com/apache/hadoop/pull/7943 Hexiaoqiao commented on PR #7943: URL: https://github.com/apache/hadoop/pull/7943#issuecomment-3268849533 Committed. Thanks @slfan1989 and @KeeProMise . ahmarsuhail commented on PR #7930: URL: https://github.com/apache/hadoop/pull/7930#issuecomment-3298194311 Hey @Hexiaoqiao, just curious, number of open PRs is still 1.1K, is that expected? Hexiaoqiao commented on PR #7930: URL: https://github.com/apache/hadoop/pull/7930#issuecomment-3298596092 Thanks @ahmarsuhail , Yes, it works fine from my side. ref: https://github.com/apache/hadoop/actions/workflows/stale.yml It will keep for a long time because rate limit. I didn't dig if the rate limit could be tuning or disable. Any thought?", "created": "2025-09-04T07:59:48.000+0000", "updated": "2025-09-16T12:42:55.000+0000", "derived": {"summary_task": "Summarize this issue: Close stale PRs on GitHub which updated over 100 days ago, base on the voting thread: https://lists.apache.org/thread/7x6c029fp9k62bxt2995dz6q6j6sg0bh", "classification_task": "Classify the issue priority and type: Close stale PRs on GitHub which updated over 100 days ago, base on the voting thread: https://lists.apache.org/thread/7x6c029fp9k62bxt2995dz6q6j6sg0bh", "qna_task": "Question: What is this issue about?\nAnswer: Close stale PRs updated over 100 days ago."}}
{"id": "13627987", "key": "HADOOP-19674", "project": "HADOOP", "summary": "[JDK 17] Implementation of JAXB-API has not been found on module path or classpath", "description": "When i try to run the *org.apache.hadoop.yarn.webapp.TestWebApp* tests over JDK17 i am facing the following errors: {noformat} Caused by: A MultiException has 2 exceptions. They are: 1. javax.xml.bind.JAXBException: Implementation of JAXB-API has not been found on module path or classpath. 2. java.lang.IllegalStateException: Unable to perform operation: create on org.apache.hadoop.yarn.webapp.MyTestJAXBContextResolver {noformat} Repro steps: - run: ./start-build-env.sh ubuntu_24 - in docker run: mvn clean install -Pjdk17+ -T 32 -DskipTests - in hadoop-yarn-common modul run: mvn test -Dtest=org.apache.hadoop.yarn.webapp.TestWebApp I found a similar error here: https://issues.apache.org/jira/browse/HDDS-5068 Based on my understanding the problem is the JDK11+ environments does not have the JAXB runtime, so we have to explicit provide them. Maybe this is just a temporal solution, till the whole Jakarta upgrade can be done. {panel:title=Full error log} {code} [ERROR] testRobotsText Time elapsed: 0.064 s <<< ERROR! org.apache.hadoop.yarn.webapp.WebAppException: Error starting http server at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:506) at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:492) at org.apache.hadoop.yarn.webapp.TestWebApp.testRobotsText(TestWebApp.java:324) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:569) at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725) at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60) at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131) at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149) at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140) at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84) at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115) at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105) at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106) at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64) at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45) at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37) at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104) at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98) at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214) at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210) at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135) at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66) at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151) at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141) at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139) at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138) at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95) at java.base/java.util.ArrayList.forEach(ArrayList.java:1511) at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41) at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155) at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141) at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139) at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138) at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95) at java.base/java.util.ArrayList.forEach(ArrayList.java:1511) at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41) at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155) at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141) at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139) at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138) at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95) at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35) at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57) at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52) at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114) at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86) at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86) at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53) at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:142) at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:113) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418) Caused by: java.io.IOException: Unable to initialize WebAppContext at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1453) at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:503) ... 71 more Caused by: javax.servlet.ServletException: org.glassfish.jersey.servlet.ServletContainer-640d604==org.glassfish.jersey.servlet.ServletContainer@f679d7ba{jsp=null,order=-1,inst=true,async=true,src=EMBEDDED:null,STARTED} at org.eclipse.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:650) at org.eclipse.jetty.servlet.ServletHolder.initialize(ServletHolder.java:415) at org.eclipse.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:750) at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357) at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510) at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499) at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310) at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735) at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762) at org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774) at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379) at org.eclipse.jetty.webapp.WebAppContext.startWebapp(WebAppContext.java:1449) at org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1414) at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916) at org.eclipse.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288) at org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:524) at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73) at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169) at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:117) at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97) at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73) at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169) at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:110) at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97) at org.eclipse.jetty.server.handler.StatisticsHandler.doStart(StatisticsHandler.java:264) at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73) at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169) at org.eclipse.jetty.server.Server.start(Server.java:423) at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:110) at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97) at org.eclipse.jetty.server.Server.doStart(Server.java:387) at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73) at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1416) ... 72 more Caused by: A MultiException has 2 exceptions. They are: 1. javax.xml.bind.JAXBException: Implementation of JAXB-API has not been found on module path or classpath. 2. java.lang.IllegalStateException: Unable to perform operation: create on org.apache.hadoop.yarn.webapp.MyTestJAXBContextResolver at org.jvnet.hk2.internal.ClazzCreator.create(ClazzCreator.java:368) at org.jvnet.hk2.internal.SystemDescriptor.create(SystemDescriptor.java:463) at org.jvnet.hk2.internal.SingletonContext$1.compute(SingletonContext.java:59) at org.jvnet.hk2.internal.SingletonContext$1.compute(SingletonContext.java:47) at org.glassfish.hk2.utilities.cache.Cache$OriginThreadAwareFuture$1.call(Cache.java:74) at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) at org.glassfish.hk2.utilities.cache.Cache$OriginThreadAwareFuture.run(Cache.java:131) at org.glassfish.hk2.utilities.cache.Cache.compute(Cache.java:176) at org.jvnet.hk2.internal.SingletonContext.findOrCreate(SingletonContext.java:98) at org.jvnet.hk2.internal.Utilities.createService(Utilities.java:2102) at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetAllServiceHandles(ServiceLocatorImpl.java:1481) at org.jvnet.hk2.internal.ServiceLocatorImpl.getAllServices(ServiceLocatorImpl.java:799) at org.glassfish.jersey.inject.hk2.AbstractHk2InjectionManager.getAllInstances(AbstractHk2InjectionManager.java:171) at org.glassfish.jersey.inject.hk2.ImmediateHk2InjectionManager.getAllInstances(ImmediateHk2InjectionManager.java:30) at org.glassfish.jersey.internal.ContextResolverFactory$ContextResolversConfigurator.postInit(ContextResolverFactory.java:69) at org.glassfish.jersey.server.ApplicationHandler.lambda$initialize$2(ApplicationHandler.java:353) at java.base/java.util.Arrays$ArrayList.forEach(Arrays.java:4204) at org.glassfish.jersey.server.ApplicationHandler.initialize(ApplicationHandler.java:353) at org.glassfish.jersey.server.ApplicationHandler.lambda$initialize$1(ApplicationHandler.java:297) at org.glassfish.jersey.internal.Errors.process(Errors.java:292) at org.glassfish.jersey.internal.Errors.process(Errors.java:274) at org.glassfish.jersey.internal.Errors.processWithException(Errors.java:232) at org.glassfish.jersey.server.ApplicationHandler.initialize(ApplicationHandler.java:296) at org.glassfish.jersey.server.ApplicationHandler.<init>(ApplicationHandler.java:261) at org.glassfish.jersey.servlet.WebComponent.<init>(WebComponent.java:314) at org.glassfish.jersey.servlet.ServletContainer.init(ServletContainer.java:154) at org.glassfish.jersey.servlet.ServletContainer.init(ServletContainer.java:360) at javax.servlet.GenericServlet.init(GenericServlet.java:244) at org.eclipse.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:632) ... 104 more Caused by: javax.xml.bind.JAXBException: Implementation of JAXB-API has not been found on module path or classpath. - with linked exception: [java.lang.ClassNotFoundException: com.sun.xml.bind.v2.ContextFactory] at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:232) at javax.xml.bind.ContextFinder.find(ContextFinder.java:375) at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:691) at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:632) at org.glassfish.jersey.jettison.JettisonJaxbContext.<init>(JettisonJaxbContext.java:73) at org.glassfish.jersey.jettison.JettisonJaxbContext.<init>(JettisonJaxbContext.java:54) at org.apache.hadoop.yarn.webapp.MyTestJAXBContextResolver.<init>(MyTestJAXBContextResolver.java:45) at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500) at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481) at org.glassfish.hk2.utilities.reflection.ReflectionHelper.makeMe(ReflectionHelper.java:1356) at org.jvnet.hk2.internal.ClazzCreator.createMe(ClazzCreator.java:248) at org.jvnet.hk2.internal.ClazzCreator.create(ClazzCreator.java:342) ... 132 more Caused by: java.lang.ClassNotFoundException: com.sun.xml.bind.v2.ContextFactory at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) at org.eclipse.jetty.webapp.WebAppClassLoader.loadClass(WebAppClassLoader.java:538) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) at javax.xml.bind.ServiceLoaderUtil.nullSafeLoadClass(ServiceLoaderUtil.java:92) at javax.xml.bind.ServiceLoaderUtil.safeLoadClass(ServiceLoaderUtil.java:125) at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:230) ... 146 more {code} {panel}", "comments": "Thank you. I was able to repro the problem Do you plan to provide a patch [~bkosztolnik] ? K0K0V0K opened a new pull request, #7928: URL: https://github.com/apache/hadoop/pull/7928 https://issues.apache.org/jira/browse/HADOOP-19674 ### Description of PR When we try to create an instance of `JettisonJaxbContext` a `JAXBContext.newInstance` call will happen. With my understanding JAXB is removed since JDK11 ([source](https://docs.oracle.com/en/java/javase/24/migrate/removed-tools-and-components.html#GUID-11F78105-D735-430D-92DD-6C37958FCBC3)) and if we would like to access JAXB, then we should explicit include them to the jars. So a new **jaxb-runtime** dependency is included in the code base. I used test scope where ever it was possible but for example in the `MRClientService` the `JAXBContextResolver` is in use, so i left the test scope there. ### How was this patch tested? Unit tests - I created a JDK8 build and run the `org.apache.hadoop.yarn.webapp.TestWebApp` test - I created a JDK17 build and run the` org.apache.hadoop.yarn.webapp.TestWebApp` test ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? Hi [~stoty]! Yes, just now. I am not 100% sure this is a good solution in point of architecture, security or long term maintenance, so i am grateful to every review. Thanks! I have looked into this a bit: In 3.4, jaxb-impl is a transitive dependency of jersey-json: {noformat} [INFO] +- com.github.pjfanning:jersey-json:jar:1.22.0:compile [INFO] | \\- com.sun.xml.bind:jaxb-impl:jar:2.2.3-1:compile [INFO] | \\- javax.xml.bind:jaxb-api:jar:2.2.11:compile {noformat} In 3.5 Jersey has been upgraded, and we've lost the transitive dependency. Hadoop does use java.xml.bind, so in the modules where it's used it should be added as a compile / test dependency (depending on where it is used). It is also required by Jetty (and possibly other libraries) so it may need to be added to some modules that do not directly use javax.bind. Yes, at some point we should update to Jakarta, but that is a future issue, IMO adding the dependency now is fine. stoty commented on code in PR #7928: URL: https://github.com/apache/hadoop/pull/7928#discussion_r2319677719 ########## hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/pom.xml: ########## @@ -144,6 +144,11 @@ <artifactId>jersey-media-jaxb</artifactId> <scope>test</scope> </dependency> + <dependency> Review Comment: How did you decide where to add jaxb-imp and with what scope ? ########## hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-globalpolicygenerator/pom.xml: ########## @@ -122,6 +122,12 @@ <artifactId>jersey-test-framework-provider-jetty</artifactId> <scope>test</scope> </dependency> + <dependency> + <groupId>org.glassfish.jaxb</groupId> + <artifactId>jaxb-runtime</artifactId> + <version>${jaxb.version}</version> Review Comment: The version should be set in once in the hadoop-project pom dependencyManagement section ########## hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/pom.xml: ########## @@ -208,6 +208,12 @@ <artifactId>jersey-media-json-jettison</artifactId> <scope>test</scope> </dependency> + <dependency> + <groupId>org.glassfish.jaxb</groupId> + <artifactId>jaxb-runtime</artifactId> + <version>${jaxb.version}</version> + <scope>test</scope> Review Comment: This is needed by Jetty. (at least) This should be compile scope, as javax.xml.bind is directly used in the main code. K0K0V0K commented on code in PR #7928: URL: https://github.com/apache/hadoop/pull/7928#discussion_r2319820574 ########## hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/pom.xml: ########## @@ -144,6 +144,11 @@ <artifactId>jersey-media-jaxb</artifactId> <scope>test</scope> </dependency> + <dependency> Review Comment: I checked the usage of the `JettisonJaxbContext` in the projects where only used in the tests i added with test scope, otherwise i set no scope. K0K0V0K commented on code in PR #7928: URL: https://github.com/apache/hadoop/pull/7928#discussion_r2319871051 ########## hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/pom.xml: ########## @@ -208,6 +208,12 @@ <artifactId>jersey-media-json-jettison</artifactId> <scope>test</scope> </dependency> + <dependency> + <groupId>org.glassfish.jaxb</groupId> + <artifactId>jaxb-runtime</artifactId> + <version>${jaxb.version}</version> + <scope>test</scope> Review Comment: I was not sure this needed in every case for the Jetty. But in that case if i see well we will need this every where where we are using the HttpServer2. So if i see well this will be needed in HDFS NN also. In that case shall we add this to the just to the `hadoop-common` project and let others use it transitively? K0K0V0K commented on code in PR #7928: URL: https://github.com/apache/hadoop/pull/7928#discussion_r2319871051 ########## hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/pom.xml: ########## @@ -208,6 +208,12 @@ <artifactId>jersey-media-json-jettison</artifactId> <scope>test</scope> </dependency> + <dependency> + <groupId>org.glassfish.jaxb</groupId> + <artifactId>jaxb-runtime</artifactId> + <version>${jaxb.version}</version> + <scope>test</scope> Review Comment: I was not sure this needed in every case for the Jetty. But in that case if i see well we will need this every where where we are using the HttpServer2. So if i see well this will be needed in HDFS NN also. In that case shall we add this to the `hadoop-common` project only and let others use it transitively? susheelgupta7 commented on PR #7928: URL: https://github.com/apache/hadoop/pull/7928#issuecomment-3252006029 Build issue observed on JDK 8 The Maven enforcer plugin failed due to a dependency convergence conflict for jaxb-runtime: ```java [ERROR] Rule 0: org.apache.maven.enforcer.rules.dependency.DependencyConvergence failed with message: [ERROR] Failed while enforcing releasability. [ERROR] [ERROR] Dependency convergence error for org.glassfish.jaxb:jaxb-runtime:jar:2.3.1 paths to dependency are: [ERROR] +-org.apache.hadoop:hadoop-mapreduce-client-app:jar:3.5.0-SNAPSHOT [ERROR] +-org.apache.hadoop:hadoop-yarn-server-web-proxy:jar:3.5.0-SNAPSHOT:compile [ERROR] +-org.apache.hadoop:hadoop-yarn-server-common:jar:3.5.0-SNAPSHOT:compile [ERROR] +-org.ehcache:ehcache:jar:3.8.2:compile [ERROR] +-org.glassfish.jaxb:jaxb-runtime:jar:2.3.1:compile [ERROR] and [ERROR] +-org.apache.hadoop:hadoop-mapreduce-client-app:jar:3.5.0-SNAPSHOT [ERROR] +-org.glassfish.jaxb:jaxb-runtime:jar:2.3.9:compile ``` stoty commented on code in PR #7928: URL: https://github.com/apache/hadoop/pull/7928#discussion_r2321030300 ########## hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/pom.xml: ########## @@ -208,6 +208,12 @@ <artifactId>jersey-media-json-jettison</artifactId> <scope>test</scope> </dependency> + <dependency> + <groupId>org.glassfish.jaxb</groupId> + <artifactId>jaxb-runtime</artifactId> + <version>${jaxb.version}</version> + <scope>test</scope> Review Comment: > I was not sure this needed in every case for the Jetty. The stack strace seems to be coming from Jetty initialization. > But in that case if i see well we will need this every where where we are using the HttpServer2. So if i see well this will be needed in HDFS NN also. Yes, that's likely. > > In that case shall we add this to the `hadoop-common` project only and let others use it transitively? I think it's better to add it separately where needed. Most modules don't use Jetty. stoty commented on code in PR #7928: URL: https://github.com/apache/hadoop/pull/7928#discussion_r2321033382 ########## hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/pom.xml: ########## @@ -208,6 +208,12 @@ <artifactId>jersey-media-json-jettison</artifactId> <scope>test</scope> </dependency> + <dependency> + <groupId>org.glassfish.jaxb</groupId> + <artifactId>jaxb-runtime</artifactId> + <version>${jaxb.version}</version> + <scope>test</scope> Review Comment: One other issue is the binary assembly We also need to make sure that jaxb-impl is included there. K0K0V0K commented on PR #7928: URL: https://github.com/apache/hadoop/pull/7928#issuecomment-3252296131 > Build issue observed on JDK 8 The Maven enforcer plugin failed due to a dependency convergence conflict for jaxb-runtime: > > ```java > [ERROR] Rule 0: org.apache.maven.enforcer.rules.dependency.DependencyConvergence failed with message: > [ERROR] Failed while enforcing releasability. > [ERROR] > [ERROR] Dependency convergence error for org.glassfish.jaxb:jaxb-runtime:jar:2.3.1 paths to dependency are: > [ERROR] +-org.apache.hadoop:hadoop-mapreduce-client-app:jar:3.5.0-SNAPSHOT > [ERROR] +-org.apache.hadoop:hadoop-yarn-server-web-proxy:jar:3.5.0-SNAPSHOT:compile > [ERROR] +-org.apache.hadoop:hadoop-yarn-server-common:jar:3.5.0-SNAPSHOT:compile > [ERROR] +-org.ehcache:ehcache:jar:3.8.2:compile > [ERROR] +-org.glassfish.jaxb:jaxb-runtime:jar:2.3.1:compile > [ERROR] and > [ERROR] +-org.apache.hadoop:hadoop-mapreduce-client-app:jar:3.5.0-SNAPSHOT > [ERROR] +-org.glassfish.jaxb:jaxb-runtime:jar:2.3.9:compile > ``` Thanks @susheelgupta7 for the finding! K0K0V0K commented on code in PR #7928: URL: https://github.com/apache/hadoop/pull/7928#discussion_r2322802024 ########## hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/pom.xml: ########## @@ -208,6 +208,12 @@ <artifactId>jersey-media-json-jettison</artifactId> <scope>test</scope> </dependency> + <dependency> + <groupId>org.glassfish.jaxb</groupId> + <artifactId>jaxb-runtime</artifactId> + <version>${jaxb.version}</version> + <scope>test</scope> Review Comment: I tried to upload a corrected version. With my understanding every Hadoop based Jetty server is instance of the `HttpServer2`, what is in the `hadoop-common`, so i added the dependency there. Some other modules what use` hadoop-common`, but not the Jetty server are already excludes the `jetty-server`. I extended these cases with `jaxb-runtime` exculsion. This means: not 100% these modules dont need `jaxb-runtime`, but pretty sure not because the Jetty. In the httpfs modul i dont know why but seems like the `jetty-server` transitive dependency is excluded from `hadoop-common`, but there is an explicit dependency for it also in the pom. I run a clean install with -Pdist and i can see the jaxb-runtime jar in the `hadoop-dist/target/hadoop-3.5.0-SNAPSHOT/share/hadoop/common/lib/jaxb-runtime-2.3.9.jar` path stoty commented on code in PR #7928: URL: https://github.com/apache/hadoop/pull/7928#discussion_r2322987219 ########## hadoop-client-modules/hadoop-client/pom.xml: ########## @@ -53,6 +53,10 @@ <groupId>org.eclipse.jetty</groupId> <artifactId>jetty-server</artifactId> </exclusion> + <exclusion> Review Comment: These are for java 8, right ? ########## hadoop-client-modules/hadoop-client/pom.xml: ########## @@ -53,6 +53,10 @@ <groupId>org.eclipse.jetty</groupId> <artifactId>jetty-server</artifactId> </exclusion> + <exclusion> Review Comment: These are for java 8, right ? stoty commented on code in PR #7928: URL: https://github.com/apache/hadoop/pull/7928#discussion_r2323018571 ########## hadoop-project/pom.xml: ########## @@ -2152,6 +2155,11 @@ <artifactId>jersey-media-json-jettison</artifactId> <version>${jersey2.version}</version> </dependency> + <dependency> Review Comment: Since we're globally dependency managing jaxb-runtime, I think that we don't need all the excludes in this patch. They should all be dependency managed to 2.3.9 by maven, and the dependency convergence check should pass. stoty commented on code in PR #7928: URL: https://github.com/apache/hadoop/pull/7928#discussion_r2323027545 ########## hadoop-project/pom.xml: ########## @@ -2152,6 +2155,11 @@ <artifactId>jersey-media-json-jettison</artifactId> <version>${jersey2.version}</version> </dependency> + <dependency> Review Comment: I think that the reported error was for the earlier patch without the global dependency management. hadoop-yetus commented on PR #7928: URL: https://github.com/apache/hadoop/pull/7928#issuecomment-3256455260 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 54s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 20s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 38m 12s | | trunk passed | | +1 :green_heart: | compile | 18m 54s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 16m 43s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 8m 2s | | trunk passed | | +1 :green_heart: | javadoc | 7m 13s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 6m 15s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 150m 1s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 40s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 4m 24s | | the patch passed | | +1 :green_heart: | compile | 18m 21s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 18m 21s | | the patch passed | | +1 :green_heart: | compile | 16m 30s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 16m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 7m 41s | | the patch passed | | +1 :green_heart: | javadoc | 7m 7s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 6m 11s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 65m 10s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 31s | | hadoop-project in the patch passed. | | +1 :green_heart: | unit | 24m 7s | | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 3m 52s | | hadoop-kms in the patch passed. | | +1 :green_heart: | unit | 6m 0s | | hadoop-hdfs-httpfs in the patch passed. | | +1 :green_heart: | unit | 6m 16s | | hadoop-yarn-server-common in the patch passed. | | +1 :green_heart: | unit | 1m 35s | | hadoop-yarn-server-web-proxy in the patch passed. | | -1 :x: | unit | 145m 56s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/2/artifact/out/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt) | hadoop-mapreduce-client in the patch passed. | | +1 :green_heart: | unit | 2m 38s | | hadoop-yarn-services-api in the patch passed. | | +1 :green_heart: | unit | 0m 53s | | hadoop-client in the patch passed. | | +1 :green_heart: | asflicense | 1m 39s | | The patch does not generate ASF License warnings. | | | | 451m 50s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.mapreduce.v2.TestUberAM | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7928 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux 4eb1be1e663a 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 6086bd5ba19b4e4de56b0eba69be70962117a85a | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/2/testReport/ | | Max. process+thread count | 1273 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api hadoop-client-modules/hadoop-client U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/2/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7928: URL: https://github.com/apache/hadoop/pull/7928#issuecomment-3257284211 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 54s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 4s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 42m 58s | | trunk passed | | +1 :green_heart: | compile | 18m 38s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 35s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 24m 49s | | trunk passed | | +1 :green_heart: | javadoc | 11m 2s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 9m 33s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 62m 19s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 37s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 42m 48s | | the patch passed | | +1 :green_heart: | compile | 19m 10s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 19m 10s | | the patch passed | | +1 :green_heart: | compile | 16m 22s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 16m 22s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 21m 21s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 11m 2s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 8m 13s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 62m 4s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 488m 32s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/3/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 15s | | The patch does not generate ASF License warnings. | | | | 831m 35s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.TestRollingUpgrade | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7928 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux a9562a3960c9 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 450d122aa0923580d4ddfe2b8ad9a734265fe7d9 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/3/testReport/ | | Max. process+thread count | 2197 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api hadoop-client-modules/hadoop-client . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/3/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. K0K0V0K commented on code in PR #7928: URL: https://github.com/apache/hadoop/pull/7928#discussion_r2324319353 ########## hadoop-client-modules/hadoop-client/pom.xml: ########## @@ -53,6 +53,10 @@ <groupId>org.eclipse.jetty</groupId> <artifactId>jetty-server</artifactId> </exclusion> + <exclusion> + <groupId>org.glassfish.jaxb</groupId> + <artifactId>jaxb-runtime</artifactId> + </exclusion> Review Comment: I think this one is necessary otherwise at the end on install i am facing this error: ``` [INFO] Total time: 05:20 min (Wall Clock) [INFO] Finished at: 2025-09-05T09:02:10+02:00 [INFO] ------------------------------------------------------------------------ [ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:3.5.0:enforce (enforce-banned-dependencies) on project hadoop-client-check-test-invariants: [ERROR] Rule 1: org.apache.maven.plugins.enforcer.BanDuplicateClasses failed with message: [ERROR] Duplicate classes found: [ERROR] [ERROR] Found in: [ERROR] org.apache.hadoop:hadoop-client-minicluster:jar:3.5.0-SNAPSHOT:compile [ERROR] org.apache.hadoop:hadoop-client-runtime:jar:3.5.0-SNAPSHOT:compile [ERROR] Duplicate classes: [ERROR] org/apache/hadoop/shaded/com/sun/activation/registries/MimeTypeEntry.class [ERROR] org/apache/hadoop/shaded/com/sun/istack/localization/LocalizableMessageFactory$ResourceBundleSupplier.class [ERROR] org/apache/hadoop/shaded/com/sun/activation/registries/MailcapTokenizer.class [ERROR] org/apache/hadoop/shaded/com/sun/istack/FinalArrayList.class [ERROR] org/apache/hadoop/shaded/com/sun/istack/Pool$Impl.class [ERROR] org/apache/hadoop/shaded/com/sun/activation/registries/MailcapParseException.class [ERROR] org/apache/hadoop/shaded/com/sun/activation/registries/LogSupport.class [ERROR] META-INF/versions/9/com/sun/istack/logging/StackHelper.class [ERROR] org/apache/hadoop/shaded/com/sun/istack/NotNull.class [ERROR] org/apache/hadoop/shaded/com/sun/istack/XMLStreamException2.class [ERROR] org/apache/hadoop/shaded/com/sun/activation/viewers/TextEditor.class [ERROR] org/apache/hadoop/shaded/com/sun/istack/XMLStreamReaderToContentHandler$1.class [ERROR] org/apache/hadoop/shaded/com/sun/istack/localization/Localizable.class [ERROR] org/apache/hadoop/shaded/com/sun/activation/viewers/ImageViewerCanvas.class [ERROR] org/apache/hadoop/shaded/com/sun/activation/viewers/TextViewer.class [ERROR] org/apache/hadoop/shaded/com/sun/istack/Interned.class [ERROR] org/apache/hadoop/shaded/com/sun/istack/localization/Localizer.class [ERROR] org/apache/hadoop/shaded/com/sun/istack/localization/LocalizableMessageFactory.class [ERROR] org/apache/hadoop/shaded/com/sun/activation/registries/MailcapFile.class [ERROR] org/apache/hadoop/shaded/com/sun/istack/logging/Logger.class [ERROR] org/apache/hadoop/shaded/com/sun/istack/ByteArrayDataSource.class [ERROR] org/apache/hadoop/shaded/com/sun/istack/Nullable.class [ERROR] org/apache/hadoop/shaded/com/sun/istack/localization/NullLocalizable.class [ERROR] org/apache/hadoop/shaded/com/sun/istack/SAXException2.class [ERROR] org/apache/hadoop/shaded/com/sun/activation/registries/MimeTypeFile.class [ERROR] org/apache/hadoop/shaded/com/sun/istack/logging/StackHelper.class [ERROR] org/apache/hadoop/shaded/com/sun/activation/registries/LineTokenizer.class [ERROR] org/apache/hadoop/shaded/com/sun/istack/Builder.class [ERROR] org/apache/hadoop/shaded/com/sun/istack/FragmentContentHandler.class [ERROR] org/apache/hadoop/shaded/com/sun/istack/Pool.class [ERROR] org/apache/hadoop/shaded/com/sun/istack/localization/LocalizableMessage.class [ERROR] org/apache/hadoop/shaded/com/sun/activation/viewers/ImageViewer.class [ERROR] org/apache/hadoop/shaded/com/sun/istack/SAXParseException2.class [ERROR] org/apache/hadoop/shaded/com/sun/istack/XMLStreamReaderToContentHandler.class ``` stoty commented on code in PR #7928: URL: https://github.com/apache/hadoop/pull/7928#discussion_r2324475248 ########## hadoop-client-modules/hadoop-client/pom.xml: ########## @@ -53,6 +53,10 @@ <groupId>org.eclipse.jetty</groupId> <artifactId>jetty-server</artifactId> </exclusion> + <exclusion> + <groupId>org.glassfish.jaxb</groupId> + <artifactId>jaxb-runtime</artifactId> + </exclusion> Review Comment: I have checked 3.4.2, and these classes are not present there. Based on that, it seems that hadoop-client-runtime indeed does not include jaxb-impl traditionally. hadoop-yetus commented on PR #7928: URL: https://github.com/apache/hadoop/pull/7928#issuecomment-3259566698 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 15m 34s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 17s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 5m 15s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/7/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | +1 :green_heart: | compile | 17m 32s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 14m 30s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 23m 18s | | trunk passed | | +1 :green_heart: | javadoc | 10m 7s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 8m 13s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 56m 8s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 37s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 36m 55s | | the patch passed | | +1 :green_heart: | compile | 17m 1s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 17m 1s | | the patch passed | | +1 :green_heart: | compile | 14m 40s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 14m 40s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 22m 13s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 10m 38s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 8m 20s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 61m 21s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 446m 39s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/7/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 10s | | The patch does not generate ASF License warnings. | | | | 744m 47s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/7/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7928 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux a22a22156e0a 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c5a26a7263c9a5c356216b6426d5fe903213cd15 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/7/testReport/ | | Max. process+thread count | 3607 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api hadoop-client-modules/hadoop-client . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/7/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7928: URL: https://github.com/apache/hadoop/pull/7928#issuecomment-3259596835 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 56s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 7m 56s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 40m 57s | | trunk passed | | +1 :green_heart: | compile | 19m 53s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 46s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 23m 46s | | trunk passed | | +1 :green_heart: | javadoc | 10m 34s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 51s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 58m 27s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 37s | | Maven dependency ordering for patch | | -1 :x: | mvninstall | 37m 21s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/4/artifact/out/patch-mvninstall-root.txt) | root in the patch failed. | | +1 :green_heart: | compile | 17m 10s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 17m 10s | | the patch passed | | +1 :green_heart: | compile | 15m 11s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 11s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 21m 2s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | -1 :x: | javadoc | 9m 54s | [/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/4/artifact/out/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 7m 32s | [/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/4/artifact/out/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | shadedclient | 61m 30s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 480m 5s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/4/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 14s | | The patch does not generate ASF License warnings. | | | | 810m 1s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints | | | hadoop.hdfs.server.namenode.TestNameNodeMXBean | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7928 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux bd7276601e6c 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 835934188aaa69fac727bbe178075cadfc87e0a9 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/4/testReport/ | | Max. process+thread count | 2322 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api hadoop-client-modules/hadoop-client . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/4/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7928: URL: https://github.com/apache/hadoop/pull/7928#issuecomment-3259909698 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 51s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 7m 51s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 37m 23s | | trunk passed | | +1 :green_heart: | compile | 19m 47s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 17m 16s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 24m 2s | | trunk passed | | +1 :green_heart: | javadoc | 11m 10s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 8m 52s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 59m 48s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 35s | | Maven dependency ordering for patch | | -1 :x: | mvninstall | 35m 24s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/5/artifact/out/patch-mvninstall-root.txt) | root in the patch failed. | | +1 :green_heart: | compile | 18m 7s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 18m 7s | | the patch passed | | +1 :green_heart: | compile | 17m 15s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 17m 15s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 22m 30s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 1s | | No new issues. | | -1 :x: | javadoc | 11m 51s | [/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/5/artifact/out/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 8m 21s | [/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/5/artifact/out/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | shadedclient | 63m 0s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 611m 20s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/5/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 18s | | The patch does not generate ASF License warnings. | | | | 944m 54s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7928 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux f7e642f299bc 5.15.0-151-generic #161-Ubuntu SMP Tue Jul 22 14:25:40 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 835934188aaa69fac727bbe178075cadfc87e0a9 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/5/testReport/ | | Max. process+thread count | 3138 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api hadoop-client-modules/hadoop-client . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/5/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7928: URL: https://github.com/apache/hadoop/pull/7928#issuecomment-3263251404 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 22m 49s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 7m 46s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 37m 36s | | trunk passed | | +1 :green_heart: | compile | 17m 44s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 22s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 23m 15s | | trunk passed | | +1 :green_heart: | javadoc | 10m 21s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 43s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 58m 26s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 37s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 41m 8s | | the patch passed | | +1 :green_heart: | compile | 17m 44s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 17m 44s | | the patch passed | | +1 :green_heart: | compile | 15m 13s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 13s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 20m 23s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 10m 6s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 45s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 59m 47s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 481m 54s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/8/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 15s | | The patch does not generate ASF License warnings. | | | | 824m 39s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/8/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7928 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux 117987a87daa 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7d1831a9a0ae7b1f73edfce600e8a818b5ec5de6 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/8/testReport/ | | Max. process+thread count | 3135 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api hadoop-client-modules/hadoop-client . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/8/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on PR #7928: URL: https://github.com/apache/hadoop/pull/7928#issuecomment-3263561534 @K0K0V0K Thank you for your contribution! LGTM. However, we need to trigger the compilation again as the current process seems to have been interrupted. I also noticed some warning messages. Could you please double-check? ``` [WARNING] Some dependencies of Maven Plugins are expected to be in provided scope. Please make sure that dependencies listed below declared in POM have set '<scope>provided</scope>' as well. The following dependencies are in wrong scope: * org.apache.maven:maven-plugin-api:jar:3.9.5:compile * org.apache.maven:maven-model:jar:3.9.5:compile * org.apache.maven:maven-artifact:jar:3.9.5:compile * org.apache.maven:maven-core:jar:3.9.5:compile * org.apache.maven:maven-settings:jar:3.9.5:compile * org.apache.maven:maven-settings-builder:jar:3.9.5:compile * org.apache.maven:maven-builder-support:jar:3.9.5:compile * org.apache.maven:maven-repository-metadata:jar:3.9.5:compile * org.apache.maven:maven-model-builder:jar:3.9.5:compile * org.apache.maven:maven-resolver-provider:jar:3.9.5:compile ``` K0K0V0K commented on PR #7928: URL: https://github.com/apache/hadoop/pull/7928#issuecomment-3265388602 Hi @slfan1989! Thanks for the review. I checked the latest console, output and seems like the waring is not present anymore: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/8/consoleFull hadoop-yetus commented on PR #7928: URL: https://github.com/apache/hadoop/pull/7928#issuecomment-3268664561 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 55s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 7m 51s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 38m 27s | | trunk passed | | +1 :green_heart: | compile | 17m 56s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 16m 29s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 23m 9s | | trunk passed | | +1 :green_heart: | javadoc | 10m 16s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 46s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 58m 11s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 39s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 41m 29s | | the patch passed | | +1 :green_heart: | compile | 18m 0s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 18m 0s | | the patch passed | | +1 :green_heart: | compile | 15m 40s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 40s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 19m 58s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 10m 17s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 44s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 59m 50s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 656m 49s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/9/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 17s | | The patch does not generate ASF License warnings. | | | | 980m 24s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.TestRollingUpgrade | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/9/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7928 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux ca1384aa87d7 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 56b01d2c3fd0ade4f966d40068b9fdc9c53dc7ff | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/9/testReport/ | | Max. process+thread count | 3135 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api hadoop-client-modules/hadoop-client . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7928/9/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. brumi1024 merged PR #7928: URL: https://github.com/apache/hadoop/pull/7928 K0K0V0K commented on PR #7928: URL: https://github.com/apache/hadoop/pull/7928#issuecomment-3270292832 Thanks! @stoty @slfan1989 @brumi1024 @susheelgupta7 for the review and for the help here!", "created": "2025-09-03T11:30:33.000+0000", "updated": "2025-09-09T13:06:35.000+0000", "derived": {"summary_task": "Summarize this issue: When i try to run the *org.apache.hadoop.yarn.webapp.TestWebApp* tests over JDK17 i am facing the following errors: {noformat} Caused by: A MultiException has 2 exceptions. They are: 1. javax.xml.bind.JAXBException: Implementation of JAXB-API has not been found on module path or classpath. 2. java.lang.IllegalStateException: Unable to perform operation: create on org.apache.hadoop.yarn.webapp.MyTe", "classification_task": "Classify the issue priority and type: When i try to run the *org.apache.hadoop.yarn.webapp.TestWebApp* tests over JDK17 i am facing the following errors: {noformat} Caused by: A MultiException has 2 exceptions. They are: 1. javax.xml.bind.JAXBException: Implementation of JAXB-API has not been found on module path or classpath. 2. java.lang.IllegalStateException: Unable to perform operation: create on org.apache.hadoop.yarn.webapp.MyTe", "qna_task": "Question: What is this issue about?\nAnswer: [JDK 17] Implementation of JAXB-API has not been found on module path or classpath"}}
{"id": "13627890", "key": "HADOOP-19673", "project": "HADOOP", "summary": "BloomMapFile: invalid io.mapfile.bloom.error.rate (\u22640 or \u22651) causes NaN/zero vector size and writer construction failure", "description": "{{BloomMapFile.Writer#initBloomFilter(Configuration)}} computes the Bloom filter vector size as: {code:java} int numKeys = conf.getInt(IO_MAPFILE_BLOOM_SIZE_KEY, IO_MAPFILE_BLOOM_SIZE_DEFAULT); float errorRate = conf.getFloat(IO_MAPFILE_BLOOM_ERROR_RATE_KEY, IO_MAPFILE_BLOOM_ERROR_RATE_DEFAULT); int vectorSize = (int) Math.ceil( (double)(-HASH_COUNT * numKeys) / Math.log(1.0 - Math.pow(errorRate, 1.0 / HASH_COUNT)) ); {code} When {{io.mapfile.bloom.error.rate}} is *\u2264 0* or {*}\u2265 1{*}: * {{Math.pow(errorRate, 1/k)}} produces *NaN* (negative base with non-integer exponent) or an invalid value; * {{Math.log(1 - NaN)}} becomes {*}NaN{*}; * {{Math.ceil(NaN)}} cast to {{int}} yields {*}0{*}, so {{{}vectorSize == 0{}}}; * constructing {{DynamicBloomFilter}} subsequently fails, and {{BloomMapFile.Writer}} construction fails (observed as assertion failure in tests). The code misses input validation for {{io.mapfile.bloom.error.rate}} which should be strictly within {{{}(0, 1){}}}. With invalid values, the math silently degrades to NaN/0 and fails at runtime. *Reproduction* Injected values: {{io.mapfile.bloom.error.rate = 0,-1}} Test: {{org.apache.hadoop.io.TestBloomMapFile#testBloomMapFileConstructors}} {code:java} [INFO] Running org.apache.hadoop.io.TestBloomMapFile [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.358 s <<< FAILURE! - in org.apache.hadoop.io.TestBloomMapFile [ERROR] org.apache.hadoop.io.TestBloomMapFile.testBloomMapFileConstructors Time elapsed: 0.272 s <<< FAILURE! java.lang.AssertionError: testBloomMapFileConstructors error !!! at org.apache.hadoop.io.TestBloomMapFile.testBloomMapFileConstructors(TestBloomMapFile.java:287{code}", "comments": "", "created": "2025-09-02T14:20:00.000+0000", "updated": "2025-09-07T07:57:06.000+0000", "derived": {"summary_task": "Summarize this issue: {{BloomMapFile.Writer#initBloomFilter(Configuration)}} computes the Bloom filter vector size as: {code:java} int numKeys = conf.getInt(IO_MAPFILE_BLOOM_SIZE_KEY, IO_MAPFILE_BLOOM_SIZE_DEFAULT); float errorRate = conf.getFloat(IO_MAPFILE_BLOOM_ERROR_RATE_KEY, IO_MAPFILE_BLOOM_ERROR_RATE_DEFAULT); int vectorSize = (int) Math.ceil( (double)(-HASH_COUNT * numKeys) / Math.log(1.0 - Math.pow(errorRate, ", "classification_task": "Classify the issue priority and type: {{BloomMapFile.Writer#initBloomFilter(Configuration)}} computes the Bloom filter vector size as: {code:java} int numKeys = conf.getInt(IO_MAPFILE_BLOOM_SIZE_KEY, IO_MAPFILE_BLOOM_SIZE_DEFAULT); float errorRate = conf.getFloat(IO_MAPFILE_BLOOM_ERROR_RATE_KEY, IO_MAPFILE_BLOOM_ERROR_RATE_DEFAULT); int vectorSize = (int) Math.ceil( (double)(-HASH_COUNT * numKeys) / Math.log(1.0 - Math.pow(errorRate, ", "qna_task": "Question: What is this issue about?\nAnswer: BloomMapFile: invalid io.mapfile.bloom.error.rate (\u22640 or \u22651) causes NaN/zero vector size and writer construction failure"}}
{"id": "13627795", "key": "HADOOP-19672", "project": "HADOOP", "summary": "ABFS: Network Error-Based Client Switchover: Apache to JDK (continuous failure))", "description": "", "comments": "bhattmanish98 opened a new pull request, #7967: URL: https://github.com/apache/hadoop/pull/7967 JIRA \u2013 https://issues.apache.org/jira/browse/HADOOP-19672 In case of a network error while using the Apache client, we allow the client to switch over from Apache to JDK. This network fallback occurs in two scenarios: 1. During file system initialization \u2013 When warming up the cache, if no connection is created (indicating an issue with the Apache client), the system will fall back to the JDK. 2. During a network call \u2013 If an I/O or Unknown Host exception occurs for three consecutive retries, the system will fall back to the JDK. This fallback is applied at the JVM level, so all file system calls will use the JDK client once the switch occurs. There is also a possibility of recovery. During cache warmup, if connections are successfully created using the Apache client, the system will automatically switch back to the Apache client. hadoop-yetus commented on PR #7967: URL: https://github.com/apache/hadoop/pull/7967#issuecomment-3293191749 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 36s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 57m 10s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 46s | | trunk passed | | +1 :green_heart: | javadoc | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 12s | | trunk passed | | +1 :green_heart: | shadedclient | 35m 50s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 1 new + 2 unchanged - 0 fixed = 3 total (was 2) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 29s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 27s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 10s | | the patch passed | | +1 :green_heart: | shadedclient | 35m 16s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 59s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 41s | | The patch does not generate ASF License warnings. | | | | 144m 47s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7967 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux b7ef8b2a88ce 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1dad672faf42fbfb0ada2a95456b922a50b2becf | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/1/testReport/ | | Max. process+thread count | 693 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7967: URL: https://github.com/apache/hadoop/pull/7967#issuecomment-3293931499 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 15m 49s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 50m 45s | | trunk passed | | +1 :green_heart: | compile | 0m 44s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 40s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | +1 :green_heart: | javadoc | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 38s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 11s | | trunk passed | | +1 :green_heart: | shadedclient | 35m 48s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 21s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 29s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 27s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 39m 10s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 6s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 156m 38s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7967 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux f1f4dba85ce3 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 6faa4c9c8ad8083e0a636b6439146a24c66c0ce3 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/2/testReport/ | | Max. process+thread count | 555 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. anmolanmol1234 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2381728892 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsApacheHttpClient.java: ########## @@ -73,18 +80,21 @@ static boolean usable() { } AbfsApacheHttpClient(DelegatingSSLSocketFactory delegatingSSLSocketFactory, - final AbfsConfiguration abfsConfiguration, final KeepAliveCache keepAliveCache, - URL baseUrl) { + final AbfsConfiguration abfsConfiguration, + final KeepAliveCache keepAliveCache, + URL baseUrl, + final boolean isCacheWarmupNeeded) { final AbfsConnectionManager connMgr = new AbfsConnectionManager( createSocketFactoryRegistry( new SSLConnectionSocketFactory(delegatingSSLSocketFactory, getDefaultHostnameVerifier())), new AbfsHttpClientConnectionFactory(), keepAliveCache, - abfsConfiguration, baseUrl); + abfsConfiguration, baseUrl, isCacheWarmupNeeded); final HttpClientBuilder builder = HttpClients.custom(); builder.setConnectionManager(connMgr) .setRequestExecutor( - new AbfsManagedHttpRequestExecutor(abfsConfiguration.getHttpReadTimeout())) + new AbfsManagedHttpRequestExecutor( Review Comment: As we were discussing last time, if we keep it to read timeout the 100 continue timeout would become 30 seconds, this should be another config for 100 continue timeout anmolanmol1234 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2381745959 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -254,7 +259,8 @@ private AbfsClient(final URL baseUrl, abfsApacheHttpClient = new AbfsApacheHttpClient( DelegatingSSLSocketFactory.getDefaultFactory(), - abfsConfiguration, keepAliveCache, baseUrl); + abfsConfiguration, keepAliveCache, baseUrl, + abfsConfiguration.getFsConfiguredServiceType() == abfsServiceType); Review Comment: Can you add some comments around this change as to how it would affect the need for cache warmup anmolanmol1234 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2381793492 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsConnectionManager.java: ########## @@ -91,26 +92,34 @@ class AbfsConnectionManager implements HttpClientConnectionManager { /** * The base host for which connections are managed. */ - private HttpHost baseHost; + private final HttpHost baseHost; AbfsConnectionManager(Registry<ConnectionSocketFactory> socketFactoryRegistry, - AbfsHttpClientConnectionFactory connectionFactory, KeepAliveCache kac, - final AbfsConfiguration abfsConfiguration, final URL baseUrl) { + AbfsHttpClientConnectionFactory connectionFactory, + KeepAliveCache kac, + final AbfsConfiguration abfsConfiguration, + final URL baseUrl, + final boolean isCacheWarmupNeeded) { this.httpConnectionFactory = connectionFactory; this.kac = kac; this.connectionOperator = new DefaultHttpClientConnectionOperator( socketFactoryRegistry, null, null); this.abfsConfiguration = abfsConfiguration; - if (abfsConfiguration.getApacheCacheWarmupCount() > 0 + this.baseHost = new HttpHost(baseUrl.getHost(), + baseUrl.getDefaultPort(), baseUrl.getProtocol()); + if (isCacheWarmupNeeded && abfsConfiguration.getApacheCacheWarmupCount() > 0 && kac.getFixedThreadPool() != null) { // Warm up the cache with connections. LOG.debug(\"Warming up the KeepAliveCache with {} connections\", abfsConfiguration.getApacheCacheWarmupCount()); - this.baseHost = new HttpHost(baseUrl.getHost(), - baseUrl.getDefaultPort(), baseUrl.getProtocol()); HttpRoute route = new HttpRoute(baseHost, null, true); - cacheExtraConnection(route, + int totalConnectionsCreated = cacheExtraConnection(route, abfsConfiguration.getApacheCacheWarmupCount()); + if (totalConnectionsCreated == 0) { Review Comment: even if we fail or catch rejected exception for any one of the tasks we want to register fallback as the successfully submitted tasks might have increased the count of totalConnectionsCreated anmolanmol1234 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2381815558 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestApacheClientConnectionPool.java: ########## @@ -118,6 +121,38 @@ public void testConnectedConnectionLogging() throws Exception { .isEqualTo(4); } + /** + * Test to verify that the ApacheHttpClient falls back to JDK client + * when connection warmup fails. + * This test is applicable only for ApacheHttpClient. + */ + @Test + public void testApacheClientFallbackDuringConnectionWarmup() + throws Exception { + try (KeepAliveCache keepAliveCache = new KeepAliveCache( + new AbfsConfiguration(new Configuration(), EMPTY_STRING))) { + // Create a connection manager with invalid URL to force fallback to JDK client + // during connection warmup. + // This is to simulate failure during connection warmup in the connection manager. + // The invalid URL will cause the connection manager to fail to create connections + // during warmup, forcing it to fall back to JDK client. + final AbfsConnectionManager connMgr = new AbfsConnectionManager( + RegistryBuilder.<ConnectionSocketFactory>create() + .register(HTTPS_SCHEME, new SSLConnectionSocketFactory( + DelegatingSSLSocketFactory.getDefaultFactory(), + getDefaultHostnameVerifier())) + .build(), + new AbfsHttpClientConnectionFactory(), keepAliveCache, + new AbfsConfiguration(new Configuration(), EMPTY_STRING), + new URL(\"https://test.com\"), true); + + Assertions.assertThat(AbfsApacheHttpClient.usable()) + .describedAs(\"Apache HttpClient should be not usable\") Review Comment: Can you make one http call and validate now jdk is being used, user agent can be used for validation anujmodi2021 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2382005373 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientHandler.java: ########## @@ -68,13 +68,13 @@ public AbfsClientHandler(final URL baseUrl, final SASTokenProvider sasTokenProvider, final EncryptionContextProvider encryptionContextProvider, final AbfsClientContext abfsClientContext) throws IOException { + initServiceType(abfsConfiguration); Review Comment: Why this change? ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java: ########## @@ -188,7 +189,7 @@ public AbfsBlobClient(final URL baseUrl, final EncryptionContextProvider encryptionContextProvider, final AbfsClientContext abfsClientContext) throws IOException { super(baseUrl, sharedKeyCredentials, abfsConfiguration, tokenProvider, - encryptionContextProvider, abfsClientContext); + encryptionContextProvider, abfsClientContext, AbfsServiceType.BLOB); Review Comment: Why this change? ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsApacheHttpClient.java: ########## @@ -73,18 +80,21 @@ static boolean usable() { } AbfsApacheHttpClient(DelegatingSSLSocketFactory delegatingSSLSocketFactory, - final AbfsConfiguration abfsConfiguration, final KeepAliveCache keepAliveCache, - URL baseUrl) { + final AbfsConfiguration abfsConfiguration, + final KeepAliveCache keepAliveCache, + URL baseUrl, + final boolean isCacheWarmupNeeded) { final AbfsConnectionManager connMgr = new AbfsConnectionManager( createSocketFactoryRegistry( new SSLConnectionSocketFactory(delegatingSSLSocketFactory, getDefaultHostnameVerifier())), new AbfsHttpClientConnectionFactory(), keepAliveCache, - abfsConfiguration, baseUrl); + abfsConfiguration, baseUrl, isCacheWarmupNeeded); final HttpClientBuilder builder = HttpClients.custom(); builder.setConnectionManager(connMgr) .setRequestExecutor( - new AbfsManagedHttpRequestExecutor(abfsConfiguration.getHttpReadTimeout())) + new AbfsManagedHttpRequestExecutor( Review Comment: +1 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -254,7 +259,8 @@ private AbfsClient(final URL baseUrl, abfsApacheHttpClient = new AbfsApacheHttpClient( DelegatingSSLSocketFactory.getDefaultFactory(), - abfsConfiguration, keepAliveCache, baseUrl); + abfsConfiguration, keepAliveCache, baseUrl, + abfsConfiguration.getFsConfiguredServiceType() == abfsServiceType); Review Comment: +1 What are we trying to achieve here? anmolanmol1234 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2381793492 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsConnectionManager.java: ########## @@ -91,26 +92,34 @@ class AbfsConnectionManager implements HttpClientConnectionManager { /** * The base host for which connections are managed. */ - private HttpHost baseHost; + private final HttpHost baseHost; AbfsConnectionManager(Registry<ConnectionSocketFactory> socketFactoryRegistry, - AbfsHttpClientConnectionFactory connectionFactory, KeepAliveCache kac, - final AbfsConfiguration abfsConfiguration, final URL baseUrl) { + AbfsHttpClientConnectionFactory connectionFactory, + KeepAliveCache kac, + final AbfsConfiguration abfsConfiguration, + final URL baseUrl, + final boolean isCacheWarmupNeeded) { this.httpConnectionFactory = connectionFactory; this.kac = kac; this.connectionOperator = new DefaultHttpClientConnectionOperator( socketFactoryRegistry, null, null); this.abfsConfiguration = abfsConfiguration; - if (abfsConfiguration.getApacheCacheWarmupCount() > 0 + this.baseHost = new HttpHost(baseUrl.getHost(), + baseUrl.getDefaultPort(), baseUrl.getProtocol()); + if (isCacheWarmupNeeded && abfsConfiguration.getApacheCacheWarmupCount() > 0 && kac.getFixedThreadPool() != null) { // Warm up the cache with connections. LOG.debug(\"Warming up the KeepAliveCache with {} connections\", abfsConfiguration.getApacheCacheWarmupCount()); - this.baseHost = new HttpHost(baseUrl.getHost(), - baseUrl.getDefaultPort(), baseUrl.getProtocol()); HttpRoute route = new HttpRoute(baseHost, null, true); - cacheExtraConnection(route, + int totalConnectionsCreated = cacheExtraConnection(route, abfsConfiguration.getApacheCacheWarmupCount()); + if (totalConnectionsCreated == 0) { Review Comment: even if we fail or catch rejected exception for any one of the tasks we want to register fallback ? as the successfully submitted tasks might have increased the count of totalConnectionsCreated manika137 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2387212747 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -214,7 +214,7 @@ public final class FileSystemConfigurations { public static final long THOUSAND = 1000L; public static final HttpOperationType DEFAULT_NETWORKING_LIBRARY - = HttpOperationType.JDK_HTTP_URL_CONNECTION; + = HttpOperationType.APACHE_HTTP_CLIENT; Review Comment: Nit: We should change this in our md file as well manika137 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2390726345 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -214,7 +214,7 @@ public final class FileSystemConfigurations { public static final long THOUSAND = 1000L; public static final HttpOperationType DEFAULT_NETWORKING_LIBRARY - = HttpOperationType.JDK_HTTP_URL_CONNECTION; + = HttpOperationType.APACHE_HTTP_CLIENT; public static final int DEFAULT_APACHE_HTTP_CLIENT_MAX_IO_EXCEPTION_RETRIES = 3; Review Comment: where are we checking after 3 retries we'll fallback to JDK? manika137 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2390726345 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -214,7 +214,7 @@ public final class FileSystemConfigurations { public static final long THOUSAND = 1000L; public static final HttpOperationType DEFAULT_NETWORKING_LIBRARY - = HttpOperationType.JDK_HTTP_URL_CONNECTION; + = HttpOperationType.APACHE_HTTP_CLIENT; public static final int DEFAULT_APACHE_HTTP_CLIENT_MAX_IO_EXCEPTION_RETRIES = 3; Review Comment: where are we checking after 3 retries we'll fallback to JDK? bhattmanish98 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2393513847 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsApacheHttpClient.java: ########## @@ -73,18 +80,21 @@ static boolean usable() { } AbfsApacheHttpClient(DelegatingSSLSocketFactory delegatingSSLSocketFactory, - final AbfsConfiguration abfsConfiguration, final KeepAliveCache keepAliveCache, - URL baseUrl) { + final AbfsConfiguration abfsConfiguration, + final KeepAliveCache keepAliveCache, + URL baseUrl, + final boolean isCacheWarmupNeeded) { final AbfsConnectionManager connMgr = new AbfsConnectionManager( createSocketFactoryRegistry( new SSLConnectionSocketFactory(delegatingSSLSocketFactory, getDefaultHostnameVerifier())), new AbfsHttpClientConnectionFactory(), keepAliveCache, - abfsConfiguration, baseUrl); + abfsConfiguration, baseUrl, isCacheWarmupNeeded); final HttpClientBuilder builder = HttpClients.custom(); builder.setConnectionManager(connMgr) .setRequestExecutor( - new AbfsManagedHttpRequestExecutor(abfsConfiguration.getHttpReadTimeout())) + new AbfsManagedHttpRequestExecutor( Review Comment: Sure, will create a new config for read timeout and use that when 100 continue is enabled. bhattmanish98 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2393517614 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -254,7 +259,8 @@ private AbfsClient(final URL baseUrl, abfsApacheHttpClient = new AbfsApacheHttpClient( DelegatingSSLSocketFactory.getDefaultFactory(), - abfsConfiguration, keepAliveCache, baseUrl); + abfsConfiguration, keepAliveCache, baseUrl, + abfsConfiguration.getFsConfiguredServiceType() == abfsServiceType); Review Comment: The reason for this change: Since the keep alive cache is on client level and we were doing cache warmup for both the client separately. Now with this change, we will do cache warmup only for the default client, not for both the clients. bhattmanish98 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2393517614 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -254,7 +259,8 @@ private AbfsClient(final URL baseUrl, abfsApacheHttpClient = new AbfsApacheHttpClient( DelegatingSSLSocketFactory.getDefaultFactory(), - abfsConfiguration, keepAliveCache, baseUrl); + abfsConfiguration, keepAliveCache, baseUrl, + abfsConfiguration.getFsConfiguredServiceType() == abfsServiceType); Review Comment: The reason for this change: Since the keep alive cache is on client level and we were doing cache warmup for both the client separately. Now with this change, we will do cache warmup only for the default client, not for both the clients. Will add the comment in the code as well bhattmanish98 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2393520715 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestApacheClientConnectionPool.java: ########## @@ -118,6 +121,38 @@ public void testConnectedConnectionLogging() throws Exception { .isEqualTo(4); } + /** + * Test to verify that the ApacheHttpClient falls back to JDK client + * when connection warmup fails. + * This test is applicable only for ApacheHttpClient. + */ + @Test + public void testApacheClientFallbackDuringConnectionWarmup() + throws Exception { + try (KeepAliveCache keepAliveCache = new KeepAliveCache( + new AbfsConfiguration(new Configuration(), EMPTY_STRING))) { + // Create a connection manager with invalid URL to force fallback to JDK client + // during connection warmup. + // This is to simulate failure during connection warmup in the connection manager. + // The invalid URL will cause the connection manager to fail to create connections + // during warmup, forcing it to fall back to JDK client. + final AbfsConnectionManager connMgr = new AbfsConnectionManager( + RegistryBuilder.<ConnectionSocketFactory>create() + .register(HTTPS_SCHEME, new SSLConnectionSocketFactory( + DelegatingSSLSocketFactory.getDefaultFactory(), + getDefaultHostnameVerifier())) + .build(), + new AbfsHttpClientConnectionFactory(), keepAliveCache, + new AbfsConfiguration(new Configuration(), EMPTY_STRING), + new URL(\"https://test.com\"), true); + + Assertions.assertThat(AbfsApacheHttpClient.usable()) + .describedAs(\"Apache HttpClient should be not usable\") Review Comment: Sure, will do that. bhattmanish98 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2393522594 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java: ########## @@ -188,7 +189,7 @@ public AbfsBlobClient(final URL baseUrl, final EncryptionContextProvider encryptionContextProvider, final AbfsClientContext abfsClientContext) throws IOException { super(baseUrl, sharedKeyCredentials, abfsConfiguration, tokenProvider, - encryptionContextProvider, abfsClientContext); + encryptionContextProvider, abfsClientContext, AbfsServiceType.BLOB); Review Comment: As described in the previous comment, we need to find out default client and for that we are comparing these values with the default value configured. bhattmanish98 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2393523578 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -214,7 +214,7 @@ public final class FileSystemConfigurations { public static final long THOUSAND = 1000L; public static final HttpOperationType DEFAULT_NETWORKING_LIBRARY - = HttpOperationType.JDK_HTTP_URL_CONNECTION; + = HttpOperationType.APACHE_HTTP_CLIENT; Review Comment: Sure, will make the change in .md file as well bhattmanish98 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2393620859 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsConnectionManager.java: ########## @@ -91,26 +92,34 @@ class AbfsConnectionManager implements HttpClientConnectionManager { /** * The base host for which connections are managed. */ - private HttpHost baseHost; + private final HttpHost baseHost; AbfsConnectionManager(Registry<ConnectionSocketFactory> socketFactoryRegistry, - AbfsHttpClientConnectionFactory connectionFactory, KeepAliveCache kac, - final AbfsConfiguration abfsConfiguration, final URL baseUrl) { + AbfsHttpClientConnectionFactory connectionFactory, + KeepAliveCache kac, + final AbfsConfiguration abfsConfiguration, + final URL baseUrl, + final boolean isCacheWarmupNeeded) { this.httpConnectionFactory = connectionFactory; this.kac = kac; this.connectionOperator = new DefaultHttpClientConnectionOperator( socketFactoryRegistry, null, null); this.abfsConfiguration = abfsConfiguration; - if (abfsConfiguration.getApacheCacheWarmupCount() > 0 + this.baseHost = new HttpHost(baseUrl.getHost(), + baseUrl.getDefaultPort(), baseUrl.getProtocol()); + if (isCacheWarmupNeeded && abfsConfiguration.getApacheCacheWarmupCount() > 0 && kac.getFixedThreadPool() != null) { // Warm up the cache with connections. LOG.debug(\"Warming up the KeepAliveCache with {} connections\", abfsConfiguration.getApacheCacheWarmupCount()); - this.baseHost = new HttpHost(baseUrl.getHost(), - baseUrl.getDefaultPort(), baseUrl.getProtocol()); HttpRoute route = new HttpRoute(baseHost, null, true); - cacheExtraConnection(route, + int totalConnectionsCreated = cacheExtraConnection(route, abfsConfiguration.getApacheCacheWarmupCount()); + if (totalConnectionsCreated == 0) { Review Comment: Yes, make sense. I have updated the returned value in case of rejected exception; other thing will remain the same. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientHandler.java: ########## @@ -68,13 +68,13 @@ public AbfsClientHandler(final URL baseUrl, final SASTokenProvider sasTokenProvider, final EncryptionContextProvider encryptionContextProvider, final AbfsClientContext abfsClientContext) throws IOException { + initServiceType(abfsConfiguration); Review Comment: This will initialize the default and ingress service types. This is needed before crating the clients so that we can do cache warmup only for default client. hadoop-yetus commented on PR #7967: URL: https://github.com/apache/hadoop/pull/7967#issuecomment-3378551948 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 48s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 1s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 54m 51s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/3/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 43s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 10s | | trunk passed | | +1 :green_heart: | shadedclient | 39m 40s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 36s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 36s | | the patch passed | | +1 :green_heart: | compile | 0m 31s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 31s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 21s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 34s | | the patch passed | | +1 :green_heart: | javadoc | 0m 30s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 11s | | the patch passed | | +1 :green_heart: | shadedclient | 40m 2s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 8s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 150m 19s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7967 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 6778220a7832 5.15.0-157-generic #167-Ubuntu SMP Wed Sep 17 21:35:53 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 44765c0ae41d29a78198cb113bc06780be7092af | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/3/testReport/ | | Max. process+thread count | 561 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/3/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. anmolanmol1234 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2415677758 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientHandler.java: ########## @@ -68,6 +68,9 @@ public AbfsClientHandler(final URL baseUrl, final SASTokenProvider sasTokenProvider, final EncryptionContextProvider encryptionContextProvider, final AbfsClientContext abfsClientContext) throws IOException { + // This will initialize the default and ingress service types. + // This is needed before crating the clients so that we can do cache warmup Review Comment: nit: typo creating hadoop-yetus commented on PR #7967: URL: https://github.com/apache/hadoop/pull/7967#issuecomment-3386392364 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 50s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 53m 46s | | trunk passed | | +1 :green_heart: | compile | 0m 44s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 43s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 11s | | trunk passed | | +1 :green_heart: | shadedclient | 40m 20s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 36s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 36s | | the patch passed | | +1 :green_heart: | compile | 0m 31s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 31s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 21s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 35s | | the patch passed | | +1 :green_heart: | javadoc | 0m 28s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 40m 32s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 7s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 150m 27s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7967 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux a733619d79ed 5.15.0-157-generic #167-Ubuntu SMP Wed Sep 17 21:35:53 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7ff70868cc0483c55d875f66cf2379f04e75075e | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/4/testReport/ | | Max. process+thread count | 526 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/4/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. bhattmanish98 commented on PR #7967: URL: https://github.com/apache/hadoop/pull/7967#issuecomment-3388294934 ============================================================ HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 872, Failures: 0, Errors: 0, Skipped: 217 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ HNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 5 [WARNING] Tests run: 875, Failures: 0, Errors: 0, Skipped: 169 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 10 ============================================================ NonHNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 714, Failures: 0, Errors: 0, Skipped: 282 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11 ============================================================ AppendBlob-HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 872, Failures: 0, Errors: 0, Skipped: 228 [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 9 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-SharedKey-Blob ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 721, Failures: 0, Errors: 0, Skipped: 140 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11 ============================================================ NonHNS-OAuth-DFS ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 711, Failures: 0, Errors: 0, Skipped: 284 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 718, Failures: 0, Errors: 0, Skipped: 152 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ AppendBlob-NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 713, Failures: 0, Errors: 0, Skipped: 198 [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ HNS-Oauth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 746, Failures: 0, Errors: 0, Skipped: 226 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-OAuth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 711, Failures: 0, Errors: 0, Skipped: 281 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 anujmodi2021 merged PR #7967: URL: https://github.com/apache/hadoop/pull/7967 steveloughran commented on PR #7967: URL: https://github.com/apache/hadoop/pull/7967#issuecomment-3401193052 I understand why the startup mechanism makes sense, but I'm curious about doing it on a live connection. What problems were occurring with hostname lookup that changing client would fix? Do you want this in 3.4.3? if so do a backport PR ASAP bhattmanish98 commented on PR #7967: URL: https://github.com/apache/hadoop/pull/7967#issuecomment-3432065456 @steveloughran We\u2019re performing cache refresh when the number of available connections in the cache drops below a certain threshold. The warmup normally happens during file system initialization, but if the file system is already active and making multiple parallel network calls, creating new connections on demand with the Apache client tends to take longer. To avoid delays in such cases, we asynchronously pre-create and cache a few additional connections whenever the pool runs low. This ensures that subsequent network calls can use already-established connections and spend time on actual data processing rather than connection setup. bhattmanish98 commented on PR #7967: URL: https://github.com/apache/hadoop/pull/7967#issuecomment-3435042669 > Do you want this in 3.4.3? if so do a backport PR ASAP The previous PR that includes the Apache client changes was also not included in release 3.4.3. We\u2019re planning to include this change in the next major release.", "created": "2025-09-01T10:54:52.000+0000", "updated": "2025-10-23T04:38:43.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: ABFS: Network Error-Based Client Switchover: Apache to JDK (continuous failure))"}}
{"id": "13627731", "key": "HADOOP-19671", "project": "HADOOP", "summary": "Migrate to AssertJ for Assertion Verification", "description": "Currently, the unit tests in the project do not fully leverage modern assertion capabilities, resulting in lower readability and maintainability of the test code. To improve test clarity and extensibility, we plan to migrate the existing assertion library to {*}AssertJ{*}. *Objective:* * Migrate all assertions in existing tests from JUnit or other assertion libraries to AssertJ. * Utilize AssertJ\u2019s rich assertion methods (e.g., fluent API, more readable assertions) to enhance the expressiveness of unit tests. * Ensure that all existing unit tests continue to run correctly after migration. *Implementation Steps:* # Analyze existing unit test code to identify assertions that need to be replaced. # Replace existing assertions with AssertJ assertion syntax. # Run unit tests to ensure the tests pass and function correctly after migration. # Update relevant documentation to ensure the team is aware of how to use AssertJ for assertions.", "comments": "[~stevel@apache.org] The JUnit 5 upgrade for Hadoop is nearing completion, and I will soon initiate a new upgrade plan to migrate the unit tests to AssertJ in order to improve test readability and maintainability. We need to establish a new set of unit testing standards. If you have any relevant suggestions or rules, feel free to share.", "created": "2025-08-30T06:33:03.000+0000", "updated": "2025-08-30T06:39:18.000+0000", "derived": {"summary_task": "Summarize this issue: Currently, the unit tests in the project do not fully leverage modern assertion capabilities, resulting in lower readability and maintainability of the test code. To improve test clarity and extensibility, we plan to migrate the existing assertion library to {*}AssertJ{*}. *Objective:* * Migrate all assertions in existing tests from JUnit or other assertion libraries to AssertJ. * Utilize AssertJ\u2019", "classification_task": "Classify the issue priority and type: Currently, the unit tests in the project do not fully leverage modern assertion capabilities, resulting in lower readability and maintainability of the test code. To improve test clarity and extensibility, we plan to migrate the existing assertion library to {*}AssertJ{*}. *Objective:* * Migrate all assertions in existing tests from JUnit or other assertion libraries to AssertJ. * Utilize AssertJ\u2019", "qna_task": "Question: What is this issue about?\nAnswer: Migrate to AssertJ for Assertion Verification"}}
{"id": "13627643", "key": "HADOOP-19670", "project": "HADOOP", "summary": "Replace Thread with SubjectPreservingThread", "description": "", "comments": "", "created": "2025-08-29T08:22:44.000+0000", "updated": "2025-09-24T16:11:48.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Replace Thread with SubjectPreservingThread"}}
{"id": "13627641", "key": "HADOOP-19669", "project": "HADOOP", "summary": "Update Daemon to restore pre JDK22 Subject behaviour in Threads", "description": "", "comments": "Merged into HADOOP-196668", "created": "2025-08-29T07:57:27.000+0000", "updated": "2025-09-10T13:21:24.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Update Daemon to restore pre JDK22 Subject behaviour in Threads"}}
{"id": "13627637", "key": "HADOOP-19668", "project": "HADOOP", "summary": "Add SubjectInheritingThread to restore pre JDK22 Subject behaviour in Threads", "description": "This is the first part of HADOOP-19574 which adds the compatibility code, and fixes Daemon, but does not replace native Thread instances.", "comments": "", "created": "2025-08-29T07:23:38.000+0000", "updated": "2025-09-10T13:21:24.000+0000", "derived": {"summary_task": "Summarize this issue: This is the first part of HADOOP-19574 which adds the compatibility code, and fixes Daemon, but does not replace native Thread instances.", "classification_task": "Classify the issue priority and type: This is the first part of HADOOP-19574 which adds the compatibility code, and fixes Daemon, but does not replace native Thread instances.", "qna_task": "Question: What is this issue about?\nAnswer: Add SubjectInheritingThread to restore pre JDK22 Subject behaviour in Threads"}}
{"id": "13627442", "key": "HADOOP-19667", "project": "HADOOP", "summary": "Clarify legal value constraints for hadoop.security.crypto.buffer.size (min=512; floor to cipher block size)", "description": "In core-default.xml, the property hadoop.security.crypto.buffer.size is currently documented as \u201cThe buffer size used by CryptoInputStream and CryptoOutputStream.\u201d It does not specify the legal value constraints. {code:java} <property> <name>hadoop.security.crypto.buffer.size</name> <value>8192</value> <description>The buffer size used by CryptoInputStream and CryptoOutputStream. </description> </property> {code} The runtime enforces two hidden constraints that are not documented: 1. Minimum value is 512 bytes. Values below 512 cause IllegalArgumentException at stream construction time. 2. Block-size flooring: The effective buffer size is floored to a multiple of the cipher algorithm\u2019s block size (e.g., 16 bytes for AES/CTR/NoPadding). As a result, users may be surprised that: 1. Setting a value like 4100 results in an actual capacity of 4096. 2. Setting values <512 fails fast with IllegalArgumentException. *Expected* core-default.xml (and user-facing docs) should explicitly document: 1. Minimum legal value: 512 bytes. 2. The effective value is floored to the nearest multiple of the cipher algorithm block size (e.g., 16 for AES).", "comments": "please provide a pull request in trunk which updates the file with the relevant information. hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/TransparentEncryption.md zzz0706 opened a new pull request, #7907: URL: https://github.com/apache/hadoop/pull/7907 ### Description of PR Docs-only change to **TransparentEncryption.md** clarifying the hidden constraints for `hadoop.security.crypto.buffer.size`: - **Minimum value is 512 bytes** (values <512 throw `IllegalArgumentException` at stream construction). - **Block alignment:** the effective value is **floored** to the nearest multiple of the cipher algorithm **block size** (e.g., 16 for AES/CTR/NoPadding). Examples: `4100 -> 4096`, `8195 -> 8192`. This is a minimal wording update (single-sentence addition) to avoid operator surprise, with no behavior change. **Target branch:** `trunk` (per contributor guide) **JIRA:** HADOOP-19667 **Files changed:** - `hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/TransparentEncryption.md` ### How was this patch tested? - Docs-only; built site/module locally to validate formatting: Thanks for the guidance. I\u2019ve opened a PR against trunk updating the requested file. hadoop-yetus commented on PR #7907: URL: https://github.com/apache/hadoop/pull/7907#issuecomment-3228575609 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 14m 55s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 40m 25s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 19s | | trunk passed | | +1 :green_heart: | shadedclient | 77m 7s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 1m 4s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 1m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 39m 27s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 39s | | The patch does not generate ASF License warnings. | | | | 134m 51s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7907/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7907 | | JIRA Issue | HADOOP-19667 | | Optional Tests | dupname asflicense mvnsite codespell detsecrets markdownlint | | uname | Linux 04003553fdc2 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / ce5422f5a2696efe7d65ae74ebcb3413fe33abde | | Max. process+thread count | 667 (vs. ulimit of 5500) | | modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7907/1/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-27T10:08:02.000+0000", "updated": "2025-08-27T15:02:06.000+0000", "derived": {"summary_task": "Summarize this issue: In core-default.xml, the property hadoop.security.crypto.buffer.size is currently documented as \u201cThe buffer size used by CryptoInputStream and CryptoOutputStream.\u201d It does not specify the legal value constraints. {code:java} <property> <name>hadoop.security.crypto.buffer.size</name> <value>8192</value> <description>The buffer size used by CryptoInputStream and CryptoOutputStream. </description> </", "classification_task": "Classify the issue priority and type: In core-default.xml, the property hadoop.security.crypto.buffer.size is currently documented as \u201cThe buffer size used by CryptoInputStream and CryptoOutputStream.\u201d It does not specify the legal value constraints. {code:java} <property> <name>hadoop.security.crypto.buffer.size</name> <value>8192</value> <description>The buffer size used by CryptoInputStream and CryptoOutputStream. </description> </", "qna_task": "Question: What is this issue about?\nAnswer: Clarify legal value constraints for hadoop.security.crypto.buffer.size (min=512; floor to cipher block size)"}}
{"id": "13627413", "key": "HADOOP-19666", "project": "HADOOP", "summary": "Add hardware-accelerated CRC32 support for riscv64 using the v,zbc,zvbc extension", "description": "This PR implements vector-accelerated CRC32 using the RISC-V V, Zbc and Zvbc instruction sets, with full functional verification and performance testing completed. The implementation uses the vclmul.v and vclmulh.v (carry-less multiply) instructions for data folding and computes the final checksum via Barrett reduction. Key Features: 1. Runtime Hardware Detection The PR uses kernel hardware probing and cpuinfo parsing to dynamically detect hardware support for CRC32 acceleration (via v, zbc, and zvbc extensions) at runtime. 2. Performance Improvement Hardware-accelerated CRC32 achieves a performance boost of over *3x* compared to the software implementation.", "comments": "leiwen2025 opened a new pull request, #7912: URL: https://github.com/apache/hadoop/pull/7912 This PR implements vector-accelerated CRC32 using the RISC-V V, Zbc and Zvbc instruction sets, with full functional verification and performance testing completed. The implementation uses the vclmul.v and vclmulh.v (carry-less multiply) instructions for data folding and computes the final checksum via Barrett reduction. Key Features: 1. Runtime Hardware Detection The PR uses kernel hardware probing and cpuinfo parsing to dynamically detect hardware support for CRC32 acceleration (via v, zbc, and zvbc extensions) at runtime. 2. Performance Improvement Hardware-accelerated CRC32 achieves a performance boost of over **3X** compared to the software implementation. <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR ### How was this patch tested? ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #7912: URL: https://github.com/apache/hadoop/pull/7912#issuecomment-3232576752 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 13m 5s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 26m 30s | | trunk passed | | +1 :green_heart: | compile | 7m 30s | | trunk passed | | -1 :x: | mvnsite | 1m 29s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in trunk failed. | | +1 :green_heart: | shadedclient | 56m 43s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 39s | | the patch passed | | +1 :green_heart: | compile | 7m 3s | | the patch passed | | +1 :green_heart: | cc | 7m 3s | | the patch passed | | +1 :green_heart: | golang | 7m 3s | | the patch passed | | +1 :green_heart: | javac | 7m 3s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/artifact/out/blanks-eol.txt) | The patch has 20 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -1 :x: | blanks | 0m 0s | [/blanks-tabs.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/artifact/out/blanks-tabs.txt) | The patch 2 line(s) with tabs. | | -1 :x: | mvnsite | 1m 29s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch failed. | | +1 :green_heart: | shadedclient | 22m 57s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 19m 44s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 20s | | The patch does not generate ASF License warnings. | | | | 123m 6s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7912 | | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang | | uname | Linux 3c797fab6900 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 67c832e1dc930b52b9a68c261f372b14e6cf1639 | | Default Java | Red Hat, Inc.-1.8.0_312-b07 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/testReport/ | | Max. process+thread count | 1262 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/console | | versions | git=2.27.0 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7912: URL: https://github.com/apache/hadoop/pull/7912#issuecomment-3233497529 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 26m 26s | | trunk passed | | +1 :green_heart: | compile | 7m 37s | | trunk passed | | -1 :x: | mvnsite | 1m 30s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/2/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in trunk failed. | | +1 :green_heart: | shadedclient | 57m 17s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 44s | | the patch passed | | +1 :green_heart: | compile | 6m 57s | | the patch passed | | +1 :green_heart: | cc | 6m 57s | | the patch passed | | +1 :green_heart: | golang | 6m 57s | | the patch passed | | +1 :green_heart: | javac | 6m 57s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 1m 29s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/2/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch failed. | | +1 :green_heart: | shadedclient | 23m 35s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 19m 38s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 20s | | The patch does not generate ASF License warnings. | | | | 111m 26s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7912 | | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang | | uname | Linux 9af82b84cde2 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / be2c9c5dfefc7f4ad7591c58f7857b177bef5b0e | | Default Java | Red Hat, Inc.-1.8.0_312-b07 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/2/testReport/ | | Max. process+thread count | 3150 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/2/console | | versions | git=2.27.0 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. leiwen2025 commented on PR #7912: URL: https://github.com/apache/hadoop/pull/7912#issuecomment-3409341544 @steveloughran @ayushtkn Hi, this PR has been open for a while. Could you please take a look when you have time? Thanks! steveloughran commented on PR #7912: URL: https://github.com/apache/hadoop/pull/7912#issuecomment-3416334594 It's been a long time since I did C code, I'll have to stare at this a while. In #7903 that creation of the file bulk_crc32_riscv.c should be what the new code goes into; work with @PeterPtroc to get something you are both happy with *and tested*. Having two people who are set up to build and test this on riscv hardware is exactly what we need to get this done how does this differ from HADOOP-19655? I think that PR got there just before this one, so takes precedence number-wise; I'm not in a position to evaluate the code. Can you and [~peterptroc] collaborate on this? I'll give you both credit in the patches leiwen2025 commented on PR #7912: URL: https://github.com/apache/hadoop/pull/7912#issuecomment-3448014826 @steveloughran Thanks for the review! I'll be happy to work with @PeterPtroc on this. If someone in the community could help connect me with Peter, that'd be much appreciated. I'd like to coordinate testing and integration on RISC-V. Thanks! This patch implements CRC32 on RISC-V using Zvbc vector instructions, while HADOOP-19655 uses Zbc scalar ones. Happy to work together with [~peterptroc] to coordinate and test both implementations.", "created": "2025-08-27T03:08:41.000+0000", "updated": "2025-10-26T05:02:39.000+0000", "derived": {"summary_task": "Summarize this issue: This PR implements vector-accelerated CRC32 using the RISC-V V, Zbc and Zvbc instruction sets, with full functional verification and performance testing completed. The implementation uses the vclmul.v and vclmulh.v (carry-less multiply) instructions for data folding and computes the final checksum via Barrett reduction. Key Features: 1. Runtime Hardware Detection The PR uses kernel hardware probin", "classification_task": "Classify the issue priority and type: This PR implements vector-accelerated CRC32 using the RISC-V V, Zbc and Zvbc instruction sets, with full functional verification and performance testing completed. The implementation uses the vclmul.v and vclmulh.v (carry-less multiply) instructions for data folding and computes the final checksum via Barrett reduction. Key Features: 1. Runtime Hardware Detection The PR uses kernel hardware probin", "qna_task": "Question: What is this issue about?\nAnswer: Add hardware-accelerated CRC32 support for riscv64 using the v,zbc,zvbc extension"}}
{"id": "13627372", "key": "HADOOP-19665", "project": "HADOOP", "summary": "[kms] Negative hadoop.security.kms.client.encrypted.key.cache.expiry causes KMSClientProvider init failure with generic IllegalArgumentException;", "description": "When the client-side config hadoop.security.kms.client.encrypted.key.cache.expiry is set to a negative value in core-site.xml, any tool that initializes KMSClientProvider (e.g., KeyShell) fails immediately with: {code:java} java.lang.IllegalArgumentException: expiry must be > 0 at org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:xxx) at org.apache.hadoop.crypto.key.kms.KMSClientProvider.<init>(KMSClientProvider.java:xxx) ... {code} This is a controlled failure (JVM doesn\u2019t crash), but the error message does not mention which property and what value triggered it. Users typically see a stack trace without a clear remediation hint. *Expected behavior* Fail fast with a clear configuration error that names the property and value, e.g.: Invalid configuration: hadoop.security.kms.client.encrypted.key.cache.expiry = -1 (must be > 0 ms) *Steps to Reproduce* 1. In the client core-site.xml, set: {code:xml} <property> <name>hadoop.security.kms.client.encrypted.key.cache.expiry</name> <value>-1</value> </property> {code} 2. Ensure the conf is active (echo $HADOOP_CONF_DIR points to this dir). 3. Run: {code:java} ./bin/hadoop key list -provider kms://http@localhost:9600/kms -metadata {code}", "comments": "", "created": "2025-08-26T14:11:12.000+0000", "updated": "2025-09-07T07:39:00.000+0000", "derived": {"summary_task": "Summarize this issue: When the client-side config hadoop.security.kms.client.encrypted.key.cache.expiry is set to a negative value in core-site.xml, any tool that initializes KMSClientProvider (e.g., KeyShell) fails immediately with: {code:java} java.lang.IllegalArgumentException: expiry must be > 0 at org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:xxx) at org.apache.hadoop.crypto.key.kms.KMSClientP", "classification_task": "Classify the issue priority and type: When the client-side config hadoop.security.kms.client.encrypted.key.cache.expiry is set to a negative value in core-site.xml, any tool that initializes KMSClientProvider (e.g., KeyShell) fails immediately with: {code:java} java.lang.IllegalArgumentException: expiry must be > 0 at org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:xxx) at org.apache.hadoop.crypto.key.kms.KMSClientP", "qna_task": "Question: What is this issue about?\nAnswer: [kms] Negative hadoop.security.kms.client.encrypted.key.cache.expiry causes KMSClientProvider init failure with generic IllegalArgumentException; "}}
{"id": "13627343", "key": "HADOOP-19664", "project": "HADOOP", "summary": "S3A Analytics-Accelerator: Move AAL to use Java sync client", "description": "Java sync client is giving the best performance for our use case, especially for readVectored() where a large number of requests can be made concurrently.", "comments": "sync or async? Sync. It's using async currently. But the readVectored + AAL use case is not ideal for the async client. As we already have our own thread pool, and each thread is responsible for making a single S3 request, and start reading data from that input stream immediately to fill the internal buffers.. With the async client, this means you need to join() immediately, and when at a higher concurrency things get stuck in the Netty thread pool and the AsyncResponseTransformer.toBlockingInputStream() of s3AsyncClient .getObject(builder.build(), AsyncResponseTransformer.toBlockingInputStream()). S3Async client works well (I think) when you have high concurrency but don't need to join on the data immediately, so the netty io pool is sufficient to satisfy those requests. internal benchmarking has been showing a 4-5% improvement with the Sync client consistently. ahmarsuhail opened a new pull request, #7909: URL: https://github.com/apache/hadoop/pull/7909 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR Moves AAL to use the Java sync client as this is giving us the best performance in internal benchmarks. ### How was this patch tested? Tested in ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? ahmarsuhail commented on PR #7909: URL: https://github.com/apache/hadoop/pull/7909#issuecomment-3228451753 @steveloughran @mukund-thakur @shameersss1 small PR to move to AAL to use the Java sync client hadoop-yetus commented on PR #7909: URL: https://github.com/apache/hadoop/pull/7909#issuecomment-3228959531 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 23s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 15s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 22m 54s | | trunk passed | | +1 :green_heart: | compile | 8m 28s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 7m 31s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 2m 1s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 1s | | trunk passed | | +1 :green_heart: | javadoc | 1m 0s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 58s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 30s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 21m 40s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 25s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 8m 9s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 9s | | the patch passed | | +1 :green_heart: | compile | 8m 25s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 8m 25s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 2m 25s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7909/1/artifact/out/results-checkstyle-root.txt) | root: The patch generated 1 new + 1 unchanged - 0 fixed = 2 total (was 1) | | +1 :green_heart: | mvnsite | 0m 50s | | the patch passed | | +1 :green_heart: | javadoc | 0m 44s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 56s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 18s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 24m 21s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 25s | | hadoop-project in the patch passed. | | +1 :green_heart: | unit | 2m 44s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 42s | | The patch does not generate ASF License warnings. | | | | 130m 37s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7909/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7909 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 9825a7b540bf 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 77adf896d34aa6259543a9fc93161a029a208438 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7909/1/testReport/ | | Max. process+thread count | 555 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7909/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. mukund-thakur commented on PR #7909: URL: https://github.com/apache/hadoop/pull/7909#issuecomment-3246075386 checkstyle failure. ./hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/streams/TestStreamFactories.java:26:import software.amazon.awssdk.services.s3.S3AsyncClient;:8: Unused import - software.amazon.awssdk.services.s3.S3AsyncClient. [UnusedImports] steveloughran commented on PR #7909: URL: https://github.com/apache/hadoop/pull/7909#issuecomment-3291747586 (oh, and include S3A: in the commit message. THX) ahmarsuhail commented on PR #7909: URL: https://github.com/apache/hadoop/pull/7909#issuecomment-3297498659 Test results are all good, except the known `ITestS3AContractOpen>AbstractContractOpenTest.testInputStreamReadNegativePosition` failure. Will merge. I don't see any failures in ITestS3AContractAnalyticsStreamVectoredRead that Steve saw in the SDK upgrade PR: https://github.com/apache/hadoop/pull/7882, will take a look at that separately. ahmarsuhail merged PR #7909: URL: https://github.com/apache/hadoop/pull/7909 hadoop-yetus commented on PR #7909: URL: https://github.com/apache/hadoop/pull/7909#issuecomment-3298248022 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 28s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 26m 11s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7909/2/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | +1 :green_heart: | compile | 9m 1s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 8m 18s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 2m 24s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 53s | | trunk passed | | +1 :green_heart: | javadoc | 0m 51s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 48s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 24s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 23m 50s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 25s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 27s | | the patch passed | | +1 :green_heart: | compile | 9m 8s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 9m 8s | | the patch passed | | +1 :green_heart: | compile | 8m 15s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 8m 15s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 2m 16s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 51s | | the patch passed | | +1 :green_heart: | javadoc | 0m 49s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 48s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 21s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 23m 47s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 18s | | hadoop-project in the patch passed. | | +1 :green_heart: | unit | 2m 19s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 136m 42s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7909/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7909 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux f17818206b60 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 112594c60b788e6c2f51021c0bcd017b3e0467ed | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7909/2/testReport/ | | Max. process+thread count | 554 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7909/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7909: URL: https://github.com/apache/hadoop/pull/7909#issuecomment-3298290502 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 2s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 26m 25s | | trunk passed | | +1 :green_heart: | compile | 9m 31s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 8m 14s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 2m 9s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 53s | | trunk passed | | +1 :green_heart: | javadoc | 0m 50s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 44s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 24s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 23m 13s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 25s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 28s | | the patch passed | | +1 :green_heart: | compile | 9m 5s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 9m 5s | | the patch passed | | +1 :green_heart: | compile | 8m 2s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 8m 2s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 2m 8s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 50s | | the patch passed | | +1 :green_heart: | javadoc | 0m 47s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 45s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 20s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 22m 46s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 26s | | hadoop-project in the patch passed. | | +1 :green_heart: | unit | 2m 42s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 45s | | The patch does not generate ASF License warnings. | | | | 135m 27s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7909/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7909 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 304e885133c5 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 65d16df11981860e9a6ef04a84e13672d7bb1ee3 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7909/3/testReport/ | | Max. process+thread count | 553 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7909/3/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. steveloughran commented on PR #7909: URL: https://github.com/apache/hadoop/pull/7909#issuecomment-3304093797 @ahmarsuhail can you do a followup to add the library to LICENSE-binary? thanks", "created": "2025-08-26T10:26:15.000+0000", "updated": "2025-09-18T13:22:04.000+0000", "derived": {"summary_task": "Summarize this issue: Java sync client is giving the best performance for our use case, especially for readVectored() where a large number of requests can be made concurrently.", "classification_task": "Classify the issue priority and type: Java sync client is giving the best performance for our use case, especially for readVectored() where a large number of requests can be made concurrently.", "qna_task": "Question: What is this issue about?\nAnswer: S3A Analytics-Accelerator: Move AAL to use Java sync client"}}
{"id": "13627337", "key": "HADOOP-19663", "project": "HADOOP", "summary": "Add RISC-V build infrastructure and placeholder implementation for CRC32 acceleration", "description": "Establish the foundational build infrastructure for RISC-V CRC32 hardware acceleration support while maintaining full backward compatibility with existing software implementations.", "comments": "PeterPtroc opened a new pull request, #7903: URL: https://github.com/apache/hadoop/pull/7903 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR Add a RISC-V-specific compilation unit: org/apache/hadoop/util/bulk_crc32_riscv.c. - Contains a no-op constructor reserved for future HW capability detection and dispatch. - Keeps runtime behavior unchanged (falls back to the generic software path in bulk_crc32.c). - Wire CMake to select bulk_crc32_riscv.c on riscv32/riscv64, mirroring other platforms. This PR establishes the foundational build infrastructure for future RISC-V Zbc (CLMUL) CRC32/CRC32C acceleration without changing current behavior. Follow-ups (HADOOP-19655) will introduce HW-accelerated implementations and runtime dispatch. ### How was this patch tested? - Ensured native build for hadoop-common compiles cleanly with RISC-V selection. - Verified by test_bulk_crc32. - No new tests added, as this patch is scaffolding-only without any behavior change. ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #7903: URL: https://github.com/apache/hadoop/pull/7903#issuecomment-3225752648 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 22m 29s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 41m 17s | | trunk passed | | +1 :green_heart: | compile | 14m 21s | | trunk passed | | -1 :x: | mvnsite | 1m 56s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/1/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in trunk failed. | | +1 :green_heart: | shadedclient | 94m 44s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 1m 11s | | the patch passed | | +1 :green_heart: | compile | 13m 13s | | the patch passed | | +1 :green_heart: | cc | 13m 13s | | the patch passed | | +1 :green_heart: | golang | 13m 13s | | the patch passed | | +1 :green_heart: | javac | 13m 13s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 1m 54s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/1/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch failed. | | +1 :green_heart: | shadedclient | 38m 33s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 23m 54s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 57s | | The patch does not generate ASF License warnings. | | | | 198m 14s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7903 | | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang | | uname | Linux af19edaaec5b 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1b159b6f8fadc7e4229a3aa0eeaa184b6d2f25cc | | Default Java | Red Hat, Inc.-1.8.0_312-b07 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/1/testReport/ | | Max. process+thread count | 1279 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/1/console | | versions | git=2.27.0 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7903: URL: https://github.com/apache/hadoop/pull/7903#issuecomment-3226747056 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 41m 2s | | trunk passed | | +1 :green_heart: | compile | 14m 4s | | trunk passed | | -1 :x: | mvnsite | 1m 55s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/2/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in trunk failed. | | +1 :green_heart: | shadedclient | 94m 56s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 1m 12s | | the patch passed | | +1 :green_heart: | compile | 13m 25s | | the patch passed | | +1 :green_heart: | cc | 13m 25s | | the patch passed | | +1 :green_heart: | golang | 13m 25s | | the patch passed | | +1 :green_heart: | javac | 13m 25s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 1m 57s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/2/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch failed. | | +1 :green_heart: | shadedclient | 38m 42s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 23m 39s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 53s | | The patch does not generate ASF License warnings. | | | | 176m 33s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7903 | | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang | | uname | Linux c0759ac9ab3e 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 3d607d41a2a311260c28800135009dfca09fb4f3 | | Default Java | Red Hat, Inc.-1.8.0_312-b07 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/2/testReport/ | | Max. process+thread count | 2145 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/2/console | | versions | git=2.27.0 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. PeterPtroc commented on PR #7903: URL: https://github.com/apache/hadoop/pull/7903#issuecomment-3228926064 Due to some CI infrastructure issues, I will paste the result of validating this patch on a RISC-V machine. Below are the command and the results. Command\uff1a ``` mvn -Pnative \\ -Dtest=org.apache.hadoop.util.TestNativeCrc32 \\ -Djava.library.path=\"$HADOOP_COMMON_LIB_NATIVE_DIR\" \\ test ``` Results ``` [INFO] ------------------------------------------------------- [INFO] T E S T S [INFO] ------------------------------------------------------- [INFO] Running org.apache.hadoop.util.TestNativeCrc32 [INFO] Tests run: 22, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 30.72 s PeterPtroc commented on PR #7903: URL: https://github.com/apache/hadoop/pull/7903#issuecomment-3228966982 Hi @pan3793 @slfan1989 , could you please take a look when you have a moment? This PR adds RISC-V CRC32 scaffolding and keeps behavior unchanged. Happy to address any feedback. Thanks! pan3793 commented on PR #7903: URL: https://github.com/apache/hadoop/pull/7903#issuecomment-3232286543 @PeterPtroc I suppose most developers here do not have RISC-V env, is it possible to have a docs about how to verify it by leveraging QEMU or some common tools? PeterPtroc commented on PR #7903: URL: https://github.com/apache/hadoop/pull/7903#issuecomment-3235836592 @pan3793 Thanks for the suggestion! Below is a concise doc to verify the correctness of the crc32riscv implementation: I mainly verify on RISC\u2011V by using QEMU together with the openEuler RISC\u2011V image. Download the image - https://dl-cdn.openeuler.openatom.cn/openEuler-25.03/virtual_machine_img/riscv64/ For me, from the above link, download these four files: RISCV_VIRT_CODE.fd, RISCV_VIRT_VARS.fd, openEuler-25.03-riscv64.qcow2.xz, and start_vm.sh; then log in as root with the password: openEuler12#$. Install required packages ````bash yum install -y gcc gcc-c++ gcc-gfortran libgcc cmake yum install -y wget openssl openssl-devel zlib zlib-devel automake libtool make libstdc++-static glibc-static git snappy snappy-devel fuse fuse-devel doxygen clang cyrus-sasl cyrus-sasl-devel libtirpc libtirpc-devel yum install -y java-17-openjdk.riscv64 java-17-openjdk-devel.riscv64 java-17-openjdk-headless.riscv64 ```` Install Protobuf 2.5.0 (with RISC\u2011V patches) ````bash mkdir protobuf && cd protobuf # Fetch sources git clone https://gitee.com/src-openeuler/protobuf2.git cd protobuf2 tar -xjf protobuf-2.5.0.tar.bz2 cp *.patch protobuf-2.5.0 && cd protobuf-2.5.0 # Apply patches (adds riscv64 support and build fixes) patch -p1 < 0001-Add-generic-GCC-support-for-atomic-operations.patch patch -p1 < protobuf-2.5.0-gtest.patch patch -p1 < protobuf-2.5.0-java-fixes.patch patch -p1 < protobuf-2.5.0-makefile.patch patch -p1 < add-riscv64-support.patch # Autotools setup libtoolize yum install -y automake automake-1.17 -a chmod +x configure # Configure, build, install ./configure --build=riscv64-unknown-linux --prefix=/usr/local/protobuf-2.5.0 make make check make install ldconfig # Publish protoc 2.5.0 into local Maven repo (riscv64 classifier) mvn install:install-file \\ -DgroupId=com.google.protobuf \\ -DartifactId=protoc \\ -Dversion=2.5.0 \\ -Dclassifier=linux-riscv64 \\ -Dpackaging=exe \\ -Dfile=/usr/local/protobuf-2.5.0/bin/protoc cd .. ```` Install Protobuf 3.25.5 ````bash # Download and unpack wget -c https://github.com/protocolbuffers/protobuf/releases/download/v25.5/protobuf-25.5.tar.gz tar -xzf protobuf-25.5.tar.gz cd protobuf-25.5 # Abseil dependency git clone https://github.com/abseil/abseil-cpp third_party/abseil-cpp # Configure and build cmake ./ \\ -DCMAKE_BUILD_TYPE=RELEASE \\ -Dprotobuf_BUILD_TESTS=off \\ -DCMAKE_CXX_STANDARD=20 \\ -DCMAKE_INSTALL_PREFIX=/usr/local/protobuf-3.25.5 make install -j \"$(nproc)\" # Publish protoc 3.25.5 into local Maven repo (riscv64 classifier) mvn install:install-file \\ -DgroupId=com.google.protobuf \\ -DartifactId=protoc \\ -Dversion=3.25.5 \\ -Dclassifier=linux-riscv64 \\ -Dpackaging=exe \\ -Dfile=/usr/local/protobuf-3.25.5/bin/protoc # Make protoc available on PATH and verify sudo ln -sfn /usr/local/protobuf-3.25.5/bin/protoc /usr/local/bin/protoc protoc --version ```` Verify CRC32 using Hadoop native ````bash # Clone Hadoop git clone https://github.com/apache/hadoop.git cd hadoop # Increase Maven memory export MAVEN_OPTS=\"-Xmx8g -Xms6g\" # Build Hadoop Common (native enabled) nohup mvn -pl hadoop-common-project/hadoop-common -am -Pnative -DskipTests clean install > build.log 2>&1 & # Point to built native library directory export HADOOP_COMMON_LIB_NATIVE_DIR=\"$PWD/hadoop-common-project/hadoop-common/target/native/target/usr/local/lib\" export LD_LIBRARY_PATH=\"$HADOOP_COMMON_LIB_NATIVE_DIR:$LD_LIBRARY_PATH\" # Run the CRC32 native test nohup mvn -Pnative -Dtest=org.apache.hadoop.util.TestNativeCrc32 \\ -Djava.library.path=\"$HADOOP_COMMON_LIB_NATIVE_DIR\" test > test.log 2>&1 & PeterPtroc commented on PR #7903: URL: https://github.com/apache/hadoop/pull/7903#issuecomment-3240157752 Hi @cnauroth , could you please have a look? This PR adds RISC-V CRC32 scaffolding and keeps behavior unchanged. Thanks! PeterPtroc commented on PR #7903: URL: https://github.com/apache/hadoop/pull/7903#issuecomment-3269017471 Hi @brumi1024 , this PR has been open for a while. Could you please take a look when you have time? Thanks! PeterPtroc commented on PR #7903: URL: https://github.com/apache/hadoop/pull/7903#issuecomment-3279207867 @steveloughran Hi, this PR has been open for a while. Could you please take a look when you have time? Thanks! steveloughran commented on code in PR #7903: URL: https://github.com/apache/hadoop/pull/7903#discussion_r2414452670 ########## hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/util/bulk_crc32_riscv.c: ########## @@ -0,0 +1,39 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +/** + * RISC-V CRC32 hardware acceleration (placeholder) + * + * Phase 1: provide a RISC-V-specific compilation unit that currently makes + * no runtime changes and falls back to the generic software path in + * bulk_crc32.c. Future work will add Zbc-based acceleration and runtime + * dispatch. + */ + +#include <assert.h> +#include <stddef.h> // for size_t + +#include \"bulk_crc32.h\" +#include \"gcc_optimizations.h\" + +/* Constructor hook reserved for future HW capability detection and + * function-pointer dispatch. Intentionally a no-op for the initial phase. */ +void __attribute__((constructor)) init_riscv_crc_support(void) +{ + /* No-op: keep using the default software implementations. */ +} Review Comment: nit: add a newline PeterPtroc commented on code in PR #7903: URL: https://github.com/apache/hadoop/pull/7903#discussion_r2415703394 ########## hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/util/bulk_crc32_riscv.c: ########## @@ -0,0 +1,39 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +/** + * RISC-V CRC32 hardware acceleration (placeholder) + * + * Phase 1: provide a RISC-V-specific compilation unit that currently makes + * no runtime changes and falls back to the generic software path in + * bulk_crc32.c. Future work will add Zbc-based acceleration and runtime + * dispatch. + */ + +#include <assert.h> +#include <stddef.h> // for size_t + +#include \"bulk_crc32.h\" +#include \"gcc_optimizations.h\" + +/* Constructor hook reserved for future HW capability detection and + * function-pointer dispatch. Intentionally a no-op for the initial phase. */ +void __attribute__((constructor)) init_riscv_crc_support(void) +{ + /* No-op: keep using the default software implementations. */ +} Review Comment: thanks, a newline has been added hadoop-yetus commented on PR #7903: URL: https://github.com/apache/hadoop/pull/7903#issuecomment-3384419165 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 31m 41s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 0m 35s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | -1 :x: | compile | 0m 34s | [/branch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/artifact/out/branch-compile-root.txt) | root in trunk failed. | | -1 :x: | mvnsite | 0m 35s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in trunk failed. | | +1 :green_heart: | shadedclient | 3m 12s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 34s | [/patch-mvninstall-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/artifact/out/patch-mvninstall-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch failed. | | -1 :x: | compile | 0m 34s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/artifact/out/patch-compile-root.txt) | root in the patch failed. | | -1 :x: | cc | 0m 34s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/artifact/out/patch-compile-root.txt) | root in the patch failed. | | -1 :x: | golang | 0m 34s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/artifact/out/patch-compile-root.txt) | root in the patch failed. | | -1 :x: | javac | 0m 34s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/artifact/out/patch-compile-root.txt) | root in the patch failed. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 0m 35s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch failed. | | +1 :green_heart: | shadedclient | 1m 52s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 34s | [/patch-unit-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch failed. | | +0 :ok: | asflicense | 0m 34s | | ASF License check generated no output? | | | | 41m 2s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7903 | | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang | | uname | Linux 9c4b8be6f414 5.15.0-152-generic #162-Ubuntu SMP Wed Jul 23 09:48:42 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / d4a02dc4d4cdadab4fab43b36f756826e4ace9ba | | Default Java | Red Hat, Inc.-1.8.0_462-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/testReport/ | | Max. process+thread count | 29 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/3/console | | versions | git=2.43.7 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7903: URL: https://github.com/apache/hadoop/pull/7903#issuecomment-3384529552 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 44s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 1m 8s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | -1 :x: | compile | 1m 9s | [/branch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/artifact/out/branch-compile-root.txt) | root in trunk failed. | | -1 :x: | mvnsite | 0m 37s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in trunk failed. | | +1 :green_heart: | shadedclient | 4m 24s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 26s | [/patch-mvninstall-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/artifact/out/patch-mvninstall-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch failed. | | -1 :x: | compile | 0m 28s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/artifact/out/patch-compile-root.txt) | root in the patch failed. | | -1 :x: | cc | 0m 28s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/artifact/out/patch-compile-root.txt) | root in the patch failed. | | -1 :x: | golang | 0m 28s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/artifact/out/patch-compile-root.txt) | root in the patch failed. | | -1 :x: | javac | 0m 28s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/artifact/out/patch-compile-root.txt) | root in the patch failed. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 1m 31s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch failed. | | -1 :x: | shadedclient | 2m 54s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 37s | [/patch-unit-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch failed. | | +0 :ok: | asflicense | 0m 37s | | ASF License check generated no output? | | | | 12m 9s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7903 | | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang | | uname | Linux c62a15e0c2ed 5.15.0-152-generic #162-Ubuntu SMP Wed Jul 23 09:48:42 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 8fee308fe2f2e2eaf4cc8631fc41b9bcd471609d | | Default Java | Red Hat, Inc.-1.8.0_462-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/testReport/ | | Max. process+thread count | 51 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/4/console | | versions | git=2.43.7 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. PeterPtroc commented on PR #7903: URL: https://github.com/apache/hadoop/pull/7903#issuecomment-3384544541 It seems the CI failure (unable to create new native thread) is due to a resource issue on the build agent, not related to the code changes. steveloughran merged PR #7903: URL: https://github.com/apache/hadoop/pull/7903 steveloughran commented on PR #7903: URL: https://github.com/apache/hadoop/pull/7903#issuecomment-3385847214 Merged * Ignored the CI failure; it does that sometimes, and as your code is #ifdef'd out, I'm not worried. Anything bigger and we'd have to retry the CI run * added a [RISC-V] category for this change -if future work does the same then it'll be consistent. PeterPtroc commented on PR #7903: URL: https://github.com/apache/hadoop/pull/7903#issuecomment-3386121381 Thanks @pan3793 @steveloughran for the review and the merge!", "created": "2025-08-26T09:30:19.000+0000", "updated": "2025-10-09T14:21:49.000+0000", "derived": {"summary_task": "Summarize this issue: Establish the foundational build infrastructure for RISC-V CRC32 hardware acceleration support while maintaining full backward compatibility with existing software implementations.", "classification_task": "Classify the issue priority and type: Establish the foundational build infrastructure for RISC-V CRC32 hardware acceleration support while maintaining full backward compatibility with existing software implementations.", "qna_task": "Question: What is this issue about?\nAnswer: Add RISC-V build infrastructure and placeholder implementation for CRC32 acceleration"}}
{"id": "13627332", "key": "HADOOP-19662", "project": "HADOOP", "summary": "Remove duplicate include of gcc_optimizations.h in bulk_crc32_x86.c", "description": "## Description There is a duplicate include statement for `gcc_optimizations.h` in `bulk_crc32_x86.c` at lines 29-30. ## Current Code ```c #include \"gcc_optimizations.h\" #include \"gcc_optimizations.h\" ```", "comments": "PeterPtroc opened a new pull request, #7899: URL: https://github.com/apache/hadoop/pull/7899 \u2026_crc32_x86.c Removes duplicate #include \"gcc_optimizations.h\" statement. <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR Removes duplicate include statement for `gcc_optimizations.h` in `bulk_crc32_x86.c`. ### How was this patch tested? - [x] Code compiles without issues - [x] No functional changes expected ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #7899: URL: https://github.com/apache/hadoop/pull/7899#issuecomment-3224065363 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 28m 36s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 47m 46s | | trunk passed | | +1 :green_heart: | compile | 15m 57s | | trunk passed | | -1 :x: | mvnsite | 1m 50s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/1/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in trunk failed. | | +1 :green_heart: | shadedclient | 107m 57s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 1m 10s | | the patch passed | | +1 :green_heart: | compile | 14m 20s | | the patch passed | | +1 :green_heart: | cc | 14m 20s | | the patch passed | | +1 :green_heart: | golang | 14m 20s | | the patch passed | | +1 :green_heart: | javac | 14m 20s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 1m 54s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/1/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch failed. | | +1 :green_heart: | shadedclient | 43m 43s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 23m 14s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 57s | | The patch does not generate ASF License warnings. | | | | 223m 13s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7899 | | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang | | uname | Linux e8837fccd16c 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c1d9fb6a820785db99554892fe312f5135201bf3 | | Default Java | Red Hat, Inc.-1.8.0_312-b07 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/1/testReport/ | | Max. process+thread count | 1253 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/1/console | | versions | git=2.27.0 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. PeterPtroc commented on PR #7899: URL: https://github.com/apache/hadoop/pull/7899#issuecomment-3224106808 I'll push an empty commit to re-run the tests. hadoop-yetus commented on PR #7899: URL: https://github.com/apache/hadoop/pull/7899#issuecomment-3224890091 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 50s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 44m 46s | | trunk passed | | +1 :green_heart: | compile | 15m 34s | | trunk passed | | -1 :x: | mvnsite | 1m 57s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/2/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in trunk failed. | | +1 :green_heart: | shadedclient | 103m 45s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 1m 11s | | the patch passed | | +1 :green_heart: | compile | 14m 23s | | the patch passed | | +1 :green_heart: | cc | 14m 23s | | the patch passed | | +1 :green_heart: | golang | 14m 23s | | the patch passed | | +1 :green_heart: | javac | 14m 23s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 1m 53s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/2/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch failed. | | +1 :green_heart: | shadedclient | 43m 24s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 23m 9s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 57s | | The patch does not generate ASF License warnings. | | | | 190m 57s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7899 | | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang | | uname | Linux 4cb5f960ad43 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7baa586bb254bf11e9f074aaae0aebb1ddca3f87 | | Default Java | Red Hat, Inc.-1.8.0_312-b07 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/2/testReport/ | | Max. process+thread count | 1863 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/2/console | | versions | git=2.27.0 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-26T09:00:41.000+0000", "updated": "2025-08-26T16:23:08.000+0000", "derived": {"summary_task": "Summarize this issue: ## Description There is a duplicate include statement for `gcc_optimizations.h` in `bulk_crc32_x86.c` at lines 29-30. ## Current Code ```c #include \"gcc_optimizations.h\" #include \"gcc_optimizations.h\" ```", "classification_task": "Classify the issue priority and type: ## Description There is a duplicate include statement for `gcc_optimizations.h` in `bulk_crc32_x86.c` at lines 29-30. ## Current Code ```c #include \"gcc_optimizations.h\" #include \"gcc_optimizations.h\" ```", "qna_task": "Question: What is this issue about?\nAnswer:  Remove duplicate include of gcc_optimizations.h in bulk_crc32_x86.c"}}
{"id": "13627323", "key": "HADOOP-19661", "project": "HADOOP", "summary": "Migrate CentOS 8 to Rocky Linux 8 in build env Dockerfile", "description": "", "comments": "pan3793 opened a new pull request, #7900: URL: https://github.com/apache/hadoop/pull/7900 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR Rocky Linux is supposed to be a drop-in replacement for the discontinued CentOS. See more details at https://rockylinux.org/about ### How was this patch tested? ``` $ ./start-build-env.sh rockylinux_8 ... _ _ _ ______ | | | | | | | _ \\ | |_| | __ _ __| | ___ ___ _ __ | | | |_____ __ | _ |/ _` |/ _` |/ _ \\ / _ \\| '_ \\ | | | / _ \\ \\ / / | | | | (_| | (_| | (_) | (_) | |_) | | |/ / __/\\ V / \\_| |_/\\__,_|\\__,_|\\___/ \\___/| .__/ |___/ \\___| \\_(_) | | |_| This is the standard Hadoop Developer build environment. This has all the right tools installed required to build Hadoop from source. [chengpan@4af99dc981b9 hadoop]$ ``` ``` $ mvn clean install -DskipTests -Pnative -Pyarn-ui -DskipShade ... [INFO] Reactor Summary for Apache Hadoop Main 3.5.0-SNAPSHOT: [INFO] [INFO] Apache Hadoop Main ................................. SUCCESS [ 0.675 s] [INFO] Apache Hadoop Build Tools .......................... SUCCESS [ 1.518 s] [INFO] Apache Hadoop Project POM .......................... SUCCESS [ 0.651 s] [INFO] Apache Hadoop Annotations .......................... SUCCESS [ 0.686 s] [INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [ 0.072 s] [INFO] Apache Hadoop Assemblies ........................... SUCCESS [ 0.077 s] [INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [ 1.476 s] [INFO] Apache Hadoop MiniKDC .............................. SUCCESS [ 0.316 s] [INFO] Apache Hadoop Auth ................................. SUCCESS [ 2.002 s] [INFO] Apache Hadoop Auth Examples ........................ SUCCESS [ 0.535 s] [INFO] Apache Hadoop Common ............................... SUCCESS [ 18.943 s] [INFO] Apache Hadoop NFS .................................. SUCCESS [ 1.071 s] [INFO] Apache Hadoop KMS .................................. SUCCESS [ 1.085 s] [INFO] Apache Hadoop Registry ............................. SUCCESS [ 1.282 s] [INFO] Apache Hadoop Common Project ....................... SUCCESS [ 0.046 s] [INFO] Apache Hadoop HDFS Client .......................... SUCCESS [ 10.259 s] [INFO] Apache Hadoop HDFS ................................. SUCCESS [ 17.829 s] [INFO] Apache Hadoop HDFS Native Client ................... SUCCESS [02:26 min] [INFO] Apache Hadoop HttpFS ............................... SUCCESS [ 1.772 s] [INFO] Apache Hadoop HDFS-NFS ............................. SUCCESS [ 1.020 s] [INFO] Apache Hadoop YARN ................................. SUCCESS [ 0.041 s] [INFO] Apache Hadoop YARN API ............................. SUCCESS [ 5.855 s] [INFO] Apache Hadoop YARN Common .......................... SUCCESS [ 4.910 s] [INFO] Apache Hadoop YARN Server .......................... SUCCESS [ 0.038 s] [INFO] Apache Hadoop YARN Server Common ................... SUCCESS [ 3.994 s] [INFO] Apache Hadoop YARN ApplicationHistoryService ....... SUCCESS [ 1.619 s] [INFO] Apache Hadoop YARN Timeline Service ................ SUCCESS [ 1.207 s] [INFO] Apache Hadoop YARN Web Proxy ....................... SUCCESS [ 1.058 s] [INFO] Apache Hadoop YARN ResourceManager ................. SUCCESS [ 10.150 s] [INFO] Apache Hadoop YARN NodeManager ..................... SUCCESS [ 28.435 s] [INFO] Apache Hadoop YARN Server Tests .................... SUCCESS [ 1.134 s] [INFO] Apache Hadoop YARN Client .......................... SUCCESS [ 1.959 s] [INFO] Apache Hadoop MapReduce Client ..................... SUCCESS [ 0.616 s] [INFO] Apache Hadoop MapReduce Core ....................... SUCCESS [ 4.440 s] [INFO] Apache Hadoop MapReduce Common ..................... SUCCESS [ 2.062 s] [INFO] Apache Hadoop MapReduce Shuffle .................... SUCCESS [ 1.414 s] [INFO] Apache Hadoop MapReduce App ........................ SUCCESS [ 2.901 s] [INFO] Apache Hadoop MapReduce HistoryServer .............. SUCCESS [ 1.887 s] [INFO] Apache Hadoop MapReduce JobClient .................. SUCCESS [ 3.319 s] [INFO] Apache Hadoop Distributed Copy ..................... SUCCESS [ 1.954 s] [INFO] Apache Hadoop Mini-Cluster ......................... SUCCESS [ 1.113 s] [INFO] Apache Hadoop Federation Balance ................... SUCCESS [ 1.234 s] [INFO] Apache Hadoop HDFS-RBF ............................. SUCCESS [ 5.733 s] [INFO] Apache Hadoop HDFS Project ......................... SUCCESS [ 0.034 s] [INFO] Apache Hadoop YARN SharedCacheManager .............. SUCCESS [ 0.886 s] [INFO] Apache Hadoop YARN Timeline Plugin Storage ......... SUCCESS [ 0.866 s] [INFO] Apache Hadoop YARN TimelineService HBase Backend ... SUCCESS [ 0.035 s] [INFO] Apache Hadoop YARN TimelineService HBase Common .... SUCCESS [ 1.291 s] [INFO] Apache Hadoop YARN TimelineService HBase Client .... SUCCESS [ 1.709 s] [INFO] Apache Hadoop YARN TimelineService HBase Servers ... SUCCESS [ 0.036 s] [INFO] Apache Hadoop YARN TimelineService HBase Server 2.5 SUCCESS [ 1.677 s] [INFO] Apache Hadoop YARN TimelineService HBase tests ..... SUCCESS [ 1.493 s] [INFO] Apache Hadoop YARN Router .......................... SUCCESS [ 1.912 s] [INFO] Apache Hadoop YARN TimelineService DocumentStore ... SUCCESS [ 1.018 s] [INFO] Apache Hadoop YARN GlobalPolicyGenerator ........... SUCCESS [ 1.175 s] [INFO] Apache Hadoop YARN Applications .................... SUCCESS [ 0.036 s] [INFO] Apache Hadoop YARN DistributedShell ................ SUCCESS [ 1.166 s] [INFO] Apache Hadoop YARN Unmanaged Am Launcher ........... SUCCESS [ 0.689 s] [INFO] Apache Hadoop YARN Services ........................ SUCCESS [ 0.034 s] [INFO] Apache Hadoop YARN Services Core ................... SUCCESS [ 2.393 s] [INFO] Apache Hadoop YARN Services API .................... SUCCESS [ 1.552 s] [INFO] Apache Hadoop YARN Application Catalog ............. SUCCESS [ 0.034 s] [INFO] Apache Hadoop YARN Application Catalog Webapp ...... SUCCESS [ 10.674 s] [INFO] Apache Hadoop YARN Application Catalog Docker Image SUCCESS [ 0.046 s] [INFO] Apache Hadoop YARN Application MaWo ................ SUCCESS [ 0.034 s] [INFO] Apache Hadoop YARN Application MaWo Core ........... SUCCESS [ 0.781 s] [INFO] Apache Hadoop YARN Site ............................ SUCCESS [ 0.036 s] [INFO] Apache Hadoop YARN Registry ........................ SUCCESS [ 0.439 s] [INFO] Apache Hadoop YARN UI .............................. SUCCESS [ 51.592 s] [INFO] Apache Hadoop YARN CSI ............................. SUCCESS [ 2.776 s] [INFO] Apache Hadoop YARN Project ......................... SUCCESS [ 1.155 s] [INFO] Apache Hadoop MapReduce HistoryServer Plugins ...... SUCCESS [ 0.854 s] [INFO] Apache Hadoop MapReduce NativeTask ................. SUCCESS [ 19.795 s] [INFO] Apache Hadoop MapReduce Uploader ................... SUCCESS [ 0.777 s] [INFO] Apache Hadoop MapReduce Examples ................... SUCCESS [ 1.427 s] [INFO] Apache Hadoop MapReduce ............................ SUCCESS [ 1.079 s] [INFO] Apache Hadoop MapReduce Streaming .................. SUCCESS [ 1.533 s] [INFO] Apache Hadoop Client Aggregator .................... SUCCESS [ 0.642 s] [INFO] Apache Hadoop Dynamometer Workload Simulator ....... SUCCESS [ 1.110 s] [INFO] Apache Hadoop Dynamometer Cluster Simulator ........ SUCCESS [ 2.098 s] [INFO] Apache Hadoop Dynamometer Block Listing Generator .. SUCCESS [ 0.994 s] [INFO] Apache Hadoop Dynamometer Dist ..................... SUCCESS [ 1.063 s] [INFO] Apache Hadoop Dynamometer .......................... SUCCESS [ 0.034 s] [INFO] Apache Hadoop Archives ............................. SUCCESS [ 0.908 s] [INFO] Apache Hadoop Archive Logs ......................... SUCCESS [ 1.125 s] [INFO] Apache Hadoop Rumen ................................ SUCCESS [ 1.466 s] [INFO] Apache Hadoop Gridmix .............................. SUCCESS [ 1.450 s] [INFO] Apache Hadoop Data Join ............................ SUCCESS [ 1.002 s] [INFO] Apache Hadoop Extras ............................... SUCCESS [ 0.995 s] [INFO] Apache Hadoop Pipes ................................ SUCCESS [ 3.333 s] [INFO] Apache Hadoop Amazon Web Services support .......... SUCCESS [ 4.599 s] [INFO] Apache Hadoop Kafka Library support ................ SUCCESS [ 0.636 s] [INFO] Apache Hadoop Azure support ........................ SUCCESS [ 4.082 s] [INFO] Apache Hadoop Aliyun OSS support ................... SUCCESS [ 0.878 s] [INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [ 1.312 s] [INFO] Apache Hadoop Resource Estimator Service ........... SUCCESS [ 0.868 s] [INFO] Apache Hadoop Azure Data Lake support .............. SUCCESS [ 0.844 s] [INFO] Apache Hadoop Image Generation Tool ................ SUCCESS [ 0.893 s] [INFO] Apache Hadoop Tools Dist ........................... SUCCESS [ 0.624 s] [INFO] Apache Hadoop OpenStack support .................... SUCCESS [ 0.042 s] [INFO] Apache Hadoop Common Benchmark ..................... SUCCESS [ 9.870 s] [INFO] Apache Hadoop Compatibility Benchmark .............. SUCCESS [ 0.598 s] [INFO] Apache Hadoop Tools ................................ SUCCESS [ 0.032 s] [INFO] Apache Hadoop Client API ........................... SUCCESS [ 0.851 s] [INFO] Apache Hadoop Client Runtime ....................... SUCCESS [ 0.639 s] [INFO] Apache Hadoop Client Packaging Invariants .......... SUCCESS [ 0.373 s] [INFO] Apache Hadoop Client Test Minicluster .............. SUCCESS [ 0.690 s] [INFO] Apache Hadoop Client Packaging Invariants for Test . SUCCESS [ 0.171 s] [INFO] Apache Hadoop Client Packaging Integration Tests ... SUCCESS [ 0.639 s] [INFO] Apache Hadoop Distribution ......................... SUCCESS [ 0.447 s] [INFO] Apache Hadoop Client Modules ....................... SUCCESS [ 0.032 s] [INFO] Apache Hadoop Tencent COS Support .................. SUCCESS [ 0.640 s] [INFO] Apache Hadoop OBS support .......................... SUCCESS [ 1.068 s] [INFO] Apache Hadoop Volcano Engine Services support ...... SUCCESS [ 1.601 s] [INFO] Apache Hadoop Cloud Storage ........................ SUCCESS [ 0.401 s] [INFO] Apache Hadoop Cloud Storage Project ................ SUCCESS [ 0.033 s] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 07:35 min [INFO] Finished at: 2025-08-26T10:05:06Z [INFO] ------------------------------------------------------------------------ [chengpan@4af99dc981b9 hadoop]$ ``` ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #7900: URL: https://github.com/apache/hadoop/pull/7900#issuecomment-3223525111 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 0m 21s | | Dockerfile '/home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-7900/rockylinux-8/src/dev-support/docker/Dockerfile_rockylinux_8' not found. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7900 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7900/1/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7900: URL: https://github.com/apache/hadoop/pull/7900#issuecomment-3223527763 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 0m 21s | | Dockerfile '/home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-7900/rockylinux-8/src/dev-support/docker/Dockerfile_rockylinux_8' not found. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7900 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7900/2/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7900: URL: https://github.com/apache/hadoop/pull/7900#issuecomment-3223534265 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 0m 21s | | Dockerfile '/home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-7900@2/rockylinux-8/src/dev-support/docker/Dockerfile_rockylinux_8' not found. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7900 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7900/3/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. pan3793 commented on PR #7900: URL: https://github.com/apache/hadoop/pull/7900#issuecomment-3223579339 Looks like Yetus finds Dockerfile from the trunk branch instead of the PR branch, I'm not very familiar with this part. If the behavior is not easy to change, I'm afraid this PR must be committed first, then we will know what is going to happen. ``` [2025-08-26T10:13:14.863Z] Already on 'trunk' [2025-08-26T10:13:14.863Z] Your branch is up to date with 'origin/trunk'. [2025-08-26T10:13:14.863Z] HEAD is now at 9d2a83d18ba HADOOP-19636. [JDK17] Remove CentOS 7 Support and Clean Up Dockerfile. (#7822) [2025-08-26T10:13:15.444Z] [2025-08-26T10:13:15.444Z] Testing https://github.com/apache/hadoop/pull/7900 patch on trunk. [2025-08-26T10:13:15.444Z] ERROR: Dockerfile '/home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-7900@2/rockylinux-8/src/dev-support/docker/Dockerfile_rockylinux_8' not found. ``` slfan1989 commented on PR #7900: URL: https://github.com/apache/hadoop/pull/7900#issuecomment-3226472958 > Looks like Yetus finds Dockerfile from the trunk branch instead of the PR branch, I'm not very familiar with this part. If the behavior is not easy to change, I'm afraid this PR must be committed first, then we will know what is going to happen. > > ``` > [2025-08-26T10:13:14.863Z] Already on 'trunk' > [2025-08-26T10:13:14.863Z] Your branch is up to date with 'origin/trunk'. > [2025-08-26T10:13:14.863Z] HEAD is now at 9d2a83d18ba HADOOP-19636. [JDK17] Remove CentOS 7 Support and Clean Up Dockerfile. (#7822) > [2025-08-26T10:13:15.444Z] > [2025-08-26T10:13:15.444Z] Testing https://github.com/apache/hadoop/pull/7900 patch on trunk. > [2025-08-26T10:13:15.444Z] ERROR: Dockerfile '/home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-7900@2/rockylinux-8/src/dev-support/docker/Dockerfile_rockylinux_8' not found. > ``` @pan3793 Currently, Hadoop's Yetus performs the build twice: first with the trunk code, and then again after applying the patch. At this point, I\u2019m not certain whether what you pointed out is actually an issue. @aajisaka @ayushtkn @GauthamBanasandra Could you please take a look at this issue? Thank you very much! aajisaka commented on PR #7900: URL: https://github.com/apache/hadoop/pull/7900#issuecomment-3226645951 Would you push the commit to a new branch and then create a test PR based on the branch to check Yetus is working fine? hadoop-yetus commented on PR #7900: URL: https://github.com/apache/hadoop/pull/7900#issuecomment-3226672783 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 0m 20s | | Dockerfile '/home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-7900/rockylinux-8/src/dev-support/docker/Dockerfile_rockylinux_8' not found. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7900 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7900/4/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. pan3793 commented on PR #7900: URL: https://github.com/apache/hadoop/pull/7900#issuecomment-3226674179 > Would you push the commit to a new branch and then create a test PR based on the branch to check Yetus is working fine? @aajisaka I don't have the permission to commit hadoop repo ... slfan1989 commented on PR #7900: URL: https://github.com/apache/hadoop/pull/7900#issuecomment-3226783929 > > Would you push the commit to a new branch and then create a test PR based on the branch to check Yetus is working fine? > > @aajisaka I don't have the permission to commit hadoop repo ... @pan3793 let\u2019s keep the file name unchanged as `Dockerfile_centos_8`. If needed, I can help create a branch. pan3793 commented on PR #7900: URL: https://github.com/apache/hadoop/pull/7900#issuecomment-3227608632 with the help of @slfan1989, branch `HADOOP-19661` was created at `apache/hadoop` repo, I opened https://github.com/apache/hadoop/pull/7898 targeting branch `HADOOP-19661`, the `Dockerfile_rockylinux_8 not found` issue is gone, but it seems platform change is not detected thus the Docker building test was skipped ... <img width=\"1201\" height=\"408\" alt=\"image\" src=\"https://github.com/user-attachments/assets/cb8f314b-3c7a-41b0-81ee-cea21d3f04e3\" /> @aajisaka do you have other suggestions? slfan1989 commented on PR #7900: URL: https://github.com/apache/hadoop/pull/7900#issuecomment-3231574620 > with the help of @slfan1989, branch `HADOOP-19661` was created at `apache/hadoop` repo, I opened #7898 targeting branch `HADOOP-19661`, the `Dockerfile_rockylinux_8 not found` issue is gone, but it seems platform change is not detected thus the Docker building test was skipped ... > > <img alt=\"image\" width=\"1201\" height=\"408\" src=\"https://private-user-images.githubusercontent.com/26535726/482609828-cb8f314b-3c7a-41b0-81ee-cea21d3f04e3.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTYzNDk4ODgsIm5iZiI6MTc1NjM0OTU4OCwicGF0aCI6Ii8yNjUzNTcyNi80ODI2MDk4MjgtY2I4ZjMxNGItM2M3YS00MWIwLTgxZWUtY2VhMjFkM2YwNGUzLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA4MjglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwODI4VDAyNTMwOFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWJiNTY3MTUzMmY0ZWJlYWIzMTMwODhhODMyNzhlMjcxZGQxNzhiYjcwYzdkNTI1NDU1Zjk0M2U4ZTAwODAyZDgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.8bGH6guvb6ZgikJ021xLnCWaef1WTDj3ge5b1cbxxcA\"> > @aajisaka do you have other suggestions? @pan3793 Although we can compile locally, there are some issues with Yetus, so I cannot confirm whether this PR will affect other team members' code submissions. I still need @aajisaka @GauthamBanasandra to help verify it. cc: @steveloughran @ayushtkn pan3793 opened a new pull request, #7913: URL: https://github.com/apache/hadoop/pull/7913 (no comment) hadoop-yetus commented on PR #7900: URL: https://github.com/apache/hadoop/pull/7900#issuecomment-3233914038 (!) A patch to the testing environment has been detected. Re-executing against the patched versions to perform further tests. The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7900/5/console in case of problems. pan3793 closed pull request #7913: DO-NOT-MERGE. HADOOP-19661. Test auxiliary patch URL: https://github.com/apache/hadoop/pull/7913 hadoop-yetus commented on PR #7913: URL: https://github.com/apache/hadoop/pull/7913#issuecomment-3234393664 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 14m 45s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ HADOOP-19661 Compile Tests _ | | +1 :green_heart: | mvninstall | 41m 44s | | HADOOP-19661 passed | | +1 :green_heart: | compile | 3m 36s | | HADOOP-19661 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 3m 36s | | HADOOP-19661 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 0m 27s | | HADOOP-19661 passed | | +1 :green_heart: | shadedclient | 85m 0s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 16s | | the patch passed | | +1 :green_heart: | compile | 3m 46s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | cc | 3m 46s | | the patch passed | | +1 :green_heart: | golang | 3m 46s | | the patch passed | | +1 :green_heart: | javac | 3m 46s | | the patch passed | | +1 :green_heart: | compile | 3m 40s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | cc | 3m 40s | | the patch passed | | +1 :green_heart: | golang | 3m 40s | | the patch passed | | +1 :green_heart: | javac | 3m 40s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 17s | | the patch passed | | +1 :green_heart: | shadedclient | 35m 19s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 10m 8s | | hadoop-hdfs-native-client in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 156m 33s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7913/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7913 | | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang | | uname | Linux c6281ba91ece 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | HADOOP-19661 / df44bca1341e8d0b97bc0d7d75bc90fb50ec87b8 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7913/1/testReport/ | | Max. process+thread count | 553 (vs. ulimit of 5500) | | modules | C: hadoop-hdfs-project/hadoop-hdfs-native-client U: hadoop-hdfs-project/hadoop-hdfs-native-client | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7913/1/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7900: URL: https://github.com/apache/hadoop/pull/7900#issuecomment-3234661592 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 23m 24s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | hadolint | 0m 0s | | hadolint was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 44m 28s | | trunk passed | | +1 :green_heart: | mvnsite | 23m 54s | | trunk passed | | +1 :green_heart: | shadedclient | 106m 45s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 107m 19s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 34m 26s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 20m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 60m 45s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 16m 17s | | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 45s | | The patch does not generate ASF License warnings. | | | | 245m 14s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7900/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7900 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs hadolint mvnsite unit jsonlint | | uname | Linux 5842e2ca7744 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / b2e037215b456012892b30f6e4adcca349e53794 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7900/5/testReport/ | | Max. process+thread count | 554 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7900/5/console | | versions | git=2.43.7 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. pan3793 commented on PR #7900: URL: https://github.com/apache/hadoop/pull/7900#issuecomment-3235763986 I have to rename it back to workaround the `Dockerfile_rockylinux_8 not found` issue in https://github.com/apache/hadoop/pull/7900/commits/b2e037215b456012892b30f6e4adcca349e53794. @slfan1989 @aajisaka, please take a look, and ping me to revert the renaming change if you think it's ready to go. slfan1989 commented on PR #7900: URL: https://github.com/apache/hadoop/pull/7900#issuecomment-3235795315 @pan3793 LGTM. I don't see any issues. I checked the compilation results and didn't find any problems. slfan1989 commented on PR #7900: URL: https://github.com/apache/hadoop/pull/7900#issuecomment-3235798705 @cnauroth @stoty Could you please help review this PR? Thank you very much! cnauroth closed pull request #7900: HADOOP-19661. Migrate CentOS 8 to Rocky Linux 8 in build env Dockerfile URL: https://github.com/apache/hadoop/pull/7900 hadoop-yetus commented on PR #7900: URL: https://github.com/apache/hadoop/pull/7900#issuecomment-3235821612 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 20s | | https://github.com/apache/hadoop/pull/7900 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7900 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7900/6/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. pan3793 commented on PR #7900: URL: https://github.com/apache/hadoop/pull/7900#issuecomment-3235821982 @cnauroth I need to revert the renaming change before committing to trunk ... let me send a follow up to address it cnauroth commented on PR #7900: URL: https://github.com/apache/hadoop/pull/7900#issuecomment-3235825803 > @cnauroth I need to revert the renaming change before committing to trunk ... let me send a follow up to address it @pan3793 , oops, I committed. :-D LMK, and I'll watch for more patches required. pan3793 opened a new pull request, #7917: URL: https://github.com/apache/hadoop/pull/7917 I have to keep the Dockerfile name to bypass Yetus issue, see discussion in https://github.com/apache/hadoop/pull/7900 But should rename it back before committing to trunk, since the thing has already happened, send a followup to fix it. pan3793 commented on PR #7917: URL: https://github.com/apache/hadoop/pull/7917#issuecomment-3235833407 @cnauroth hadoop-yetus commented on PR #7917: URL: https://github.com/apache/hadoop/pull/7917#issuecomment-3235839287 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 0m 21s | | Dockerfile '/home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-7917/rockylinux-8/src/dev-support/docker/Dockerfile_rockylinux_8' not found. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7917 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7917/1/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. pan3793 commented on PR #7917: URL: https://github.com/apache/hadoop/pull/7917#issuecomment-3235842248 no need to wait for Yetus's report since Yetus will fail with `Dockerfile_rockylinux_8 not found` immediately slfan1989 commented on PR #7917: URL: https://github.com/apache/hadoop/pull/7917#issuecomment-3235854584 @cnauroth I suggest waiting for a day to see if tomorrow's auto-build email triggers RockyLinux_8 completely. If it doesn't trigger properly, we'll look for a solution. cc: @pan3793 slfan1989 commented on PR #7917: URL: https://github.com/apache/hadoop/pull/7917#issuecomment-3235873037 > @cnauroth I suggest waiting for a day to see if tomorrow's auto-build email triggers RockyLinux_8 completely. If it doesn't trigger properly, we'll look for a solution. > > cc: @pan3793 @cnauroth @pan3793 After offline discussion with Pan, we've decided to merge this PR for now. If any issues arise, we'll make updates accordingly. slfan1989 merged PR #7917: URL: https://github.com/apache/hadoop/pull/7917 pan3793 opened a new pull request, #7931: URL: https://github.com/apache/hadoop/pull/7931 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR The issue is identified by https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7760/ ``` 00:08:13 Reason | Tests 00:08:13 Failed junit tests | hadoop.fs.compat.common.TestHdfsCompatShellCommand 00:08:13 | hadoop.fs.compat.common.TestHdfsCompatDefaultSuites 00:08:13 | hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints ``` ### How was this patch tested? Manually checked the failed tests after installing the missing deps inside the dev container. ``` $ ./start-build-env.sh rockylinux_8 ``` ``` $ mvn clean install -DskipTests -Pnative ``` ``` $ mvn test -pl :hadoop-compat-bench -Dtest=hadoop.fs.compat.common.TestHdfsCompatShellCommand ... [INFO] T E S T S [INFO] ------------------------------------------------------- [INFO] Running org.apache.hadoop.fs.compat.common.TestHdfsCompatShellCommand [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.64 s hadoop-yetus commented on PR #7931: URL: https://github.com/apache/hadoop/pull/7931#issuecomment-3253960302 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 24m 9s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 52s | | Maven dependency ordering for branch | | +1 :green_heart: | shadedclient | 44m 17s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 46s | | Maven dependency ordering for patch | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 34m 16s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 58s | | The patch does not generate ASF License warnings. | | | | 106m 24s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7931/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7931 | | Optional Tests | dupname asflicense codespell detsecrets jsonlint | | uname | Linux e8aea56e1b0c 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 2ba58898a54a722abb39526a9e95468802f00470 | | Max. process+thread count | 554 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7931/1/console | | versions | git=2.43.7 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7931: URL: https://github.com/apache/hadoop/pull/7931#issuecomment-3254275709 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 23m 55s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 0m 24s | | Maven dependency ordering for branch | | +1 :green_heart: | shadedclient | 26m 23s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 22s | | Maven dependency ordering for patch | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 25m 16s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 33s | | The patch does not generate ASF License warnings. | | | | 79m 7s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7931/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7931 | | Optional Tests | dupname asflicense codespell detsecrets jsonlint | | uname | Linux 912e624ed05f 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 2ba58898a54a722abb39526a9e95468802f00470 | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7931/1/console | | versions | git=2.30.2 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7931: URL: https://github.com/apache/hadoop/pull/7931#issuecomment-3254585161 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 0m 33s | | Maven dependency ordering for branch | | +1 :green_heart: | shadedclient | 34m 45s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 33s | | Maven dependency ordering for patch | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 34m 5s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 72m 16s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7931/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7931 | | Optional Tests | dupname asflicense codespell detsecrets jsonlint | | uname | Linux 53ab742d41d0 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 2ba58898a54a722abb39526a9e95468802f00470 | | Max. process+thread count | 561 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7931/1/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 merged PR #7931: URL: https://github.com/apache/hadoop/pull/7931 slfan1989 commented on PR #7931: URL: https://github.com/apache/hadoop/pull/7931#issuecomment-3257273745 @pan3793 Thanks for the contribution! Merged into trunk. pan3793 opened a new pull request, #7938: URL: https://github.com/apache/hadoop/pull/7938 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR I see some CI starts to fail due to https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7760/27/console ``` 23:59:25 58.82 Last metadata expiration check: 0:00:40 ago on Sat Sep 6 15:58:37 2025. 23:59:25 59.80 No match for argument: bats ``` ### How was this patch tested? ``` $ ./start-build-env.sh rockylinux_8 ... _ _ _ ______ | | | | | | | _ \\ | |_| | __ _ __| | ___ ___ _ __ | | | |_____ __ | _ |/ _` |/ _` |/ _ \\ / _ \\| '_ \\ | | | / _ \\ \\ / / | | | | (_| | (_| | (_) | (_) | |_) | | |/ / __/\\ V / \\_| |_/\\__,_|\\__,_|\\___/ \\___/| .__/ |___/ \\___| \\_(_) | | |_| This is the standard Hadoop Developer build environment. This has all the right tools installed required to build Hadoop from source. [chengpan@c79114c5d3d4 hadoop]$ ``` ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #7938: URL: https://github.com/apache/hadoop/pull/7938#issuecomment-3263376365 (!) A patch to the testing environment has been detected. Re-executing against the patched versions to perform further tests. The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7938/1/console in case of problems. hadoop-yetus commented on PR #7938: URL: https://github.com/apache/hadoop/pull/7938#issuecomment-3263454078 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 31m 16s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | hadolint | 0m 0s | | hadolint was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 21s | | Maven dependency ordering for branch | | +1 :green_heart: | shadedclient | 52m 11s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 46s | | Maven dependency ordering for patch | | +1 :green_heart: | blanks | 0m 1s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 41m 40s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 52s | | The patch does not generate ASF License warnings. | | | | 128m 32s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7938/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7938 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs | | uname | Linux 11b4c3233f9f 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 117e7c2d608809997f7eeb57f6f6a5e924595dc7 | | Max. process+thread count | 538 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7938/1/console | | versions | git=2.43.7 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7938: URL: https://github.com/apache/hadoop/pull/7938#issuecomment-3263456222 (!) A patch to the testing environment has been detected. Re-executing against the patched versions to perform further tests. The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7938/1/console in case of problems. hadoop-yetus commented on PR #7938: URL: https://github.com/apache/hadoop/pull/7938#issuecomment-3263508181 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 51s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 0m 31s | | Maven dependency ordering for branch | | +1 :green_heart: | shadedclient | 40m 55s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 31s | | Maven dependency ordering for patch | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | hadolint | 0m 1s | | No new issues. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | shadedclient | 39m 50s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 85m 13s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7938/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7938 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs | | uname | Linux 1632828a4b30 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 117e7c2d608809997f7eeb57f6f6a5e924595dc7 | | Max. process+thread count | 525 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7938/1/console | | versions | git=2.25.1 maven=3.6.3 hadolint=1.11.1-0-g0e692dd shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on PR #7938: URL: https://github.com/apache/hadoop/pull/7938#issuecomment-3264759129 @pan3793 Thank you for your contribution, but I think we still need other members to take another look. @GauthamBanasandra Could you please help review this PR? Thank you very much! slfan1989 commented on PR #7938: URL: https://github.com/apache/hadoop/pull/7938#issuecomment-3276952959 @GauthamBanasandra Thanks for helping with the review \u2014 LGTM. slfan1989 commented on PR #7938: URL: https://github.com/apache/hadoop/pull/7938#issuecomment-3276955337 @pan3793 Thank you for your contribution! If there are no further comments today, this PR will be merged. slfan1989 merged PR #7938: URL: https://github.com/apache/hadoop/pull/7938 slfan1989 commented on PR #7938: URL: https://github.com/apache/hadoop/pull/7938#issuecomment-3282884265 @pan3793 Thanks for the contribution! @GauthamBanasandra Thanks for the review!", "created": "2025-08-26T08:04:43.000+0000", "updated": "2025-09-11T22:56:17.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Migrate CentOS 8 to Rocky Linux 8 in build env Dockerfile"}}
{"id": "13627314", "key": "HADOOP-19660", "project": "HADOOP", "summary": "ABFS: Proposed Enhancement in WorkloadIdentityTokenProvider", "description": "Externally Reported Enhancement: *Current Limitation* The current WorkloadIdentityTokenProvider implementation works well for file-based token scenarios, but it's tightly coupled to file system operations and cannot be easily extended for alternative token sources {*}Use Case{*}: *Kubernetes TokenRequest API* In modern Kubernetes environments, the recommended approach is to use the TokenRequest API to generate short-lived, on-demand service account tokens rather than relying on projected volume mounts. *Proposed Enhancement* I propose modifying WorkloadIdentityTokenProvider to accept a Supplier for token retrieval instead of being hardcoded to file operations:", "comments": "[~anujmodi] can you assign this to me? Draft PR without test to see if the structure looks okay - [https://github.com/apache/hadoop/pull/7901] cc: [~anujmodi] Its not showing your name somehow hadoop-yetus commented on PR #7901: URL: https://github.com/apache/hadoop/pull/7901#issuecomment-3224527888 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 21m 38s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 1s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 45m 27s | | trunk passed | | +1 :green_heart: | compile | 0m 44s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 42s | | trunk passed | | +1 :green_heart: | javadoc | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 10s | | trunk passed | | +1 :green_heart: | shadedclient | 41m 37s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/1/artifact/out/blanks-eol.txt) | The patch has 17 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 0m 23s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 1 new + 2 unchanged - 0 fixed = 3 total (was 2) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 29s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 11s | | the patch passed | | +1 :green_heart: | shadedclient | 43m 13s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 54s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 39s | | The patch does not generate ASF License warnings. | | | | 166m 34s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7901 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 91770e23ae15 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / dee5a7d480a91f1099224b2565ed93ab8587e580 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/1/testReport/ | | Max. process+thread count | 607 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. anujmodi2021 commented on PR #7901: URL: https://github.com/apache/hadoop/pull/7901#issuecomment-3256894074 Thanks for the patch @kunalmnnit We will review this PR and add comments if any. anmolanmol1234 commented on code in PR #7901: URL: https://github.com/apache/hadoop/pull/7901#discussion_r2344037166 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/WorkloadIdentityTokenProvider.java: ########## @@ -38,11 +38,72 @@ public class WorkloadIdentityTokenProvider extends AccessTokenProvider { private static final String EMPTY_TOKEN_FILE_ERROR = \"Empty token file found at specified path: \"; private static final String TOKEN_FILE_READ_ERROR = \"Error reading token file at specified path: \"; + /** + * Internal implementation of ClientAssertionProvider for file-based token reading. + * This provides backward compatibility for the file-based constructor. + */ + private static class FileBasedClientAssertionProvider implements ClientAssertionProvider { + private final String tokenFile; + + public FileBasedClientAssertionProvider(String tokenFile) { + this.tokenFile = tokenFile; + } + + @Override + public void initialize(Configuration configuration, String accountName) throws IOException { + // No initialization needed for file-based provider + } + + @Override + public String getClientAssertion() throws IOException { + String clientAssertion = \"\"; + try { + File file = new File(tokenFile); + clientAssertion = FileUtils.readFileToString(file, \"UTF-8\"); Review Comment: encoding should come from constants, should not be hardcoded anmolanmol1234 commented on code in PR #7901: URL: https://github.com/apache/hadoop/pull/7901#discussion_r2344048057 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/WorkloadIdentityTokenProvider.java: ########## @@ -38,11 +38,72 @@ public class WorkloadIdentityTokenProvider extends AccessTokenProvider { private static final String EMPTY_TOKEN_FILE_ERROR = \"Empty token file found at specified path: \"; private static final String TOKEN_FILE_READ_ERROR = \"Error reading token file at specified path: \"; + /** + * Internal implementation of ClientAssertionProvider for file-based token reading. + * This provides backward compatibility for the file-based constructor. + */ + private static class FileBasedClientAssertionProvider implements ClientAssertionProvider { + private final String tokenFile; + + public FileBasedClientAssertionProvider(String tokenFile) { + this.tokenFile = tokenFile; + } + + @Override + public void initialize(Configuration configuration, String accountName) throws IOException { + // No initialization needed for file-based provider + } + + @Override + public String getClientAssertion() throws IOException { + String clientAssertion = \"\"; + try { + File file = new File(tokenFile); + clientAssertion = FileUtils.readFileToString(file, \"UTF-8\"); + } catch (Exception e) { + throw new IOException(TOKEN_FILE_READ_ERROR + tokenFile, e); + } + if (Strings.isNullOrEmpty(clientAssertion)) { Review Comment: If the token only contains whitespaces the empty check will pass, token can be trimmed before checking for empty. anmolanmol1234 commented on code in PR #7901: URL: https://github.com/apache/hadoop/pull/7901#discussion_r2344049708 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/WorkloadIdentityTokenProvider.java: ########## @@ -38,11 +38,72 @@ public class WorkloadIdentityTokenProvider extends AccessTokenProvider { private static final String EMPTY_TOKEN_FILE_ERROR = \"Empty token file found at specified path: \"; private static final String TOKEN_FILE_READ_ERROR = \"Error reading token file at specified path: \"; + /** + * Internal implementation of ClientAssertionProvider for file-based token reading. + * This provides backward compatibility for the file-based constructor. + */ + private static class FileBasedClientAssertionProvider implements ClientAssertionProvider { + private final String tokenFile; + + public FileBasedClientAssertionProvider(String tokenFile) { + this.tokenFile = tokenFile; + } + + @Override + public void initialize(Configuration configuration, String accountName) throws IOException { + // No initialization needed for file-based provider + } + + @Override + public String getClientAssertion() throws IOException { + String clientAssertion = \"\"; Review Comment: use constant for EMPTY_STRING anmolanmol1234 commented on code in PR #7901: URL: https://github.com/apache/hadoop/pull/7901#discussion_r2344054829 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/WorkloadIdentityTokenProvider.java: ########## @@ -38,11 +38,72 @@ public class WorkloadIdentityTokenProvider extends AccessTokenProvider { private static final String EMPTY_TOKEN_FILE_ERROR = \"Empty token file found at specified path: \"; private static final String TOKEN_FILE_READ_ERROR = \"Error reading token file at specified path: \"; + /** + * Internal implementation of ClientAssertionProvider for file-based token reading. + * This provides backward compatibility for the file-based constructor. + */ + private static class FileBasedClientAssertionProvider implements ClientAssertionProvider { + private final String tokenFile; + + public FileBasedClientAssertionProvider(String tokenFile) { + this.tokenFile = tokenFile; + } + + @Override + public void initialize(Configuration configuration, String accountName) throws IOException { + // No initialization needed for file-based provider + } + + @Override + public String getClientAssertion() throws IOException { + String clientAssertion = \"\"; + try { + File file = new File(tokenFile); + clientAssertion = FileUtils.readFileToString(file, \"UTF-8\"); Review Comment: Here we are reading the whole token file as a string every time getClientAssertion() is called. If file is large or accessed frequently, it could be inefficient. Can we cache the value until the token provider explicitly refreshes ? anmolanmol1234 commented on code in PR #7901: URL: https://github.com/apache/hadoop/pull/7901#discussion_r2344089164 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/WorkloadIdentityTokenProvider.java: ########## @@ -20,13 +20,13 @@ import java.io.File; import java.io.IOException; - -import org.slf4j.Logger; -import org.slf4j.LoggerFactory; import org.apache.commons.io.FileUtils; import org.apache.hadoop.classification.VisibleForTesting; +import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.thirdparty.com.google.common.base.Strings; import org.apache.hadoop.util.Preconditions; +import org.slf4j.Logger; Review Comment: Import ordering is incorrect import java.io.File; import java.io.IOException; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.apache.commons.io.FileUtils; import org.apache.hadoop.classification.VisibleForTesting; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.thirdparty.com.google.common.base.Strings; import org.apache.hadoop.util.Preconditions; kunalmnnit commented on code in PR #7901: URL: https://github.com/apache/hadoop/pull/7901#discussion_r2347334850 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/WorkloadIdentityTokenProvider.java: ########## @@ -38,11 +38,72 @@ public class WorkloadIdentityTokenProvider extends AccessTokenProvider { private static final String EMPTY_TOKEN_FILE_ERROR = \"Empty token file found at specified path: \"; private static final String TOKEN_FILE_READ_ERROR = \"Error reading token file at specified path: \"; + /** + * Internal implementation of ClientAssertionProvider for file-based token reading. + * This provides backward compatibility for the file-based constructor. + */ + private static class FileBasedClientAssertionProvider implements ClientAssertionProvider { + private final String tokenFile; + + public FileBasedClientAssertionProvider(String tokenFile) { + this.tokenFile = tokenFile; + } + + @Override + public void initialize(Configuration configuration, String accountName) throws IOException { + // No initialization needed for file-based provider + } + + @Override + public String getClientAssertion() throws IOException { + String clientAssertion = \"\"; + try { + File file = new File(tokenFile); + clientAssertion = FileUtils.readFileToString(file, \"UTF-8\"); Review Comment: Can we take this optimization in subsequent PR since this was existing piece of code? https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/WorkloadIdentityTokenProvider.java#L103-L115 kunalmnnit commented on code in PR #7901: URL: https://github.com/apache/hadoop/pull/7901#discussion_r2347335431 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/WorkloadIdentityTokenProvider.java: ########## @@ -38,11 +38,72 @@ public class WorkloadIdentityTokenProvider extends AccessTokenProvider { private static final String EMPTY_TOKEN_FILE_ERROR = \"Empty token file found at specified path: \"; private static final String TOKEN_FILE_READ_ERROR = \"Error reading token file at specified path: \"; + /** + * Internal implementation of ClientAssertionProvider for file-based token reading. + * This provides backward compatibility for the file-based constructor. + */ + private static class FileBasedClientAssertionProvider implements ClientAssertionProvider { + private final String tokenFile; + + public FileBasedClientAssertionProvider(String tokenFile) { + this.tokenFile = tokenFile; + } + + @Override + public void initialize(Configuration configuration, String accountName) throws IOException { + // No initialization needed for file-based provider + } + + @Override + public String getClientAssertion() throws IOException { + String clientAssertion = \"\"; + try { + File file = new File(tokenFile); + clientAssertion = FileUtils.readFileToString(file, \"UTF-8\"); Review Comment: Additionally, this will only be invoked when the actual AAD token is expired which is roughly every hour and directly coincides with expiry of KSA token so don't think this will be unnecessarily invoked. wdyt? kunalmnnit commented on PR #7901: URL: https://github.com/apache/hadoop/pull/7901#issuecomment-3289649227 Thanks @anmolanmol1234 for the review. PTAL again for second pass. hadoop-yetus commented on PR #7901: URL: https://github.com/apache/hadoop/pull/7901#issuecomment-3289669768 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 21m 33s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 56m 0s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 43s | | trunk passed | | +1 :green_heart: | javadoc | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 16s | | trunk passed | | +1 :green_heart: | shadedclient | 41m 20s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 35s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 31s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 31s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/2/artifact/out/blanks-eol.txt) | The patch has 17 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | +1 :green_heart: | checkstyle | 0m 21s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 30s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 41m 18s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 53s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 174m 53s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7901 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux e0210d80824e 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / e368af43811f9fdd335c0e43ac1d5e0c2d1f33f0 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/2/testReport/ | | Max. process+thread count | 575 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7901: URL: https://github.com/apache/hadoop/pull/7901#issuecomment-3289765623 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 55s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 58m 34s | | trunk passed | | +1 :green_heart: | compile | 0m 45s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 36s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 43s | | trunk passed | | +1 :green_heart: | javadoc | 0m 44s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 12s | | trunk passed | | +1 :green_heart: | shadedclient | 41m 9s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 36s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 36s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/3/artifact/out/blanks-eol.txt) | The patch has 17 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | +1 :green_heart: | checkstyle | 0m 21s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | +1 :green_heart: | javadoc | 0m 31s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 28s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 14s | | the patch passed | | +1 :green_heart: | shadedclient | 41m 13s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 56s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 39s | | The patch does not generate ASF License warnings. | | | | 156m 44s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7901 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 3f98b80109aa 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 33ec902359aea0f83b7a9dfe6cea176f740f3878 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/3/testReport/ | | Max. process+thread count | 584 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/3/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7901: URL: https://github.com/apache/hadoop/pull/7901#issuecomment-3289771396 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 9m 30s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 33m 47s | | trunk passed | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 23s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 30s | | trunk passed | | +1 :green_heart: | javadoc | 0m 27s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 21s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 45s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 5s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/5/artifact/out/blanks-eol.txt) | The patch has 45 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | +1 :green_heart: | checkstyle | 0m 13s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 20s | | the patch passed | | +1 :green_heart: | javadoc | 0m 17s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 45s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 48s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 25s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 25s | | The patch does not generate ASF License warnings. | | | | 94m 55s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7901 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 9639e6676688 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / d5030434aa067cdc77378bb13bac5579ecaea881 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/5/testReport/ | | Max. process+thread count | 545 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/5/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7901: URL: https://github.com/apache/hadoop/pull/7901#issuecomment-3289810843 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 50s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 58m 20s | | trunk passed | | +1 :green_heart: | compile | 0m 48s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 36s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 46s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 12s | | trunk passed | | +1 :green_heart: | shadedclient | 42m 10s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 35s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/4/artifact/out/blanks-eol.txt) | The patch has 16 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | +1 :green_heart: | checkstyle | 0m 21s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 30s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 40m 58s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 53s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 157m 10s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7901 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 42faac865c3a 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 992ac17ec35adde9c4f27c307c0362a5d6d79cb0 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/4/testReport/ | | Max. process+thread count | 587 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/4/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7901: URL: https://github.com/apache/hadoop/pull/7901#issuecomment-3291004596 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 51s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 57m 16s | | trunk passed | | +1 :green_heart: | compile | 0m 44s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 42s | | trunk passed | | +1 :green_heart: | javadoc | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 9s | | trunk passed | | +1 :green_heart: | shadedclient | 41m 3s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 35s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/6/artifact/out/blanks-eol.txt) | The patch has 33 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | +1 :green_heart: | checkstyle | 0m 21s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 30s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 27s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 40m 46s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 51s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 154m 30s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/6/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7901 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 06eef491e2ea 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 9495f55f855656096b3d593248a2e8b74f51de26 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/6/testReport/ | | Max. process+thread count | 533 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/6/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7901: URL: https://github.com/apache/hadoop/pull/7901#issuecomment-3291863724 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 51s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 55m 50s | | trunk passed | | +1 :green_heart: | compile | 0m 47s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 40s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 36s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 13s | | trunk passed | | +1 :green_heart: | shadedclient | 44m 15s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 37s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 37s | | the patch passed | | +1 :green_heart: | compile | 0m 31s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 31s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/7/artifact/out/blanks-eol.txt) | The patch has 3 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | +1 :green_heart: | checkstyle | 0m 23s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 36s | | the patch passed | | +1 :green_heart: | javadoc | 0m 31s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 28s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 17s | | the patch passed | | +1 :green_heart: | shadedclient | 43m 26s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 56s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 159m 23s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/7/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7901 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 7f151f599e70 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 68a5b0b9735b0bfd79a577d7ce292ad2399be467 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/7/testReport/ | | Max. process+thread count | 530 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/7/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7901: URL: https://github.com/apache/hadoop/pull/7901#issuecomment-3293541604 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 54s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 56m 51s | | trunk passed | | +1 :green_heart: | compile | 0m 46s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | +1 :green_heart: | javadoc | 0m 45s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 21s | | trunk passed | | +1 :green_heart: | shadedclient | 45m 0s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 37s | | the patch passed | | +1 :green_heart: | compile | 0m 44s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 44s | | the patch passed | | +1 :green_heart: | compile | 0m 35s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 35s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 24s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 37s | | the patch passed | | +1 :green_heart: | javadoc | 0m 36s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 31s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 22s | | the patch passed | | +1 :green_heart: | shadedclient | 44m 22s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 7s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 41s | | The patch does not generate ASF License warnings. | | | | 163m 6s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/8/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7901 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 587337ec2872 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 31906ea3983a99845ff3205fdb889fafdb7fe89b | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/8/testReport/ | | Max. process+thread count | 525 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7901/8/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. kunalmnnit commented on PR #7901: URL: https://github.com/apache/hadoop/pull/7901#issuecomment-3319670981 @anujmodi2021 @anmolanmol1234 Appreciate your review here again. Thanks! kunalmnnit commented on PR #7901: URL: https://github.com/apache/hadoop/pull/7901#issuecomment-3336856485 @anmolanmol1234 Gentle bump if you could please have a pass. This change would be really helpful for us to use k8s TokenRequest API kunalmnnit commented on PR #7901: URL: https://github.com/apache/hadoop/pull/7901#issuecomment-3337137345 @anmolanmol1234 Thank you for the approval. Would you know who would be able to help with merge? I do not seem to have access anmolanmol1234 commented on PR #7901: URL: https://github.com/apache/hadoop/pull/7901#issuecomment-3337219417 @anujmodi2021 should be able to help with the merge anujmodi2021 merged PR #7901: URL: https://github.com/apache/hadoop/pull/7901", "created": "2025-08-26T06:53:52.000+0000", "updated": "2025-09-26T16:37:27.000+0000", "derived": {"summary_task": "Summarize this issue: Externally Reported Enhancement: *Current Limitation* The current WorkloadIdentityTokenProvider implementation works well for file-based token scenarios, but it's tightly coupled to file system operations and cannot be easily extended for alternative token sources {*}Use Case{*}: *Kubernetes TokenRequest API* In modern Kubernetes environments, the recommended approach is to use the TokenRequest AP", "classification_task": "Classify the issue priority and type: Externally Reported Enhancement: *Current Limitation* The current WorkloadIdentityTokenProvider implementation works well for file-based token scenarios, but it's tightly coupled to file system operations and cannot be easily extended for alternative token sources {*}Use Case{*}: *Kubernetes TokenRequest API* In modern Kubernetes environments, the recommended approach is to use the TokenRequest AP", "qna_task": "Question: What is this issue about?\nAnswer: ABFS: Proposed Enhancement in WorkloadIdentityTokenProvider"}}
{"id": "13627296", "key": "HADOOP-19659", "project": "HADOOP", "summary": "Upgrade Debian 10 to 11 in build env Dockerfile", "description": "Debian 10 EOL, and the apt repo is unavailable {code:bash} docker run --rm -it debian:10 bash root@bc2a4c509cb3:/# apt update Ign:1 http://deb.debian.org/debian buster InRelease Ign:2 http://deb.debian.org/debian-security buster/updates InRelease Ign:3 http://deb.debian.org/debian buster-updates InRelease Err:4 http://deb.debian.org/debian buster Release 404 Not Found [IP: 151.101.90.132 80] Err:5 http://deb.debian.org/debian-security buster/updates Release 404 Not Found [IP: 151.101.90.132 80] Err:6 http://deb.debian.org/debian buster-updates Release 404 Not Found [IP: 151.101.90.132 80] Reading package lists... Done E: The repository 'http://deb.debian.org/debian buster Release' does not have a Release file. N: Updating from such a repository can't be done securely, and is therefore disabled by default. N: See apt-secure(8) manpage for repository creation and user configuration details. E: The repository 'http://deb.debian.org/debian-security buster/updates Release' does not have a Release file. N: Updating from such a repository can't be done securely, and is therefore disabled by default. N: See apt-secure(8) manpage for repository creation and user configuration details. E: The repository 'http://deb.debian.org/debian buster-updates Release' does not have a Release file. N: Updating from such a repository can't be done securely, and is therefore disabled by default. N: See apt-secure(8) manpage for repository creation and user configuration details. root@bc2a4c509cb3:/# {code}", "comments": "pan3793 opened a new pull request, #7898: URL: https://github.com/apache/hadoop/pull/7898 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR Debian 10 EOL, and the apt repo is unavailable, this PR upgrades it to Debian 11 ``` docker run --rm -it debian:10 bash root@bc2a4c509cb3:/# apt update Ign:1 http://deb.debian.org/debian buster InRelease Ign:2 http://deb.debian.org/debian-security buster/updates InRelease Ign:3 http://deb.debian.org/debian buster-updates InRelease Err:4 http://deb.debian.org/debian buster Release 404 Not Found [IP: 151.101.90.132 80] Err:5 http://deb.debian.org/debian-security buster/updates Release 404 Not Found [IP: 151.101.90.132 80] Err:6 http://deb.debian.org/debian buster-updates Release 404 Not Found [IP: 151.101.90.132 80] Reading package lists... Done E: The repository 'http://deb.debian.org/debian buster Release' does not have a Release file. N: Updating from such a repository can't be done securely, and is therefore disabled by default. N: See apt-secure(8) manpage for repository creation and user configuration details. E: The repository 'http://deb.debian.org/debian-security buster/updates Release' does not have a Release file. N: Updating from such a repository can't be done securely, and is therefore disabled by default. N: See apt-secure(8) manpage for repository creation and user configuration details. E: The repository 'http://deb.debian.org/debian buster-updates Release' does not have a Release file. N: Updating from such a repository can't be done securely, and is therefore disabled by default. N: See apt-secure(8) manpage for repository creation and user configuration details. root@bc2a4c509cb3:/# ``` [Debian 11 will be EOL after 31 Aug 2026](https://endoflife.date/debian). I didn't switch it to Debian 12 or 13 because the new version does not have `openjdk-11-jdk` in the apt repo. ### How was this patch tested? ``` $ ./start-build-env.sh debian_11 ... _ _ _ ______ | | | | | | | _ \\ | |_| | __ _ __| | ___ ___ _ __ | | | |_____ __ | _ |/ _` |/ _` |/ _ \\ / _ \\| '_ \\ | | | / _ \\ \\ / / | | | | (_| | (_| | (_) | (_) | |_) | | |/ / __/\\ V / \\_| |_/\\__,_|\\__,_|\\___/ \\___/| .__/ |___/ \\___| \\_(_) | | |_| This is the standard Hadoop Developer build environment. This has all the right tools installed required to build Hadoop from source. chengpan@c85a4426ba52:~/hadoop$ ``` ``` $ mvn clean install -DskipTests -Pnative -Pyarn-ui -DskipShade ... [INFO] Reactor Summary for Apache Hadoop Main 3.5.0-SNAPSHOT: [INFO] [INFO] Apache Hadoop Main ................................. SUCCESS [ 0.662 s] [INFO] Apache Hadoop Build Tools .......................... SUCCESS [ 1.010 s] [INFO] Apache Hadoop Project POM .......................... SUCCESS [ 0.592 s] [INFO] Apache Hadoop Annotations .......................... SUCCESS [ 0.618 s] [INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [ 0.069 s] [INFO] Apache Hadoop Assemblies ........................... SUCCESS [ 0.109 s] [INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [ 1.754 s] [INFO] Apache Hadoop MiniKDC .............................. SUCCESS [ 0.416 s] [INFO] Apache Hadoop Auth ................................. SUCCESS [ 2.437 s] [INFO] Apache Hadoop Auth Examples ........................ SUCCESS [ 0.581 s] [INFO] Apache Hadoop Common ............................... SUCCESS [ 22.868 s] [INFO] Apache Hadoop NFS .................................. SUCCESS [ 1.273 s] [INFO] Apache Hadoop KMS .................................. SUCCESS [ 1.393 s] [INFO] Apache Hadoop Registry ............................. SUCCESS [ 1.533 s] [INFO] Apache Hadoop Common Project ....................... SUCCESS [ 0.040 s] [INFO] Apache Hadoop HDFS Client .......................... SUCCESS [ 11.693 s] [INFO] Apache Hadoop HDFS ................................. SUCCESS [ 22.103 s] [INFO] Apache Hadoop HDFS Native Client ................... SUCCESS [02:13 min] [INFO] Apache Hadoop HttpFS ............................... SUCCESS [ 2.320 s] [INFO] Apache Hadoop HDFS-NFS ............................. SUCCESS [ 1.312 s] [INFO] Apache Hadoop YARN ................................. SUCCESS [ 0.042 s] [INFO] Apache Hadoop YARN API ............................. SUCCESS [ 7.877 s] [INFO] Apache Hadoop YARN Common .......................... SUCCESS [ 6.436 s] [INFO] Apache Hadoop YARN Server .......................... SUCCESS [ 0.038 s] [INFO] Apache Hadoop YARN Server Common ................... SUCCESS [ 5.022 s] [INFO] Apache Hadoop YARN ApplicationHistoryService ....... SUCCESS [ 2.078 s] [INFO] Apache Hadoop YARN Timeline Service ................ SUCCESS [ 1.541 s] [INFO] Apache Hadoop YARN Web Proxy ....................... SUCCESS [ 1.331 s] [INFO] Apache Hadoop YARN ResourceManager ................. SUCCESS [ 13.285 s] [INFO] Apache Hadoop YARN NodeManager ..................... SUCCESS [ 24.058 s] [INFO] Apache Hadoop YARN Server Tests .................... SUCCESS [ 1.467 s] [INFO] Apache Hadoop YARN Client .......................... SUCCESS [ 2.682 s] [INFO] Apache Hadoop MapReduce Client ..................... SUCCESS [ 0.682 s] [INFO] Apache Hadoop MapReduce Core ....................... SUCCESS [ 5.570 s] [INFO] Apache Hadoop MapReduce Common ..................... SUCCESS [ 2.672 s] [INFO] Apache Hadoop MapReduce Shuffle .................... SUCCESS [ 1.767 s] [INFO] Apache Hadoop MapReduce App ........................ SUCCESS [ 3.697 s] [INFO] Apache Hadoop MapReduce HistoryServer .............. SUCCESS [ 2.380 s] [INFO] Apache Hadoop MapReduce JobClient .................. SUCCESS [ 4.119 s] [INFO] Apache Hadoop Distributed Copy ..................... SUCCESS [ 2.581 s] [INFO] Apache Hadoop Mini-Cluster ......................... SUCCESS [ 1.206 s] [INFO] Apache Hadoop Federation Balance ................... SUCCESS [ 1.630 s] [INFO] Apache Hadoop HDFS-RBF ............................. SUCCESS [ 15.224 s] [INFO] Apache Hadoop HDFS Project ......................... SUCCESS [ 0.035 s] [INFO] Apache Hadoop YARN SharedCacheManager .............. SUCCESS [ 1.195 s] [INFO] Apache Hadoop YARN Timeline Plugin Storage ......... SUCCESS [ 1.104 s] [INFO] Apache Hadoop YARN TimelineService HBase Backend ... SUCCESS [ 0.034 s] [INFO] Apache Hadoop YARN TimelineService HBase Common .... SUCCESS [ 1.532 s] [INFO] Apache Hadoop YARN TimelineService HBase Client .... SUCCESS [ 4.403 s] [INFO] Apache Hadoop YARN TimelineService HBase Servers ... SUCCESS [ 0.039 s] [INFO] Apache Hadoop YARN TimelineService HBase Server 2.5 SUCCESS [ 1.835 s] [INFO] Apache Hadoop YARN TimelineService HBase tests ..... SUCCESS [ 1.916 s] [INFO] Apache Hadoop YARN Router .......................... SUCCESS [ 2.665 s] [INFO] Apache Hadoop YARN TimelineService DocumentStore ... SUCCESS [ 1.284 s] [INFO] Apache Hadoop YARN GlobalPolicyGenerator ........... SUCCESS [ 1.474 s] [INFO] Apache Hadoop YARN Applications .................... SUCCESS [ 0.033 s] [INFO] Apache Hadoop YARN DistributedShell ................ SUCCESS [ 1.413 s] [INFO] Apache Hadoop YARN Unmanaged Am Launcher ........... SUCCESS [ 0.925 s] [INFO] Apache Hadoop YARN Services ........................ SUCCESS [ 0.033 s] [INFO] Apache Hadoop YARN Services Core ................... SUCCESS [ 3.138 s] [INFO] Apache Hadoop YARN Services API .................... SUCCESS [ 1.914 s] [INFO] Apache Hadoop YARN Application Catalog ............. SUCCESS [ 0.038 s] [INFO] Apache Hadoop YARN Application Catalog Webapp ...... SUCCESS [ 11.624 s] [INFO] Apache Hadoop YARN Application Catalog Docker Image SUCCESS [ 0.048 s] [INFO] Apache Hadoop YARN Application MaWo ................ SUCCESS [ 0.035 s] [INFO] Apache Hadoop YARN Application MaWo Core ........... SUCCESS [ 0.991 s] [INFO] Apache Hadoop YARN Site ............................ SUCCESS [ 0.075 s] [INFO] Apache Hadoop YARN Registry ........................ SUCCESS [ 0.458 s] [INFO] Apache Hadoop YARN UI .............................. SUCCESS [02:07 min] [INFO] Apache Hadoop YARN CSI ............................. SUCCESS [ 3.403 s] [INFO] Apache Hadoop YARN Project ......................... SUCCESS [ 1.283 s] [INFO] Apache Hadoop MapReduce HistoryServer Plugins ...... SUCCESS [ 1.046 s] [INFO] Apache Hadoop MapReduce NativeTask ................. SUCCESS [ 18.413 s] [INFO] Apache Hadoop MapReduce Uploader ................... SUCCESS [ 1.040 s] [INFO] Apache Hadoop MapReduce Examples ................... SUCCESS [ 1.912 s] [INFO] Apache Hadoop MapReduce ............................ SUCCESS [ 1.140 s] [INFO] Apache Hadoop MapReduce Streaming .................. SUCCESS [ 2.034 s] [INFO] Apache Hadoop Client Aggregator .................... SUCCESS [ 0.728 s] [INFO] Apache Hadoop Dynamometer Workload Simulator ....... SUCCESS [ 1.363 s] [INFO] Apache Hadoop Dynamometer Cluster Simulator ........ SUCCESS [ 1.878 s] [INFO] Apache Hadoop Dynamometer Block Listing Generator .. SUCCESS [ 1.241 s] [INFO] Apache Hadoop Dynamometer Dist ..................... SUCCESS [ 1.161 s] [INFO] Apache Hadoop Dynamometer .......................... SUCCESS [ 0.034 s] [INFO] Apache Hadoop Archives ............................. SUCCESS [ 1.310 s] [INFO] Apache Hadoop Archive Logs ......................... SUCCESS [ 1.536 s] [INFO] Apache Hadoop Rumen ................................ SUCCESS [ 1.914 s] [INFO] Apache Hadoop Gridmix .............................. SUCCESS [ 1.920 s] [INFO] Apache Hadoop Data Join ............................ SUCCESS [ 1.347 s] [INFO] Apache Hadoop Extras ............................... SUCCESS [ 1.377 s] [INFO] Apache Hadoop Pipes ................................ SUCCESS [ 2.679 s] [INFO] Apache Hadoop Amazon Web Services support .......... SUCCESS [ 7.690 s] [INFO] Apache Hadoop Kafka Library support ................ SUCCESS [ 0.901 s] [INFO] Apache Hadoop Azure support ........................ SUCCESS [ 5.553 s] [INFO] Apache Hadoop Aliyun OSS support ................... SUCCESS [ 1.183 s] [INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [ 1.679 s] [INFO] Apache Hadoop Resource Estimator Service ........... SUCCESS [ 1.213 s] [INFO] Apache Hadoop Azure Data Lake support .............. SUCCESS [ 1.156 s] [INFO] Apache Hadoop Image Generation Tool ................ SUCCESS [ 1.256 s] [INFO] Apache Hadoop Tools Dist ........................... SUCCESS [ 0.704 s] [INFO] Apache Hadoop OpenStack support .................... SUCCESS [ 0.047 s] [INFO] Apache Hadoop Common Benchmark ..................... SUCCESS [ 9.812 s] [INFO] Apache Hadoop Compatibility Benchmark .............. SUCCESS [ 0.853 s] [INFO] Apache Hadoop Tools ................................ SUCCESS [ 0.032 s] [INFO] Apache Hadoop Client API ........................... SUCCESS [ 0.911 s] [INFO] Apache Hadoop Client Runtime ....................... SUCCESS [ 0.763 s] [INFO] Apache Hadoop Client Packaging Invariants .......... SUCCESS [ 0.367 s] [INFO] Apache Hadoop Client Test Minicluster .............. SUCCESS [ 0.756 s] [INFO] Apache Hadoop Client Packaging Invariants for Test . SUCCESS [ 0.174 s] [INFO] Apache Hadoop Client Packaging Integration Tests ... SUCCESS [ 0.778 s] [INFO] Apache Hadoop Distribution ......................... SUCCESS [ 0.493 s] [INFO] Apache Hadoop Client Modules ....................... SUCCESS [ 0.034 s] [INFO] Apache Hadoop Tencent COS Support .................. SUCCESS [ 56.113 s] [INFO] Apache Hadoop OBS support .......................... SUCCESS [ 1.335 s] [INFO] Apache Hadoop Volcano Engine Services support ...... SUCCESS [ 2.005 s] [INFO] Apache Hadoop Cloud Storage ........................ SUCCESS [ 0.444 s] [INFO] Apache Hadoop Cloud Storage Project ................ SUCCESS [ 0.032 s] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 10:25 min [INFO] Finished at: 2025-08-26T05:03:57Z [INFO] ------------------------------------------------------------------------ ``` ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? pan3793 commented on code in PR #7898: URL: https://github.com/apache/hadoop/pull/7898#discussion_r2299802155 ########## start-build-env.sh: ########## @@ -93,7 +93,7 @@ RUN userdel -r \\$(getent passwd ${USER_ID} | cut -d: -f1) 2>/dev/null || : RUN groupadd --non-unique -g ${GROUP_ID} ${USER_NAME} RUN useradd -g ${GROUP_ID} -u ${USER_ID} -k /root -m ${USER_NAME} -d \"${DOCKER_HOME_DIR}\" RUN echo \"${USER_NAME} ALL=NOPASSWD: ALL\" > \"/etc/sudoers.d/hadoop-build-${USER_ID}\" -ENV HOME \"${DOCKER_HOME_DIR}\" +ENV HOME=\"${DOCKER_HOME_DIR}\" Review Comment: update because ``` 1 warning found (use docker --debug to expand): - LegacyKeyValueFormat: \"ENV key=value\" should be used instead of legacy \"ENV key value\" format (line 7) ``` slfan1989 commented on PR #7898: URL: https://github.com/apache/hadoop/pull/7898#issuecomment-3223070232 LGTM. hadoop-yetus commented on PR #7898: URL: https://github.com/apache/hadoop/pull/7898#issuecomment-3223228361 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 22m 6s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | hadolint | 0m 0s | | hadolint was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 40m 22s | | trunk passed | | -1 :x: | mvnsite | 17m 26s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/1/artifact/out/branch-mvnsite-root.txt) | root in trunk failed. | | +1 :green_heart: | shadedclient | 94m 11s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 32m 16s | | the patch passed | | +1 :green_heart: | blanks | 0m 1s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 11m 20s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/1/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | +1 :green_heart: | shadedclient | 47m 38s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 14m 13s | | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 57s | | The patch does not generate ASF License warnings. | | | | 214m 23s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7898 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs hadolint mvnsite unit jsonlint | | uname | Linux 71b5c8e33180 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f40defe72ea1f0f9d9778a4da76097fa2e1fa02b | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/1/testReport/ | | Max. process+thread count | 707 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/1/console | | versions | git=2.27.0 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. pan3793 commented on PR #7898: URL: https://github.com/apache/hadoop/pull/7898#issuecomment-3223320943 > We need to solve the compilation error issue. The building error occurs on CentOS 8 stage, looks like I need to fix it first hadoop-yetus commented on PR #7898: URL: https://github.com/apache/hadoop/pull/7898#issuecomment-3226674242 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 0m 19s | | Dockerfile '/home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-7898/rockylinux-8/src/dev-support/docker/Dockerfile_rockylinux_8' not found. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7898 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/2/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7898: URL: https://github.com/apache/hadoop/pull/7898#issuecomment-3226834289 (!) A patch to the testing environment has been detected. Re-executing against the patched versions to perform further tests. The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/3/console in case of problems. hadoop-yetus commented on PR #7898: URL: https://github.com/apache/hadoop/pull/7898#issuecomment-3227579915 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 20m 58s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 1s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ HADOOP-19661 Compile Tests _ | | +1 :green_heart: | mvninstall | 45m 24s | | HADOOP-19661 passed | | +1 :green_heart: | mvnsite | 22m 56s | | HADOOP-19661 passed | | +1 :green_heart: | shadedclient | 40m 21s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 36m 26s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | hadolint | 0m 2s | | No new issues. | | +1 :green_heart: | mvnsite | 20m 1s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | shadedclient | 40m 8s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 16m 4s | | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 5s | | The patch does not generate ASF License warnings. | | | | 246m 11s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7898 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs mvnsite unit jsonlint | | uname | Linux 1b828c1875fb 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | HADOOP-19661 / 7f02ba4fa4464804763bb80838673e37361a7f6b | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/3/testReport/ | | Max. process+thread count | 588 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/3/console | | versions | git=2.25.1 maven=3.6.3 hadolint=1.11.1-0-g0e692dd shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. stoty commented on PR #7898: URL: https://github.com/apache/hadoop/pull/7898#issuecomment-3228059389 I think it would be useful to run at least the native (C code) tests. hadoop-yetus commented on PR #7898: URL: https://github.com/apache/hadoop/pull/7898#issuecomment-3238250392 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 36m 32s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | shellcheck | 0m 1s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +0 :ok: | hadolint | 0m 1s | | hadolint was not available. | | +0 :ok: | jsonlint | 0m 1s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 34s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 39m 49s | | trunk passed | | +1 :green_heart: | compile | 17m 55s | | trunk passed | | +1 :green_heart: | checkstyle | 5m 58s | | trunk passed | | +1 :green_heart: | mvnsite | 24m 46s | | trunk passed | | +1 :green_heart: | javadoc | 8m 39s | | trunk passed | | +1 :green_heart: | spotbugs | 39m 38s | | trunk passed | | +1 :green_heart: | shadedclient | 83m 13s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 83m 51s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 43s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 39m 7s | | the patch passed | | +1 :green_heart: | compile | 16m 47s | | the patch passed | | +1 :green_heart: | javac | 16m 47s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 6m 13s | | the patch passed | | +1 :green_heart: | mvnsite | 24m 36s | | the patch passed | | +1 :green_heart: | javadoc | 10m 7s | | the patch passed | | +1 :green_heart: | spotbugs | 42m 23s | | the patch passed | | +1 :green_heart: | shadedclient | 83m 31s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 438m 25s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/5/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 2m 27s | | The patch does not generate ASF License warnings. | | | | 861m 21s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.compat.common.TestHdfsCompatShellCommand | | | hadoop.fs.compat.common.TestHdfsCompatDefaultSuites | | | hadoop.hdfs.tools.TestDFSAdmin | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7898 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs hadolint mvnsite unit jsonlint compile javac javadoc mvninstall shadedclient spotbugs checkstyle | | uname | Linux b15de39f0892 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / d7d9893cf80efe73d802c253eb12508143e8a636 | | Default Java | Red Hat, Inc.-1.8.0_462-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/5/testReport/ | | Max. process+thread count | 2833 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/5/console | | versions | git=2.43.7 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on code in PR #7898: URL: https://github.com/apache/hadoop/pull/7898#discussion_r2311396990 ########## hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/cli/TestCLI.java: ########## @@ -43,7 +43,6 @@ public void tearDown() throws Exception { @Override protected CommandExecutor.Result execute(CLICommand cmd) throws Exception { return cmd.getExecutor(\"\", conf).executeCommand(cmd.getCmd()); - Review Comment: The changes in Debian are as expected, and we can roll back the modifications to this unit as they are unrelated to our main changes. pan3793 commented on PR #7898: URL: https://github.com/apache/hadoop/pull/7898#issuecomment-3241344735 @slfan1989, unnecessary changes are reverted, should be good to go slfan1989 commented on PR #7898: URL: https://github.com/apache/hadoop/pull/7898#issuecomment-3241445604 > @slfan1989, unnecessary changes are reverted, should be good to go @pan3793 Thank you for your contribution, LGTM. I will merge this PR soon. hadoop-yetus commented on PR #7898: URL: https://github.com/apache/hadoop/pull/7898#issuecomment-3242127699 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 51s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | shellcheck | 0m 1s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +0 :ok: | hadolint | 0m 1s | | hadolint was not available. | | +0 :ok: | jsonlint | 0m 1s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 45m 10s | | trunk passed | | +1 :green_heart: | mvnsite | 25m 17s | | trunk passed | | +1 :green_heart: | shadedclient | 111m 52s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 112m 30s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 13s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 21m 1s | | the patch passed | | +1 :green_heart: | shadedclient | 62m 8s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 15m 40s | | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 56s | | The patch does not generate ASF License warnings. | | | | 234m 15s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/6/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7898 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs hadolint mvnsite unit jsonlint | | uname | Linux f4938eb59c7e 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 131b7dbbd530ccac5ad0d41775484cb7bc2306e2 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/6/testReport/ | | Max. process+thread count | 524 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7898/6/console | | versions | git=2.43.7 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 merged PR #7898: URL: https://github.com/apache/hadoop/pull/7898 slfan1989 commented on PR #7898: URL: https://github.com/apache/hadoop/pull/7898#issuecomment-3242753656 @pan3793 Thanks for the contribution! Merged into trunk. pan3793 opened a new pull request, #7932: URL: https://github.com/apache/hadoop/pull/7932 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR Fix the issue that `kill` command is not found ``` [INFO] ------------------------------------------------------- [INFO] T E S T S [INFO] ------------------------------------------------------- [INFO] Running org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree [ERROR] Tests run: 5, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 7.950 s <<< FAILURE! hadoop-yetus commented on PR #7932: URL: https://github.com/apache/hadoop/pull/7932#issuecomment-3257889051 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 3m 0s | | Docker failed to build run-specific yetus/hadoop:tp-500}. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7932 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7932/1/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7932: URL: https://github.com/apache/hadoop/pull/7932#issuecomment-3258044364 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 2m 35s | | Docker failed to build run-specific yetus/hadoop:tp-8139}. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7932 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7932/2/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7932: URL: https://github.com/apache/hadoop/pull/7932#issuecomment-3258108386 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 2m 35s | | Docker failed to build run-specific yetus/hadoop:tp-26905}. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7932 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7932/3/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7932: URL: https://github.com/apache/hadoop/pull/7932#issuecomment-3262448562 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 2m 40s | | Docker failed to build run-specific yetus/hadoop:tp-22814}. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7932 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7932/4/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on PR #7932: URL: https://github.com/apache/hadoop/pull/7932#issuecomment-3283858265 @pan3793 Thanks for the contribution! LGTM. I plan to merge this PR shortly. hadoop-yetus commented on PR #7932: URL: https://github.com/apache/hadoop/pull/7932#issuecomment-3284225600 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 30m 3s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 16s | | Maven dependency ordering for branch | | +1 :green_heart: | shadedclient | 50m 57s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 45s | | Maven dependency ordering for patch | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 41m 9s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 58s | | The patch does not generate ASF License warnings. | | | | 125m 49s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7932/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7932 | | Optional Tests | dupname asflicense codespell detsecrets jsonlint | | uname | Linux 43d6fc34e5bc 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / df78520ad4a7647e76e2bafd68ab5ce287573783 | | Max. process+thread count | 529 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7932/5/console | | versions | git=2.43.7 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7932: URL: https://github.com/apache/hadoop/pull/7932#issuecomment-3284551565 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 27m 0s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 0m 26s | | Maven dependency ordering for branch | | +1 :green_heart: | shadedclient | 30m 36s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 22s | | Maven dependency ordering for patch | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 31m 5s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 33s | | The patch does not generate ASF License warnings. | | | | 91m 20s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7932/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7932 | | Optional Tests | dupname asflicense codespell detsecrets jsonlint | | uname | Linux 74aa9d50bdcb 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / df78520ad4a7647e76e2bafd68ab5ce287573783 | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7932/5/console | | versions | git=2.30.2 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7932: URL: https://github.com/apache/hadoop/pull/7932#issuecomment-3284832335 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 50s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 0m 32s | | Maven dependency ordering for branch | | +1 :green_heart: | shadedclient | 40m 46s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 32s | | Maven dependency ordering for patch | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 40m 23s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 84m 46s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7932/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7932 | | Optional Tests | dupname asflicense codespell detsecrets jsonlint | | uname | Linux a725f8615e43 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / df78520ad4a7647e76e2bafd68ab5ce287573783 | | Max. process+thread count | 530 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7932/5/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 merged PR #7932: URL: https://github.com/apache/hadoop/pull/7932 slfan1989 commented on PR #7932: URL: https://github.com/apache/hadoop/pull/7932#issuecomment-3289300445 @pan3793 Thanks for the contribution!", "created": "2025-08-26T03:33:48.000+0000", "updated": "2025-09-14T07:13:37.000+0000", "derived": {"summary_task": "Summarize this issue: Debian 10 EOL, and the apt repo is unavailable {code:bash} docker run --rm -it debian:10 bash root@bc2a4c509cb3:/# apt update Ign:1 http://deb.debian.org/debian buster InRelease Ign:2 http://deb.debian.org/debian-security buster/updates InRelease Ign:3 http://deb.debian.org/debian buster-updates InRelease Err:4 http://deb.debian.org/debian buster Release 404 Not Found [IP: 151.101.90.132 80] Err:5", "classification_task": "Classify the issue priority and type: Debian 10 EOL, and the apt repo is unavailable {code:bash} docker run --rm -it debian:10 bash root@bc2a4c509cb3:/# apt update Ign:1 http://deb.debian.org/debian buster InRelease Ign:2 http://deb.debian.org/debian-security buster/updates InRelease Ign:3 http://deb.debian.org/debian buster-updates InRelease Err:4 http://deb.debian.org/debian buster Release 404 Not Found [IP: 151.101.90.132 80] Err:5", "qna_task": "Question: What is this issue about?\nAnswer: Upgrade Debian 10 to 11 in build env Dockerfile"}}
{"id": "13626944", "key": "HADOOP-19658", "project": "HADOOP", "summary": "ABFS: [FNS Over Blob] Support create and rename idempotency on FNS Blob from client side", "description": "Support create and rename idempotency on FNS Blob from client side", "comments": "hadoop-yetus commented on PR #7914: URL: https://github.com/apache/hadoop/pull/7914#issuecomment-3234744350 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 50s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 4 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 46m 3s | | trunk passed | | +1 :green_heart: | compile | 0m 46s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | +1 :green_heart: | javadoc | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 17s | | trunk passed | | +1 :green_heart: | shadedclient | 42m 7s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 37s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 37s | | the patch passed | | +1 :green_heart: | compile | 0m 32s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 32s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 23s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) | | +1 :green_heart: | mvnsite | 0m 37s | | the patch passed | | +1 :green_heart: | javadoc | 0m 31s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 27s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 19s | | the patch passed | | +1 :green_heart: | shadedclient | 42m 29s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 0s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 40s | | The patch does not generate ASF License warnings. | | | | 147m 33s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7914 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux bf9d5b01e663 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 3ef89e1b92e564468a24cb8d200567b30f283b10 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/1/testReport/ | | Max. process+thread count | 529 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7914: URL: https://github.com/apache/hadoop/pull/7914#issuecomment-3234745585 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 51s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 4 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 45m 45s | | trunk passed | | +1 :green_heart: | compile | 0m 45s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 47s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 12s | | trunk passed | | +1 :green_heart: | shadedclient | 40m 51s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 39s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 39s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 33s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 23s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) | | +1 :green_heart: | mvnsite | 0m 37s | | the patch passed | | +1 :green_heart: | javadoc | 0m 34s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 28s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 16s | | the patch passed | | +1 :green_heart: | shadedclient | 41m 22s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 59s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 144m 16s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7914 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 5ba8302b1e20 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 3ef89e1b92e564468a24cb8d200567b30f283b10 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/2/testReport/ | | Max. process+thread count | 529 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. anujmodi2021 commented on code in PR #7914: URL: https://github.com/apache/hadoop/pull/7914#discussion_r2309688341 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemCreate.java: ########## @@ -2236,6 +2238,98 @@ public void testFailureInGetPathStatusDuringCreateRecovery() throws Exception { } } + /** + * Test to simulate a successful create operation followed by a connection reset + * on the response, triggering a retry. + * + * This test verifies that the create operation is retried in the event of a + * connection reset during the response phase. The test creates a mock + * AzureBlobFileSystem and its associated components to simulate the create + * operation and the connection reset. It then verifies that the create + * operation is retried once before succeeding. + * + * @throws Exception if an error occurs during the test execution. + */ + @Test + public void testCreateIdempotencyForNonHnsBlob() throws Exception { + assumeThat(isAppendBlobEnabled()).as(\"Not valid for APPEND BLOB\").isFalse(); + // Create a spy of AzureBlobFileSystem + try (AzureBlobFileSystem fs = Mockito.spy( + (AzureBlobFileSystem) FileSystem.newInstance(getRawConfiguration()))) { + assumeHnsDisabled(); + // Create a spy of AzureBlobFileSystemStore + AzureBlobFileSystemStore store = Mockito.spy(fs.getAbfsStore()); + assumeBlobServiceType(); Review Comment: We can move all assume before any other statement ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemRename.java: ########## @@ -1702,6 +1705,85 @@ public void testRenamePathRetryIdempotency() throws Exception { } } + /** + * Test to simulate a successful copy blob operation followed by a connection reset + * on the response, triggering a retry. + * + * This test verifies that the copy blob operation is retried in the event of a + * connection reset during the response phase. The test creates a mock + * AzureBlobFileSystem and its associated components to simulate the copy blob + * operation and the connection reset. It then verifies that the create + * operation is retried once before succeeding. + * + * @throws Exception if an error occurs during the test execution. + */ + @Test + public void testRenameIdempotencyForNonHnsBlob() throws Exception { + assumeThat(isAppendBlobEnabled()).as(\"Not valid for APPEND BLOB\").isFalse(); + // Create a spy of AzureBlobFileSystem + try (AzureBlobFileSystem fs = Mockito.spy( + (AzureBlobFileSystem) FileSystem.newInstance(getRawConfiguration()))) { + assumeHnsDisabled(); Review Comment: Same here, move all assume to first few lines ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java: ########## @@ -509,9 +509,30 @@ public AbfsRestOperation createPath(final String path, final TracingContext tracingContext) throws AzureBlobFileSystemException { AbfsRestOperation op; if (isFileCreation) { - // Create a file with the specified parameters - op = createFile(path, overwrite, permissions, isAppendBlob, eTag, - contextEncryptionAdapter, tracingContext); + AbfsRestOperation statusOp = null; + try { + // Check if the file already exists by calling GetPathStatus + statusOp = getPathStatus(path, false, tracingContext, null); Review Comment: In case of override true, flow might come here with already a Head call done on path. Can we avoid this head call in that case? hadoop-yetus commented on PR #7914: URL: https://github.com/apache/hadoop/pull/7914#issuecomment-3236588656 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 21m 17s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 4 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 44m 33s | | trunk passed | | +1 :green_heart: | compile | 0m 44s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 43s | | trunk passed | | +1 :green_heart: | javadoc | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 12s | | trunk passed | | +1 :green_heart: | shadedclient | 41m 43s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 22s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 29s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 41m 35s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 53s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 163m 44s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7914 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 08a9e7f8f5d9 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 99bc8f51316f94c4c90e429cce43b8e1bfbbf9f7 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/3/testReport/ | | Max. process+thread count | 529 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/3/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7914: URL: https://github.com/apache/hadoop/pull/7914#issuecomment-3237131417 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 51s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 4 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 44m 57s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 43s | | trunk passed | | +1 :green_heart: | javadoc | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 12s | | trunk passed | | +1 :green_heart: | shadedclient | 41m 55s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 35s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 22s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 29s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 42m 2s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 53s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 144m 22s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7914 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 8b01375fdcf8 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 5bbeb2334a61e143c467bda06c0c3aaca8dfcfae | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/4/testReport/ | | Max. process+thread count | 534 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/4/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. anujmodi2021 commented on code in PR #7914: URL: https://github.com/apache/hadoop/pull/7914#discussion_r2313015686 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java: ########## @@ -427,6 +427,8 @@ public static String containerProperty(String property, String fsName, String ac public static final String FS_AZURE_BLOB_DIR_DELETE_MAX_THREAD = \"fs.azure.blob.dir.delete.max.thread\"; /**Flag to enable/disable sending client transactional ID during create/rename operations: {@value}*/ public static final String FS_AZURE_ENABLE_CLIENT_TRANSACTION_ID = \"fs.azure.enable.client.transaction.id\"; + /**Flag to enable/disable create idempotency during create operation: {@value}*/ + public static final String FS_AZURE_ENABLE_CREATE_IDEMPOTENCY = \"fs.azure.enable.create.idempotency\"; Review Comment: This is only for Blob Idempotency, may be we can keep config name accordingly ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -239,5 +239,7 @@ public final class FileSystemConfigurations { public static final boolean DEFAULT_FS_AZURE_ENABLE_CLIENT_TRANSACTION_ID = true; + public static final boolean DEFAULT_FS_AZURE_ENABLE_ENABLE_CREATE_IDEMPOTENCY = true; Review Comment: Typo, ENABLE added twice ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsIoUtils.java: ########## @@ -54,7 +56,15 @@ public static void dumpHeadersToDebugLog(final String origin, if (key == null) { key = \"HTTP Response\"; } - String values = StringUtils.join(\";\", entry.getValue()); + List<String> valuesList = entry.getValue(); Review Comment: Why this change? anmolanmol1234 commented on code in PR #7914: URL: https://github.com/apache/hadoop/pull/7914#discussion_r2313023120 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsIoUtils.java: ########## @@ -54,7 +56,15 @@ public static void dumpHeadersToDebugLog(final String origin, if (key == null) { key = \"HTTP Response\"; } - String values = StringUtils.join(\";\", entry.getValue()); + List<String> valuesList = entry.getValue(); Review Comment: Due to null pointer exceptions on enabling AbfsIoUtils logging if value is null. anmolanmol1234 commented on code in PR #7914: URL: https://github.com/apache/hadoop/pull/7914#discussion_r2313027848 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java: ########## @@ -427,6 +427,8 @@ public static String containerProperty(String property, String fsName, String ac public static final String FS_AZURE_BLOB_DIR_DELETE_MAX_THREAD = \"fs.azure.blob.dir.delete.max.thread\"; /**Flag to enable/disable sending client transactional ID during create/rename operations: {@value}*/ public static final String FS_AZURE_ENABLE_CLIENT_TRANSACTION_ID = \"fs.azure.enable.client.transaction.id\"; + /**Flag to enable/disable create idempotency during create operation: {@value}*/ + public static final String FS_AZURE_ENABLE_CREATE_IDEMPOTENCY = \"fs.azure.enable.create.idempotency\"; Review Comment: taken ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -239,5 +239,7 @@ public final class FileSystemConfigurations { public static final boolean DEFAULT_FS_AZURE_ENABLE_CLIENT_TRANSACTION_ID = true; + public static final boolean DEFAULT_FS_AZURE_ENABLE_ENABLE_CREATE_IDEMPOTENCY = true; Review Comment: taken hadoop-yetus commented on PR #7914: URL: https://github.com/apache/hadoop/pull/7914#issuecomment-3241561632 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 52s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 4 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 44m 21s | | trunk passed | | +1 :green_heart: | compile | 0m 44s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 10s | | trunk passed | | +1 :green_heart: | shadedclient | 41m 6s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 35s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 21s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | +1 :green_heart: | javadoc | 0m 29s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 41m 5s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 54s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 142m 37s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7914 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 820b22f42029 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 61f56c9f98a43e05563fbe7b4fb4683a5e9912ea | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/5/testReport/ | | Max. process+thread count | 528 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/5/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. bhattmanish98 commented on code in PR #7914: URL: https://github.com/apache/hadoop/pull/7914#discussion_r2313575382 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -1043,6 +1047,12 @@ public String getAzureAtomicRenameDirs() { } public boolean isConditionalCreateOverwriteEnabled() { + // If either the configured FS service type or the ingress service type is BLOB, + // conditional create-overwrite is not used. + if (getFsConfiguredServiceType() == AbfsServiceType.BLOB Review Comment: why is this change needed? hadoop-yetus commented on PR #7914: URL: https://github.com/apache/hadoop/pull/7914#issuecomment-3241959922 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 56s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 4 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 45m 52s | | trunk passed | | +1 :green_heart: | compile | 0m 46s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 37s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 42s | | trunk passed | | +1 :green_heart: | javadoc | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 12s | | trunk passed | | +1 :green_heart: | shadedclient | 41m 5s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 35s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 22s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | +1 :green_heart: | javadoc | 0m 29s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 40m 59s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 53s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 143m 30s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/6/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7914 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux ba0b32cbb7fa 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 417c97d09cd9160e443ed9e355437d9d49037297 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/6/testReport/ | | Max. process+thread count | 599 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/6/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7914: URL: https://github.com/apache/hadoop/pull/7914#issuecomment-3243070175 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 20m 54s | | Docker mode activated. | | -1 :x: | patch | 0m 5s | | https://github.com/apache/hadoop/pull/7914 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/7/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7914 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/7/console | | versions | git=2.25.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7914: URL: https://github.com/apache/hadoop/pull/7914#issuecomment-3243278526 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 49s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 6 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 45m 51s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 36s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 10s | | trunk passed | | +1 :green_heart: | shadedclient | 41m 14s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 41m 37s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 35s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/8/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 1 new + 2 unchanged - 0 fixed = 3 total (was 2) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 29s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 27s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 41m 19s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 53s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 143m 58s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/8/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7914 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux d4a1cb1bcff9 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 4948a4f00c6d3afad2416ed34d88533c201b4c24 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/8/testReport/ | | Max. process+thread count | 525 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/8/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. anmolanmol1234 commented on PR #7914: URL: https://github.com/apache/hadoop/pull/7914#issuecomment-3244719265 ============================================================ HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 819, Failures: 0, Errors: 0, Skipped: 167 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ HNS-SharedKey-DFS ============================================================ [ERROR] org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemFileStatus.testLastModifiedTime anujmodi2021 merged PR #7914: URL: https://github.com/apache/hadoop/pull/7914", "created": "2025-08-21T12:01:50.000+0000", "updated": "2025-09-03T04:12:38.000+0000", "derived": {"summary_task": "Summarize this issue: Support create and rename idempotency on FNS Blob from client side", "classification_task": "Classify the issue priority and type: Support create and rename idempotency on FNS Blob from client side", "qna_task": "Question: What is this issue about?\nAnswer: ABFS: [FNS Over Blob] Support create and rename idempotency on FNS Blob from client side"}}
{"id": "13626756", "key": "HADOOP-19657", "project": "HADOOP", "summary": "Update 3.4.2 docs landing page to highlight changes shipped in the release", "description": "The landing page for the 3.4.2 docs mostly lists changes that already shipped in 3.4.0. Update this to highlight 3.4.2 changes.", "comments": "ahmarsuhail opened a new pull request, #7887: URL: https://github.com/apache/hadoop/pull/7887 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR Updates landing page with 3.4.2 changes. ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? ahmarsuhail commented on PR #7887: URL: https://github.com/apache/hadoop/pull/7887#issuecomment-3200748949 @anujmodi2021 could you please review the ABFS changes, and let me know if there's anything else you want to highlight. @steveloughran anything else in S3A we want to highlight? I'll merge this in tomorrow and kick off the new build. hadoop-yetus commented on PR #7887: URL: https://github.com/apache/hadoop/pull/7887#issuecomment-3201009350 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 7m 4s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ branch-3.4.2 Compile Tests _ | | +1 :green_heart: | mvninstall | 24m 22s | | branch-3.4.2 passed | | +1 :green_heart: | mvnsite | 0m 17s | | branch-3.4.2 passed | | +1 :green_heart: | shadedclient | 43m 32s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 10s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 12s | | the patch passed | | +1 :green_heart: | shadedclient | 18m 37s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 20s | | The patch does not generate ASF License warnings. | | | | 71m 0s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7887/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7887 | | Optional Tests | dupname asflicense mvnsite codespell detsecrets | | uname | Linux 85cc1ac66933 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | branch-3.4.2 / 62e2fc17c26865e8b191c7b34617af0ea7a5fa95 | | Max. process+thread count | 717 (vs. ulimit of 5500) | | modules | C: hadoop-project U: hadoop-project | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7887/1/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. iwasakims commented on code in PR #7887: URL: https://github.com/apache/hadoop/pull/7887#discussion_r2286735439 ########## hadoop-project/src/site/markdown/index.md.vm: ########## @@ -23,75 +23,33 @@ Overview of Changes Users are encouraged to read the full set of release notes. This page provides an overview of the major changes. -Bulk Delete API ----------------------------------------- - -[HADOOP-18679](https://issues.apache.org/jira/browse/HADOOP-18679) Bulk Delete API. - -This release provides an API to perform bulk delete of files/objects -in an object store or filesystem. - New binary distribution ----------------------- -[HADOOP-19083](https://issues.apache.org/jira/browse/HADOOP-19083) provide hadoop binary tarball without aws v2 sdk - -Hadoop has added a new variant of the binary distribution tarball, labeled with \"lean\" in the file -name. This tarball excludes the full AWS SDK v2 bundle, resulting in approximately 50% reduction in -file size. +As of v3.4.2, Hadoop will only be distributed with a lean tarball, which excludes the full AWS SDK v2 bundle to reduce +overall file size. This release has been tested with AWS SDK v2 2.29.52, which can be downloaded from Maven +[here](https://mvnrepository.com/artifact/software.amazon.awssdk/aws-sdk-java/2.29.52). S3A improvements ---------------- **Improvement** -[HADOOP-18886](https://issues.apache.org/jira/browse/HADOOP-18886) S3A: AWS SDK V2 Migration: stabilization and S3Express - -This release completes stabilization efforts on the AWS SDK v2 migration and support of Amazon S3 -Express One Zone storage. S3 Select is no longer supported. - -[HADOOP-18993](https://issues.apache.org/jira/browse/HADOOP-18993) S3A: Add option fs.s3a.classloader.isolation (#6301) - -This introduces configuration property `fs.s3a.classloader.isolation`, which defaults to `true`. -Set to `false` to disable S3A classloader isolation, which can be useful for installing custom -credential providers in user-provided jars. - -[HADOOP-19047](https://issues.apache.org/jira/browse/HADOOP-19047) Support InMemory Tracking Of S3A Magic Commits - -The S3A magic committer now supports configuration property -`fs.s3a.committer.magic.track.commits.in.memory.enabled`. Set this to `true` to track commits in -memory instead of on the file system, which reduces the number of remote calls. +[HADOOP-19363](https://issues.apache.org/jira/browse/HADOOP-19363) S3A: Support analytics-accelerator-s3 input streams +for parquet read performance. -[HADOOP-19161](https://issues.apache.org/jira/browse/HADOOP-19161) S3A: option \u201cfs.s3a.performance.flags\u201d to take list of performance flags - -S3A now supports configuration property `fs.s3a.performance.flag` for controlling activation of -multiple performance optimizations. Refer to the S3A performance documentation for details. +HADOOP-19256](https://issues.apache.org/jira/browse/HADOOP-19256) S3A: Adds support for S3 Conditional Writes. Review Comment: formatting error due to missing leading `[`. @ahmarsuhail anujmodi2021 commented on code in PR #7887: URL: https://github.com/apache/hadoop/pull/7887#discussion_r2286959916 ########## hadoop-project/src/site/markdown/index.md.vm: ########## @@ -23,75 +23,33 @@ Overview of Changes Users are encouraged to read the full set of release notes. This page provides an overview of the major changes. -Bulk Delete API ----------------------------------------- - -[HADOOP-18679](https://issues.apache.org/jira/browse/HADOOP-18679) Bulk Delete API. - -This release provides an API to perform bulk delete of files/objects -in an object store or filesystem. - New binary distribution ----------------------- -[HADOOP-19083](https://issues.apache.org/jira/browse/HADOOP-19083) provide hadoop binary tarball without aws v2 sdk - -Hadoop has added a new variant of the binary distribution tarball, labeled with \"lean\" in the file -name. This tarball excludes the full AWS SDK v2 bundle, resulting in approximately 50% reduction in -file size. +As of v3.4.2, Hadoop will only be distributed with a lean tarball, which excludes the full AWS SDK v2 bundle to reduce +overall file size. This release has been tested with AWS SDK v2 2.29.52, which can be downloaded from Maven +[here](https://mvnrepository.com/artifact/software.amazon.awssdk/aws-sdk-java/2.29.52). S3A improvements ---------------- **Improvement** -[HADOOP-18886](https://issues.apache.org/jira/browse/HADOOP-18886) S3A: AWS SDK V2 Migration: stabilization and S3Express - -This release completes stabilization efforts on the AWS SDK v2 migration and support of Amazon S3 -Express One Zone storage. S3 Select is no longer supported. - -[HADOOP-18993](https://issues.apache.org/jira/browse/HADOOP-18993) S3A: Add option fs.s3a.classloader.isolation (#6301) - -This introduces configuration property `fs.s3a.classloader.isolation`, which defaults to `true`. -Set to `false` to disable S3A classloader isolation, which can be useful for installing custom -credential providers in user-provided jars. - -[HADOOP-19047](https://issues.apache.org/jira/browse/HADOOP-19047) Support InMemory Tracking Of S3A Magic Commits - -The S3A magic committer now supports configuration property -`fs.s3a.committer.magic.track.commits.in.memory.enabled`. Set this to `true` to track commits in -memory instead of on the file system, which reduces the number of remote calls. +[HADOOP-19363](https://issues.apache.org/jira/browse/HADOOP-19363) S3A: Support analytics-accelerator-s3 input streams +for parquet read performance. -[HADOOP-19161](https://issues.apache.org/jira/browse/HADOOP-19161) S3A: option \u201cfs.s3a.performance.flags\u201d to take list of performance flags - -S3A now supports configuration property `fs.s3a.performance.flag` for controlling activation of -multiple performance optimizations. Refer to the S3A performance documentation for details. +HADOOP-19256](https://issues.apache.org/jira/browse/HADOOP-19256) S3A: Adds support for S3 Conditional Writes. ABFS improvements ----------------- **Improvement** -[HADOOP-18516](https://issues.apache.org/jira/browse/HADOOP-18516) [ABFS]: Support fixed SAS token config in addition to Custom SASTokenProvider Implementation - -ABFS now supports authentication via a fixed Shared Access Signature token. Refer to ABFS -documentation of configuration property `fs.azure.sas.fixed.token` for details. - -[HADOOP-19089](https://issues.apache.org/jira/browse/HADOOP-19089) [ABFS] Reverting Back Support of setXAttr() and getXAttr() on root path - -[HADOOP-18869](https://issues.apache.org/jira/browse/HADOOP-18869) previously implemented support for xattrs on the root path in the 3.4.0 release. Support for this has been removed in 3.4.1 to prevent the need for calling container APIs. - -[HADOOP-19178](https://issues.apache.org/jira/browse/HADOOP-19178) WASB Driver Deprecation and eventual removal - -This release announces deprecation of the WASB file system in favor of ABFS. Refer to ABFS -documentation for additional guidance. - -**Bug** - -[HADOOP-18542](https://issues.apache.org/jira/browse/HADOOP-18542) Azure Token provider requires tenant and client IDs despite being optional +[HADOOP-19226](https://issues.apache.org/jira/browse/HADOOP-19226) ABFS: [FnsOverBlob] Implementing Azure Rest APIs on +Blob Endpoint for AbfsBlobClient. -It is no longer necessary to specify a tenant and client ID in configuration for MSI authentication -when running in an Azure instance. +[HADOOP-19474](https://issues.apache.org/jira/browse/HADOOP-19474) ABFS: [FnsOverBlob] Listing Optimizations to avoid Review Comment: https://issues.apache.org/jira/browse/HADOOP-19543 also can be added. ########## hadoop-project/src/site/markdown/index.md.vm: ########## @@ -23,75 +23,33 @@ Overview of Changes Users are encouraged to read the full set of release notes. This page provides an overview of the major changes. -Bulk Delete API ----------------------------------------- - -[HADOOP-18679](https://issues.apache.org/jira/browse/HADOOP-18679) Bulk Delete API. - -This release provides an API to perform bulk delete of files/objects -in an object store or filesystem. - New binary distribution ----------------------- -[HADOOP-19083](https://issues.apache.org/jira/browse/HADOOP-19083) provide hadoop binary tarball without aws v2 sdk - -Hadoop has added a new variant of the binary distribution tarball, labeled with \"lean\" in the file -name. This tarball excludes the full AWS SDK v2 bundle, resulting in approximately 50% reduction in -file size. +As of v3.4.2, Hadoop will only be distributed with a lean tarball, which excludes the full AWS SDK v2 bundle to reduce +overall file size. This release has been tested with AWS SDK v2 2.29.52, which can be downloaded from Maven +[here](https://mvnrepository.com/artifact/software.amazon.awssdk/aws-sdk-java/2.29.52). S3A improvements ---------------- **Improvement** -[HADOOP-18886](https://issues.apache.org/jira/browse/HADOOP-18886) S3A: AWS SDK V2 Migration: stabilization and S3Express - -This release completes stabilization efforts on the AWS SDK v2 migration and support of Amazon S3 -Express One Zone storage. S3 Select is no longer supported. - -[HADOOP-18993](https://issues.apache.org/jira/browse/HADOOP-18993) S3A: Add option fs.s3a.classloader.isolation (#6301) - -This introduces configuration property `fs.s3a.classloader.isolation`, which defaults to `true`. -Set to `false` to disable S3A classloader isolation, which can be useful for installing custom -credential providers in user-provided jars. - -[HADOOP-19047](https://issues.apache.org/jira/browse/HADOOP-19047) Support InMemory Tracking Of S3A Magic Commits - -The S3A magic committer now supports configuration property -`fs.s3a.committer.magic.track.commits.in.memory.enabled`. Set this to `true` to track commits in -memory instead of on the file system, which reduces the number of remote calls. +[HADOOP-19363](https://issues.apache.org/jira/browse/HADOOP-19363) S3A: Support analytics-accelerator-s3 input streams +for parquet read performance. -[HADOOP-19161](https://issues.apache.org/jira/browse/HADOOP-19161) S3A: option \u201cfs.s3a.performance.flags\u201d to take list of performance flags - -S3A now supports configuration property `fs.s3a.performance.flag` for controlling activation of -multiple performance optimizations. Refer to the S3A performance documentation for details. +HADOOP-19256](https://issues.apache.org/jira/browse/HADOOP-19256) S3A: Adds support for S3 Conditional Writes. ABFS improvements ----------------- **Improvement** -[HADOOP-18516](https://issues.apache.org/jira/browse/HADOOP-18516) [ABFS]: Support fixed SAS token config in addition to Custom SASTokenProvider Implementation - -ABFS now supports authentication via a fixed Shared Access Signature token. Refer to ABFS -documentation of configuration property `fs.azure.sas.fixed.token` for details. - -[HADOOP-19089](https://issues.apache.org/jira/browse/HADOOP-19089) [ABFS] Reverting Back Support of setXAttr() and getXAttr() on root path - -[HADOOP-18869](https://issues.apache.org/jira/browse/HADOOP-18869) previously implemented support for xattrs on the root path in the 3.4.0 release. Support for this has been removed in 3.4.1 to prevent the need for calling container APIs. - -[HADOOP-19178](https://issues.apache.org/jira/browse/HADOOP-19178) WASB Driver Deprecation and eventual removal - -This release announces deprecation of the WASB file system in favor of ABFS. Refer to ABFS -documentation for additional guidance. - -**Bug** - -[HADOOP-18542](https://issues.apache.org/jira/browse/HADOOP-18542) Azure Token provider requires tenant and client IDs despite being optional +[HADOOP-19226](https://issues.apache.org/jira/browse/HADOOP-19226) ABFS: [FnsOverBlob] Implementing Azure Rest APIs on Review Comment: For FNSOverBlob support,may be we can use parent JIRA here as it will track all the related work items. John6665 commented on PR #7887: URL: https://github.com/apache/hadoop/pull/7887#issuecomment-3204892829 Good job ahmarsuhail merged PR #7887: URL: https://github.com/apache/hadoop/pull/7887 hadoop-yetus commented on PR #7887: URL: https://github.com/apache/hadoop/pull/7887#issuecomment-3205356855 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 38s | | https://github.com/apache/hadoop/pull/7887 does not apply to branch-3.4.2. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7887 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7887/2/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-19T13:16:06.000+0000", "updated": "2025-08-21T01:39:59.000+0000", "derived": {"summary_task": "Summarize this issue: The landing page for the 3.4.2 docs mostly lists changes that already shipped in 3.4.0. Update this to highlight 3.4.2 changes.", "classification_task": "Classify the issue priority and type: The landing page for the 3.4.2 docs mostly lists changes that already shipped in 3.4.0. Update this to highlight 3.4.2 changes.", "qna_task": "Question: What is this issue about?\nAnswer: Update 3.4.2 docs landing page to highlight changes shipped in the release"}}
{"id": "13626749", "key": "HADOOP-19656", "project": "HADOOP", "summary": "Fix hadoop-client-minicluster", "description": "", "comments": "can you please fill the description of the ticket, it isn't conclusive from the ticket, like what is broken which needs to be fixed I should close this ticket, the issue was fixed by HADOOP-19652", "created": "2025-08-19T12:53:49.000+0000", "updated": "2025-09-05T09:18:52.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Fix hadoop-client-minicluster"}}
{"id": "13626707", "key": "HADOOP-19655", "project": "HADOOP", "summary": "Add RISC-V Zbc (CLMUL) hardware-accelerated CRC32/CRC32C implementation", "description": "This patch introduces hardware-accelerated CRC32 and CRC32C algorithms for RISC-V platforms supporting the Zbc extension (CLMUL instructions) in bulk_crc32_riscv.c. Key changes: * Implements optimized CRC32 and CRC32C routines using CLMUL instructions for zlib and Castagnoli polynomials. * Automatically switches to hardware acceleration when Zbc is available, otherwise falls back to generic table-based software implementation. * Maintains compatibility with platforms lacking Zbc support. This optimization improves CRC performance on RISC-V CPUs with Zbc extension.", "comments": "PeterPtroc opened a new pull request, #7896: URL: https://github.com/apache/hadoop/pull/7896 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR - Implements a CLMUL-based 16B fold + Barrett reduction algorithm, adapted from riscv-crc32-clmul. - True interleaved (round-robin) multi-block pipeline (1\u20133 blocks) to increase ILP. - Small buffers and tails fall back to the existing table-based software path. - Runtime gating: - Double-checked detection: \u201czbc\u201d in /proc/cpuinfo AND a SIGILL-safe CLMUL probe. ### How was this patch tested? - Build (native profile): - mvn -pl hadoop-common-project/hadoop-common -am -Pnative -DskipTests clean install - Run benchmark: - mvn -Pnative -DskipTests -Dexec.classpathScope=test -Dexec.mainClass=org.apache.hadoop.util.Crc32PerformanceTest ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #7896: URL: https://github.com/apache/hadoop/pull/7896#issuecomment-3222853817 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 27m 44s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 44m 27s | | trunk passed | | +1 :green_heart: | compile | 18m 7s | | trunk passed | | -1 :x: | mvnsite | 1m 45s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/2/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in trunk failed. | | +1 :green_heart: | shadedclient | 105m 59s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 1m 10s | | the patch passed | | +1 :green_heart: | compile | 16m 23s | | the patch passed | | +1 :green_heart: | cc | 16m 23s | | the patch passed | | +1 :green_heart: | golang | 16m 23s | | the patch passed | | +1 :green_heart: | javac | 16m 23s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 1m 47s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/2/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch failed. | | +1 :green_heart: | shadedclient | 43m 12s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 24m 53s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 47s | | The patch does not generate ASF License warnings. | | | | 223m 28s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7896 | | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang | | uname | Linux 4727479d09e6 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 58baa6eb96c75dd288767f650918073c5c094a6f | | Default Java | Red Hat, Inc.-1.8.0_312-b07 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/2/testReport/ | | Max. process+thread count | 3133 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/2/console | | versions | git=2.27.0 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7896: URL: https://github.com/apache/hadoop/pull/7896#issuecomment-3222859121 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 29m 24s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 46m 46s | | trunk passed | | +1 :green_heart: | compile | 17m 41s | | trunk passed | | -1 :x: | mvnsite | 1m 45s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/1/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in trunk failed. | | +1 :green_heart: | shadedclient | 107m 55s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 1m 12s | | the patch passed | | +1 :green_heart: | compile | 16m 38s | | the patch passed | | +1 :green_heart: | cc | 16m 38s | | the patch passed | | +1 :green_heart: | golang | 16m 38s | | the patch passed | | +1 :green_heart: | javac | 16m 38s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 1m 43s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/1/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch failed. | | +1 :green_heart: | shadedclient | 42m 59s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 24m 53s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 54s | | The patch does not generate ASF License warnings. | | | | 227m 14s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7896 | | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang | | uname | Linux 262a84b27cca 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 58baa6eb96c75dd288767f650918073c5c094a6f | | Default Java | Red Hat, Inc.-1.8.0_312-b07 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/1/testReport/ | | Max. process+thread count | 3133 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/1/console | | versions | git=2.27.0 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-19T02:52:30.000+0000", "updated": "2025-10-17T16:49:52.000+0000", "derived": {"summary_task": "Summarize this issue: This patch introduces hardware-accelerated CRC32 and CRC32C algorithms for RISC-V platforms supporting the Zbc extension (CLMUL instructions) in bulk_crc32_riscv.c. Key changes: * Implements optimized CRC32 and CRC32C routines using CLMUL instructions for zlib and Castagnoli polynomials. * Automatically switches to hardware acceleration when Zbc is available, otherwise falls back to generic table-", "classification_task": "Classify the issue priority and type: This patch introduces hardware-accelerated CRC32 and CRC32C algorithms for RISC-V platforms supporting the Zbc extension (CLMUL instructions) in bulk_crc32_riscv.c. Key changes: * Implements optimized CRC32 and CRC32C routines using CLMUL instructions for zlib and Castagnoli polynomials. * Automatically switches to hardware acceleration when Zbc is available, otherwise falls back to generic table-", "qna_task": "Question: What is this issue about?\nAnswer: Add RISC-V Zbc (CLMUL) hardware-accelerated CRC32/CRC32C implementation"}}
{"id": "13626670", "key": "HADOOP-19654", "project": "HADOOP", "summary": "Upgrade AWS SDK to 2.35.4", "description": "Upgrade to a recent version of 2.33.x or later while off the critical path of things. HADOOP-19485 froze the sdk at a version which worked with third party stores. Apparently the new version works; early tests show that Bulk Delete calls with third party stores complain about lack of md5 headers, so some tuning is clearly going to be needed.", "comments": "steveloughran opened a new pull request, #7882: URL: https://github.com/apache/hadoop/pull/7882 ### How was this patch tested? Testing in progress; still trying to get the ITests working. JUnit5 update complicates things here, as it highlights that minicluster tests aren't working. ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? pan3793 commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3201364651 > JUnit5 update complicates things here, as it highlights that minicluster tests aren't working. I found `hadoop-client-runtime` and `hadoop-client-minicluster` broken during integration with Spark, HADOOP-19652 plus YARN-11824 recovers that, is it the same issue? steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3201641390 @pan3793 maybe. what is unrelated is out the box the SDK doesn't do bulk delete with third party stores which support it (Dell ECS). ``` org.apache.hadoop.fs.s3a.AWSBadRequestException: bulkDelete on job-00-fork-0001/test/org.apache.hadoop.fs.contract.s3a.ITestS3AContractBulkDelete: software.amazon.awssdk.services.s3.model.InvalidRequestException: Missing required header for this request: Content-MD5 (Service: S3, Status Code: 400, Request ID: 0c07c87d:196d43d824a:d5329:91d, Extended Request ID: 85e1d41b57b608d4e58222b552dea52902e93b05a12f63f54730ae77769df8d1) (SDK Attempt Count: 1):InvalidRequest: Missing required header for this request: Content-MD5 (Service: S3, Status Code: 400, Request ID: 0c07c87d:196d43d824a:d5329:91d, Extended Request ID: 85e1d41b57b608d4e58222b552dea52902e93b05a12f63f54730ae77769df8d1) (SDK Attempt Count: 1) -- ``` steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3201646178 @pan3793 no, it's lifecycle related. Test needs to set up that minicluster before the test cases. and that's somehow not happening hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3209266234 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 32s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 49s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 34m 10s | | trunk passed | | +1 :green_heart: | compile | 17m 32s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 14m 18s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 4m 12s | | trunk passed | | +1 :green_heart: | mvnsite | 23m 24s | | trunk passed | | +1 :green_heart: | javadoc | 9m 45s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 49s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 21s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 65m 42s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 1m 15s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 32m 7s | | the patch passed | | +1 :green_heart: | compile | 15m 9s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 9s | | the patch passed | | +1 :green_heart: | compile | 13m 51s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 51s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 4m 18s | | the patch passed | | +1 :green_heart: | mvnsite | 18m 45s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 9m 33s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 48s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 21s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 66m 27s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 368m 52s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/2/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 19s | | The patch does not generate ASF License warnings. | | | | 735m 38s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux cb65e960fd1f 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0d3f20b487ebe8cca5f4b91a3197d7e6cc639901 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/2/testReport/ | | Max. process+thread count | 3658 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3221833528 regressions ## everywhere No logging. Instead we get ``` SLF4J: Failed to load class \"org.slf4j.impl.StaticMDCBinder\". SLF4J: Defaulting to no-operation MDCAdapter implementation. SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. ``` `ITestS3AContractAnalyticsStreamVectoredRead` failures -stream closed. more on this once I've looked at it. If it is an SDK issue, major regression, though it may be something needing changes in the aal libary ## s3 express ``` [ERROR] ITestTreewalkProblems.testDistCp:319->lambda$testDistCp$3:320 [Exit code of distcp -useiterator -update -delete -direct s3a://stevel--usw2-az1--x-s3/job-00-fork-0005/test/testDistCp/src s3a://stevel--usw2-az1--x-s3/job-00-fork-0005/test/testDistCp/dest] ``` assumption: now that the store has lifecycle rules, you don't get prefix listings when there's an in-progress upload. Fix: change test but also path capability warning of inconsistency. this is good. Operation costs/auditing count an extra HTTP request, so cost tests fail. I suspect it is always calling CreateSession, but without logging can't be sure hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3222650336 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 32s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 10 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 57s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 32m 35s | | trunk passed | | +1 :green_heart: | compile | 15m 39s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 50s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 4m 15s | | trunk passed | | +1 :green_heart: | mvnsite | 23m 16s | | trunk passed | | +1 :green_heart: | javadoc | 9m 41s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 52s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 21s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 65m 47s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 1m 3s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 32m 0s | | the patch passed | | +1 :green_heart: | compile | 15m 22s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 22s | | the patch passed | | +1 :green_heart: | compile | 13m 52s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 52s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/3/artifact/out/blanks-eol.txt) | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 4m 11s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/3/artifact/out/results-checkstyle-root.txt) | root: The patch generated 9 new + 42 unchanged - 5 fixed = 51 total (was 47) | | +1 :green_heart: | mvnsite | 18m 51s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 9m 36s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 53s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 21s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 65m 59s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 371m 3s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/3/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 17s | | The patch does not generate ASF License warnings. | | | | 733m 32s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.TestRollingUpgrade | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux 880f3cb624ae 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 5b9a7e32525c27e876698f49e88ab520eae2d8c4 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/3/testReport/ | | Max. process+thread count | 3821 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/3/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3296808329 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 10 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 10m 31s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 24m 17s | | trunk passed | | +1 :green_heart: | compile | 9m 23s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 7m 50s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 2m 0s | | trunk passed | | +1 :green_heart: | mvnsite | 19m 57s | | trunk passed | | +1 :green_heart: | javadoc | 5m 17s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 37s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 11s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 40m 37s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 40s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 23m 52s | | the patch passed | | +1 :green_heart: | compile | 8m 3s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 3s | | the patch passed | | +1 :green_heart: | compile | 7m 24s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 7m 24s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/6/artifact/out/blanks-eol.txt) | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 1m 54s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/6/artifact/out/results-checkstyle-root.txt) | root: The patch generated 9 new + 42 unchanged - 5 fixed = 51 total (was 47) | | +1 :green_heart: | mvnsite | 11m 32s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 5m 26s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 5m 7s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 15s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 39m 23s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 678m 20s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/6/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 8s | | The patch does not generate ASF License warnings. | | | | 913m 40s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.mapreduce.v2.TestUberAM | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/6/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux 113d355d9ed2 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / cc31e5be98b54ee418f5ddad4696de2d40e099a0 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/6/testReport/ | | Max. process+thread count | 4200 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/6/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3298415046 Thanks @steveloughran, PR looks good overall. Are then failures in `ITestS3AContractAnalyticsStreamVectoredRead` intermittent? I've not been able to reproduce, am running the test on this SDK upgrade branch. ahmarsuhail commented on code in PR #7882: URL: https://github.com/apache/hadoop/pull/7882#discussion_r2352378026 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/ITestS3APutIfMatchAndIfNoneMatch.java: ########## @@ -390,7 +416,7 @@ public void testIfNoneMatchOverwriteWithEmptyFile() throws Throwable { // close the stream, should throw RemoteFileChangedException RemoteFileChangedException exception = intercept(RemoteFileChangedException.class, stream::close); - assertS3ExceptionStatusCode(SC_412_PRECONDITION_FAILED, exception); + verifyS3ExceptionStatusCode(SC_412_PRECONDITION_FAILED, exception); Review Comment: do you know what the difference is with the other tests here? As in, why with S3 express is it ok to assert that we'll get a 412, whereas the others tests will throw a 200? ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/ITestAssumeRole.java: ########## @@ -203,7 +206,7 @@ protected Configuration createValidRoleConf() throws JsonProcessingException { conf.set(ASSUMED_ROLE_SESSION_DURATION, \"45m\"); // disable create session so there's no need to // add a role policy for it. - disableCreateSession(conf); + //disableCreateSession(conf); Review Comment: nit: can just cut this instead of commenting it out, since we're skipping these tests if S3 Express is enabled hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3301096683 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 10 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 10m 32s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 23m 50s | | trunk passed | | +1 :green_heart: | compile | 8m 32s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 7m 30s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 58s | | trunk passed | | +1 :green_heart: | mvnsite | 14m 30s | | trunk passed | | +1 :green_heart: | javadoc | 5m 33s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 5m 5s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 15s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 38m 32s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 36s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 23m 26s | | the patch passed | | +1 :green_heart: | compile | 8m 17s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 17s | | the patch passed | | +1 :green_heart: | compile | 7m 15s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 7m 15s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/7/artifact/out/blanks-eol.txt) | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 1m 58s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/7/artifact/out/results-checkstyle-root.txt) | root: The patch generated 9 new + 42 unchanged - 5 fixed = 51 total (was 47) | | +1 :green_heart: | mvnsite | 12m 9s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 5m 27s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 5m 4s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 15s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 38m 17s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 678m 56s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/7/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 11s | | The patch does not generate ASF License warnings. | | | | 905m 0s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.mapreduce.v2.TestUberAM | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/7/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux 3b890eb50412 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 3351e41830fbc9230ffe18bd88bfc0e2a60b20bd | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/7/testReport/ | | Max. process+thread count | 4379 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/7/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3304013014 I've attached a log of a test run against an s3 express bucket where the test `ITestAWSStatisticCollection.testSDKMetricsCostOfGetFileStatusOnFile()` is failing because the AWS SDK stats report 2 http requests for the probe. I'd thought it was create-session related but it isn't: it looks like somehow the stream is broken. This happens reliably on every test runs. The relevant stuff is at line 564 where a HEAD request fails because the stream is broken \"end of stream\". ``` 2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"HEAD /test/testSDKMetricsCostOfGetFileStatusOnFile HTTP/1.1[\\r][\\n]\" 2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"Host: stevel--usw2-az1--x-s3.s3express-usw2-az1.us-west-2.amazonaws.com[\\r][\\n]\" 2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"amz-sdk-invocation-id: 1804bbcd-04de-cba8-8055-6a09917ca20d[\\r][\\n]\" 2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"amz-sdk-request: attempt=1; max=3[\\r][\\n]\" 2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"Authorization: AWS4-HMAC-SHA256 Credential=AKIA/20250917/us-west-2/s3express/aws4_request, SignedHeaders=amz-sdk-invocation-id;amz-sdk-request;host;referer;x-amz-content-sha256;x-amz-date, Signature=228a46bb1d008468d38afd0da0ed7b4c354ab12631a63bf4283cb23dc02527a3[\\r][\\n]\" 2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"Referer: https://audit.example.org/hadoop/1/op_get_file_status/cf739331-1f2e-42dd-a5d9-f564d6023a23-00000008/?op=op_get_file_status&p1=test/testSDKMetricsCostOfGetFileStatusOnFile&pr=stevel&ps=282e3c5d-c1bd-4859-94b9-82e77ff225d1&id=cf739331-1f2e-42dd-a5d9-f564d6023a23-00000008&t0=1&fs=cf739331-1f2e-42dd-a5d9-f564d6023a23&t1=1&ts=1758131029311[\\r][\\n]\" 2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"User-Agent: Hadoop 3.5.0-SNAPSHOT aws-sdk-java/2.33.8 md/io#sync md/http#Apache ua/2.1 api/S3#2.33.x os/Mac_OS_X#15.6.1 lang/java#17.0.8 md/OpenJDK_64-Bit_Server_VM#17.0.8+7-LTS md/vendor#Amazon.com_Inc. md/en_GB m/F,G hll/cross-region[\\r][\\n]\" 2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"x-amz-content-sha256: UNSIGNED-PAYLOAD[\\r][\\n]\" 2025-09-17 18:43:49,314 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"X-Amz-Date: 20250917T174349Z[\\r][\\n]\" 2025-09-17 18:43:49,314 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"Connection: Keep-Alive[\\r][\\n]\" 2025-09-17 18:43:49,314 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"[\\r][\\n]\" 2025-09-17 18:43:49,314 [setup] DEBUG http.wire (Wire.java:wire(87)) - http-outgoing-1 << \"end of stream\" 2025-09-17 18:43:49,314 [setup] DEBUG awssdk.request (LoggerAdapter.java:debug(125)) - Retryable error detected. Will retry in 51ms. Request attempt number 1 software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: The target server failed to respond at software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:130) at software.amazon.awssdk.core.exception.SdkClientException.create(SdkClientException.java:47) ``` The second request always works. ``` 2025-09-17 18:43:49,672 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"HEAD /test/testSDKMetricsCostOfGetFileStatusOnFile HTTP/1.1[\\r][\\n]\" 2025-09-17 18:43:49,673 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"Host: stevel--usw2-az1--x-s3.s3express-usw2-az1.us-west-2.amazonaws.com[\\r][\\n]\" 2025-09-17 18:43:49,673 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"amz-sdk-invocation-id: 1804bbcd-04de-cba8-8055-6a09917ca20d[\\r][\\n]\" 2025-09-17 18:43:49,673 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"amz-sdk-request: attempt=2; max=3[\\r][\\n]\" 2025-09-17 18:43:49,673 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"Authorization: AWS4-HMAC-SHA256 Credential=AKIA/20250917/us-west-2/s3express/aws4_request, SignedHeaders=amz-sdk-invocation-id;amz-sdk-request;host;referer;x-amz-content-sha256;x-amz-date, Signature=920d981fad319228c969f5df7f5c1a3c7e4d3c0e2f45ff53bba73e6cf47c5871[\\r][\\n]\" 2025-09-17 18:43:49,673 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"Referer: https://audit.example.org/hadoop/1/op_get_file_status/cf739331-1f2e-42dd-a5d9-f564d6023a23-00000008/?op=op_get_file_status&p1=test/testSDKMetricsCostOfGetFileStatusOnFile&pr=stevel&ps=282e3c5d-c1bd-4859-94b9-82e77ff225d1&id=cf739331-1f2e-42dd-a5d9-f564d6023a23-00000008&t0=1&fs=cf739331-1f2e-42dd-a5d9-f564d6023a23&t1=1&ts=1758131029311[\\r][\\n]\" 2025-09-17 18:43:49,673 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"User-Agent: Hadoop 3.5.0-SNAPSHOT aws-sdk-java/2.33.8 md/io#sync md/http#Apache ua/2.1 api/S3#2.33.x os/Mac_OS_X#15.6.1 lang/java#17.0.8 md/OpenJDK_64-Bit_Server_VM#17.0.8+7-LTS md/vendor#Amazon.com_Inc. md/en_GB m/F,G hll/cross-region[\\r][\\n]\" 2025-09-17 18:43:49,673 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"x-amz-content-sha256: UNSIGNED-PAYLOAD[\\r][\\n]\" 2025-09-17 18:43:49,673 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"X-Amz-Date: 20250917T174349Z[\\r][\\n]\" 2025-09-17 18:43:49,674 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"Connection: Keep-Alive[\\r][\\n]\" 2025-09-17 18:43:49,674 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"[\\r][\\n]\" 2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"HTTP/1.1 200 OK[\\r][\\n]\" 2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"server: AmazonS3[\\r][\\n]\" 2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"x-amz-request-id: 01869434dd00019958c6871b05090b3f875a3c90[\\r][\\n]\" 2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"x-amz-id-2: 9GqfbNyMyUs6[\\r][\\n]\" 2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"etag: \"6036aaaf62444466bf0a21cc7518f738\"[\\r][\\n]\" 2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"accept-ranges: bytes[\\r][\\n]\" 2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"last-modified: Wed, 17 Sep 2025 17:43:49 GMT[\\r][\\n]\" 2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"x-amz-storage-class: EXPRESS_ONEZONE[\\r][\\n]\" 2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"content-type: application/octet-stream[\\r][\\n]\" 2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"x-amz-server-side-encryption: AES256[\\r][\\n]\" 2025-09-17 18:43:49,860 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"content-length: 0[\\r][\\n]\" 2025-09-17 18:43:49,860 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"x-amz-expiration: NotImplemented[\\r][\\n]\" 2025-09-17 18:43:49,860 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"date: Wed, 17 Sep 2025 17:43:48 GMT[\\r][\\n]\" 2025-09-17 18:43:49,860 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"[\\r][\\n]\" 2025-09-17 18:43:49,860 [setup] DEBUG awssdk.request (LoggerAdapter.java:debug(105)) - Received successful response: 200, Request ID: ``` Either the request is being rejected (why?) or the connection has gone stale. But why should it happen at exactly the same place on every single test run? [org.apache.hadoop.fs.s3a.statistics.ITestAWSStatisticCollection-output.txt](https://github.com/user-attachments/files/22391405/org.apache.hadoop.fs.s3a.statistics.ITestAWSStatisticCollection-output.txt) steveloughran commented on code in PR #7882: URL: https://github.com/apache/hadoop/pull/7882#discussion_r2356314622 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/ITestS3APutIfMatchAndIfNoneMatch.java: ########## @@ -390,7 +416,7 @@ public void testIfNoneMatchOverwriteWithEmptyFile() throws Throwable { // close the stream, should throw RemoteFileChangedException RemoteFileChangedException exception = intercept(RemoteFileChangedException.class, stream::close); - assertS3ExceptionStatusCode(SC_412_PRECONDITION_FAILED, exception); + verifyS3ExceptionStatusCode(SC_412_PRECONDITION_FAILED, exception); Review Comment: Hey, it's your server code. Go see. hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3305933937 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 34s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 11 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 12m 12s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 40m 29s | | trunk passed | | +1 :green_heart: | compile | 15m 48s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 14m 0s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 4m 18s | | trunk passed | | +1 :green_heart: | mvnsite | 21m 27s | | trunk passed | | +1 :green_heart: | javadoc | 9m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 58s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 21s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 66m 25s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 1m 3s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 40m 59s | | the patch passed | | +1 :green_heart: | compile | 15m 18s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 18s | | the patch passed | | +1 :green_heart: | compile | 13m 50s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 50s | | the patch passed | | -1 :x: | blanks | 0m 1s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/8/artifact/out/blanks-eol.txt) | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 4m 10s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/8/artifact/out/results-checkstyle-root.txt) | root: The patch generated 7 new + 42 unchanged - 5 fixed = 49 total (was 47) | | +1 :green_heart: | mvnsite | 19m 25s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 9m 38s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 50s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 21s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 66m 26s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 450m 14s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/8/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 21s | | The patch does not generate ASF License warnings. | | | | 832m 28s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/8/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux 40fa101aa5ab 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 661dc6e3caa66f1218db70d8e6959c2ee3cb0a87 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/8/testReport/ | | Max. process+thread count | 3559 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/8/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3306500643 @steveloughran discovered completely by accident, but it's something to do with the checksumming code. If you comment out these lines: ``` // builder.addPlugin(LegacyMd5Plugin.create()); // do not do request checksums as this causes third-party store problems. // builder.requestChecksumCalculation(RequestChecksumCalculation.WHEN_REQUIRED); // response checksum validation. Slow, even with CRC32 checksums. // if (parameters.isChecksumValidationEnabled()) { // builder.responseChecksumValidation(ResponseChecksumValidation.WHEN_SUPPORTED); // } ``` the test will pass. Could be something to do with s3Express not supporting md5, will look into it. ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3306741287 Specifically, it's this line: `builder.requestChecksumCalculation(RequestChecksumCalculation.WHEN_REQUIRED);` that causes this. Comment that out, or change it to `builder.requestChecksumCalculation(RequestChecksumCalculation.WHEN_SUPPORTED)`, it passes. My guess is it's something to do with S3 express not supporting MD5, but for operations where `RequestChecksumCalculation.WHEN_REQUIRED` is true, SDK calculates the m5 and then S3 express rejects it. Have asked the SDK team. steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3307416441 ok, so maybe for s3express stores we don't do legacy MD5 plugin stuff all is good? 1. Does imply the far end is breaking the connection when it is unhappy -at least our unit tests found this stuff before the cost of every HEAD doubles. 2. maybe we should make the choice of checksums an enum with md5 the default, so it is something that can be turned off/changed in future. While on the topic of S3 Express, is it now the case that because there's lifecycle rules for cleanup, LIST calls don't return prefixes of paths with incomplete uploads? If so I will need to change production code and the test -with a separate JIRA for that for completeness ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3311549488 > for s3express stores we don't do legacy MD5 plugin stuff all is good @steveloughran confirming with the SDK team, since the MD5 plugin is supposed to restore previous behaviour, the server rejecting the first request seems wrong. let's see what they have to say. > LIST calls don't return prefixes of paths with incomplete uploads Will check with S3 express team on this steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3312197290 thanks. I don't see it on tests against s3 with the 2.29.52 release, so something is changing with the requests made with new SDK + MD5 stuff. ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3318915081 @steveloughran not able to narrow this error down just yet, it looks like it's a combination of S3A's configuration of the S3 client + these new Md5 changes. ``` @Test public void testHead() throws Throwable { // S3Client s3Client = getFileSystem().getS3AInternals().getAmazonS3Client(\"test instance\"); S3Client s3Client = S3Client.builder().region(Region.US_EAST_1) .addPlugin(LegacyMd5Plugin.create()) .requestChecksumCalculation(RequestChecksumCalculation.WHEN_REQUIRED) .responseChecksumValidation(ResponseChecksumValidation.WHEN_SUPPORTED) .overrideConfiguration(o -> o.retryStrategy(b -> b.maxAttempts(1))) .build(); s3Client.headObject(HeadObjectRequest.builder().bucket(\"<>\") .key(\"<>\").build()); } ``` I see the failure when the S3A client, and don't see it when I use a newly created client. So it's not just because of `requestChecksumCalculation(RequestChecksumCalculation.WHEN_REQUIRED)` Looking into it some more. S3 express team said there have been no changes in LIST behaviour. ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3320014995 able to reproduce the issue outside of S3A. Basically did what would happen when you run a test in S3A: * a probe for the `test/` directory, and then create the `test/` directory, and then do the `headObject()` call. The head fails, but if you comment out `requestChecksumCalculation(RequestChecksumCalculation.WHEN_REQUIRED)` it works again. no idea what's going on. but have shared this local reproduction with SDK team. And rules out that it's something in the S3A code. ``` public class TestClass { S3Client s3Client; public TestClass() { this.s3Client = S3Client.builder().region(Region.US_EAST_1) .addPlugin(LegacyMd5Plugin.create()) .requestChecksumCalculation(RequestChecksumCalculation.WHEN_REQUIRED) .responseChecksumValidation(ResponseChecksumValidation.WHEN_SUPPORTED) .overrideConfiguration(o -> o.retryStrategy(b -> b.maxAttempts(1))) .build(); } public void testS3Express(String bucket, String key) { s3Client.listObjectsV2(ListObjectsV2Request.builder() .bucket(\"<>\") .maxKeys(2) .prefix(\"test/\") .build()); try { s3Client.headObject(HeadObjectRequest.builder().bucket(\"<>\") .key(\"test\") .build()); } catch (Exception e) { System.out.println(\"Exception thrown: \" + e.getMessage()); } s3Client.putObject(PutObjectRequest .builder() .bucket(\"<>\") .key(\"test/\").build(), RequestBody.empty()); s3Client.headObject(HeadObjectRequest.builder().bucket(\"<>\") .key(\"<>\") .build()); } ``` steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3325180154 well, nice and simple code snippet for the regression testing. Shows the value in having sdk metrics tied up...this is the only case which failed because it's the one asserting at the SDK level values. steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3353451859 @ahmarsuhail is there a public sdk issue for this for me to link/track? ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3355348352 Just created https://github.com/aws/aws-sdk-java-v2/issues/6459 ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3370618249 FYI @steveloughran , SDK team was able to root cause the issue, details here: https://github.com/aws/aws-sdk-java-v2/issues/6459#issuecomment-3362570846 Since it's a bit of an edge case, and the SDK retry means we recover from it anyway, you think we can go ahead with the upgrade or should we wait for the fix? hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3405709735 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 33s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 11 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 11m 32s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 27m 4s | | trunk passed | | +1 :green_heart: | compile | 15m 43s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 48s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 3m 13s | | trunk passed | | -1 :x: | mvnsite | 10m 26s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/artifact/out/branch-mvnsite-root.txt) | root in trunk failed. | | +1 :green_heart: | javadoc | 9m 10s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 8m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +0 :ok: | spotbugs | 0m 28s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | -1 :x: | spotbugs | 31m 2s | [/branch-spotbugs-root-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/artifact/out/branch-spotbugs-root-warnings.html) | root in trunk has 12 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 57m 7s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 50s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 28m 35s | | the patch passed | | +1 :green_heart: | compile | 15m 6s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 6s | | the patch passed | | +1 :green_heart: | compile | 15m 39s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 39s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/artifact/out/blanks-eol.txt) | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 3m 13s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/artifact/out/results-checkstyle-root.txt) | root: The patch generated 7 new + 42 unchanged - 5 fixed = 49 total (was 47) | | -1 :x: | mvnsite | 7m 15s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | -1 :x: | javadoc | 9m 2s | [/results-javadoc-javadoc-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/artifact/out/results-javadoc-javadoc-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 1 new + 43015 unchanged - 0 fixed = 43016 total (was 43015) | | +1 :green_heart: | javadoc | 8m 45s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +0 :ok: | spotbugs | 0m 24s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 57m 1s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 793m 21s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 44s | | The patch does not generate ASF License warnings. | | | | 1110m 44s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.tools.TestDFSAdmin | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux 503d715744fe 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / e319765ca591fc2a0968f3b2e900586bb46ce7c1 | | Default Java | Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/testReport/ | | Max. process+thread count | 3551 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3408678775 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 15 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 9s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 16m 19s | | trunk passed | | +1 :green_heart: | compile | 8m 32s | | trunk passed | | +1 :green_heart: | checkstyle | 1m 41s | | trunk passed | | -1 :x: | mvnsite | 6m 5s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/branch-mvnsite-root.txt) | root in trunk failed. | | +1 :green_heart: | javadoc | 5m 5s | | trunk passed | | +0 :ok: | spotbugs | 0m 15s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | -1 :x: | spotbugs | 0m 28s | [/branch-spotbugs-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/branch-spotbugs-hadoop-tools_hadoop-aws.txt) | hadoop-aws in trunk failed. | | -1 :x: | spotbugs | 0m 16s | [/branch-spotbugs-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/branch-spotbugs-root.txt) | root in trunk failed. | | +1 :green_heart: | shadedclient | 14m 2s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 30s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 15m 7s | | the patch passed | | +1 :green_heart: | compile | 8m 25s | | the patch passed | | +1 :green_heart: | javac | 8m 25s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/blanks-eol.txt) | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 1m 32s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/results-checkstyle-root.txt) | root: The patch generated 11 new + 47 unchanged - 6 fixed = 58 total (was 53) | | -1 :x: | mvnsite | 3m 45s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | -1 :x: | javadoc | 4m 54s | [/results-javadoc-javadoc-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/results-javadoc-javadoc-root.txt) | root generated 4 new + 43015 unchanged - 0 fixed = 43019 total (was 43015) | | +0 :ok: | spotbugs | 0m 12s | | hadoop-project has no data from spotbugs | | -1 :x: | spotbugs | 0m 26s | [/patch-spotbugs-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/patch-spotbugs-hadoop-tools_hadoop-aws.txt) | hadoop-aws in the patch failed. | | -1 :x: | spotbugs | 0m 17s | [/patch-spotbugs-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/patch-spotbugs-root.txt) | root in the patch failed. | | +1 :green_heart: | shadedclient | 13m 44s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 227m 31s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 343m 23s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.tools.TestDFSAdmin | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux f49b0547d834 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c2eb04aa497f8d4648a3b457a90843ce96abe7fe | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/testReport/ | | Max. process+thread count | 5153 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/console | | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3412290514 @ahmarsuhail I'm handling the retries now by requiring the md5 plugin to be explicitly requested (i.e. third party stores); also making it easier to switch checksum generation from ALWAYS to WHEN_REQUESTED. So for AWS S3: stricter checksums, no md5. Other stores: configure it as needed. Still wondering if we should make this more automated, but not in a way which causes problems later. --- I am now seeing failings against s3 express ``` org.opentest4j.AssertionFailedError: [Counter named audit_request_execution with expected value 4] Expecting: <11L> to be equal to: <4L> but was not. Expected :4 Actual :11 <Click to see difference> at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at org.apache.hadoop.fs.statistics.IOStatisticAssertions.verifyStatisticValue(IOStatisticAssertions.java:274) at org.apache.hadoop.fs.statistics.IOStatisticAssertions.verifyStatisticCounterValue(IOStatisticAssertions.java:175) at org.apache.hadoop.fs.s3a.ITestS3AAnalyticsAcceleratorStreamReading.testMultiRowGroupParquet(ITestS3AAnalyticsAcceleratorStreamReading.java:186) at java.lang.reflect.Method.invoke(Method.java:498) at java.util.ArrayList.forEach(ArrayList.java:1259) at java.util.ArrayList.forEach(ArrayList.java:1259) ``` I'm changing this test to measure the # of audited requests before the file opening begins and then assert on the difference between them. steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3412377933 Now that 3rd party is good, I'm getting S3 express happy, mainly by test tuning. But many, many errors with vectored reads ``` [ERROR] Errors: [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testAllRangesMergedIntoOne [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testAllRangesMergedIntoOne \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testAllRangesMergedIntoOne \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testBufferSlicing [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testBufferSlicing \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testBufferSlicing \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testConsecutiveRanges [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testConsecutiveRanges \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testConsecutiveRanges \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testMultipleVectoredReads [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testMultipleVectoredReads \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testMultipleVectoredReads \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testNormalReadAfterVectoredRead [INFO] Run 1: PASS [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testNormalReadAfterVectoredRead \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testReadVectoredWithAALStatsCollection [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testReadVectoredWithAALStatsCollection \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testReadVectoredWithAALStatsCollection \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testSomeRandomNonOverlappingRanges [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testSomeRandomNonOverlappingRanges \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testSomeRandomNonOverlappingRanges \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadAfterNormalRead [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadAfterNormalRead \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadAfterNormalRead \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadAndReadFully [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead>AbstractContractVectoredReadTest.testVectoredReadAndReadFully:220 \u00bb IO test/vectored_file.txt: Stream is closed! [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead>AbstractContractVectoredReadTest.testVectoredReadAndReadFully:220 \u00bb IO test/vectored_file.txt: Stream is closed! [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadMultipleRanges [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead>AbstractContractVectoredReadTest.testVectoredReadMultipleRanges:206 \u00bb Execution java.io.IOException: Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead>AbstractContractVectoredReadTest.testVectoredReadMultipleRanges:206 \u00bb Execution java.io.IOException: Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadWholeFile [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadWholeFile \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadWholeFile \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [INFO] ``` ``` [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testReadVectoredWithAALStatsCollection ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3412492676 @steveloughran just ran with the old 2.29.x SDK, failures there too. will look into it and fix ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3415388890 This is happening because those readVectored() tests create a new `vectored-read.txt` file on the setup() before each test. Since the tests are parameterized, they run twice, once for `direct-buffer` and then for `array-buffer`. On the first run for `direct-buffer`, a HEAD for the metadata is made and cached, and the data for `vectored-read.txt` is also cached. Then the stream is `closed()` and since the file ends in `.txt`, AAL clears the data cache. (since it's a sequential format, the chances there will be a backward seek and the same data will be accessed are low so it's better to clear the data cache). The metadata cache is not cleared here (it should be, and I will make that fix). On the second run for `array-buffer`, the `vectored-file` is written again. AAL will get the metadata from the metadata cache, and use that eTag when making the GETS for the block data. Since on S3 express, the eTag is no longer the md5 of the object content, even though the object content is the same, the eTag has changed. And hence the 412s on the GETS. On consistency with caching in general: * AAL provides a `metadatastore.ttl` config, set that to 0 and HEAD responses are never cached. This solves the caching issues we had when overwrite files before, as with that `ttl` 0 we will always get the latest version of the file. * Data blocks will be removed once memory usage is > defined memory threshold (2GB), and clean up happens every 5s by default. The edge case here is that what if data usage is always below 2GB, and data blocks never get evicted? This is why the `metadatastore.ttl` was introduced. * Our `BlockKey` which is the key under which file data is stored is a combination of the S3URI + eTag. If the eTag changes, then we'll have a different BlockKey, which means we don't have any data stored for it. For example: ``` * Data is written to A.parquet, etag is \"1234\". * A.parquet is read fully in to the cache, with key \"A.parquet + 1234\" * A.parquet is overwritten, etag is \"6789\". * A.parquet is opened for reading again: If metadata ttl has not yet expired, and metadata cache has eTag as `1234`, so AAL will return data from the data cache using key \"A.parquet + 1234\". If the requested data is not in the data cache, we'll make a GET with the outdated eTag as `1234` and this will fail with a 412. If metadata TTL has expired, a new HEAD request is made, and we now have the eTag `6789`, this will now create a new BlockKey \"A.parquet + 6789\", and since there is no data stored here, will make GETS for the data. ``` With this we ensure two things: 1/ Once a stream opened it will always serve bytes from the same object version, or fail. 2/ Data will be stale at maximum metadata.tll milliseconds, with the exception of stream's lifetime. Basically, if your data changes often, set the metadataTTL to 0, and AAL will always get the latest data. Otherwise we have eventually consistency. ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3415396379 The TLDR is: * I will make a small fix to clear the metadata cache on stream close for sequential formats (which fixes this issue) * Setting the `metadata.ttl` also fixes this issue. I've tested with both, and all `ITestS3AContractAnalyticsStreamVectoredRead` test cases pass. steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3416145759 good explanation. Though I would have expected a bit broader test coverage of your own stores; something to look for on the next library update. Can I also get improvements in error translation too -we need the error string including request IDs. Relying on the stack entry below to print it isn't enough, as deep exception nesting (hive, spark) can lose that. steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3416153341 one more thing here: make sure you can handle `null` as an etag in the cache. Not all stores have it, which is why it can be turned off for classic input stream version checking. You won't be able to detect overwrites, but we can just document having a short TTL here. ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3422219254 @steveloughran updated exception handling: https://github.com/awslabs/analytics-accelerator-s3/pull/361, next release will have include the requestIDs in the message, eg: ``` java.io.IOException: Server error accessing s3://xxx, request failed with: At least one of the pre-conditions you specified did not hold (Service: S3, Status Code: 412, Request ID: xxxx, Extended Request ID: xxxx) ``` The null as `eTag` will require more work, the only way to do that reliably is to disable the caching fully and provide a pass through stream. Do you know which stores don't support eTags? steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3428848006 latest iteration works with third party stores without MPU (so no magic or use of memory for upload buffering), or bulk delete. tested google gcs, only underful buffers which can be ignored. ``` [ERROR] Failures: [ERROR] ITestS3AContractUnbuffer>AbstractContractUnbufferTest.testMultipleUnbuffers:108->AbstractContractUnbufferTest.validateFullFileContents:141->AbstractContractUnbufferTest.validateFileContents:148 failed to read expected number of bytes from stream. This may be transient ==> expected: <1024> but was: <533> [ERROR] ITestS3AContractUnbuffer>AbstractContractUnbufferTest.testUnbufferAfterRead:61->AbstractContractUnbufferTest.validateFullFileContents:141->AbstractContractUnbufferTest.validateFileContents:148 failed to read expected number of bytes from stream. This may be transient ==> expected: <1024> but was: <533> [ERROR] ITestS3AContractUnbuffer>AbstractContractUnbufferTest.testUnbufferBeforeRead:71->AbstractContractUnbufferTest.validateFullFileContents:141->AbstractContractUnbufferTest.validateFileContents:148 failed to read expected number of bytes from stream. This may be transient ==> expected: <1024> but was: <539> [ERROR] ITestS3AContractUnbuffer>AbstractContractUnbufferTest.testUnbufferOnClosedFile:91->AbstractContractUnbufferTest.validateFullFileContents:141->AbstractContractUnbufferTest.validateFileContents:148 failed to read expected number of bytes from stream. This may be transient ==> expected: <1024> but was: <539> [INFO] [ERROR] Tests run: 1253, Failures: 4, Errors: 0, Skipped: 450 [INFO] ``` steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3428880917 @ahmarsuhail I think Apache Ozone is the one. I just added an `etag` command to cloudstore to print this stuff out and experimented with various stores: https://github.com/steveloughran/cloudstore/blob/main/src/main/site/etag.md dell ECS and Google both supply etags. We don't retrieve them for directory markers anyway, which isn't an issue * I've updated the third-party docs to cover etags in more detail, and say \"switch to classic and disable version checking\" * I do think the cache needs to handle null/empty string tags, somehow. Certainly by not caching metadata. hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3432106892 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 15m 2s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 38 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 10m 43s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 29m 3s | | trunk passed | | +1 :green_heart: | compile | 15m 53s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 41s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 3m 15s | | trunk passed | | -1 :x: | mvnsite | 10m 51s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/branch-mvnsite-root.txt) | root in trunk failed. | | +1 :green_heart: | javadoc | 9m 36s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 8m 31s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +0 :ok: | spotbugs | 0m 28s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | -1 :x: | spotbugs | 1m 18s | [/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html) | hadoop-tools/hadoop-aws in trunk has 188 extant spotbugs warnings. | | -1 :x: | spotbugs | 36m 27s | [/branch-spotbugs-root-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/branch-spotbugs-root-warnings.html) | root in trunk has 9241 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 61m 43s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 48s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 27m 26s | | the patch passed | | +1 :green_heart: | compile | 14m 53s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 14m 53s | | the patch passed | | +1 :green_heart: | compile | 15m 24s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 24s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/blanks-eol.txt) | The patch has 24 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 3m 15s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/results-checkstyle-root.txt) | root: The patch generated 14 new + 79 unchanged - 6 fixed = 93 total (was 85) | | -1 :x: | mvnsite | 7m 2s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | -1 :x: | javadoc | 9m 37s | [/results-javadoc-javadoc-root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/results-javadoc-javadoc-root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) | root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 4 new + 46184 unchanged - 0 fixed = 46188 total (was 46184) | | -1 :x: | javadoc | 8m 41s | [/results-javadoc-javadoc-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/results-javadoc-javadoc-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 4 new + 43015 unchanged - 0 fixed = 43019 total (was 43015) | | +0 :ok: | spotbugs | 0m 22s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 62m 29s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 741m 49s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 44s | | The patch does not generate ASF License warnings. | | | | 1085m 5s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.sls.appmaster.TestAMSimulator | | | hadoop.security.ssl.TestDelegatingSSLSocketFactory | | | hadoop.yarn.service.TestYarnNativeServices | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux 8fbc6faf5962 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1cd8a2820c4aadeca61f3a7449c7d98fd34bb9d8 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/testReport/ | | Max. process+thread count | 3717 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3434634222 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 8m 27s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 39 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 7m 18s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 15m 15s | | trunk passed | | +1 :green_heart: | compile | 8m 18s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 8m 18s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 1m 25s | | trunk passed | | -1 :x: | mvnsite | 6m 10s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/branch-mvnsite-root.txt) | root in trunk failed. | | +1 :green_heart: | javadoc | 5m 31s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 43s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +0 :ok: | spotbugs | 0m 15s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | -1 :x: | spotbugs | 0m 41s | [/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html) | hadoop-tools/hadoop-aws in trunk has 188 extant spotbugs warnings. | | -1 :x: | spotbugs | 18m 42s | [/branch-spotbugs-root-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/branch-spotbugs-root-warnings.html) | root in trunk has 9241 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 32m 22s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 33s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 15m 11s | | the patch passed | | +1 :green_heart: | compile | 7m 52s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 7m 52s | | the patch passed | | +1 :green_heart: | compile | 8m 17s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 17s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/blanks-eol.txt) | The patch has 24 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 1m 33s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/results-checkstyle-root.txt) | root: The patch generated 14 new + 79 unchanged - 6 fixed = 93 total (was 85) | | -1 :x: | mvnsite | 3m 43s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | -1 :x: | javadoc | 5m 23s | [/results-javadoc-javadoc-root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/results-javadoc-javadoc-root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) | root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 4 new + 46184 unchanged - 0 fixed = 46188 total (was 46184) | | -1 :x: | javadoc | 4m 38s | [/results-javadoc-javadoc-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/results-javadoc-javadoc-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 4 new + 43015 unchanged - 0 fixed = 43019 total (was 43015) | | +0 :ok: | spotbugs | 0m 12s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 32m 15s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 594m 47s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 0m 50s | | The patch does not generate ASF License warnings. | | | | 779m 27s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.yarn.server.resourcemanager.reservation.TestCapacityOverTimePolicy | | | hadoop.hdfs.server.balancer.TestBalancerWithHANameNodes | | | hadoop.hdfs.server.federation.router.async.TestRouterAsyncRpcClient | | | hadoop.security.ssl.TestDelegatingSSLSocketFactory | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux c90f744f2220 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 96c3f38ea5e033636e1acdb8fe2ed4b398bedb08 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/testReport/ | | Max. process+thread count | 4624 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3437736945 fun test run today, against s3 london. Most of the multipart upload/commit tests were failing \"missing part\", from cli or IDE. Testing with S3 express was happy. (`-Dparallel-tests -DtestsThreadCount=8 -Panalytics -Dscale`) ``` [ERROR] ITestS3AHugeMagicCommits.test_030_postCreationAssertions:192 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: JAEYPCZ4P3JYGMTD, Extended Request ID: O/135mw9Xd2aEuFUh0ICWYc8DLXSpBUWaVGkEgEFGf0xO8o+XlZXY0hI+mvennOGt+C/UI7mNrQ=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: JAEYPCZ4P3JYGMTD, Extended Request ID: O/135mw9Xd2aEuFUh0ICWYc8DLXSpBUWaVGkEgEFGf0xO8o+XlZXY0hI+mvennOGt+C/UI7mNrQ=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeMagicCommits>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin in s3a://stevel-london/job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/array/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/bytebuffer/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: K0K75V8AH7SVBHS3, Extended Request ID: kDosbp+Z2PLZn9tVtRF9QfOqh1MgLbIKYaYFn2JeIptXlBV4v1a/wFukoXnaF7fCp6zx3vR8feE0fScUJEw+WhNW9lzu9dBxssOA62UA2kg=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: K0K75V8AH7SVBHS3, Extended Request ID: kDosbp+Z2PLZn9tVtRF9QfOqh1MgLbIKYaYFn2JeIptXlBV4v1a/wFukoXnaF7fCp6zx3vR8feE0fScUJEw+WhNW9lzu9dBxssOA62UA2kg=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/disk/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 73T4YAYRWE63WAW5, Extended Request ID: 6ucEY2heh2NsxE8dBrlZp9AE4Tb+hbvnyxea1/yp5H85BEvkQdYsfNlRH5XZM1g4hHPDSoGMVtM=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 73T4YAYRWE63WAW5, Extended Request ID: 6ucEY2heh2NsxE8dBrlZp9AE4Tb+hbvnyxea1/yp5H85BEvkQdYsfNlRH5XZM1g4hHPDSoGMVtM=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/disk/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: ZSY181YB49GQFR83, Extended Request ID: FrPEfsXO3Gbhxi3m4ZmyYSiyfscQ1QSm/1lKjRPLHEbLWH5vtGked+fHvZl281Dm6u013/5VP6pj42h4XISftk7p9uEIDGw31E7Ymcoviq4=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: ZSY181YB49GQFR83, Extended Request ID: FrPEfsXO3Gbhxi3m4ZmyYSiyfscQ1QSm/1lKjRPLHEbLWH5vtGked+fHvZl281Dm6u013/5VP6pj42h4XISftk7p9uEIDGw31E7Ymcoviq4=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesStorageClass.test_010_CreateHugeFile:74->AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/array/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: APYCQNP1GY02DGDE, Extended Request ID: lE0hQJ67sSwCYSMmO7tDEAvEIOCcpwIbLdfqqrNTpWT0bHIaacaIEzZusajj79rnFQlWudxsMHBIUXdS9ELiKR0T923lcULZy4Essx1LoTs=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: APYCQNP1GY02DGDE, Extended Request ID: lE0hQJ67sSwCYSMmO7tDEAvEIOCcpwIbLdfqqrNTpWT0bHIaacaIEzZusajj79rnFQlWudxsMHBIUXdS9ELiKR0T923lcULZy4Essx1LoTs=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesStorageClass.test_030_postCreationAssertions:81->AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesStorageClass>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesStorageClass.test_100_renameHugeFile:108->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [INFO] [ERROR] Tests run: 124, Failures: 1, Errors: 30, Skipped: 13 [INFO] ``` This has to be some transient issue with my s3 london bucket, as if in progress upload parts were not being retained. Never seen this before; the expiry time is set to 24h When these uploads fail we do leave incomplete uploads in progress: ``` Listing uploads under path \"\" job-00-fork-0005/test/testCommitOperations 141OKG11JHhWF1GOnunHUd9ZzBJ8cUG9z0LsW_4wUGgCXCvDMQM3kRi5IOCUV8FdCHtg_w8SlipfubRtzCQoT5yEpOLv.cWOiOwjEaBzUjnuJORppfXuKy1piHpLnu98 job-00-fork-0005/test/testIfMatchTwoMultipartUploadsRaceConditionOneClosesFirst yBJpm3zh4DjNQIDtyWgEmWVCk5sehVz5Vzn3QGr_tQT2iOonRp5ErXsQy24yIvnzRxBCZqVapy5VepLeu2udZBT5EXLnKRA3bchvzjtKDlipywSzYlL2N_xLUDCT359I job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads AnspJPHUoPJqg61t28OvLfAogi6G9ocyx1Dm6XY2C.a_H_onklM0Nr0LIXaPiYlQjZIiH0fTsQ1e2KhEjS9pGxvSKOXq_4YibiGZmFC6rBolmfACMqIRpoeaqYDgzYW4 job-00-fork-0005/test/testMagicWriteRecovery/file.txt KpvoTuVh85Wzm9XuU1EuxbATjb6D.Zv8vEj3z2S6AvJBHCBssy4iphxNhTkLDs7ceEwak4IPtdXED1vRf3geXT7MRMJn8d6feafvHVEgzbD31odpzTLmOaPrU_mFQXGV job-00-fork-0005/test/testMagicWriteRecovery/file.txt CnrbWU3pzgEGvjRuDuaP43Xcv1eBF5aLknqYaZA1vwO3b1QUIu9QJSiZjuLMYKT9GKw1QXwqoKo4iuxTY1a18bARx4XMEiL98kZBv0TPMaAfXE.70Olh8Q2kTyDlUCSh job-00-fork-0005/test/testMagicWriteRecovery/file.txt dEVGPBRsuOAzL5pGA02ve9qJhAlNK8lb8khF6laKjo9U0j_aG1xLkHEfPLrmcrcsLxC3R755Yv_uKbzY_Vnoc.nXCprvutM1TZmLLN_7LHrQ0tY0IjYSS6hVzDVlHbvC job-00-fork-0006/test/restricted/testCommitEmptyFile/empty-commit.txt NOCjVJqycZhkalrvU26F5oIaJP51q055et2N6b74.2JVjiKL8KwrhOhdrtumOrZ2tZWNqaK4iKZ_iosqgehJOiPbWJwxvrfvA5V.dAUTLNqjtEf5tfWh0UXu.vahDy_S5SSgNLFXK.VB82i5MZtOcw-- job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin lsYNpdn_oiWLwEVvvM621hCvIwDVaL4y_bbwVpQouW1OBThA.P9cR8fZtxvBjGdMY41UH0dTjxGHtF3BXEY8WXqmcnO9QHs_Jy.os781pE3MGzqgzFyxmd0yN6LFcTbq test/restricted/testCommitEmptyFile/empty-commit.txt T3W9V56Bv_FMhKpgcBgJ1H2wOBkPKk23T0JomesBzZyqiIAu3NiROibAgoZUhWSdoTKSJoOgcn3UWYGOvGBbsHteS_N_c1QoTEp0GE7PNlzDfs1GheJ5SOpUgaEY6MaYdNe0mn0gY48FDXpVB2nqiA-- test/restricted/testCommitEmptyFile/empty-commit.txt .cr4b3xkfze4N24Bj3PAm_ACIyIVuTU4DueDktU1abNu2LJWXH2HKnUu1oOjfnnQwnUXp4VmXBVbZ5aq8E8gVCxN.Oyb7hmGVtESmRjpqIXSW80JrB_0_dqXe.uAT.JH7kEWywAlb4NIqJ5Xz99tvA-- Total 10 uploads found. ``` Most interesting here is `testIfNoneMatchTwoConcurrentMultipartUploads`, because this initiates then completes an MPU, so as to create a zero byte file. It doesn't upload any parts. The attempt to complete failed. ``` [ERROR] ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchTwoConcurrentMultipartUploads:380->createFileWithFlags:190 \u00bb AWSBadRequest Completing multipart upload on job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 9JCJ6M5QRDGJNYYS, Extended Request ID: Z7Q7+LA0o/5B4xoIGhgo+tVppawZ0UBj7X4RNb+0m9RbOAOwD/Apv1o+KmnW0aypjwmfFlarxjo=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 9JCJ6M5QRDGJNYYS, Extended Request ID: Z7Q7+LA0o/5B4xoIGhgo+tVppawZ0UBj7X4RNb+0m9RbOAOwD/Apv1o+KmnW0aypjwmfFlarxjo=) (SDK Attempt Count: 1) ``` Yet the uploads list afterwards finds it ``` job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads AnspJPHUoPJqg61t28OvLfAogi6G9ocyx1Dm6XY2C.a_H_onklM0Nr0LIXaPiYlQjZIiH0fTsQ1e2KhEjS9pGxvSKOXq_4YibiGZmFC6rBolmfACMqIRpoeaqYDgzYW4 ``` I have to conclude that the list of pending uploads was briefly offline/inconsistent. This is presumably so, so rare that there's almost no point retrying here. With no retries, every active write/job would have failed, even though the system had recovered within a minute. Maybe we should retry here? I remember a long long time ago the v1 sdk didn't retry on failures of the final POST to commit an upload, and how that sporadically caused problems. Retrying on MPU failures will allow for recovery in the presence of a transient failure here, and the cost of \"deletion of all pending uploads will take longer to fail all active uploads\". hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3441063874 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 19s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 39 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 10m 42s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 15m 48s | | trunk passed | | +1 :green_heart: | compile | 8m 10s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 8m 15s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 1m 36s | | trunk passed | | -1 :x: | mvnsite | 6m 44s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/17/artifact/out/branch-mvnsite-root.txt) | root in trunk failed. | | +1 :green_heart: | javadoc | 5m 35s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 51s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +0 :ok: | spotbugs | 0m 15s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | -1 :x: | spotbugs | 0m 41s | [/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/17/artifact/out/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html) | hadoop-tools/hadoop-aws in trunk has 188 extant spotbugs warnings. | | -1 :x: | spotbugs | 19m 4s | [/branch-spotbugs-root-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/17/artifact/out/branch-spotbugs-root-warnings.html) | root in trunk has 9241 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 34m 44s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 34s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 16m 48s | | the patch passed | | +1 :green_heart: | compile | 8m 49s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 49s | | the patch passed | | +1 :green_heart: | compile | 9m 26s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 9m 26s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 41s | | root: The patch generated 0 new + 79 unchanged - 6 fixed = 79 total (was 85) | | -1 :x: | mvnsite | 4m 11s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/17/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 5m 17s | | root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 0 new + 46182 unchanged - 2 fixed = 46182 total (was 46184) | | +1 :green_heart: | javadoc | 4m 44s | | root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 0 new + 43013 unchanged - 2 fixed = 43013 total (was 43015) | | +0 :ok: | spotbugs | 0m 11s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 34m 53s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 591m 32s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/17/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 0m 51s | | The patch does not generate ASF License warnings. | | | | 781m 38s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.hdfs.tools.TestDFSAdmin | | | hadoop.security.ssl.TestDelegatingSSLSocketFactory | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/17/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux 54d25015775c 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / fa906dcf97ff8829f50184906bd7433bc2a0a73a | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/17/testReport/ | | Max. process+thread count | 4675 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/17/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3444243683 OK, this is all related to checksums on multipart puts. If you declare that checksums are always required on requests, you MUST define a checksum algorithm to use for multipart put, otherwise upload completions fail. I have no idea why, will file some SDK bug report to say \"this is wrong\" and simply change our settings to - checksums NOT always required - MD5 always enabled - checksum algorithm is CRC32C (will test with third party store) checksums in MPUs breaks a couple of the multipart uploader tests; more worried that about a ITestS3AOpenCost test failing with checksum verification being enabled (slow, expensive). I need to make sure that this is not an SDK regression. steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3444263138 Its a change in the default value: downloads have checksums verified unless you say \"no\". we say no. AWS SDK issue #6518 shows how checksum generation on uploaded data (fs.s3a.create.checksum) must be set if request checksum calculation is enabled (fs.s3a.checksum.generation) Checksum validation has also been enabled by default; {{ITestS3AOpenCost.testStreamIsNotChecksummed()}} caught that change. It looks like the SDK has really embraced checksums, which first broke compatibility with other stores, but which has also surfaced problems within their own code. All checksum logic will be off by default; MD5 headers will be attached now hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3446382053 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 23s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 44 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 10m 1s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 15m 50s | | trunk passed | | +1 :green_heart: | compile | 8m 12s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 8m 23s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 1m 36s | | trunk passed | | -1 :x: | mvnsite | 5m 54s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/branch-mvnsite-root.txt) | root in trunk failed. | | +1 :green_heart: | javadoc | 5m 18s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 41s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +0 :ok: | spotbugs | 0m 16s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | -1 :x: | spotbugs | 1m 27s | [/branch-spotbugs-hadoop-common-project_hadoop-common-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/branch-spotbugs-hadoop-common-project_hadoop-common-warnings.html) | hadoop-common-project/hadoop-common in trunk has 448 extant spotbugs warnings. | | -1 :x: | spotbugs | 0m 39s | [/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html) | hadoop-tools/hadoop-aws in trunk has 188 extant spotbugs warnings. | | -1 :x: | spotbugs | 18m 32s | [/branch-spotbugs-root-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/branch-spotbugs-root-warnings.html) | root in trunk has 9241 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 32m 9s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 29s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 15m 37s | | the patch passed | | +1 :green_heart: | compile | 8m 6s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 6s | | the patch passed | | +1 :green_heart: | compile | 8m 20s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 20s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/blanks-eol.txt) | The patch has 6 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 1m 30s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/results-checkstyle-root.txt) | root: The patch generated 6 new + 83 unchanged - 6 fixed = 89 total (was 89) | | -1 :x: | mvnsite | 3m 47s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 5m 16s | | root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 0 new + 46182 unchanged - 2 fixed = 46182 total (was 46184) | | +1 :green_heart: | javadoc | 4m 41s | | root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 0 new + 43013 unchanged - 2 fixed = 43013 total (was 43015) | | +0 :ok: | spotbugs | 0m 11s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 32m 34s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 587m 3s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 0m 50s | | The patch does not generate ASF License warnings. | | | | 770m 3s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.service.TestYarnNativeServices | | | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.hdfs.tools.TestDFSAdmin | | | hadoop.security.ssl.TestDelegatingSSLSocketFactory | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint markdownlint shellcheck shelldocs | | uname | Linux cc0336ad4f43 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 149e98291ada965d1dc2b85b4214f235bb4b5c5d | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/testReport/ | | Max. process+thread count | 4586 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-18T16:47:04.000+0000", "updated": "2025-10-25T09:26:53.000+0000", "derived": {"summary_task": "Summarize this issue: Upgrade to a recent version of 2.33.x or later while off the critical path of things. HADOOP-19485 froze the sdk at a version which worked with third party stores. Apparently the new version works; early tests show that Bulk Delete calls with third party stores complain about lack of md5 headers, so some tuning is clearly going to be needed.", "classification_task": "Classify the issue priority and type: Upgrade to a recent version of 2.33.x or later while off the critical path of things. HADOOP-19485 froze the sdk at a version which worked with third party stores. Apparently the new version works; early tests show that Bulk Delete calls with third party stores complain about lack of md5 headers, so some tuning is clearly going to be needed.", "qna_task": "Question: What is this issue about?\nAnswer: Upgrade AWS SDK to 2.35.4"}}
{"id": "13626577", "key": "HADOOP-19653", "project": "HADOOP", "summary": "[JDK25] Bump ByteBuddy 1.17.6 and ASM 9.8 to support Java 25 bytecode", "description": "", "comments": "pan3793 opened a new pull request, #7879: URL: https://github.com/apache/hadoop/pull/7879 \u2026tecode <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR ``` ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? slfan1989 commented on code in PR #7879: URL: https://github.com/apache/hadoop/pull/7879#discussion_r2281285636 ########## hadoop-project/pom.xml: ########## @@ -146,7 +146,7 @@ <netty4.version>4.1.118.Final</netty4.version> <snappy-java.version>1.1.10.4</snappy-java.version> <lz4-java.version>1.7.1</lz4-java.version> - <byte-buddy.version>1.15.11</byte-buddy.version> + <byte-buddy.version>1.17.6</byte-buddy.version> Review Comment: Should the LICENSE-binary be updated? pan3793 commented on code in PR #7879: URL: https://github.com/apache/hadoop/pull/7879#discussion_r2281290385 ########## hadoop-project/pom.xml: ########## @@ -146,7 +146,7 @@ <netty4.version>4.1.118.Final</netty4.version> <snappy-java.version>1.1.10.4</snappy-java.version> <lz4-java.version>1.7.1</lz4-java.version> - <byte-buddy.version>1.15.11</byte-buddy.version> + <byte-buddy.version>1.17.6</byte-buddy.version> Review Comment: they are test-only deps, not present in LICENSE/NOTICE files slfan1989 commented on code in PR #7879: URL: https://github.com/apache/hadoop/pull/7879#discussion_r2281293747 ########## hadoop-project/pom.xml: ########## @@ -146,7 +146,7 @@ <netty4.version>4.1.118.Final</netty4.version> <snappy-java.version>1.1.10.4</snappy-java.version> <lz4-java.version>1.7.1</lz4-java.version> - <byte-buddy.version>1.15.11</byte-buddy.version> + <byte-buddy.version>1.17.6</byte-buddy.version> Review Comment: LGTM. hadoop-yetus commented on PR #7879: URL: https://github.com/apache/hadoop/pull/7879#issuecomment-3195141474 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 34s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 16s | | trunk passed | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 0m 28s | | trunk passed | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 76m 14s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 15s | | the patch passed | | +1 :green_heart: | compile | 0m 15s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 15s | | the patch passed | | +1 :green_heart: | compile | 0m 14s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 14s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 17s | | the patch passed | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 36m 45s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 19s | | hadoop-project in the patch passed. | | +1 :green_heart: | asflicense | 0m 39s | | The patch does not generate ASF License warnings. | | | | 117m 4s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7879/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7879 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux 395fb749b4d0 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / fee75ef1efffd7f6da1fa67bdefd54b076f51df3 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7879/1/testReport/ | | Max. process+thread count | 549 (vs. ulimit of 5500) | | modules | C: hadoop-project U: hadoop-project | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7879/1/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. stoty commented on PR #7879: URL: https://github.com/apache/hadoop/pull/7879#issuecomment-3196190107 Have you tried building with Java 25 ? Last time I updated ByteBuddy, I run into maven-shade-plugin incompatibilities. pan3793 commented on PR #7879: URL: https://github.com/apache/hadoop/pull/7879#issuecomment-3196229409 @stoty this PR indeed fixes the Java 25 building stoty commented on PR #7879: URL: https://github.com/apache/hadoop/pull/7879#issuecomment-3196566191 Thanks. I can also successfully build with the patch. The shade plugin problem doesn't surface because we're actually compiling JDK17 bytecode. pan3793 commented on PR #7879: URL: https://github.com/apache/hadoop/pull/7879#issuecomment-3196591873 > The shade plugin problem doesn't surface because we're actually compiling JDK17 bytecode. @stoty This is actually addressed by upgrading ASM deps used by `maven-shade-plugin`, without this change, you will see error ``` [ERROR] Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.6.0:shade (default) on project hadoop-client-minicluster: Error creating shaded jar: Problem shading JAR /home/chengpan/.m2/repository/net/bytebuddy/byte-buddy/1.17.6/byte-buddy-1.17.6.jar entry META-INF/versions/24/net/bytebuddy/jar/asmjdkbridge/JdkClassWriter$1.class: java.lang.IllegalArgumentException: Unsupported class file major version 68 -> [Help 1] ``` stoty commented on PR #7879: URL: https://github.com/apache/hadoop/pull/7879#issuecomment-3196844666 I've also tried to set the target version to 25, but some other plugin failed, the build didn't even get as far the shade plugin. But again, that is not a major issue, as we're building for Java 17 compatibility. slfan1989 merged PR #7879: URL: https://github.com/apache/hadoop/pull/7879 slfan1989 commented on PR #7879: URL: https://github.com/apache/hadoop/pull/7879#issuecomment-3198832279 @pan3793 Thanks for the contribution! @stoty Thanks for the review!", "created": "2025-08-18T03:11:13.000+0000", "updated": "2025-08-19T00:44:29.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: [JDK25] Bump ByteBuddy 1.17.6 and ASM 9.8 to support Java 25 bytecode"}}
{"id": "13626576", "key": "HADOOP-19652", "project": "HADOOP", "summary": "Fix dependency exclusion list of hadoop-client-runtime.", "description": "", "comments": "pan3793 opened a new pull request, #7878: URL: https://github.com/apache/hadoop/pull/7878 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR When trying to use the Hadoop trunk version client with Spark 4.0.0, `NoClassDefFoundError` was raised. ``` Exception in thread \"main\" java.lang.NoClassDefFoundError: org/apache/hadoop/shaded/javax/ws/rs/WebApplicationException at java.base/java.lang.ClassLoader.defineClass1(Native Method) at java.base/java.lang.ClassLoader.defineClass(ClassLoader.java:962) at java.base/java.security.SecureClassLoader.defineClass(SecureClassLoader.java:144) at java.base/jdk.internal.loader.BuiltinClassLoader.defineClass(BuiltinClassLoader.java:776) at java.base/jdk.internal.loader.BuiltinClassLoader.findClassOnClassPathOrNull(BuiltinClassLoader.java:691) at java.base/jdk.internal.loader.BuiltinClassLoader.loadClassOrNull(BuiltinClassLoader.java:620) at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:578) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:490) at org.apache.spark.deploy.yarn.YarnRMClient.getAmIpFilterParams(YarnRMClient.scala:109) at org.apache.spark.deploy.yarn.ApplicationMaster.addAmIpFilter(ApplicationMaster.scala:698) at org.apache.spark.deploy.yarn.ApplicationMaster.runExecutorLauncher(ApplicationMaster.scala:555) at org.apache.spark.deploy.yarn.ApplicationMaster.run(ApplicationMaster.scala:265) at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:942) at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:941) at java.base/jdk.internal.vm.ScopedValueContainer.callWithoutScope(ScopedValueContainer.java:162) at java.base/jdk.internal.vm.ScopedValueContainer.call(ScopedValueContainer.java:147) at java.base/java.lang.ScopedValue$Carrier.call(ScopedValue.java:419) at java.base/javax.security.auth.Subject.callAs(Subject.java:331) at org.apache.hadoop.util.SubjectUtil.callAs(SubjectUtil.java:134) at org.apache.hadoop.util.SubjectUtil.doAs(SubjectUtil.java:166) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:2039) at org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:941) at org.apache.spark.deploy.yarn.ExecutorLauncher$.main(ApplicationMaster.scala:973) at org.apache.spark.deploy.yarn.ExecutorLauncher.main(ApplicationMaster.scala) Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.shaded.javax.ws.rs.WebApplicationException at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:580) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:490) ... 24 more ``` ### How was this patch tested? Hadoop Client Runtime 3.4.2 RC2 <img width=\"407\" height=\"459\" alt=\"image\" src=\"https://github.com/user-attachments/assets/9ba6eb0c-2d52-4034-8f97-01c73195d795\" /> Hadoop Client Runtime 3.5.0-SNAPSHOT trunk <img width=\"516\" height=\"432\" alt=\"image\" src=\"https://github.com/user-attachments/assets/81115ab8-3f86-4336-9d95-fe61093b09d1\" /> Hadoop Client Runtime 3.5.0-SNAPSHOT HADOOP-19652 <img width=\"509\" height=\"433\" alt=\"image\" src=\"https://github.com/user-attachments/assets/eb08a2d8-6b52-45f4-b56e-4ea3a92e950a\" /> Tested by submitting a Spark application to a YARN cluster. ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? pan3793 commented on PR #7878: URL: https://github.com/apache/hadoop/pull/7878#issuecomment-3194939043 cc @slfan1989 @cnauroth, this is caused by Jersey upgrading. pan3793 commented on PR #7878: URL: https://github.com/apache/hadoop/pull/7878#issuecomment-3194946944 AFAIK, there is no solid integration test for the Hadoop Shaded client, I will continue to test it with Spark and fix issues I encounter. hadoop-yetus commented on PR #7878: URL: https://github.com/apache/hadoop/pull/7878#issuecomment-3195086656 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 33s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 10s | | trunk passed | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 0m 29s | | trunk passed | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 76m 30s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 5m 50s | | the patch passed | | +1 :green_heart: | compile | 0m 16s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 16s | | the patch passed | | +1 :green_heart: | compile | 0m 16s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 16s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 17s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | shadedclient | 35m 43s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 20s | | hadoop-client-runtime in the patch passed. | | +1 :green_heart: | asflicense | 0m 39s | | The patch does not generate ASF License warnings. | | | | 121m 57s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7878/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7878 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux 84d5837e24b5 5.15.0-140-generic #150-Ubuntu SMP Sat Apr 12 06:00:09 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 16187caf9fa1ca13f0b60d9a5e2dabd40c2aba12 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7878/1/testReport/ | | Max. process+thread count | 648 (vs. ulimit of 5500) | | modules | C: hadoop-client-modules/hadoop-client-runtime U: hadoop-client-modules/hadoop-client-runtime | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7878/1/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on code in PR #7878: URL: https://github.com/apache/hadoop/pull/7878#discussion_r2281293084 ########## hadoop-client-modules/hadoop-client-runtime/pom.xml: ########## @@ -176,14 +176,11 @@ <exclude>org.glassfish.jersey.core:*</exclude> <exclude>org.glassfish.hk2.external:*</exclude> <exclude>org.glassfish.jaxb:*</exclude> - <exclude>jakarta.ws.rs:*</exclude> <exclude>jakarta.annotation:*</exclude> <exclude>jakarta.validation:*</exclude> - <exclude>jakarta.servlet:*</exclude> - <exclude>javax.annotation:*</exclude> <exclude>org.hamcrest:*</exclude> + <exclude>org.javassist:*</exclude> Review Comment: Can this line remain unchanged? pan3793 commented on code in PR #7878: URL: https://github.com/apache/hadoop/pull/7878#discussion_r2281304950 ########## hadoop-client-modules/hadoop-client-runtime/pom.xml: ########## @@ -176,14 +176,11 @@ <exclude>org.glassfish.jersey.core:*</exclude> <exclude>org.glassfish.hk2.external:*</exclude> <exclude>org.glassfish.jaxb:*</exclude> - <exclude>jakarta.ws.rs:*</exclude> <exclude>jakarta.annotation:*</exclude> <exclude>jakarta.validation:*</exclude> - <exclude>jakarta.servlet:*</exclude> - <exclude>javax.annotation:*</exclude> <exclude>org.hamcrest:*</exclude> + <exclude>org.javassist:*</exclude> Review Comment: this is the corrected version of ``` <exclude>javassist:*</exclude> ``` pan3793 commented on PR #7878: URL: https://github.com/apache/hadoop/pull/7878#issuecomment-3195164574 Not a real error, just caused by token expiration, push a new empty commit to retrigger CI. ``` 12:47:42 ============================================================================ 12:47:42 ============================================================================ 12:47:42 Adding comment to Github 12:47:42 ============================================================================ 12:47:42 ============================================================================ 12:47:42 12:47:42 12:47:46 ERROR: Failed to write github status. Token expired or missing repo:status write? 12:47:46 ERROR: Failed to write github status. Token expired or missing repo:status write? ``` https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7878/1/console hadoop-yetus commented on PR #7878: URL: https://github.com/apache/hadoop/pull/7878#issuecomment-3195503178 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 48s | | trunk passed | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 0m 29s | | trunk passed | | +1 :green_heart: | javadoc | 0m 31s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 76m 3s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 5m 47s | | the patch passed | | +1 :green_heart: | compile | 0m 14s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 14s | | the patch passed | | +1 :green_heart: | compile | 0m 15s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 15s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 17s | | the patch passed | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | shadedclient | 35m 29s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 20s | | hadoop-client-runtime in the patch passed. | | +1 :green_heart: | asflicense | 0m 49s | | The patch does not generate ASF License warnings. | | | | 121m 21s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7878/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7878 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux f59cfeb748b8 5.15.0-140-generic #150-Ubuntu SMP Sat Apr 12 06:00:09 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 830cdae5363f8db2dede0b6097accfce6a51f7b9 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7878/2/testReport/ | | Max. process+thread count | 560 (vs. ulimit of 5500) | | modules | C: hadoop-client-modules/hadoop-client-runtime U: hadoop-client-modules/hadoop-client-runtime | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7878/2/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on PR #7878: URL: https://github.com/apache/hadoop/pull/7878#issuecomment-3196039890 @pan3793 https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7878/2/artifact/out/patch-shadedclient.txt ``` ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:3.5.0:enforce (enforce-banned-dependencies) on project hadoop-client-check-test-invariants: [ERROR] Rule 1: org.apache.maven.plugins.enforcer.BanDuplicateClasses failed with message: [ERROR] Duplicate classes found: [ERROR] [ERROR] Found in: [ERROR] org.apache.hadoop:hadoop-client-minicluster:jar:3.5.0-SNAPSHOT:compile [ERROR] org.apache.hadoop:hadoop-client-runtime:jar:3.5.0-SNAPSHOT:compile ...... [ERROR] After correcting the problems, you can resume the build with the command [ERROR] mvn <args> -rf :hadoop-client-check-test-invariants ``` pan3793 commented on PR #7878: URL: https://github.com/apache/hadoop/pull/7878#issuecomment-3196049320 @slfan1989 I see, will fix soon hadoop-yetus commented on PR #7878: URL: https://github.com/apache/hadoop/pull/7878#issuecomment-3197368091 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 31s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 54s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 32m 40s | | trunk passed | | +1 :green_heart: | compile | 0m 27s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 27s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 0m 56s | | trunk passed | | +1 :green_heart: | javadoc | 0m 53s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 50s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 79m 43s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 34s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 22m 9s | | the patch passed | | +1 :green_heart: | compile | 0m 17s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 17s | | the patch passed | | +1 :green_heart: | compile | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 17s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 35s | | the patch passed | | +1 :green_heart: | javadoc | 0m 32s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 32s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 36m 37s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 20s | | hadoop-client-runtime in the patch passed. | | +1 :green_heart: | unit | 0m 21s | | hadoop-client-minicluster in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 143m 23s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7878/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7878 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux 5bff763e4c87 5.15.0-140-generic #150-Ubuntu SMP Sat Apr 12 06:00:09 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0e4ad11e173fee8758c0267f4ee503448bfcdc97 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7878/3/testReport/ | | Max. process+thread count | 546 (vs. ulimit of 5500) | | modules | C: hadoop-client-modules/hadoop-client-runtime hadoop-client-modules/hadoop-client-minicluster U: hadoop-client-modules | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7878/3/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on PR #7878: URL: https://github.com/apache/hadoop/pull/7878#issuecomment-3198828530 > @slfan1989 I see, will fix soon @pan3793 Thank you for your contribution! I don\u2019t see any other issues. Let\u2019s wait for one more day, and if there are no further comments, I\u2019ll merge this PR. slfan1989 merged PR #7878: URL: https://github.com/apache/hadoop/pull/7878 slfan1989 commented on PR #7878: URL: https://github.com/apache/hadoop/pull/7878#issuecomment-3203410020 @pan3793 Thanks for the contribution! Merged into trunk.", "created": "2025-08-18T02:31:16.000+0000", "updated": "2025-09-29T11:05:01.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Fix dependency exclusion list of hadoop-client-runtime."}}
{"id": "13626490", "key": "HADOOP-19651", "project": "HADOOP", "summary": "Upgrade libopenssl to 3.5.2-1 needed for rsync", "description": "The currently used libopenssl-3.1.4-1 is no longer available on the msys repo - https://repo.msys2.org/msys/x86_64/libopenssl-3.1.4-1-x86_64.pkg.tar.zst The Jenkins CI thus fails to download the same and thus it fails to build the docker image for Windows. {code} 00:25:33 SUCCESS: Specified value was saved. 00:25:46 Removing intermediate container 5ce7355571a1 00:25:46 ---> a13a4bc69545 00:25:46 Step 21/78 : RUN powershell Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl-3.1.4-1-x86_64.pkg.tar.zst -OutFile $Env:TEMP\\libopenssl-3.1.4-1-x86_64.pkg.tar.zst 00:25:46 ---> Running in d2dafad446f9 00:25:54 \u001b[91mInvoke-WebRequest : The remote server returned an error: (404) Not Found. 00:25:54 \u001b[0m\u001b[91mAt line:1 char:1 00:25:54 \u001b[0m\u001b[91m+ Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl- ... 00:25:54 + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 00:25:54 \u001b[0m\u001b[91m + CategoryInfo : InvalidOperation: (System.Net.HttpWebRequest:Htt {code} Thus, we need to upgrade to the latest version to address this.", "comments": "GauthamBanasandra opened a new pull request, #7875: URL: https://github.com/apache/hadoop/pull/7875 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR The currently used `libopenssl-3.1.4-1` is no longer available on the msys repo - https://repo.msys2.org/msys/x86_64/libopenssl-3.1.4-1-x86_64.pkg.tar.zst The Jenkins CI thus fails to download the same and thus it fails to build the docker image for Windows. ``` 00:25:33 SUCCESS: Specified value was saved. 00:25:46 Removing intermediate container 5ce7355571a1 00:25:46 ---> a13a4bc69545 00:25:46 Step 21/78 : RUN powershell Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl-3.1.4-1-x86_64.pkg.tar.zst -OutFile $Env:TEMP\\libopenssl-3.1.4-1-x86_64.pkg.tar.zst 00:25:46 ---> Running in d2dafad446f9 00:25:54 \u001b[91mInvoke-WebRequest : The remote server returned an error: (404) Not Found. 00:25:54 \u001b[0m\u001b[91mAt line:1 char:1 00:25:54 \u001b[0m\u001b[91m+ Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl- ... 00:25:54 + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 00:25:54 \u001b[0m\u001b[91m + CategoryInfo : InvalidOperation: (System.Net.HttpWebRequest:Htt ``` Thus, we need to upgrade to the latest available version to address this. ### How was this patch tested? Jenkins CI - In progress. ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #7875: URL: https://github.com/apache/hadoop/pull/7875#issuecomment-3192521324 (!) A patch to the testing environment has been detected. Re-executing against the patched versions to perform further tests. The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7875/1/console in case of problems. hadoop-yetus commented on PR #7875: URL: https://github.com/apache/hadoop/pull/7875#issuecomment-3192810626 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 25m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 10m 21s | | Maven dependency ordering for branch | | +1 :green_heart: | shadedclient | 54m 29s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 32s | | Maven dependency ordering for patch | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 40m 17s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 40s | | The patch does not generate ASF License warnings. | | | | 123m 20s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7875/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7875 | | Optional Tests | dupname asflicense | | uname | Linux 7a9ce5519195 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 988bb50936dcb2b9808ed2ffaa95851a4c228fd6 | | Max. process+thread count | 556 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7875/1/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. GauthamBanasandra commented on PR #7875: URL: https://github.com/apache/hadoop/pull/7875#issuecomment-3193865491 Build is going through now - https://ci-hadoop.apache.org/view/Hadoop/job/hadoop-qbt-trunk-java8-win10-x86_64/1030/console. GauthamBanasandra merged PR #7875: URL: https://github.com/apache/hadoop/pull/7875 Merged PR to trunk - https://github.com/apache/hadoop/pull/7875.", "created": "2025-08-15T19:19:04.000+0000", "updated": "2025-08-16T19:37:06.000+0000", "derived": {"summary_task": "Summarize this issue: The currently used libopenssl-3.1.4-1 is no longer available on the msys repo - https://repo.msys2.org/msys/x86_64/libopenssl-3.1.4-1-x86_64.pkg.tar.zst The Jenkins CI thus fails to download the same and thus it fails to build the docker image for Windows. {code} 00:25:33 SUCCESS: Specified value was saved. 00:25:46 Removing intermediate container 5ce7355571a1 00:25:46 ---> a13a4bc69545 00:25:46 S", "classification_task": "Classify the issue priority and type: The currently used libopenssl-3.1.4-1 is no longer available on the msys repo - https://repo.msys2.org/msys/x86_64/libopenssl-3.1.4-1-x86_64.pkg.tar.zst The Jenkins CI thus fails to download the same and thus it fails to build the docker image for Windows. {code} 00:25:33 SUCCESS: Specified value was saved. 00:25:46 Removing intermediate container 5ce7355571a1 00:25:46 ---> a13a4bc69545 00:25:46 S", "qna_task": "Question: What is this issue about?\nAnswer: Upgrade libopenssl to 3.5.2-1 needed for rsync"}}
{"id": "13626478", "key": "HADOOP-19650", "project": "HADOOP", "summary": "ABFS: NPE when close() called on uninitialized filesystem", "description": "code {code} public void testABFSConstructor() throws Throwable { new AzureBlobFileSystem().close(); } {code} stack {code} [ERROR] org.apache.hadoop.validator.TestRuntimeValid.testABFSConstructor -- Time elapsed: 0.003 s <<< ERROR! java.lang.NullPointerException: Cannot invoke \"org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getClient()\" because \"this.abfsStore\" is null at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.close(AzureBlobFileSystem.java:800) at org.apache.hadoop.validator.TestRuntimeValid.testABFSConstructor(TestRuntimeValid.java:49) {code}", "comments": "Thanks for reporting this. Will work on the patch on priority. hadoop-yetus commented on PR #7880: URL: https://github.com/apache/hadoop/pull/7880#issuecomment-3197386039 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 51s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 44m 9s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 42s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 11s | | trunk passed | | +1 :green_heart: | shadedclient | 41m 9s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 35s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 21s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 29s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 27s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 40m 42s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 11s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 141m 38s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7880 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 160d07de834c 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / fd85bee514d271d663f801bf3498b3145ec08fec | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/2/testReport/ | | Max. process+thread count | 530 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. steveloughran commented on code in PR #7880: URL: https://github.com/apache/hadoop/pull/7880#discussion_r2282920640 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java: ########## @@ -148,7 +150,7 @@ public class AzureBlobFileSystem extends FileSystem private URI uri; private Path workingDir; private AzureBlobFileSystemStore abfsStore; - private boolean isClosed; + private boolean isClosed = true; Review Comment: so this really means inited and closed. Mention that in javadocs. hadoop-yetus commented on PR #7880: URL: https://github.com/apache/hadoop/pull/7880#issuecomment-3197699593 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 15m 15s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 14s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | +1 :green_heart: | javadoc | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 36s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 13s | | trunk passed | | +1 :green_heart: | shadedclient | 35m 33s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 35s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/3/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 2 new + 1 unchanged - 0 fixed = 3 total (was 1) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 29s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 10s | | the patch passed | | +1 :green_heart: | shadedclient | 35m 42s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 0s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 140m 32s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7880 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 741539520d55 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 18fa0f27d00c849b858b19383a3c213a71a9aacb | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/3/testReport/ | | Max. process+thread count | 564 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/3/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7880: URL: https://github.com/apache/hadoop/pull/7880#issuecomment-3201478753 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 50s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 45m 31s | | trunk passed | | +1 :green_heart: | compile | 0m 44s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 42s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 11s | | trunk passed | | +1 :green_heart: | shadedclient | 41m 34s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/4/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 2 new + 1 unchanged - 0 fixed = 3 total (was 1) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 28s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 41m 45s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 10s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 144m 38s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7880 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 4b13a9c47991 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 62141e105067253aaf4a3fc516b5c06f84f8da9f | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/4/testReport/ | | Max. process+thread count | 558 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/4/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. anujmodi2021 commented on PR #7880: URL: https://github.com/apache/hadoop/pull/7880#issuecomment-3204164409 ------------------------------ :::: AGGREGATED TEST RESULT :::: ============================================================ HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 862, Failures: 0, Errors: 0, Skipped: 209 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ HNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 865, Failures: 0, Errors: 0, Skipped: 161 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 10 ============================================================ NonHNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 704, Failures: 0, Errors: 0, Skipped: 240 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11 ============================================================ AppendBlob-HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 862, Failures: 0, Errors: 0, Skipped: 220 [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-SharedKey-Blob ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 709, Failures: 0, Errors: 0, Skipped: 135 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 5 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11 ============================================================ NonHNS-OAuth-DFS ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 701, Failures: 0, Errors: 0, Skipped: 242 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 706, Failures: 0, Errors: 0, Skipped: 147 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 5 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ AppendBlob-NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 703, Failures: 0, Errors: 0, Skipped: 189 [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 6 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ HNS-Oauth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 736, Failures: 0, Errors: 0, Skipped: 216 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-OAuth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 701, Failures: 0, Errors: 0, Skipped: 239 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 Time taken: 199 mins 4 secs. anujmodi2021 commented on code in PR #7880: URL: https://github.com/apache/hadoop/pull/7880#discussion_r2286986125 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java: ########## @@ -148,7 +150,7 @@ public class AzureBlobFileSystem extends FileSystem private URI uri; private Path workingDir; private AzureBlobFileSystemStore abfsStore; - private boolean isClosed; + private boolean isClosed = true; Review Comment: Yes, added the javadcoc. bhattmanish98 commented on code in PR #7880: URL: https://github.com/apache/hadoop/pull/7880#discussion_r2287000680 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java: ########## @@ -795,10 +801,10 @@ public boolean mkdirs(final Path f, final FsPermission permission) throws IOExce @Override public synchronized void close() throws IOException { - if (isClosed) { + if (isClosed()) { Review Comment: Should we throw an exception in case someone is trying to close non initialized or already closed file system? bhattmanish98 commented on code in PR #7880: URL: https://github.com/apache/hadoop/pull/7880#discussion_r2287009604 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestFileSystemInitialization.java: ########## @@ -81,21 +89,123 @@ public void testFileSystemCapabilities() throws Throwable { final Path p = new Path(\"}\"); // etags always present - Assertions.assertThat(fs.hasPathCapability(p, ETAGS_AVAILABLE)) + assertThat(fs.hasPathCapability(p, ETAGS_AVAILABLE)) .describedAs(\"path capability %s in %s\", ETAGS_AVAILABLE, fs) .isTrue(); // readahead always correct - Assertions.assertThat(fs.hasPathCapability(p, CAPABILITY_SAFE_READAHEAD)) + assertThat(fs.hasPathCapability(p, CAPABILITY_SAFE_READAHEAD)) .describedAs(\"path capability %s in %s\", CAPABILITY_SAFE_READAHEAD, fs) .isTrue(); // etags-over-rename and ACLs are either both true or both false. final boolean etagsAcrossRename = fs.hasPathCapability(p, ETAGS_PRESERVED_IN_RENAME); final boolean acls = fs.hasPathCapability(p, FS_ACLS); - Assertions.assertThat(etagsAcrossRename) + assertThat(etagsAcrossRename) .describedAs(\"capabilities %s=%s and %s=%s in %s\", ETAGS_PRESERVED_IN_RENAME, etagsAcrossRename, FS_ACLS, acls, fs) .isEqualTo(acls); } + + /** + * Test that the AzureBlobFileSystem close without init works + * @throws Exception if an error occurs + */ + @Test + public void testABFSCloseWithoutInit() throws Exception { + AzureBlobFileSystem fs = new AzureBlobFileSystem(); + assertThat(fs.isClosed()).isTrue(); + fs.close(); + fs.initialize(this.getFileSystem().getUri(), getRawConfiguration()); + assertThat(fs.isClosed()).isFalse(); + fs.close(); + assertThat(fs.isClosed()).isTrue(); + } + + /** + * Test that the AzureBlobFileSystem throws an exception + * when trying to perform an operation without initialization. + * @throws Exception if an error occurs + */ + @Test + public void testABFSUninitializedFileSystem() throws Exception { + AzureBlobFileSystem fs = new AzureBlobFileSystem(); + assertThat(fs.isClosed()).isTrue(); + Path testPath = new Path(\"testPath\"); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + fs::toString); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + () -> fs.open(testPath, ONE_MB)); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + () -> fs.create(testPath, FsPermission.getDefault(), false, ONE_MB, + fs.getDefaultReplication(testPath), ONE_MB, null)); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + () -> fs.createNonRecursive(testPath, FsPermission.getDefault(), false, ONE_MB, + fs.getDefaultReplication(testPath), ONE_MB, null)); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + () -> fs.append(testPath, ONE_MB, null)); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + () -> fs.rename(testPath, testPath)); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + () -> fs.delete(testPath, true)); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + () -> fs.listStatus(testPath)); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + () -> fs.mkdirs(testPath, FsPermission.getDefault())); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + () -> fs.getFileStatus(testPath)); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + () -> fs.breakLease(testPath)); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + () -> fs.makeQualified(testPath)); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + () -> fs.setOwner(testPath, \"\", \"\")); Review Comment: EMPTY_STRING can be used here hadoop-yetus commented on PR #7880: URL: https://github.com/apache/hadoop/pull/7880#issuecomment-3204282967 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 41s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 30m 22s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | -1 :x: | compile | 0m 31s | [/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 30s | [/branch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/branch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -0 :warning: | checkstyle | 0m 29s | [/buildtool-branch-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/buildtool-branch-checkstyle-hadoop-tools_hadoop-azure.txt) | The patch fails to run checkstyle in hadoop-azure | | -1 :x: | mvnsite | 0m 30s | [/branch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/branch-mvnsite-hadoop-tools_hadoop-azure.txt) | hadoop-azure in trunk failed. | | -1 :x: | javadoc | 0m 30s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 30s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | spotbugs | 0m 30s | [/branch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure.txt) | hadoop-azure in trunk failed. | | +1 :green_heart: | shadedclient | 3m 51s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 24s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | compile | 0m 23s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 23s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 24s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 0m 24s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 21s | [/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt) | The patch fails to run checkstyle in hadoop-azure | | -1 :x: | mvnsite | 0m 42s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | javadoc | 0m 23s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 24s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | spotbugs | 0m 24s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | +1 :green_heart: | shadedclient | 5m 1s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 24s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | +0 :ok: | asflicense | 0m 24s | | ASF License check generated no output? | | | | 45m 10s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7880 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 4535df39a5aa 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 579d0674d77eb78689c28a0df8aa4b49aac222c8 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/testReport/ | | Max. process+thread count | 65 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/6/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7880: URL: https://github.com/apache/hadoop/pull/7880#issuecomment-3204495167 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 49s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 45m 13s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 37s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 42s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 9s | | trunk passed | | +1 :green_heart: | shadedclient | 42m 54s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 21s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | +1 :green_heart: | javadoc | 0m 28s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 40m 48s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 9s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 144m 24s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7880 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 84fe5e904fca 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 77e3546d9fc5ff4a180abe2300956dbbd003e079 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/5/testReport/ | | Max. process+thread count | 576 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/5/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. anujmodi2021 commented on code in PR #7880: URL: https://github.com/apache/hadoop/pull/7880#discussion_r2287442313 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java: ########## @@ -795,10 +801,10 @@ public boolean mkdirs(final Path f, final FsPermission permission) throws IOExce @Override public synchronized void close() throws IOException { - if (isClosed) { + if (isClosed()) { Review Comment: fs.close() is supposed to be idempotent IMO anujmodi2021 commented on code in PR #7880: URL: https://github.com/apache/hadoop/pull/7880#discussion_r2287443157 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestFileSystemInitialization.java: ########## @@ -81,21 +89,123 @@ public void testFileSystemCapabilities() throws Throwable { final Path p = new Path(\"}\"); // etags always present - Assertions.assertThat(fs.hasPathCapability(p, ETAGS_AVAILABLE)) + assertThat(fs.hasPathCapability(p, ETAGS_AVAILABLE)) .describedAs(\"path capability %s in %s\", ETAGS_AVAILABLE, fs) .isTrue(); // readahead always correct - Assertions.assertThat(fs.hasPathCapability(p, CAPABILITY_SAFE_READAHEAD)) + assertThat(fs.hasPathCapability(p, CAPABILITY_SAFE_READAHEAD)) .describedAs(\"path capability %s in %s\", CAPABILITY_SAFE_READAHEAD, fs) .isTrue(); // etags-over-rename and ACLs are either both true or both false. final boolean etagsAcrossRename = fs.hasPathCapability(p, ETAGS_PRESERVED_IN_RENAME); final boolean acls = fs.hasPathCapability(p, FS_ACLS); - Assertions.assertThat(etagsAcrossRename) + assertThat(etagsAcrossRename) .describedAs(\"capabilities %s=%s and %s=%s in %s\", ETAGS_PRESERVED_IN_RENAME, etagsAcrossRename, FS_ACLS, acls, fs) .isEqualTo(acls); } + + /** + * Test that the AzureBlobFileSystem close without init works + * @throws Exception if an error occurs + */ + @Test + public void testABFSCloseWithoutInit() throws Exception { + AzureBlobFileSystem fs = new AzureBlobFileSystem(); + assertThat(fs.isClosed()).isTrue(); + fs.close(); + fs.initialize(this.getFileSystem().getUri(), getRawConfiguration()); + assertThat(fs.isClosed()).isFalse(); + fs.close(); + assertThat(fs.isClosed()).isTrue(); + } + + /** + * Test that the AzureBlobFileSystem throws an exception + * when trying to perform an operation without initialization. + * @throws Exception if an error occurs + */ + @Test + public void testABFSUninitializedFileSystem() throws Exception { + AzureBlobFileSystem fs = new AzureBlobFileSystem(); + assertThat(fs.isClosed()).isTrue(); + Path testPath = new Path(\"testPath\"); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + fs::toString); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + () -> fs.open(testPath, ONE_MB)); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + () -> fs.create(testPath, FsPermission.getDefault(), false, ONE_MB, + fs.getDefaultReplication(testPath), ONE_MB, null)); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + () -> fs.createNonRecursive(testPath, FsPermission.getDefault(), false, ONE_MB, + fs.getDefaultReplication(testPath), ONE_MB, null)); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + () -> fs.append(testPath, ONE_MB, null)); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + () -> fs.rename(testPath, testPath)); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + () -> fs.delete(testPath, true)); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + () -> fs.listStatus(testPath)); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + () -> fs.mkdirs(testPath, FsPermission.getDefault())); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + () -> fs.getFileStatus(testPath)); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + () -> fs.breakLease(testPath)); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + () -> fs.makeQualified(testPath)); + + intercept(IllegalStateException.class, ERR_INVALID_ABFS_STATE, + () -> fs.setOwner(testPath, \"\", \"\")); Review Comment: Will take up this. bhattmanish98 commented on code in PR #7880: URL: https://github.com/apache/hadoop/pull/7880#discussion_r2287814760 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java: ########## @@ -795,10 +801,10 @@ public boolean mkdirs(final Path f, final FsPermission permission) throws IOExce @Override public synchronized void close() throws IOException { - if (isClosed) { + if (isClosed()) { Review Comment: There is chance that more two threads can attempt to close store and client twice. bhattmanish98 commented on code in PR #7880: URL: https://github.com/apache/hadoop/pull/7880#discussion_r2287814760 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java: ########## @@ -795,10 +801,10 @@ public boolean mkdirs(final Path f, final FsPermission permission) throws IOExce @Override public synchronized void close() throws IOException { - if (isClosed) { + if (isClosed()) { Review Comment: There is chance that more two threads can attempt to close store and client which can cause similar issue what we got now. steveloughran commented on code in PR #7880: URL: https://github.com/apache/hadoop/pull/7880#discussion_r2288107530 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java: ########## @@ -795,10 +801,10 @@ public boolean mkdirs(final Path f, final FsPermission permission) throws IOExce @Override public synchronized void close() throws IOException { - if (isClosed) { + if (isClosed()) { Review Comment: @bhattmanish98 I don't see concurrency issues because close() is synchronized, and once closed it can't be closed again. Of course, once one thread has closed it, nobody else can use the instance. fix: don't do that. anujmodi2021 commented on PR #7880: URL: https://github.com/apache/hadoop/pull/7880#issuecomment-3206294039 > +1 > > ready to merge. Sure, just did a commit to rerun yetus, will merge as soon as green. bhattmanish98 commented on code in PR #7880: URL: https://github.com/apache/hadoop/pull/7880#discussion_r2288137399 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java: ########## @@ -795,10 +801,10 @@ public boolean mkdirs(final Path f, final FsPermission permission) throws IOExce @Override public synchronized void close() throws IOException { - if (isClosed) { + if (isClosed()) { Review Comment: @steveloughran Make sense, I overlooked synchronized in the method definition. PR Looks good. Approved the changes. hadoop-yetus commented on PR #7880: URL: https://github.com/apache/hadoop/pull/7880#issuecomment-3206925500 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 50s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 45m 52s | | trunk passed | | +1 :green_heart: | compile | 0m 44s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 43s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 12s | | trunk passed | | +1 :green_heart: | shadedclient | 42m 39s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 35s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 21s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 29s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 41m 26s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 10s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 145m 34s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/7/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7880 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux a9e8d7a37f31 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 5a51d6a271f282878677fdab891549d77dac2e6d | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/7/testReport/ | | Max. process+thread count | 536 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/7/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. anujmodi2021 merged PR #7880: URL: https://github.com/apache/hadoop/pull/7880 anujmodi2021 opened a new pull request, #7888: URL: https://github.com/apache/hadoop/pull/7888 ### Description of PR PR on trunk: https://github.com/apache/hadoop/pull/7880 JIRA: https://issues.apache.org/jira/browse/HADOOP-19650 anujmodi2021 opened a new pull request, #7889: URL: https://github.com/apache/hadoop/pull/7889 ### Description of PR PR on trunk: https://github.com/apache/hadoop/pull/7880 JIRA: https://issues.apache.org/jira/browse/HADOOP-19650 anujmodi2021 commented on PR #7880: URL: https://github.com/apache/hadoop/pull/7880#issuecomment-3207359625 Will do backport for 3.4 and 3.4.2 hadoop-yetus commented on PR #7889: URL: https://github.com/apache/hadoop/pull/7889#issuecomment-3207450187 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 6m 54s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ branch-3.4 Compile Tests _ | | +1 :green_heart: | mvninstall | 26m 26s | | branch-3.4 passed | | +1 :green_heart: | compile | 0m 22s | | branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 20s | | branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 18s | | branch-3.4 passed | | +1 :green_heart: | mvnsite | 0m 24s | | branch-3.4 passed | | +1 :green_heart: | javadoc | 0m 23s | | branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 20s | | branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 41s | | branch-3.4 passed | | +1 :green_heart: | shadedclient | 19m 39s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 21s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 15s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 15s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 12s | | hadoop-tools/hadoop-azure: The patch generated 0 new + 1 unchanged - 1 fixed = 1 total (was 2) | | +1 :green_heart: | mvnsite | 0m 19s | | the patch passed | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 45s | | the patch passed | | +1 :green_heart: | shadedclient | 19m 33s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 3s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 25s | | The patch does not generate ASF License warnings. | | | | 81m 35s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7889/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7889 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 6822d049569c 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | branch-3.4 / cc5116580b5f7b89333cf0c606432b50e29a543b | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7889/1/testReport/ | | Max. process+thread count | 559 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7889/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7888: URL: https://github.com/apache/hadoop/pull/7888#issuecomment-3207584935 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 11m 45s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ branch-3.4.2 Compile Tests _ | | +1 :green_heart: | mvninstall | 37m 15s | | branch-3.4.2 passed | | +1 :green_heart: | compile | 0m 40s | | branch-3.4.2 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 36s | | branch-3.4.2 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 32s | | branch-3.4.2 passed | | +1 :green_heart: | mvnsite | 0m 42s | | branch-3.4.2 passed | | +1 :green_heart: | javadoc | 0m 39s | | branch-3.4.2 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 35s | | branch-3.4.2 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 9s | | branch-3.4.2 passed | | +1 :green_heart: | shadedclient | 33m 16s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 32s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 28s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 28s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 20s | | hadoop-tools/hadoop-azure: The patch generated 0 new + 1 unchanged - 1 fixed = 1 total (was 2) | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | +1 :green_heart: | javadoc | 0m 28s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 32m 23s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 26s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 128m 43s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7888/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7888 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 5e18fda5c880 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | branch-3.4.2 / eb7913ebe57cf38ee41eba62cdba53d5d9a25bff | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7888/1/testReport/ | | Max. process+thread count | 699 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7888/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. anujmodi2021 commented on PR #7888: URL: https://github.com/apache/hadoop/pull/7888#issuecomment-3207755446 FYI, @steveloughran @ahmarsuhail Thanks anujmodi2021 commented on PR #7889: URL: https://github.com/apache/hadoop/pull/7889#issuecomment-3207761504 @steveloughran @ahmarsuhail Let me know if this is good to merge. Thanks ahmarsuhail merged PR #7888: URL: https://github.com/apache/hadoop/pull/7888 ahmarsuhail merged PR #7889: URL: https://github.com/apache/hadoop/pull/7889 ahmarsuhail opened a new pull request, #7920: URL: https://github.com/apache/hadoop/pull/7920 Reverts apache/hadoop#7888 ahmarsuhail closed pull request #7920: Revert \"HADOOP-19650. [ABFS][Backport-3.4.2] Fixing NPE when close() called on uninitialized AzureBlobFileSystem\" URL: https://github.com/apache/hadoop/pull/7920 hadoop-yetus commented on PR #7920: URL: https://github.com/apache/hadoop/pull/7920#issuecomment-3236755969 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 40s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ branch-3.4.2 Compile Tests _ | | +1 :green_heart: | mvninstall | 37m 27s | | branch-3.4.2 passed | | +1 :green_heart: | compile | 0m 39s | | branch-3.4.2 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 35s | | branch-3.4.2 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 31s | | branch-3.4.2 passed | | +1 :green_heart: | mvnsite | 0m 41s | | branch-3.4.2 passed | | +1 :green_heart: | javadoc | 0m 38s | | branch-3.4.2 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 35s | | branch-3.4.2 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 11s | | branch-3.4.2 passed | | +1 :green_heart: | shadedclient | 36m 57s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7920/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 1 new + 1 unchanged - 0 fixed = 2 total (was 1) | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | +1 :green_heart: | javadoc | 0m 30s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 12s | | the patch passed | | +1 :green_heart: | shadedclient | 38m 44s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 28s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 128m 0s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7920/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7920 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 15ea4ce1ec3b 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | branch-3.4.2 / 88dcaa58e8f0ca5850444ce56867884b3149f101 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7920/1/testReport/ | | Max. process+thread count | 742 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7920/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-15T17:27:57.000+0000", "updated": "2025-08-29T11:44:15.000+0000", "derived": {"summary_task": "Summarize this issue: code {code} public void testABFSConstructor() throws Throwable { new AzureBlobFileSystem().close(); } {code} stack {code} [ERROR] org.apache.hadoop.validator.TestRuntimeValid.testABFSConstructor -- Time elapsed: 0.003 s <<< ERROR! java.lang.NullPointerException: Cannot invoke \"org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getClient()\" because \"this.abfsStore\" is null at org.apache.hadoop.", "classification_task": "Classify the issue priority and type: code {code} public void testABFSConstructor() throws Throwable { new AzureBlobFileSystem().close(); } {code} stack {code} [ERROR] org.apache.hadoop.validator.TestRuntimeValid.testABFSConstructor -- Time elapsed: 0.003 s <<< ERROR! java.lang.NullPointerException: Cannot invoke \"org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getClient()\" because \"this.abfsStore\" is null at org.apache.hadoop.", "qna_task": "Question: What is this issue about?\nAnswer: ABFS: NPE when close() called on uninitialized filesystem"}}
{"id": "13626235", "key": "HADOOP-19649", "project": "HADOOP", "summary": "ABFS: Fixing Test Failures and Wrong assumptions after Junit Upgrade", "description": "After https://issues.apache.org/jira/browse/HADOOP-19425 most of the integration tests are getting skipped. All tests need to be fixed with this PR", "comments": "anujmodi2021 opened a new pull request, #7868: URL: https://github.com/apache/hadoop/pull/7868 ### Description of PR After https://issues.apache.org/jira/browse/HADOOP-19425 most of the integration tests are getting skipped. All tests need to be fixed with this PR ### How was this patch tested? Test Suite ran. Copilot commented on code in PR #7868: URL: https://github.com/apache/hadoop/pull/7868#discussion_r2272827582 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemAppend.java: ########## @@ -608,12 +608,12 @@ public void testCreateExplicitDirectoryOverDfsAppendOverBlob() **/ @Test public void testRecreateAppendAndFlush() throws IOException { + assumeThat(isAppendBlobEnabled()).as(\"Not valid for APPEND BLOB\").isFalse(); + assumeThat(getIngressServiceType()).isEqualTo(AbfsServiceType.BLOB); Review Comment: This assumption is placed after the assertThrows() call begins, but should be before it. The assumption should be checked before setting up the exception assertion to ensure the test conditions are met first. ```suggestion assumeThat(getIngressServiceType()).isEqualTo(AbfsServiceType.BLOB); ``` hadoop-yetus commented on PR #7868: URL: https://github.com/apache/hadoop/pull/7868#issuecomment-3183429913 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 14m 4s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 9 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 41s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 11s | | trunk passed | | +1 :green_heart: | shadedclient | 35m 21s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 22s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 29s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 28s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 35m 51s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 16s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 39s | | The patch does not generate ASF License warnings. | | | | 140m 11s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7868/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7868 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 51d70a663e18 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 64f15bfbb96b28d201067683e6e619496d0683da | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7868/1/testReport/ | | Max. process+thread count | 719 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7868/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on PR #7868: URL: https://github.com/apache/hadoop/pull/7868#issuecomment-3186336426 @anujmodi2021 Thank you for your contribution! LGTM. anujmodi2021 commented on PR #7868: URL: https://github.com/apache/hadoop/pull/7868#issuecomment-3187064274 ------------------------------ :::: AGGREGATED TEST RESULT :::: ============================================================ HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 209 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ HNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 863, Failures: 0, Errors: 0, Skipped: 161 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 10 ============================================================ NonHNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 702, Failures: 0, Errors: 0, Skipped: 240 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11 ============================================================ AppendBlob-HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 220 [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-SharedKey-Blob ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 707, Failures: 0, Errors: 0, Skipped: 135 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 5 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11 ============================================================ NonHNS-OAuth-DFS ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 699, Failures: 0, Errors: 0, Skipped: 242 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 704, Failures: 0, Errors: 0, Skipped: 147 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 5 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ AppendBlob-NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 701, Failures: 0, Errors: 0, Skipped: 189 [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 6 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ HNS-Oauth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 734, Failures: 0, Errors: 0, Skipped: 216 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-OAuth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 699, Failures: 0, Errors: 0, Skipped: 239 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 slfan1989 commented on PR #7868: URL: https://github.com/apache/hadoop/pull/7868#issuecomment-3190296832 @anujmodi2021 Thank you for your contribution! I noticed a small improvement we could make in this PR: removing the junit-vintage-engine dependency. This dependency is used for running mixed JUnit 4 and JUnit 5 tests, but since this module has been fully migrated to JUnit 5, I believe we can safely remove it. https://github.com/apache/hadoop/blob/1abdf72dca0c530c265859362b2a2d574d8c9d72/hadoop-tools/hadoop-azure/pom.xml#L363-L367 anmolanmol1234 commented on code in PR #7868: URL: https://github.com/apache/hadoop/pull/7868#discussion_r2281356462 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFilesystemAcl.java: ########## @@ -56,33 +56,55 @@ * Test acl operations. */ public class ITestAzureBlobFilesystemAcl extends AbstractAbfsIntegrationTest { + Review Comment: revert changes with extra spaces anmolanmol1234 commented on code in PR #7868: URL: https://github.com/apache/hadoop/pull/7868#discussion_r2281359848 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFilesystemAcl.java: ########## @@ -238,41 +276,46 @@ public void testModifyAclEntriesStickyBit() throws Exception { fs.modifyAclEntries(path, aclSpec); AclStatus s = fs.getAclStatus(path); AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]); - assertArrayEquals(new AclEntry[]{aclEntry(ACCESS, USER, FOO, READ_EXECUTE), - aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(DEFAULT, USER, ALL), - aclEntry(DEFAULT, USER, FOO, READ_EXECUTE), aclEntry(DEFAULT, GROUP, READ_EXECUTE), - aclEntry(DEFAULT, MASK, READ_EXECUTE), aclEntry(DEFAULT, OTHER, NONE)}, + assertArrayEquals(new AclEntry[]{ + aclEntry(ACCESS, USER, FOO, READ_EXECUTE), + aclEntry(ACCESS, GROUP, READ_EXECUTE), + aclEntry(DEFAULT, USER, ALL), + aclEntry(DEFAULT, USER, FOO, READ_EXECUTE), + aclEntry(DEFAULT, GROUP, READ_EXECUTE), + aclEntry(DEFAULT, MASK, READ_EXECUTE), + aclEntry(DEFAULT, OTHER, NONE) + }, returned); assertPermission(fs, (short) 01750); } @Test public void testModifyAclEntriesPathNotFound() throws Exception { - Assertions.assertThrows(FileNotFoundException.class, () -> { - final AzureBlobFileSystem fs = this.getFileSystem(); - assumeTrue(getIsNamespaceEnabled(fs)); - path = new Path(testRoot, UUID.randomUUID().toString()); - List<AclEntry> aclSpec = Lists.newArrayList( - aclEntry(ACCESS, USER, ALL), - aclEntry(ACCESS, USER, FOO, ALL), - aclEntry(ACCESS, GROUP, READ_EXECUTE), - aclEntry(ACCESS, OTHER, NONE)); - fs.modifyAclEntries(path, aclSpec); - }); + assumeTrue(getIsNamespaceEnabled(getFileSystem())); + Assertions.assertThrows(FileNotFoundException.class, () -> { + final AzureBlobFileSystem fs = this.getFileSystem(); + path = new Path(testRoot, UUID.randomUUID().toString()); + List<AclEntry> aclSpec = Lists.newArrayList( + aclEntry(ACCESS, USER, ALL), + aclEntry(ACCESS, USER, FOO, ALL), + aclEntry(ACCESS, GROUP, READ_EXECUTE), + aclEntry(ACCESS, OTHER, NONE)); + fs.modifyAclEntries(path, aclSpec); + }); } @Test public void testModifyAclEntriesDefaultOnFile() throws Exception { - Assertions.assertThrows(Exception.class, () -> { - final AzureBlobFileSystem fs = this.getFileSystem(); - assumeTrue(getIsNamespaceEnabled(fs)); - path = new Path(testRoot, UUID.randomUUID().toString()); - fs.create(path).close(); - fs.setPermission(path, FsPermission.createImmutable((short) RW_R)); - List<AclEntry> aclSpec = Lists.newArrayList( - aclEntry(DEFAULT, USER, FOO, ALL)); - fs.modifyAclEntries(path, aclSpec); - }); + Assertions.assertThrows(Exception.class, () -> { + final AzureBlobFileSystem fs = this.getFileSystem(); Review Comment: same as above, only space changes can be reverted hadoop-yetus commented on PR #7868: URL: https://github.com/apache/hadoop/pull/7868#issuecomment-3195393896 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 9 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 25m 44s | | trunk passed | | +1 :green_heart: | compile | 0m 26s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 23s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 29s | | trunk passed | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 47s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 14s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 18s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 13s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 21s | | the patch passed | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 43s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 57s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 23s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 26s | | The patch does not generate ASF License warnings. | | | | 78m 3s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7868/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7868 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 14573deebbd7 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / bf4b108ed612d69707da0fd5a95294a2734b3986 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7868/2/testReport/ | | Max. process+thread count | 700 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7868/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. anujmodi2021 commented on code in PR #7868: URL: https://github.com/apache/hadoop/pull/7868#discussion_r2282011563 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFilesystemAcl.java: ########## @@ -56,33 +56,55 @@ * Test acl operations. */ public class ITestAzureBlobFilesystemAcl extends AbstractAbfsIntegrationTest { + Review Comment: So when the last time these changes were made the proper code-style was not set by whoever made these changes. In this PR these changes came in automatically due to importing the recommended code-style. Since this is the right thing to do, I feel like it might be okay to retain these changes. Anyways this is a small PR. ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFilesystemAcl.java: ########## @@ -238,41 +276,46 @@ public void testModifyAclEntriesStickyBit() throws Exception { fs.modifyAclEntries(path, aclSpec); AclStatus s = fs.getAclStatus(path); AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]); - assertArrayEquals(new AclEntry[]{aclEntry(ACCESS, USER, FOO, READ_EXECUTE), - aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(DEFAULT, USER, ALL), - aclEntry(DEFAULT, USER, FOO, READ_EXECUTE), aclEntry(DEFAULT, GROUP, READ_EXECUTE), - aclEntry(DEFAULT, MASK, READ_EXECUTE), aclEntry(DEFAULT, OTHER, NONE)}, + assertArrayEquals(new AclEntry[]{ + aclEntry(ACCESS, USER, FOO, READ_EXECUTE), + aclEntry(ACCESS, GROUP, READ_EXECUTE), + aclEntry(DEFAULT, USER, ALL), + aclEntry(DEFAULT, USER, FOO, READ_EXECUTE), + aclEntry(DEFAULT, GROUP, READ_EXECUTE), + aclEntry(DEFAULT, MASK, READ_EXECUTE), + aclEntry(DEFAULT, OTHER, NONE) + }, returned); assertPermission(fs, (short) 01750); } @Test public void testModifyAclEntriesPathNotFound() throws Exception { - Assertions.assertThrows(FileNotFoundException.class, () -> { - final AzureBlobFileSystem fs = this.getFileSystem(); - assumeTrue(getIsNamespaceEnabled(fs)); - path = new Path(testRoot, UUID.randomUUID().toString()); - List<AclEntry> aclSpec = Lists.newArrayList( - aclEntry(ACCESS, USER, ALL), - aclEntry(ACCESS, USER, FOO, ALL), - aclEntry(ACCESS, GROUP, READ_EXECUTE), - aclEntry(ACCESS, OTHER, NONE)); - fs.modifyAclEntries(path, aclSpec); - }); + assumeTrue(getIsNamespaceEnabled(getFileSystem())); + Assertions.assertThrows(FileNotFoundException.class, () -> { + final AzureBlobFileSystem fs = this.getFileSystem(); + path = new Path(testRoot, UUID.randomUUID().toString()); + List<AclEntry> aclSpec = Lists.newArrayList( + aclEntry(ACCESS, USER, ALL), + aclEntry(ACCESS, USER, FOO, ALL), + aclEntry(ACCESS, GROUP, READ_EXECUTE), + aclEntry(ACCESS, OTHER, NONE)); + fs.modifyAclEntries(path, aclSpec); + }); } @Test public void testModifyAclEntriesDefaultOnFile() throws Exception { - Assertions.assertThrows(Exception.class, () -> { - final AzureBlobFileSystem fs = this.getFileSystem(); - assumeTrue(getIsNamespaceEnabled(fs)); - path = new Path(testRoot, UUID.randomUUID().toString()); - fs.create(path).close(); - fs.setPermission(path, FsPermission.createImmutable((short) RW_R)); - List<AclEntry> aclSpec = Lists.newArrayList( - aclEntry(DEFAULT, USER, FOO, ALL)); - fs.modifyAclEntries(path, aclSpec); - }); + Assertions.assertThrows(Exception.class, () -> { + final AzureBlobFileSystem fs = this.getFileSystem(); Review Comment: Same as above anujmodi2021 commented on PR #7868: URL: https://github.com/apache/hadoop/pull/7868#issuecomment-3196145863 ------------------------------ :::: AGGREGATED TEST RESULT :::: ============================================================ HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 209 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ HNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 863, Failures: 0, Errors: 0, Skipped: 161 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 10 ============================================================ NonHNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 702, Failures: 0, Errors: 0, Skipped: 240 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11 ============================================================ AppendBlob-HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 220 [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-SharedKey-Blob ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 707, Failures: 0, Errors: 0, Skipped: 135 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 5 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11 ============================================================ NonHNS-OAuth-DFS ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 699, Failures: 0, Errors: 0, Skipped: 242 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 704, Failures: 0, Errors: 0, Skipped: 147 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 5 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ AppendBlob-NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 701, Failures: 0, Errors: 0, Skipped: 189 [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 6 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ HNS-Oauth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 734, Failures: 0, Errors: 0, Skipped: 216 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-OAuth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 699, Failures: 0, Errors: 0, Skipped: 239 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 Time taken: 198 mins 45 secs. slfan1989 commented on PR #7868: URL: https://github.com/apache/hadoop/pull/7868#issuecomment-3198853166 Thanks everyone for pushing this PR forward and for the review. anujmodi2021 merged PR #7868: URL: https://github.com/apache/hadoop/pull/7868 anujmodi2021 commented on PR #7868: URL: https://github.com/apache/hadoop/pull/7868#issuecomment-3199223431 > Thanks everyone for pushing this PR forward and for the review. Thanks for review. Have merged the PR. anujmodi2021 commented on PR #7868: URL: https://github.com/apache/hadoop/pull/7868#issuecomment-3199224446 @slfan1989 are we pursuing this upgrade for upcoming 3.4.2 release as well? Or is it targetted for 3.5.0 only? slfan1989 commented on PR #7868: URL: https://github.com/apache/hadoop/pull/7868#issuecomment-3199287275 > @slfan1989 are we pursuing this upgrade for upcoming 3.4.2 release as well? Or is it targetted for 3.5.0 only? Personally, I think it\u2019s needed, but it hasn\u2019t gone through community discussion yet. For now, we should apply it to version 3.5 first.", "created": "2025-08-13T06:11:03.000+0000", "updated": "2025-08-26T03:57:39.000+0000", "derived": {"summary_task": "Summarize this issue: After https://issues.apache.org/jira/browse/HADOOP-19425 most of the integration tests are getting skipped. All tests need to be fixed with this PR", "classification_task": "Classify the issue priority and type: After https://issues.apache.org/jira/browse/HADOOP-19425 most of the integration tests are getting skipped. All tests need to be fixed with this PR", "qna_task": "Question: What is this issue about?\nAnswer: ABFS: Fixing Test Failures and Wrong assumptions after Junit Upgrade"}}
{"id": "13626015", "key": "HADOOP-19648", "project": "HADOOP", "summary": "cos use token credential will lost token field", "description": "Hi, I've discovered a bug when accessing COSN using temporary credentials (access key, secret key, and session token). In the org.apache.hadoop.fs.cosn.CosNativeFileSystemStore#initCOSClient method, when the client is initialized, it only passes the access key and secret key, completely ignoring the session token. This causes all subsequent operations that rely on these temporary credentials to fail. Furthermore, this re-initialization step seems unnecessary. Instead of creating a new client with incomplete credentials, the existing credential provider (which already contains the AK, SK, and token) should be passed down directly. !image-2025-08-11-10-37-12-451.png|width=1048,height=540! !image-2025-08-11-10-42-36-375.png!", "comments": "leosanqing opened a new pull request, #7866: URL: https://github.com/apache/hadoop/pull/7866 ### Description of PR In the org.apache.hadoop.fs.cosn.CosNativeFileSystemStore#initCOSClient method, when the client is initialized, it only passes the access key and secret key, completely ignoring the session token. This causes all subsequent operations that rely on these temporary credentials to fail. Furthermore, this re-initialization step seems unnecessary. Instead of creating a new client with incomplete credentials, the existing credential provider (which already contains the AK, SK, and token) should be passed down directly. ### How was this patch tested? ### For code changes: - [\u2714] Does the title or this PR starts with the corresponding JIRA issue id leosanqing closed pull request #7866: HADOOP-19648. [hotfix] Cos use token credential will lose token field URL: https://github.com/apache/hadoop/pull/7866 leosanqing opened a new pull request, #7867: URL: https://github.com/apache/hadoop/pull/7867 ### Description of PR In the org.apache.hadoop.fs.cosn.CosNativeFileSystemStore#initCOSClient method, when the client is initialized, it only passes the access key and secret key, completely ignoring the session token. This causes all subsequent operations that rely on these temporary credentials to fail. Furthermore, this re-initialization step seems unnecessary. Instead of creating a new client with incomplete credentials, the existing credential provider (which already contains the AK, SK, and token) should be passed down directly. ### How was this patch tested? ### For code changes: - [\u2714] Does the title or this PR starts with the corresponding JIRA issue id hadoop-yetus commented on PR #7867: URL: https://github.com/apache/hadoop/pull/7867#issuecomment-3173339571 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 21m 11s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 44m 48s | | trunk passed | | +1 :green_heart: | compile | 0m 27s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 26s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 30s | | trunk passed | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 27s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 45s | | trunk passed | | +1 :green_heart: | shadedclient | 40m 31s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 17s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 14s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 20s | | the patch passed | | +1 :green_heart: | javadoc | 0m 20s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 42s | | the patch passed | | +1 :green_heart: | shadedclient | 40m 35s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 22s | | hadoop-cos in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 156m 39s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7867 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 0a4e3db4c896 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a2fb2f41505dd147136a842dc66395632cee38f8 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/1/testReport/ | | Max. process+thread count | 540 (vs. ulimit of 5500) | | modules | C: hadoop-cloud-storage-project/hadoop-cos U: hadoop-cloud-storage-project/hadoop-cos | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7867: URL: https://github.com/apache/hadoop/pull/7867#issuecomment-3175078496 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 50s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 4 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 45m 44s | | trunk passed | | +1 :green_heart: | compile | 0m 27s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 26s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 30s | | trunk passed | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 27s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 45s | | trunk passed | | +1 :green_heart: | shadedclient | 41m 20s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 17s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 14s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 20s | | the patch passed | | +1 :green_heart: | javadoc | 0m 20s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 42s | | the patch passed | | +1 :green_heart: | shadedclient | 41m 8s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 21s | | hadoop-cos in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 138m 35s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7867 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint | | uname | Linux 3d08225597f2 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 4d939984a47fcbbb4e3ca1cd5ffa32a0cc953838 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/2/testReport/ | | Max. process+thread count | 535 (vs. ulimit of 5500) | | modules | C: hadoop-cloud-storage-project/hadoop-cos U: hadoop-cloud-storage-project/hadoop-cos | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. leosanqing commented on PR #7867: URL: https://github.com/apache/hadoop/pull/7867#issuecomment-3188421349 @slfan1989 @cnauroth hi guys\uff0ccould you help me review this pr\uff1fthx : ) cnauroth commented on code in PR #7867: URL: https://github.com/apache/hadoop/pull/7867#discussion_r2279916489 ########## hadoop-cloud-storage-project/hadoop-cos/src/test/java/org/apache/hadoop/fs/cosn/TestCosCredentials.java: ########## @@ -131,4 +134,44 @@ private void validateCredentials(URI uri, Configuration configuration) } } } + + @Test + public void testTmpTokenCredentialsProvider() throws Throwable { + Configuration configuration = new Configuration(); + // Set DynamicTemporaryCosnCredentialsProvider as the CosCredentials + // Provider. + configuration.set(CosNConfigKeys.COSN_CREDENTIALS_PROVIDER, + \"org.apache.hadoop.fs.cosn.auth.DynamicTemporaryCosnCredentialsProvider\"); + validateTmpTokenCredentials(this.fsUri, configuration); + } + + private void validateTmpTokenCredentials(URI uri, Configuration configuration) + throws IOException { + if (null != configuration) { + COSCredentialsProvider credentialsProvider = + CosNUtils.createCosCredentialsProviderSet(uri, configuration); + COSCredentials cosCredentials = credentialsProvider.getCredentials(); + assertNotNull(cosCredentials, \"The cos credentials obtained is null.\"); + assertTrue( + StringUtils.equalsIgnoreCase(configuration.get(CosNConfigKeys.COSN_CREDENTIALS_PROVIDER), + \"org.apache.hadoop.fs.cosn.auth.DynamicTemporaryCosnCredentialsProvider\"), + \"CredentialsProvider must be DynamicTemporaryCosnCredentialsProvider\"); + + if (!(cosCredentials instanceof BasicSessionCredentials)) { Review Comment: This could be simplified to: ``` assertTrue(cosCredentials instanceof BasicSessionCredentials, \"...\"); ``` ########## hadoop-cloud-storage-project/hadoop-cos/src/test/java/org/apache/hadoop/fs/cosn/TestCosCredentials.java: ########## @@ -131,4 +134,44 @@ private void validateCredentials(URI uri, Configuration configuration) } } } + + @Test + public void testTmpTokenCredentialsProvider() throws Throwable { + Configuration configuration = new Configuration(); + // Set DynamicTemporaryCosnCredentialsProvider as the CosCredentials + // Provider. + configuration.set(CosNConfigKeys.COSN_CREDENTIALS_PROVIDER, + \"org.apache.hadoop.fs.cosn.auth.DynamicTemporaryCosnCredentialsProvider\"); + validateTmpTokenCredentials(this.fsUri, configuration); + } + + private void validateTmpTokenCredentials(URI uri, Configuration configuration) + throws IOException { + if (null != configuration) { Review Comment: Is this null check required? It seems like `configuration` will always be non-null. ########## hadoop-cloud-storage-project/hadoop-cos/src/test/java/org/apache/hadoop/fs/cosn/TestCosCredentials.java: ########## @@ -131,4 +134,44 @@ private void validateCredentials(URI uri, Configuration configuration) } } } + + @Test + public void testTmpTokenCredentialsProvider() throws Throwable { + Configuration configuration = new Configuration(); + // Set DynamicTemporaryCosnCredentialsProvider as the CosCredentials + // Provider. + configuration.set(CosNConfigKeys.COSN_CREDENTIALS_PROVIDER, + \"org.apache.hadoop.fs.cosn.auth.DynamicTemporaryCosnCredentialsProvider\"); + validateTmpTokenCredentials(this.fsUri, configuration); + } + + private void validateTmpTokenCredentials(URI uri, Configuration configuration) + throws IOException { + if (null != configuration) { + COSCredentialsProvider credentialsProvider = + CosNUtils.createCosCredentialsProviderSet(uri, configuration); + COSCredentials cosCredentials = credentialsProvider.getCredentials(); + assertNotNull(cosCredentials, \"The cos credentials obtained is null.\"); + assertTrue( + StringUtils.equalsIgnoreCase(configuration.get(CosNConfigKeys.COSN_CREDENTIALS_PROVIDER), + \"org.apache.hadoop.fs.cosn.auth.DynamicTemporaryCosnCredentialsProvider\"), + \"CredentialsProvider must be DynamicTemporaryCosnCredentialsProvider\"); + + if (!(cosCredentials instanceof BasicSessionCredentials)) { + fail(\"cosCredentials must be instanceof BasicSessionCredentials\"); + } + + if (!StringUtils.equals(cosCredentials.getCOSAccessKeyId(), \"ak\") || !StringUtils.equals( Review Comment: Instead of conditional logic around `fail`, I suggest that this should be 3 separate `assertEquals` for access key, secret key and session token. leosanqing commented on code in PR #7867: URL: https://github.com/apache/hadoop/pull/7867#discussion_r2284575792 ########## hadoop-cloud-storage-project/hadoop-cos/src/test/java/org/apache/hadoop/fs/cosn/TestCosCredentials.java: ########## @@ -131,4 +134,44 @@ private void validateCredentials(URI uri, Configuration configuration) } } } + + @Test + public void testTmpTokenCredentialsProvider() throws Throwable { + Configuration configuration = new Configuration(); + // Set DynamicTemporaryCosnCredentialsProvider as the CosCredentials + // Provider. + configuration.set(CosNConfigKeys.COSN_CREDENTIALS_PROVIDER, + \"org.apache.hadoop.fs.cosn.auth.DynamicTemporaryCosnCredentialsProvider\"); + validateTmpTokenCredentials(this.fsUri, configuration); + } + + private void validateTmpTokenCredentials(URI uri, Configuration configuration) + throws IOException { + if (null != configuration) { Review Comment: yep, you'r right. leosanqing commented on code in PR #7867: URL: https://github.com/apache/hadoop/pull/7867#discussion_r2284603156 ########## hadoop-cloud-storage-project/hadoop-cos/src/test/java/org/apache/hadoop/fs/cosn/TestCosCredentials.java: ########## @@ -131,4 +134,44 @@ private void validateCredentials(URI uri, Configuration configuration) } } } + + @Test + public void testTmpTokenCredentialsProvider() throws Throwable { + Configuration configuration = new Configuration(); + // Set DynamicTemporaryCosnCredentialsProvider as the CosCredentials + // Provider. + configuration.set(CosNConfigKeys.COSN_CREDENTIALS_PROVIDER, + \"org.apache.hadoop.fs.cosn.auth.DynamicTemporaryCosnCredentialsProvider\"); + validateTmpTokenCredentials(this.fsUri, configuration); + } + + private void validateTmpTokenCredentials(URI uri, Configuration configuration) + throws IOException { + if (null != configuration) { + COSCredentialsProvider credentialsProvider = + CosNUtils.createCosCredentialsProviderSet(uri, configuration); + COSCredentials cosCredentials = credentialsProvider.getCredentials(); + assertNotNull(cosCredentials, \"The cos credentials obtained is null.\"); + assertTrue( + StringUtils.equalsIgnoreCase(configuration.get(CosNConfigKeys.COSN_CREDENTIALS_PROVIDER), + \"org.apache.hadoop.fs.cosn.auth.DynamicTemporaryCosnCredentialsProvider\"), + \"CredentialsProvider must be DynamicTemporaryCosnCredentialsProvider\"); + + if (!(cosCredentials instanceof BasicSessionCredentials)) { Review Comment: : ). Thx for your suggestion. I've also found a more standardized way to write the code. `assertInstanceOf(BasicSessionCredentials.class, cosCredentials, \"cosCredentials must be instanceof BasicSessionCredentials\");` leosanqing commented on PR #7867: URL: https://github.com/apache/hadoop/pull/7867#issuecomment-3199934458 > @leosanqing , thank you for the patch. I commented with a few suggestions. > > Can you or someone else make sure the integration tests are passing? I don't have the means to run those myself. Thx bro. Sure, I'v added a new ITest, and pass the test on my local env.(the failed tests are Junit method not found, I've looked up each failed tests) <img width=\"2510\" height=\"481\" alt=\"image\" src=\"https://github.com/user-attachments/assets/f3f6b152-6dc5-471f-8dbb-f21e4d55c33c\" /> When I'm not change code. This ITest will throw exception like this. <img width=\"2419\" height=\"1374\" alt=\"image\" src=\"https://github.com/user-attachments/assets/f381bc6d-0ab7-4a1e-920d-0fae8e6eb3e5\" /> and this is the new code for testing. <img width=\"2509\" height=\"475\" alt=\"image\" src=\"https://github.com/user-attachments/assets/69ad045e-449b-47ca-b13c-6d606767c099\" /> leosanqing commented on PR #7867: URL: https://github.com/apache/hadoop/pull/7867#issuecomment-3200102193 > @leosanqing , thank you for the patch. I commented with a few suggestions. > > Can you or someone else make sure the integration tests are passing? I don't have the means to run those myself. For newest, I add the test dependence, all tests are passed. <img width=\"2278\" height=\"1151\" alt=\"image\" src=\"https://github.com/user-attachments/assets/2ab96a2e-c789-4c11-b1c3-7a2d825a3f7a\" /> hadoop-yetus commented on PR #7867: URL: https://github.com/apache/hadoop/pull/7867#issuecomment-3200299993 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 37s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 40m 56s | | trunk passed | | +1 :green_heart: | compile | 0m 27s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 25s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 30s | | trunk passed | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 27s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 47s | | trunk passed | | +1 :green_heart: | shadedclient | 37m 4s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 28s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 18s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 14s | [/buildtool-patch-checkstyle-hadoop-cloud-storage-project_hadoop-cos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/3/artifact/out/buildtool-patch-checkstyle-hadoop-cloud-storage-project_hadoop-cos.txt) | The patch fails to run checkstyle in hadoop-cos | | +1 :green_heart: | mvnsite | 0m 20s | | the patch passed | | +1 :green_heart: | javadoc | 0m 20s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 20s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 46s | | the patch passed | | +1 :green_heart: | shadedclient | 41m 27s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 21s | | hadoop-cos in the patch passed. | | -1 :x: | asflicense | 0m 38s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/3/artifact/out/results-asflicense.txt) | The patch generated 2 ASF License warnings. | | | | 129m 59s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7867 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux d39db1ea78c7 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 23868390c030e1b63dce02867f5d5baf816a45c5 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/3/testReport/ | | Max. process+thread count | 556 (vs. ulimit of 5500) | | modules | C: hadoop-cloud-storage-project/hadoop-cos U: hadoop-cloud-storage-project/hadoop-cos | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/3/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7867: URL: https://github.com/apache/hadoop/pull/7867#issuecomment-3200543127 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 52s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 45m 6s | | trunk passed | | +1 :green_heart: | compile | 0m 28s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 27s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 30s | | trunk passed | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 27s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 45s | | trunk passed | | +1 :green_heart: | shadedclient | 40m 35s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 24s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 18s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 13s | [/buildtool-patch-checkstyle-hadoop-cloud-storage-project_hadoop-cos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/4/artifact/out/buildtool-patch-checkstyle-hadoop-cloud-storage-project_hadoop-cos.txt) | The patch fails to run checkstyle in hadoop-cos | | +1 :green_heart: | mvnsite | 0m 19s | | the patch passed | | +1 :green_heart: | javadoc | 0m 20s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 42s | | the patch passed | | +1 :green_heart: | shadedclient | 40m 27s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 22s | | hadoop-cos in the patch passed. | | -1 :x: | asflicense | 0m 38s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/4/artifact/out/results-asflicense.txt) | The patch generated 2 ASF License warnings. | | | | 136m 47s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7867 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 4e15f55ee773 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0a56214a7af88643a157e0c743258af4fceb4f2d | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/4/testReport/ | | Max. process+thread count | 531 (vs. ulimit of 5500) | | modules | C: hadoop-cloud-storage-project/hadoop-cos U: hadoop-cloud-storage-project/hadoop-cos | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/4/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. leosanqing commented on PR #7867: URL: https://github.com/apache/hadoop/pull/7867#issuecomment-3200678324 Sorry for committing so many times, I'm not used for hadoop code style. Pls forgive me. hadoop-yetus commented on PR #7867: URL: https://github.com/apache/hadoop/pull/7867#issuecomment-3200958862 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 19s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 26m 30s | | trunk passed | | +1 :green_heart: | compile | 0m 18s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 15s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 19s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 21s | | trunk passed | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 32s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 36s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 13s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 13s | | the patch passed | | +1 :green_heart: | compile | 0m 11s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 11s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 10s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 15s | | the patch passed | | +1 :green_heart: | javadoc | 0m 14s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 14s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 31s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 14s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 15s | | hadoop-cos in the patch passed. | | +1 :green_heart: | asflicense | 0m 25s | | The patch does not generate ASF License warnings. | | | | 76m 13s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/6/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7867 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 6c557ebeaf9a 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0bab4e6ecff4636cbb8fe5af133af5652cef157e | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/6/testReport/ | | Max. process+thread count | 553 (vs. ulimit of 5500) | | modules | C: hadoop-cloud-storage-project/hadoop-cos U: hadoop-cloud-storage-project/hadoop-cos | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/6/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7867: URL: https://github.com/apache/hadoop/pull/7867#issuecomment-3201095625 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 50s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 1s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 44m 47s | | trunk passed | | +1 :green_heart: | compile | 0m 28s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 26s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 26s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 30s | | trunk passed | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 45s | | trunk passed | | +1 :green_heart: | shadedclient | 40m 39s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 26s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 17s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 13s | [/buildtool-patch-checkstyle-hadoop-cloud-storage-project_hadoop-cos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/5/artifact/out/buildtool-patch-checkstyle-hadoop-cloud-storage-project_hadoop-cos.txt) | The patch fails to run checkstyle in hadoop-cos | | +1 :green_heart: | mvnsite | 0m 20s | | the patch passed | | +1 :green_heart: | javadoc | 0m 20s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 44s | | the patch passed | | +1 :green_heart: | shadedclient | 40m 41s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 21s | | hadoop-cos in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 136m 39s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7867 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 1b78dd2042d8 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f2dbd3e7e6603a5ceccf86d2c17acbc9ab06ee62 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/5/testReport/ | | Max. process+thread count | 587 (vs. ulimit of 5500) | | modules | C: hadoop-cloud-storage-project/hadoop-cos U: hadoop-cloud-storage-project/hadoop-cos | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7867/5/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. cnauroth commented on PR #7867: URL: https://github.com/apache/hadoop/pull/7867#issuecomment-3204031971 > Sorry for committing so many times, I'm not used for hadoop code style. Pls forgive me. No worries! We appreciate the contribution! leosanqing commented on PR #7867: URL: https://github.com/apache/hadoop/pull/7867#issuecomment-3218110763 @cnauroth Hey, bro, sorry to ping you on a weekend. Just a friendly reminder about this PR, looks like it hasn't been merged yet. Happy weekend! cnauroth closed pull request #7867: HADOOP-19648. [hotfix] Cos use token credential will lose token field URL: https://github.com/apache/hadoop/pull/7867 cnauroth commented on PR #7867: URL: https://github.com/apache/hadoop/pull/7867#issuecomment-3221459343 @leosanqing , thank you for the reminder. I have committed this to trunk. This would ship the fix in Hadoop 3.5.0. I looked at backporting to branch-3.4, but there were conflicts. If you need the patch in earlier versions, please send up a separate pull request targeting branch-3.4. Thank you for the contribution! leosanqing opened a new pull request, #7911: URL: https://github.com/apache/hadoop/pull/7911 ### Description of PR In the org.apache.hadoop.fs.cosn.CosNativeFileSystemStore#initCOSClient method, when the client is initialized, it only passes the access key and secret key, completely ignoring the session token. This causes all subsequent operations that rely on these temporary credentials to fail. Furthermore, this re-initialization step seems unnecessary. Instead of creating a new client with incomplete credentials, the existing credential provider (which already contains the AK, SK, and token) should be passed down directly. This is same as https://github.com/apache/hadoop/pull/7867,but there were some conflicts. so I send up a separate pull request targeting branch-3.4. ### How was this patch tested? ITests are passed on my local env. <img width=\"2528\" height=\"576\" alt=\"image\" src=\"https://github.com/user-attachments/assets/fff41a10-f423-46ee-a8f9-5a806175c97f\" /> ### For code changes: - [\u2714] Does the title or this PR starts with the corresponding JIRA issue id (yes)? hadoop-yetus commented on PR #7911: URL: https://github.com/apache/hadoop/pull/7911#issuecomment-3231866522 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 19m 31s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ branch-3.4 Compile Tests _ | | +1 :green_heart: | mvninstall | 41m 58s | | branch-3.4 passed | | +1 :green_heart: | compile | 0m 28s | | branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 26s | | branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 26s | | branch-3.4 passed | | +1 :green_heart: | mvnsite | 0m 30s | | branch-3.4 passed | | +1 :green_heart: | javadoc | 0m 32s | | branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 27s | | branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 45s | | branch-3.4 passed | | +1 :green_heart: | shadedclient | 36m 54s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 25s | | the patch passed | | +1 :green_heart: | compile | 0m 18s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 17s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 14s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 20s | | the patch passed | | +1 :green_heart: | javadoc | 0m 20s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 43s | | the patch passed | | +1 :green_heart: | shadedclient | 37m 24s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 21s | | hadoop-cos in the patch passed. | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 145m 34s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7911/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7911 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 5b677ee4dfa6 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | branch-3.4 / 7cf257320847af9fc5a7075abe7ffda257bee4ee | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7911/1/testReport/ | | Max. process+thread count | 527 (vs. ulimit of 5500) | | modules | C: hadoop-cloud-storage-project/hadoop-cos U: hadoop-cloud-storage-project/hadoop-cos | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7911/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. leosanqing commented on PR #7911: URL: https://github.com/apache/hadoop/pull/7911#issuecomment-3231885127 @cnauroth hi, bro, I send a new pr to merge targeting brach-3.4. Previous conflicts are test dependencies. ITest's results are here. Cloud you help me to review this pr? <img width=\"2528\" height=\"576\" alt=\"image\" src=\"https://github.com/user-attachments/assets/5098c1c2-7dd4-4b4f-afed-3de28355d47d\" /> cnauroth closed pull request #7911: HADOOP-19648. Cos use token credential will lose token field URL: https://github.com/apache/hadoop/pull/7911 cnauroth commented on PR #7911: URL: https://github.com/apache/hadoop/pull/7911#issuecomment-3238260120 Thank you again @leosanqing . I committed this to branch-3.4. leosanqing commented on PR #7911: URL: https://github.com/apache/hadoop/pull/7911#issuecomment-3238639566 > Thank you again @leosanqing . I committed this to branch-3.4. Hey\uff0cthank you for your merging. I'm not sure if branch-3.3 is still being maintained, but I have tested this PR against branch-3.3 in my local environment, and it passed. If you are still releasing new versions for 3.3, this could be merged into branch-3.3 as well, since it was the original branch where this COS feature was first introduced. cnauroth commented on PR #7911: URL: https://github.com/apache/hadoop/pull/7911#issuecomment-3245771396 > > Thank you again @leosanqing . I committed this to branch-3.4. > > Hey\uff0cthank you for your merging. > > I'm not sure if branch-3.3 is still being maintained, but I have tested this PR against branch-3.3 in my local environment, and it passed. If you are still releasing new versions for 3.3, this could be merged into branch-3.3 as well, since it was the original branch where this COS feature was first introduced. I'm not aware of any specific schedule for another 3.3 release, but there are still a few patches going in there. I merged this to branch-3.3. Thanks again, @leosanqing .", "created": "2025-08-11T02:44:05.000+0000", "updated": "2025-09-02T15:14:00.000+0000", "derived": {"summary_task": "Summarize this issue: Hi, I've discovered a bug when accessing COSN using temporary credentials (access key, secret key, and session token). In the org.apache.hadoop.fs.cosn.CosNativeFileSystemStore#initCOSClient method, when the client is initialized, it only passes the access key and secret key, completely ignoring the session token. This causes all subsequent operations that rely on these temporary credentials to fa", "classification_task": "Classify the issue priority and type: Hi, I've discovered a bug when accessing COSN using temporary credentials (access key, secret key, and session token). In the org.apache.hadoop.fs.cosn.CosNativeFileSystemStore#initCOSClient method, when the client is initialized, it only passes the access key and secret key, completely ignoring the session token. This causes all subsequent operations that rely on these temporary credentials to fa", "qna_task": "Question: What is this issue about?\nAnswer: cos use token credential will lost token field"}}
{"id": "13625746", "key": "HADOOP-19647", "project": "HADOOP", "summary": "ABFS: Read Policy set in openFileOptions should be considered for enabling various optimizations", "description": "AbfsInputStream should take in account the Read Policy set by user with Open File Options. Based on the read policy set, appropriate optimizations should be enabled and kicked in.", "comments": "key ones are random, whole file and sequential, with recognising avro, parquet, a bonus * parquet does now open files with \"parquet\" as first entry * distcp always uses whole-file where a few large 64MB+ blocks deliver great performance", "created": "2025-08-07T07:54:02.000+0000", "updated": "2025-08-08T16:09:59.000+0000", "derived": {"summary_task": "Summarize this issue: AbfsInputStream should take in account the Read Policy set by user with Open File Options. Based on the read policy set, appropriate optimizations should be enabled and kicked in.", "classification_task": "Classify the issue priority and type: AbfsInputStream should take in account the Read Policy set by user with Open File Options. Based on the read policy set, appropriate optimizations should be enabled and kicked in.", "qna_task": "Question: What is this issue about?\nAnswer: ABFS: Read Policy set in openFileOptions should be considered for enabling various optimizations"}}
{"id": "13625597", "key": "HADOOP-19646", "project": "HADOOP", "summary": "S3A: Migrate hadoop-aws module from JUnit4 Assume to JUnit5 Assumptions", "description": "This task aims to migrate the test code in the {{hadoop-aws}} module from JUnit4's {{org.junit.Assume}} API to JUnit5's {{org.junit.jupiter.api.Assumptions}} API in order to unify the testing framework and improve maintainability. h4. Scope of changes: * Replace usages of {{{}Assume.assumeTrue(...){}}}, {{{}Assume.assumeFalse(...){}}}, etc., with the corresponding methods from JUnit5, such as {{{}Assumptions.assumeTrue(...){}}}; * Ensure the assertion logic remains consistent with the original behavior; * Update any outdated import statements referencing JUnit4's {{{}Assume{}}}; * Verify that all affected unit tests pass correctly under JUnit5.", "comments": "slfan1989 opened a new pull request, #7858: URL: https://github.com/apache/hadoop/pull/7858 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR JIRA: HADOOP-19646. [JDK17] Migrate hadoop-aws module from JUnit4 Assume to JUnit5 Assumptions. ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #7858: URL: https://github.com/apache/hadoop/pull/7858#issuecomment-3157797389 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 52s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 44m 7s | | trunk passed | | +1 :green_heart: | compile | 0m 45s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 36s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 41s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 10s | | trunk passed | | +1 :green_heart: | shadedclient | 41m 47s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 0m 39s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 39s | | the patch passed | | +1 :green_heart: | compile | 0m 28s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 28s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 20s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | +1 :green_heart: | javadoc | 0m 31s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 7s | | the patch passed | | +1 :green_heart: | shadedclient | 41m 16s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 23s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 143m 0s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7858/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7858 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 5abd35351f22 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 6ca49d5da145efa997723827e84e0d9b047683a3 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7858/1/testReport/ | | Max. process+thread count | 529 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7858/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on PR #7858: URL: https://github.com/apache/hadoop/pull/7858#issuecomment-3158154572 @steveloughran @anujmodi2021 Could you please review this PR? Thank you very much! steveloughran commented on PR #7858: URL: https://github.com/apache/hadoop/pull/7858#issuecomment-3161012685 Can we move to AssertJ here? 1. it's a better syntax, and nicely extensible 1. it lets us cherrypick into java4 branches without any problems 1. everyone who has already used it knows the syntax I don't want to invest any time learning JUnit5's assert syntax, not given AssertJ is good and I'm still learning the nuances in what is a very powerful assertion language slfan1989 commented on PR #7858: URL: https://github.com/apache/hadoop/pull/7858#issuecomment-3162122354 > Can we move to AssertJ here? > > 1. it's a better syntax, and nicely extensible > 2. it lets us cherrypick into java4 branches without any problems > 3. everyone who has already used it knows the syntax > > I don't want to invest any time learning JUnit5's assert syntax, not given AssertJ is good and I'm still learning the nuances in what is a very powerful assertion language Thank you very much for your feedback! I completely agree with your suggestions. I will make improvements in this PR accordingly. Once the upgrade to JUnit 5 is fully completed, I will create a separate JIRA ticket to batch-convert JUnit 5 assertions to AssertJ. hadoop-yetus commented on PR #7858: URL: https://github.com/apache/hadoop/pull/7858#issuecomment-3169619857 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 14m 7s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 40m 11s | | trunk passed | | +1 :green_heart: | compile | 0m 56s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 37s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | +1 :green_heart: | javadoc | 0m 45s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 36s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 12s | | trunk passed | | +1 :green_heart: | shadedclient | 34m 59s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 37s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 37s | | the patch passed | | +1 :green_heart: | compile | 0m 28s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 28s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 22s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 35s | | the patch passed | | +1 :green_heart: | javadoc | 0m 31s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 27s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 35m 3s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 27s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 39s | | The patch does not generate ASF License warnings. | | | | 139m 48s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7858/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7858 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 9af81ee0d453 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 21b0dfe8a802763f56d63b29a99ee3e2281a3fa1 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7858/2/testReport/ | | Max. process+thread count | 549 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7858/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on PR #7858: URL: https://github.com/apache/hadoop/pull/7858#issuecomment-3173954615 @steveloughran Could you please help review this PR? Thank you very much! I\u2019ve updated it to use AssertJ. slfan1989 merged PR #7858: URL: https://github.com/apache/hadoop/pull/7858 slfan1989 commented on PR #7858: URL: https://github.com/apache/hadoop/pull/7858#issuecomment-3177411353 > LGTM > +1 @steveloughran Thank you very much for the review! zhtttylz opened a new pull request, #7951: URL: https://github.com/apache/hadoop/pull/7951 ### Description of PR JIRA:HADOOP-19646. [Addendum] [JDK17] Migrate hadoop-aws module from JUnit4 Assume to JUnit5 Assumptions. ### How was this patch tested? Junit Test. ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? pan3793 commented on code in PR #7951: URL: https://github.com/apache/hadoop/pull/7951#discussion_r2340668453 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/ITestTreewalkProblems.java: ########## @@ -316,7 +316,7 @@ public void testDistCp() throws Throwable { options, getConfiguration()); } else { // distcp fails if uploads are visible - intercept(org.junit.ComparisonFailure.class, () -> { + intercept(org.opentest4j.AssertionFailedError.class, () -> { Review Comment: why not import it at the beginning? zhtttylz commented on code in PR #7951: URL: https://github.com/apache/hadoop/pull/7951#discussion_r2340699995 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/ITestTreewalkProblems.java: ########## @@ -316,7 +316,7 @@ public void testDistCp() throws Throwable { options, getConfiguration()); } else { // distcp fails if uploads are visible - intercept(org.junit.ComparisonFailure.class, () -> { + intercept(org.opentest4j.AssertionFailedError.class, () -> { Review Comment: Thanks for the feedback\u2014I\u2019ll make the updates right away! hadoop-yetus commented on PR #7951: URL: https://github.com/apache/hadoop/pull/7951#issuecomment-3280996134 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 50s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 44m 35s | | trunk passed | | +1 :green_heart: | compile | 0m 47s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 35s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 31s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 42s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 10s | | trunk passed | | +1 :green_heart: | shadedclient | 41m 22s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 0m 38s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 38s | | the patch passed | | +1 :green_heart: | compile | 0m 27s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 27s | | the patch passed | | +1 :green_heart: | blanks | 0m 1s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 21s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 30s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 7s | | the patch passed | | +1 :green_heart: | shadedclient | 40m 50s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 26s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 142m 35s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7951/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7951 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 2780b877b219 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 775a0d2508eb5c067af32f20f99a59c3e6ec9748 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7951/1/testReport/ | | Max. process+thread count | 526 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7951/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7951: URL: https://github.com/apache/hadoop/pull/7951#issuecomment-3281110579 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 28m 23s | | trunk passed | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 20s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 19s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 25s | | trunk passed | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 20s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 40s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 24s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 16s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 16s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 10s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 19s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 40s | | the patch passed | | +1 :green_heart: | shadedclient | 22m 15s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 35s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 27s | | The patch does not generate ASF License warnings. | | | | 82m 40s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7951/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7951 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 05afa27f9842 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 75dad0ad76f5d20b28f9cd4db43206bbee1b3e03 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7951/2/testReport/ | | Max. process+thread count | 703 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7951/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on PR #7951: URL: https://github.com/apache/hadoop/pull/7951#issuecomment-3282915596 @ahmarsuhail Could you help review this PR? Thank you very much! ahmarsuhail commented on PR #7951: URL: https://github.com/apache/hadoop/pull/7951#issuecomment-3285924530 Thanks @zhtttylz! +1, LGTM. slfan1989 merged PR #7951: URL: https://github.com/apache/hadoop/pull/7951 slfan1989 commented on PR #7951: URL: https://github.com/apache/hadoop/pull/7951#issuecomment-3286046276 @zhtttylz Thanks for the contriburion! @pan3793 @ahmarsuhail Thanks for the review!", "created": "2025-08-06T05:12:27.000+0000", "updated": "2025-09-12T16:49:49.000+0000", "derived": {"summary_task": "Summarize this issue: This task aims to migrate the test code in the {{hadoop-aws}} module from JUnit4's {{org.junit.Assume}} API to JUnit5's {{org.junit.jupiter.api.Assumptions}} API in order to unify the testing framework and improve maintainability. h4. Scope of changes: * Replace usages of {{{}Assume.assumeTrue(...){}}}, {{{}Assume.assumeFalse(...){}}}, etc., with the corresponding methods from JUnit5, such as {{{}", "classification_task": "Classify the issue priority and type: This task aims to migrate the test code in the {{hadoop-aws}} module from JUnit4's {{org.junit.Assume}} API to JUnit5's {{org.junit.jupiter.api.Assumptions}} API in order to unify the testing framework and improve maintainability. h4. Scope of changes: * Replace usages of {{{}Assume.assumeTrue(...){}}}, {{{}Assume.assumeFalse(...){}}}, etc., with the corresponding methods from JUnit5, such as {{{}", "qna_task": "Question: What is this issue about?\nAnswer: S3A: Migrate hadoop-aws module from JUnit4 Assume to JUnit5 Assumptions"}}
{"id": "13624892", "key": "HADOOP-19645", "project": "HADOOP", "summary": "ABFS: [ReadAheadV2] Improve Metrics for Read Calls to identify type of read done.", "description": "There are a number of ways in which ABFS driver can trigger a network call to read data. We need a way to identify what type of read call was made from client. Plan is to add an indication for this in already present ClientRequestId header. Following are types of read we want to identify: # Direct Read: Read from a given position in remote file. This will be synchronous read # Normal Read: Read from current seeked position where read ahead was bypassed. This will be synchronous read. # Prefetch Read: Read triggered from background threads filling up in memory cache. This will be asynchronous read. # Missed Cache Read: Read triggered after nothing was received from read ahead. This will be synchronous read. # Footer Read: Read triggered as part of footer read optimization. This will be synchronous. # Small File Read: Read triggered as a part of small file read. This will be synchronous read. We will add another field in the Tracing Header (Client Request Id) for each request. We can call this field \"Operation Specific Header\" very similar to how we have \"Retry Header\" today. As part of this we will only use it for read operations keeping it empty for other operations. Moving ahead f we need to publish any operation specific info, same header can be used.", "comments": "hadoop-yetus commented on PR #7837: URL: https://github.com/apache/hadoop/pull/7837#issuecomment-3135193019 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 55s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 47m 25s | | trunk passed | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 37s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 31s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 42s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 11s | | trunk passed | | +1 :green_heart: | shadedclient | 42m 4s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 14 new + 1 unchanged - 0 fixed = 15 total (was 1) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | -1 :x: | javadoc | 0m 29s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 26s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | spotbugs | 1m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 42m 8s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 2m 59s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 146m 51s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.azurebfs.services.TestApacheHttpClientFallback | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7837 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux c64bcd4ab2e9 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / ea1572a40346a9ddfa4e80f4f1a4925308205175 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/testReport/ | | Max. process+thread count | 533 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7837: URL: https://github.com/apache/hadoop/pull/7837#issuecomment-3135296997 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 47s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 47m 41s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 36s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 41s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 15s | | trunk passed | | +1 :green_heart: | shadedclient | 42m 11s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 29s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 28s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 28s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 17 new + 1 unchanged - 0 fixed = 18 total (was 1) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | -1 :x: | javadoc | 0m 30s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 28s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | spotbugs | 1m 15s | | the patch passed | | +1 :green_heart: | shadedclient | 41m 28s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 2m 59s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 146m 30s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.azurebfs.services.TestApacheHttpClientFallback | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7837 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 7be945cb4d05 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 42ecdd05eb6954bdfd26d5022bf4c8a517c607ba | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/testReport/ | | Max. process+thread count | 540 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7837: URL: https://github.com/apache/hadoop/pull/7837#issuecomment-3135629353 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 28s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 2s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 2s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 45m 31s | | trunk passed | | +1 :green_heart: | compile | 0m 44s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 37s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 42s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 10s | | trunk passed | | +1 :green_heart: | shadedclient | 40m 58s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/3/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 18 new + 1 unchanged - 0 fixed = 19 total (was 1) | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | -1 :x: | javadoc | 0m 29s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/3/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 26s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/3/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | spotbugs | 1m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 46m 42s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 42s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 148m 56s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7837 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 5f43a0e97825 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 224f712fee069bd839f8c27a979367e61cec8c17 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/3/testReport/ | | Max. process+thread count | 596 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/3/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. anujmodi2021 commented on PR #7837: URL: https://github.com/apache/hadoop/pull/7837#issuecomment-3135851673 ----------------------------- :::: AGGREGATED TEST RESULT :::: ============================================================ HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 874, Failures: 0, Errors: 0, Skipped: 223 [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 34 [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23 ============================================================ HNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 874, Failures: 0, Errors: 0, Skipped: 172 [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 34 [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 10 ============================================================ NonHNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 858, Failures: 0, Errors: 0, Skipped: 395 [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 35 [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 11 ============================================================ AppendBlob-HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 874, Failures: 0, Errors: 0, Skipped: 234 [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 58 [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-SharedKey-Blob ============================================================ [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 858, Failures: 0, Errors: 0, Skipped: 285 [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 29 [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 11 ============================================================ NonHNS-OAuth-DFS ============================================================ [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 858, Failures: 0, Errors: 0, Skipped: 400 [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 35 [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24 ============================================================ NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 858, Failures: 0, Errors: 0, Skipped: 301 [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 29 [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24 ============================================================ AppendBlob-NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 858, Failures: 0, Errors: 0, Skipped: 346 [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 53 [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24 ============================================================ HNS-Oauth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 874, Failures: 0, Errors: 0, Skipped: 356 [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 34 [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-OAuth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 858, Failures: 0, Errors: 0, Skipped: 397 [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 35 [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24 Copilot commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2244303281 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java: ########## @@ -265,6 +289,34 @@ private String addFailureReasons(final String header, return String.format(\"%s_%s\", header, previousFailure); } + private String getRetryHeader(final String previousFailure, String retryPolicyAbbreviation) { + String retryHeader = String.format(\"%d\", retryCount); + if (previousFailure == null) { + return retryHeader; + } + if (CONNECTION_TIMEOUT_ABBREVIATION.equals(previousFailure) && retryPolicyAbbreviation != null) { + return String.format(\"%s_%s_%s\", retryHeader, previousFailure, retryPolicyAbbreviation); + } + return String.format(\"%s_%s\", retryHeader, previousFailure); + } + + private String getOperationSpecificHeader(FSOperationType opType) { + // Similar header can be added for other operations in the future. + switch (opType) { + case READ: + return readSpecificHeader(); + default: + return EMPTY_STRING; // no operation specific header + } + } + + private String readSpecificHeader() { + // More information on read can be added to this header in the future. + // As underscore separated values. + String readHeader = String.format(\"%s\", readType.toString()); Review Comment: The String.format with \"%s\" is unnecessary here. Use readType.toString() directly for better readability and performance. ```suggestion String readHeader = readType.toString(); ``` ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java: ########## @@ -193,31 +213,35 @@ public void setListener(Listener listener) { public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) { clientRequestId = UUID.randomUUID().toString(); switch (format) { - case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty + case ALL_ID_FORMAT: header = - clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\" - + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID - + \":\" + opType + \":\" + retryCount; - header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation); - if (!(ingressHandler.equals(EMPTY_STRING))) { - header += \":\" + ingressHandler; - } - if (!(position.equals(EMPTY_STRING))) { - header += \":\" + position; - } - if (operatedBlobCount != null) { - header += (\":\" + operatedBlobCount); - } - header += (\":\" + httpOperation.getTracingContextSuffix()); + AbfsHttpConstants.TracingHeaderVersion.V1 + \":\" + + clientCorrelationID + \":\" + + clientRequestId + \":\" + + fileSystemID + \":\" + + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + + streamID + \":\" + + opType + \":\" + + getRetryHeader(previousFailure, retryPolicyAbbreviation) + \":\" + + ingressHandler + \":\" + + position + \":\" + + operatedBlobCount + \":\" + + httpOperation.getTracingContextSuffix() + \":\" + + getOperationSpecificHeader(opType); + metricHeader += !(metricResults.trim().isEmpty()) ? metricResults : \"\"; break; case TWO_ID_FORMAT: - header = clientCorrelationID + \":\" + clientRequestId; + header = + AbfsHttpConstants.TracingHeaderVersion.V1 + \":\" + + clientCorrelationID + \":\" + clientRequestId; metricHeader += !(metricResults.trim().isEmpty()) ? metricResults : \"\"; break; default: //case SINGLE_ID_FORMAT - header = clientRequestId; + header = + AbfsHttpConstants.TracingHeaderVersion.V1 + \":\" + Review Comment: The hardcoded V1 version is used in multiple places. Consider using TracingHeaderVersion.getCurrentVersion() consistently to centralize version management. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java: ########## @@ -193,31 +213,35 @@ public void setListener(Listener listener) { public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) { clientRequestId = UUID.randomUUID().toString(); switch (format) { - case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty + case ALL_ID_FORMAT: header = - clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\" - + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID - + \":\" + opType + \":\" + retryCount; - header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation); - if (!(ingressHandler.equals(EMPTY_STRING))) { - header += \":\" + ingressHandler; - } - if (!(position.equals(EMPTY_STRING))) { - header += \":\" + position; - } - if (operatedBlobCount != null) { - header += (\":\" + operatedBlobCount); - } - header += (\":\" + httpOperation.getTracingContextSuffix()); + AbfsHttpConstants.TracingHeaderVersion.V1 + \":\" + Review Comment: The hardcoded V1 version is used in multiple places. Consider using TracingHeaderVersion.getCurrentVersion() consistently to centralize version management. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java: ########## @@ -193,31 +213,35 @@ public void setListener(Listener listener) { public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) { clientRequestId = UUID.randomUUID().toString(); switch (format) { - case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty + case ALL_ID_FORMAT: header = - clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\" - + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID - + \":\" + opType + \":\" + retryCount; - header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation); - if (!(ingressHandler.equals(EMPTY_STRING))) { - header += \":\" + ingressHandler; - } - if (!(position.equals(EMPTY_STRING))) { - header += \":\" + position; - } - if (operatedBlobCount != null) { - header += (\":\" + operatedBlobCount); - } - header += (\":\" + httpOperation.getTracingContextSuffix()); + AbfsHttpConstants.TracingHeaderVersion.V1 + \":\" + + clientCorrelationID + \":\" + + clientRequestId + \":\" + + fileSystemID + \":\" + + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + + streamID + \":\" + + opType + \":\" + + getRetryHeader(previousFailure, retryPolicyAbbreviation) + \":\" + + ingressHandler + \":\" + + position + \":\" + + operatedBlobCount + \":\" + + httpOperation.getTracingContextSuffix() + \":\" + + getOperationSpecificHeader(opType); + metricHeader += !(metricResults.trim().isEmpty()) ? metricResults : \"\"; break; case TWO_ID_FORMAT: - header = clientCorrelationID + \":\" + clientRequestId; + header = + AbfsHttpConstants.TracingHeaderVersion.V1 + \":\" + + clientCorrelationID + \":\" + clientRequestId; metricHeader += !(metricResults.trim().isEmpty()) ? metricResults : \"\"; break; default: //case SINGLE_ID_FORMAT - header = clientRequestId; + header = + AbfsHttpConstants.TracingHeaderVersion.V1 + \":\" + Review Comment: The hardcoded V1 version is used in multiple places. Consider using TracingHeaderVersion.getCurrentVersion() consistently to centralize version management. ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderValidator.java: ########## @@ -81,82 +85,93 @@ public TracingHeaderValidator(String clientCorrelationId, String fileSystemId, } private void validateTracingHeader(String tracingContextHeader) { - String[] idList = tracingContextHeader.split(\":\"); + String[] idList = tracingContextHeader.split(\":\", -1); Review Comment: [nitpick] Consider defining the split limit (-1) as a named constant to improve code readability and maintainability. ```suggestion String[] idList = tracingContextHeader.split(\":\", SPLIT_NO_LIMIT); ``` hadoop-yetus commented on PR #7837: URL: https://github.com/apache/hadoop/pull/7837#issuecomment-3138813312 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 7 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 26m 51s | | trunk passed | | +1 :green_heart: | compile | 0m 26s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 26s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 22s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 30s | | trunk passed | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 48s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 9s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 18s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 13s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/4/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 6 new + 5 unchanged - 0 fixed = 11 total (was 5) | | +1 :green_heart: | mvnsite | 0m 21s | | the patch passed | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 42s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 13s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 23s | | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 27s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/4/artifact/out/results-asflicense.txt) | The patch generated 1 ASF License warnings. | | | | 79m 44s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7837 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux ba7a35768638 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0d926b14ba007e88c0099c5880f78176988d2442 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/4/testReport/ | | Max. process+thread count | 555 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/4/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. anujmodi2021 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2245426192 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java: ########## @@ -265,6 +289,34 @@ private String addFailureReasons(final String header, return String.format(\"%s_%s\", header, previousFailure); } + private String getRetryHeader(final String previousFailure, String retryPolicyAbbreviation) { + String retryHeader = String.format(\"%d\", retryCount); + if (previousFailure == null) { + return retryHeader; + } + if (CONNECTION_TIMEOUT_ABBREVIATION.equals(previousFailure) && retryPolicyAbbreviation != null) { + return String.format(\"%s_%s_%s\", retryHeader, previousFailure, retryPolicyAbbreviation); + } + return String.format(\"%s_%s\", retryHeader, previousFailure); + } + + private String getOperationSpecificHeader(FSOperationType opType) { + // Similar header can be added for other operations in the future. + switch (opType) { + case READ: + return readSpecificHeader(); + default: + return EMPTY_STRING; // no operation specific header + } + } + + private String readSpecificHeader() { + // More information on read can be added to this header in the future. + // As underscore separated values. + String readHeader = String.format(\"%s\", readType.toString()); Review Comment: Rataining it as in future we might add more info to the same field bhattmanish98 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2245456486 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/AbfsHttpConstants.java: ########## @@ -128,6 +128,7 @@ public final class AbfsHttpConstants { public static final String STAR = \"*\"; public static final String COMMA = \",\"; public static final String COLON = \":\"; + public static final String HYPHEN = \"-\"; Review Comment: We already have CHAR_HYPHEN defined for this. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java: ########## @@ -0,0 +1,50 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.utils; + +public enum TracingHeaderVersion { + + V0(\"\", 8), + V1(\"v1\", 13); + + private final String version; + private final int fieldCount; + + TracingHeaderVersion(String version, int fieldCount) { + this.version = version; + this.fieldCount = fieldCount; + } + + @Override + public String toString() { + return version; + } + + public static TracingHeaderVersion getCurrentVersion() { + return V1; + } + + public int getFieldCount() { + return V1.fieldCount; + } + + public String getVersion() { + return V1.version; Review Comment: Same as above, it should be `return this.version`? ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java: ########## @@ -265,6 +286,34 @@ private String addFailureReasons(final String header, return String.format(\"%s_%s\", header, previousFailure); } + private String getRetryHeader(final String previousFailure, String retryPolicyAbbreviation) { Review Comment: Please add javadoc to all newly added methods ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java: ########## @@ -0,0 +1,50 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.utils; + +public enum TracingHeaderVersion { + + V0(\"\", 8), + V1(\"v1\", 13); + + private final String version; + private final int fieldCount; + + TracingHeaderVersion(String version, int fieldCount) { + this.version = version; + this.fieldCount = fieldCount; + } + + @Override + public String toString() { + return version; + } + + public static TracingHeaderVersion getCurrentVersion() { + return V1; + } + + public int getFieldCount() { + return V1.fieldCount; Review Comment: Shouldn't it be just `return this.fieldCount`? ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java: ########## @@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception { in.close(); } + @Test + public void testReadTypeInTracingContextHeader() throws Exception { Review Comment: Java Doc missing hadoop-yetus commented on PR #7837: URL: https://github.com/apache/hadoop/pull/7837#issuecomment-3140309047 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 24s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 7 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 26m 43s | | trunk passed | | +1 :green_heart: | compile | 0m 26s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 24s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 29s | | trunk passed | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 23s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 46s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 46s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 21s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 18s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 13s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 21s | | the patch passed | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 43s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 44s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 25s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 25s | | The patch does not generate ASF License warnings. | | | | 78m 32s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7837 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux b44a21d0bd2d 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 9bb6cdbeda33155f0957108cfdf87b63dcefe53a | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/5/testReport/ | | Max. process+thread count | 676 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/5/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. manika137 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2246885435 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java: ########## @@ -544,7 +555,9 @@ private int readInternal(final long position, final byte[] b, final int offset, } // got nothing from read-ahead, do our own read now - receivedBytes = readRemote(position, b, offset, length, new TracingContext(tracingContext)); + TracingContext tc = new TracingContext(tracingContext); + tc.setReadType(ReadType.MISSEDCACHE_READ); + receivedBytes = readRemote(position, b, offset, length, tc); return receivedBytes; } else { LOG.debug(\"read ahead disabled, reading remote\"); Review Comment: Should we add readtype as normal read for this TC as well? manika137 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2246929766 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java: ########## @@ -442,6 +451,7 @@ private int optimisedRead(final byte[] b, final int off, final int len, // bCursor that means the user requested data has not been read. if (fCursor < contentLength && bCursor > limit) { restorePointerState(); + tracingContext.setReadType(ReadType.NORMAL_READ); Review Comment: Before readOneBlock we're setting TC as normal read both here and line 439. In readOneBlock method- we're setting TC again to normal read- do we need it twice? We can keep it once in the method only otherwise manika137 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2246993787 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java: ########## @@ -193,32 +213,33 @@ public void setListener(Listener listener) { public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) { clientRequestId = UUID.randomUUID().toString(); switch (format) { - case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty - header = - clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\" - + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID - + \":\" + opType + \":\" + retryCount; - header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation); - if (!(ingressHandler.equals(EMPTY_STRING))) { - header += \":\" + ingressHandler; - } - if (!(position.equals(EMPTY_STRING))) { - header += \":\" + position; - } - if (operatedBlobCount != null) { - header += (\":\" + operatedBlobCount); - } - header += (\":\" + httpOperation.getTracingContextSuffix()); - metricHeader += !(metricResults.trim().isEmpty()) ? metricResults : \"\"; + case ALL_ID_FORMAT: + header = TracingHeaderVersion.V1.getVersion() + COLON + + clientCorrelationID + COLON + + clientRequestId + COLON + + fileSystemID + COLON + + getPrimaryRequestIdForHeader(retryCount > 0) + COLON + + streamID + COLON + + opType + COLON + + getRetryHeader(previousFailure, retryPolicyAbbreviation) + COLON + + ingressHandler + COLON + + position + COLON + + operatedBlobCount + COLON + + httpOperation.getTracingContextSuffix() + COLON + + getOperationSpecificHeader(opType); Review Comment: should we keep the op specific header before adding the HTTP client? It would get all req related info together and then network client. Eg- .....:RE:1_EGR:NR:JDK manika137 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2247014906 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java: ########## @@ -265,6 +286,34 @@ private String addFailureReasons(final String header, return String.format(\"%s_%s\", header, previousFailure); } + private String getRetryHeader(final String previousFailure, String retryPolicyAbbreviation) { Review Comment: we can remove the addFailureReasons method- it has no usage now manika137 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2247066421 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java: ########## @@ -0,0 +1,50 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.utils; + +public enum TracingHeaderVersion { + + V0(\"\", 8), + V1(\"v1\", 13); Review Comment: Since the next versions would be V1.1/V1.2- so should we consider starting with V1.0/V1.1? And with the version updates- would we update the version field in V1 only or new V1.1 enum? manika137 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2247336266 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java: ########## @@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception { in.close(); } + @Test + public void testReadTypeInTracingContextHeader() throws Exception { + AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem()); + AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore()); + AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration()); + AbfsClient spiedClient = Mockito.spy(spiedStore.getClient()); + Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize(); + Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize(); + Mockito.doReturn(spiedClient).when(spiedStore).getClient(); + Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore(); + Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration(); + int numOfReadCalls = 0; + int fileSize = 0; + + /* + * Test to verify Normal Read Type. + * Disabling read ahead ensures that read type is normal read. + */ + fileSize = 3 * ONE_MB; // To make sure multiple blocks are read. + numOfReadCalls += 3; // 3 blocks of 1MB each. + doReturn(false).when(spiedConfig).isReadAheadV2Enabled(); + doReturn(false).when(spiedConfig).isReadAheadEnabled(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls); + + /* + * Test to verify Missed Cache Read Type. + * Setting read ahead depth to 0 ensure that nothing can be got from prefetch. + * In such a case Input Stream will do a sequential read with missed cache read type. + */ + fileSize = ONE_MB; // To make sure only one block is read. + numOfReadCalls += 1; // 1 block of 1MB. + Mockito.doReturn(0).when(spiedConfig).getReadAheadQueueDepth(); + doReturn(true).when(spiedConfig).isReadAheadEnabled(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, MISSEDCACHE_READ, numOfReadCalls); + + /* + * Test to verify Prefetch Read Type. + * Setting read ahead depth to 2 with prefetch enabled ensures that prefetch is done. + * First read here might be Normal or Missed Cache but the rest 2 should be Prefetched Read. + */ + fileSize = 3 * ONE_MB; // To make sure multiple blocks are read. + numOfReadCalls += 3; + doReturn(true).when(spiedConfig).isReadAheadEnabled(); + Mockito.doReturn(3).when(spiedConfig).getReadAheadQueueDepth(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, PREFETCH_READ, numOfReadCalls); + + /* + * Test to verify Footer Read Type. + * Having file size less than footer read size and disabling small file opt + */ + fileSize = 8 * ONE_KB; + numOfReadCalls += 1; // Full file will be read along with footer. + doReturn(false).when(spiedConfig).readSmallFilesCompletely(); + doReturn(true).when(spiedConfig).optimizeFooterRead(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, FOOTER_READ, numOfReadCalls); + + /* + * Test to verify Small File Read Type. + * Having file size less than footer read size and disabling small file opt + */ + fileSize = 8 * ONE_KB; + numOfReadCalls += 1; // Full file will be read along with footer. + doReturn(true).when(spiedConfig).readSmallFilesCompletely(); + doReturn(false).when(spiedConfig).optimizeFooterRead(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, SMALLFILE_READ, numOfReadCalls); + } + + private void testReadTypeInTracingContextHeaderInternal(AzureBlobFileSystem fs, int fileSize, ReadType readType, int numOfReadCalls) throws Exception { + Path testPath = new Path(\"testFile\"); + byte[] fileContent = getRandomBytesArray(fileSize); + try (FSDataOutputStream oStream = fs.create(testPath)) { + oStream.write(fileContent); + oStream.flush(); + } + try (FSDataInputStream iStream = fs.open(testPath)) { + int bytesRead = iStream.read(new byte[fileSize], 0, + fileSize); + Assertions.assertThat(fileSize) + .describedAs(\"Read size should match file size\") + .isEqualTo(bytesRead); + } + + ArgumentCaptor<String> captor1 = ArgumentCaptor.forClass(String.class); + ArgumentCaptor<Long> captor2 = ArgumentCaptor.forClass(Long.class); + ArgumentCaptor<byte[]> captor3 = ArgumentCaptor.forClass(byte[].class); + ArgumentCaptor<Integer> captor4 = ArgumentCaptor.forClass(Integer.class); + ArgumentCaptor<Integer> captor5 = ArgumentCaptor.forClass(Integer.class); + ArgumentCaptor<String> captor6 = ArgumentCaptor.forClass(String.class); + ArgumentCaptor<String> captor7 = ArgumentCaptor.forClass(String.class); + ArgumentCaptor<ContextEncryptionAdapter> captor8 = ArgumentCaptor.forClass(ContextEncryptionAdapter.class); + ArgumentCaptor<TracingContext> captor9 = ArgumentCaptor.forClass(TracingContext.class); + + verify(fs.getAbfsStore().getClient(), times(numOfReadCalls)).read( + captor1.capture(), captor2.capture(), captor3.capture(), + captor4.capture(), captor5.capture(), captor6.capture(), + captor7.capture(), captor8.capture(), captor9.capture()); + TracingContext tracingContext = captor9.getAllValues().get(numOfReadCalls - 1); + verifyHeaderForReadTypeInTracingContextHeader(tracingContext, readType); + } + + private void verifyHeaderForReadTypeInTracingContextHeader(TracingContext tracingContext, ReadType readType) { + AbfsHttpOperation mockOp = Mockito.mock(AbfsHttpOperation.class); + doReturn(EMPTY_STRING).when(mockOp).getTracingContextSuffix(); + tracingContext.constructHeader(mockOp, null, null); + String[] idList = tracingContext.getHeader().split(COLON, SPLIT_NO_LIMIT); + Assertions.assertThat(idList).describedAs(\"Client Request Id should have all fields\").hasSize( + TracingHeaderVersion.getCurrentVersion().getFieldCount()); + Assertions.assertThat(tracingContext.getHeader()).describedAs(\"Operation Type Should Be Read\") + .contains(FSOperationType.READ.toString()); + Assertions.assertThat(tracingContext.getHeader()).describedAs(\"Read type in tracing context header should match\") + .contains(readType.toString()); + } + +// private testReadTypeInTracingContextHeaderInternal(ReadType readType) throws Exception { Review Comment: Nit- we can remove this anmolanmol1234 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2247415571 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java: ########## @@ -77,8 +81,7 @@ public class TracingContext { * this field shall not be set. */ private String primaryRequestIdForRetry; - - private Integer operatedBlobCount = null; + private Integer operatedBlobCount = 1; // Only relevant for rename-delete over blob endpoint where it will be explicitly set. Review Comment: why is it changed from null to 1 ? anmolanmol1234 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2247428588 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java: ########## @@ -0,0 +1,50 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.utils; + +public enum TracingHeaderVersion { + + V0(\"\", 8), + V1(\"v1\", 13); Review Comment: So every time we add a new header, we need to add a new version ? anmolanmol1234 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2247438249 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java: ########## @@ -0,0 +1,50 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.utils; + +public enum TracingHeaderVersion { + + V0(\"\", 8), + V1(\"v1\", 13); + + private final String version; + private final int fieldCount; + + TracingHeaderVersion(String version, int fieldCount) { + this.version = version; + this.fieldCount = fieldCount; + } + + @Override + public String toString() { + return version; + } + + public static TracingHeaderVersion getCurrentVersion() { + return V1; + } + + public int getFieldCount() { + return V1.fieldCount; Review Comment: +1 anmolanmol1234 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2247441472 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java: ########## @@ -0,0 +1,50 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.utils; + +public enum TracingHeaderVersion { + + V0(\"\", 8), + V1(\"v1\", 13); + + private final String version; + private final int fieldCount; + + TracingHeaderVersion(String version, int fieldCount) { + this.version = version; + this.fieldCount = fieldCount; + } + + @Override + public String toString() { + return version; + } + + public static TracingHeaderVersion getCurrentVersion() { + return V1; Review Comment: will this need to be updated everytime a new version is introduced ? anmolanmol1234 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2247441472 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java: ########## @@ -0,0 +1,50 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.utils; + +public enum TracingHeaderVersion { + + V0(\"\", 8), + V1(\"v1\", 13); + + private final String version; + private final int fieldCount; + + TracingHeaderVersion(String version, int fieldCount) { + this.version = version; + this.fieldCount = fieldCount; + } + + @Override + public String toString() { + return version; + } + + public static TracingHeaderVersion getCurrentVersion() { + return V1; Review Comment: this needs to be updated everytime a new version is introduced, can it be dynamically fetched ? anmolanmol1234 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2247447664 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java: ########## @@ -193,32 +213,33 @@ public void setListener(Listener listener) { public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) { clientRequestId = UUID.randomUUID().toString(); switch (format) { - case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty - header = - clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\" - + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID - + \":\" + opType + \":\" + retryCount; - header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation); - if (!(ingressHandler.equals(EMPTY_STRING))) { - header += \":\" + ingressHandler; - } - if (!(position.equals(EMPTY_STRING))) { - header += \":\" + position; - } - if (operatedBlobCount != null) { - header += (\":\" + operatedBlobCount); - } - header += (\":\" + httpOperation.getTracingContextSuffix()); - metricHeader += !(metricResults.trim().isEmpty()) ? metricResults : \"\"; + case ALL_ID_FORMAT: + header = TracingHeaderVersion.V1.getVersion() + COLON Review Comment: should we use getCurrentVersion here ? anmolanmol1234 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2247451122 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java: ########## @@ -193,32 +213,33 @@ public void setListener(Listener listener) { public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) { clientRequestId = UUID.randomUUID().toString(); switch (format) { - case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty - header = - clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\" - + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID - + \":\" + opType + \":\" + retryCount; - header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation); - if (!(ingressHandler.equals(EMPTY_STRING))) { - header += \":\" + ingressHandler; - } - if (!(position.equals(EMPTY_STRING))) { - header += \":\" + position; - } - if (operatedBlobCount != null) { - header += (\":\" + operatedBlobCount); - } - header += (\":\" + httpOperation.getTracingContextSuffix()); - metricHeader += !(metricResults.trim().isEmpty()) ? metricResults : \"\"; + case ALL_ID_FORMAT: + header = TracingHeaderVersion.V1.getVersion() + COLON + + clientCorrelationID + COLON + + clientRequestId + COLON + + fileSystemID + COLON + + getPrimaryRequestIdForHeader(retryCount > 0) + COLON + + streamID + COLON + + opType + COLON + + getRetryHeader(previousFailure, retryPolicyAbbreviation) + COLON + + ingressHandler + COLON Review Comment: these empty string checks are needed anmolanmol1234 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2247460310 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java: ########## @@ -193,32 +213,33 @@ public void setListener(Listener listener) { public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) { clientRequestId = UUID.randomUUID().toString(); switch (format) { - case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty - header = - clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\" - + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID - + \":\" + opType + \":\" + retryCount; - header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation); - if (!(ingressHandler.equals(EMPTY_STRING))) { - header += \":\" + ingressHandler; - } - if (!(position.equals(EMPTY_STRING))) { - header += \":\" + position; - } - if (operatedBlobCount != null) { - header += (\":\" + operatedBlobCount); - } - header += (\":\" + httpOperation.getTracingContextSuffix()); - metricHeader += !(metricResults.trim().isEmpty()) ? metricResults : \"\"; + case ALL_ID_FORMAT: + header = TracingHeaderVersion.V1.getVersion() + COLON + + clientCorrelationID + COLON + + clientRequestId + COLON + + fileSystemID + COLON + + getPrimaryRequestIdForHeader(retryCount > 0) + COLON + + streamID + COLON + + opType + COLON + + getRetryHeader(previousFailure, retryPolicyAbbreviation) + COLON + + ingressHandler + COLON + + position + COLON + + operatedBlobCount + COLON + + httpOperation.getTracingContextSuffix() + COLON + + getOperationSpecificHeader(opType); + + metricHeader += !(metricResults.trim().isEmpty()) ? metricResults : EMPTY_STRING; break; case TWO_ID_FORMAT: - header = clientCorrelationID + \":\" + clientRequestId; - metricHeader += !(metricResults.trim().isEmpty()) ? metricResults : \"\"; + header = TracingHeaderVersion.V1.getVersion() + COLON Review Comment: same as above getCurrentVersion ? anmolanmol1234 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2247472876 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestTracingContext.java: ########## @@ -326,8 +329,8 @@ fileSystemId, FSOperationType.CREATE_FILESYSTEM, tracingHeaderFormat, new Tracin } private void checkHeaderForRetryPolicyAbbreviation(String header, String expectedFailureReason, String expectedRetryPolicyAbbreviation) { - String[] headerContents = header.split(\":\"); - String previousReqContext = headerContents[6]; + String[] headerContents = header.split(\":\", SPLIT_NO_LIMIT); Review Comment: colon constant here as well since we are changing at other places anmolanmol1234 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2247491342 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java: ########## @@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception { in.close(); } + @Test + public void testReadTypeInTracingContextHeader() throws Exception { + AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem()); + AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore()); + AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration()); + AbfsClient spiedClient = Mockito.spy(spiedStore.getClient()); + Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize(); + Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize(); + Mockito.doReturn(spiedClient).when(spiedStore).getClient(); + Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore(); + Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration(); + int numOfReadCalls = 0; + int fileSize = 0; + + /* + * Test to verify Normal Read Type. + * Disabling read ahead ensures that read type is normal read. + */ + fileSize = 3 * ONE_MB; // To make sure multiple blocks are read. + numOfReadCalls += 3; // 3 blocks of 1MB each. + doReturn(false).when(spiedConfig).isReadAheadV2Enabled(); + doReturn(false).when(spiedConfig).isReadAheadEnabled(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls); Review Comment: should we also verify that it is normal_read for all the three calls made ? anmolanmol1234 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2247493079 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java: ########## @@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception { in.close(); } + @Test + public void testReadTypeInTracingContextHeader() throws Exception { + AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem()); + AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore()); + AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration()); + AbfsClient spiedClient = Mockito.spy(spiedStore.getClient()); + Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize(); + Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize(); + Mockito.doReturn(spiedClient).when(spiedStore).getClient(); + Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore(); + Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration(); + int numOfReadCalls = 0; + int fileSize = 0; + + /* + * Test to verify Normal Read Type. + * Disabling read ahead ensures that read type is normal read. + */ + fileSize = 3 * ONE_MB; // To make sure multiple blocks are read. + numOfReadCalls += 3; // 3 blocks of 1MB each. + doReturn(false).when(spiedConfig).isReadAheadV2Enabled(); + doReturn(false).when(spiedConfig).isReadAheadEnabled(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls); + + /* + * Test to verify Missed Cache Read Type. + * Setting read ahead depth to 0 ensure that nothing can be got from prefetch. + * In such a case Input Stream will do a sequential read with missed cache read type. + */ + fileSize = ONE_MB; // To make sure only one block is read. + numOfReadCalls += 1; // 1 block of 1MB. + Mockito.doReturn(0).when(spiedConfig).getReadAheadQueueDepth(); + doReturn(true).when(spiedConfig).isReadAheadEnabled(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, MISSEDCACHE_READ, numOfReadCalls); + + /* + * Test to verify Prefetch Read Type. + * Setting read ahead depth to 2 with prefetch enabled ensures that prefetch is done. + * First read here might be Normal or Missed Cache but the rest 2 should be Prefetched Read. + */ + fileSize = 3 * ONE_MB; // To make sure multiple blocks are read. + numOfReadCalls += 3; + doReturn(true).when(spiedConfig).isReadAheadEnabled(); + Mockito.doReturn(3).when(spiedConfig).getReadAheadQueueDepth(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, PREFETCH_READ, numOfReadCalls); Review Comment: same here verify that 2 calls have prefetch_read anmolanmol1234 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2247491342 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java: ########## @@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception { in.close(); } + @Test + public void testReadTypeInTracingContextHeader() throws Exception { + AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem()); + AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore()); + AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration()); + AbfsClient spiedClient = Mockito.spy(spiedStore.getClient()); + Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize(); + Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize(); + Mockito.doReturn(spiedClient).when(spiedStore).getClient(); + Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore(); + Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration(); + int numOfReadCalls = 0; + int fileSize = 0; + + /* + * Test to verify Normal Read Type. + * Disabling read ahead ensures that read type is normal read. + */ + fileSize = 3 * ONE_MB; // To make sure multiple blocks are read. + numOfReadCalls += 3; // 3 blocks of 1MB each. + doReturn(false).when(spiedConfig).isReadAheadV2Enabled(); + doReturn(false).when(spiedConfig).isReadAheadEnabled(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls); Review Comment: should we also verify that it is normal_read for all the three calls made, currently it verifies for contains anmolanmol1234 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2247501549 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java: ########## @@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception { in.close(); } + @Test + public void testReadTypeInTracingContextHeader() throws Exception { + AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem()); + AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore()); + AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration()); + AbfsClient spiedClient = Mockito.spy(spiedStore.getClient()); + Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize(); + Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize(); + Mockito.doReturn(spiedClient).when(spiedStore).getClient(); + Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore(); + Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration(); + int numOfReadCalls = 0; + int fileSize = 0; + + /* + * Test to verify Normal Read Type. + * Disabling read ahead ensures that read type is normal read. + */ + fileSize = 3 * ONE_MB; // To make sure multiple blocks are read. + numOfReadCalls += 3; // 3 blocks of 1MB each. + doReturn(false).when(spiedConfig).isReadAheadV2Enabled(); + doReturn(false).when(spiedConfig).isReadAheadEnabled(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls); + + /* + * Test to verify Missed Cache Read Type. + * Setting read ahead depth to 0 ensure that nothing can be got from prefetch. + * In such a case Input Stream will do a sequential read with missed cache read type. + */ + fileSize = ONE_MB; // To make sure only one block is read. + numOfReadCalls += 1; // 1 block of 1MB. + Mockito.doReturn(0).when(spiedConfig).getReadAheadQueueDepth(); + doReturn(true).when(spiedConfig).isReadAheadEnabled(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, MISSEDCACHE_READ, numOfReadCalls); + + /* + * Test to verify Prefetch Read Type. + * Setting read ahead depth to 2 with prefetch enabled ensures that prefetch is done. + * First read here might be Normal or Missed Cache but the rest 2 should be Prefetched Read. + */ + fileSize = 3 * ONE_MB; // To make sure multiple blocks are read. + numOfReadCalls += 3; + doReturn(true).when(spiedConfig).isReadAheadEnabled(); + Mockito.doReturn(3).when(spiedConfig).getReadAheadQueueDepth(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, PREFETCH_READ, numOfReadCalls); + + /* + * Test to verify Footer Read Type. + * Having file size less than footer read size and disabling small file opt + */ + fileSize = 8 * ONE_KB; + numOfReadCalls += 1; // Full file will be read along with footer. + doReturn(false).when(spiedConfig).readSmallFilesCompletely(); + doReturn(true).when(spiedConfig).optimizeFooterRead(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, FOOTER_READ, numOfReadCalls); + + /* + * Test to verify Small File Read Type. + * Having file size less than footer read size and disabling small file opt + */ + fileSize = 8 * ONE_KB; + numOfReadCalls += 1; // Full file will be read along with footer. + doReturn(true).when(spiedConfig).readSmallFilesCompletely(); + doReturn(false).when(spiedConfig).optimizeFooterRead(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, SMALLFILE_READ, numOfReadCalls); + } Review Comment: One test for direct read as well ? violetnspct commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2249151187 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java: ########## @@ -578,6 +591,7 @@ int readRemote(long position, byte[] b, int offset, int length, TracingContext t streamStatistics.remoteReadOperation(); } LOG.trace(\"Trigger client.read for path={} position={} offset={} length={}\", path, position, offset, length); + tracingContext.setPosition(String.valueOf(position)); Review Comment: Is there a test to verify position is correctly added to tracing context? Position is a key identifier for read operations. anujmodi2021 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2250024646 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java: ########## @@ -544,7 +555,9 @@ private int readInternal(final long position, final byte[] b, final int offset, } // got nothing from read-ahead, do our own read now - receivedBytes = readRemote(position, b, offset, length, new TracingContext(tracingContext)); + TracingContext tc = new TracingContext(tracingContext); + tc.setReadType(ReadType.MISSEDCACHE_READ); + receivedBytes = readRemote(position, b, offset, length, tc); return receivedBytes; } else { LOG.debug(\"read ahead disabled, reading remote\"); Review Comment: This is coming directly from readOneBlock() so will always be normal hadoop-yetus commented on PR #7837: URL: https://github.com/apache/hadoop/pull/7837#issuecomment-3148617353 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 7 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 25m 35s | | trunk passed | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 24s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 28s | | trunk passed | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 21s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 48s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 56s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 17s | | the patch passed | | +1 :green_heart: | compile | 0m 21s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 14s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 19s | | the patch passed | | +1 :green_heart: | javadoc | 0m 17s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 43s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 47s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 25s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 25s | | The patch does not generate ASF License warnings. | | | | 77m 23s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/6/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7837 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 5948820d65fb 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 132893fcbf3d7eb4b31ca01dbaef26c186560dd3 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/6/testReport/ | | Max. process+thread count | 555 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/6/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. anujmodi2021 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2250346945 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/AbfsHttpConstants.java: ########## @@ -128,6 +128,7 @@ public final class AbfsHttpConstants { public static final String STAR = \"*\"; public static final String COMMA = \",\"; public static final String COLON = \":\"; + public static final String HYPHEN = \"-\"; Review Comment: Taken ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java: ########## @@ -265,6 +286,34 @@ private String addFailureReasons(final String header, return String.format(\"%s_%s\", header, previousFailure); } + private String getRetryHeader(final String previousFailure, String retryPolicyAbbreviation) { Review Comment: Taken anujmodi2021 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2250347466 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java: ########## @@ -0,0 +1,50 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.utils; + +public enum TracingHeaderVersion { + + V0(\"\", 8), + V1(\"v1\", 13); + + private final String version; + private final int fieldCount; + + TracingHeaderVersion(String version, int fieldCount) { + this.version = version; + this.fieldCount = fieldCount; + } + + @Override + public String toString() { + return version; + } + + public static TracingHeaderVersion getCurrentVersion() { + return V1; + } + + public int getFieldCount() { + return V1.fieldCount; Review Comment: Fixed, Thanks for pointing out ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java: ########## @@ -0,0 +1,50 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.utils; + +public enum TracingHeaderVersion { + + V0(\"\", 8), + V1(\"v1\", 13); + + private final String version; + private final int fieldCount; + + TracingHeaderVersion(String version, int fieldCount) { + this.version = version; + this.fieldCount = fieldCount; + } + + @Override + public String toString() { + return version; + } + + public static TracingHeaderVersion getCurrentVersion() { + return V1; + } + + public int getFieldCount() { + return V1.fieldCount; + } + + public String getVersion() { + return V1.version; Review Comment: Fixed ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java: ########## @@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception { in.close(); } + @Test + public void testReadTypeInTracingContextHeader() throws Exception { Review Comment: Added anujmodi2021 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2250348114 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java: ########## @@ -442,6 +451,7 @@ private int optimisedRead(final byte[] b, final int off, final int len, // bCursor that means the user requested data has not been read. if (fCursor < contentLength && bCursor > limit) { restorePointerState(); + tracingContext.setReadType(ReadType.NORMAL_READ); Review Comment: Nice Catch, that seemed redundant, hence removed anujmodi2021 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2250348323 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java: ########## @@ -193,32 +213,33 @@ public void setListener(Listener listener) { public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) { clientRequestId = UUID.randomUUID().toString(); switch (format) { - case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty - header = - clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\" - + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID - + \":\" + opType + \":\" + retryCount; - header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation); - if (!(ingressHandler.equals(EMPTY_STRING))) { - header += \":\" + ingressHandler; - } - if (!(position.equals(EMPTY_STRING))) { - header += \":\" + position; - } - if (operatedBlobCount != null) { - header += (\":\" + operatedBlobCount); - } - header += (\":\" + httpOperation.getTracingContextSuffix()); - metricHeader += !(metricResults.trim().isEmpty()) ? metricResults : \"\"; + case ALL_ID_FORMAT: + header = TracingHeaderVersion.V1.getVersion() + COLON + + clientCorrelationID + COLON + + clientRequestId + COLON + + fileSystemID + COLON + + getPrimaryRequestIdForHeader(retryCount > 0) + COLON + + streamID + COLON + + opType + COLON + + getRetryHeader(previousFailure, retryPolicyAbbreviation) + COLON + + ingressHandler + COLON + + position + COLON + + operatedBlobCount + COLON + + httpOperation.getTracingContextSuffix() + COLON + + getOperationSpecificHeader(opType); Review Comment: Sounds Better, Taken ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java: ########## @@ -265,6 +286,34 @@ private String addFailureReasons(final String header, return String.format(\"%s_%s\", header, previousFailure); } + private String getRetryHeader(final String previousFailure, String retryPolicyAbbreviation) { Review Comment: Taken anujmodi2021 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2250349750 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java: ########## @@ -0,0 +1,50 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.utils; + +public enum TracingHeaderVersion { + + V0(\"\", 8), + V1(\"v1\", 13); Review Comment: We will have simple version strings like v0, v1, v2 and so on. This will help reduce char count in clientReqId. With any new changes in the schema of Tracing Header (add/delete/rearrange) we need to bump up version and update the schema and getCurrentVersion method to return the latest version. ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java: ########## @@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception { in.close(); } + @Test + public void testReadTypeInTracingContextHeader() throws Exception { + AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem()); + AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore()); + AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration()); + AbfsClient spiedClient = Mockito.spy(spiedStore.getClient()); + Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize(); + Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize(); + Mockito.doReturn(spiedClient).when(spiedStore).getClient(); + Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore(); + Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration(); + int numOfReadCalls = 0; + int fileSize = 0; + + /* + * Test to verify Normal Read Type. + * Disabling read ahead ensures that read type is normal read. + */ + fileSize = 3 * ONE_MB; // To make sure multiple blocks are read. + numOfReadCalls += 3; // 3 blocks of 1MB each. + doReturn(false).when(spiedConfig).isReadAheadV2Enabled(); + doReturn(false).when(spiedConfig).isReadAheadEnabled(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls); + + /* + * Test to verify Missed Cache Read Type. + * Setting read ahead depth to 0 ensure that nothing can be got from prefetch. + * In such a case Input Stream will do a sequential read with missed cache read type. + */ + fileSize = ONE_MB; // To make sure only one block is read. + numOfReadCalls += 1; // 1 block of 1MB. + Mockito.doReturn(0).when(spiedConfig).getReadAheadQueueDepth(); + doReturn(true).when(spiedConfig).isReadAheadEnabled(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, MISSEDCACHE_READ, numOfReadCalls); + + /* + * Test to verify Prefetch Read Type. + * Setting read ahead depth to 2 with prefetch enabled ensures that prefetch is done. + * First read here might be Normal or Missed Cache but the rest 2 should be Prefetched Read. + */ + fileSize = 3 * ONE_MB; // To make sure multiple blocks are read. + numOfReadCalls += 3; + doReturn(true).when(spiedConfig).isReadAheadEnabled(); + Mockito.doReturn(3).when(spiedConfig).getReadAheadQueueDepth(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, PREFETCH_READ, numOfReadCalls); + + /* + * Test to verify Footer Read Type. + * Having file size less than footer read size and disabling small file opt + */ + fileSize = 8 * ONE_KB; + numOfReadCalls += 1; // Full file will be read along with footer. + doReturn(false).when(spiedConfig).readSmallFilesCompletely(); + doReturn(true).when(spiedConfig).optimizeFooterRead(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, FOOTER_READ, numOfReadCalls); + + /* + * Test to verify Small File Read Type. + * Having file size less than footer read size and disabling small file opt + */ + fileSize = 8 * ONE_KB; + numOfReadCalls += 1; // Full file will be read along with footer. + doReturn(true).when(spiedConfig).readSmallFilesCompletely(); + doReturn(false).when(spiedConfig).optimizeFooterRead(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, SMALLFILE_READ, numOfReadCalls); + } + + private void testReadTypeInTracingContextHeaderInternal(AzureBlobFileSystem fs, int fileSize, ReadType readType, int numOfReadCalls) throws Exception { + Path testPath = new Path(\"testFile\"); + byte[] fileContent = getRandomBytesArray(fileSize); + try (FSDataOutputStream oStream = fs.create(testPath)) { + oStream.write(fileContent); + oStream.flush(); + } + try (FSDataInputStream iStream = fs.open(testPath)) { + int bytesRead = iStream.read(new byte[fileSize], 0, + fileSize); + Assertions.assertThat(fileSize) + .describedAs(\"Read size should match file size\") + .isEqualTo(bytesRead); + } + + ArgumentCaptor<String> captor1 = ArgumentCaptor.forClass(String.class); + ArgumentCaptor<Long> captor2 = ArgumentCaptor.forClass(Long.class); + ArgumentCaptor<byte[]> captor3 = ArgumentCaptor.forClass(byte[].class); + ArgumentCaptor<Integer> captor4 = ArgumentCaptor.forClass(Integer.class); + ArgumentCaptor<Integer> captor5 = ArgumentCaptor.forClass(Integer.class); + ArgumentCaptor<String> captor6 = ArgumentCaptor.forClass(String.class); + ArgumentCaptor<String> captor7 = ArgumentCaptor.forClass(String.class); + ArgumentCaptor<ContextEncryptionAdapter> captor8 = ArgumentCaptor.forClass(ContextEncryptionAdapter.class); + ArgumentCaptor<TracingContext> captor9 = ArgumentCaptor.forClass(TracingContext.class); + + verify(fs.getAbfsStore().getClient(), times(numOfReadCalls)).read( + captor1.capture(), captor2.capture(), captor3.capture(), + captor4.capture(), captor5.capture(), captor6.capture(), + captor7.capture(), captor8.capture(), captor9.capture()); + TracingContext tracingContext = captor9.getAllValues().get(numOfReadCalls - 1); + verifyHeaderForReadTypeInTracingContextHeader(tracingContext, readType); + } + + private void verifyHeaderForReadTypeInTracingContextHeader(TracingContext tracingContext, ReadType readType) { + AbfsHttpOperation mockOp = Mockito.mock(AbfsHttpOperation.class); + doReturn(EMPTY_STRING).when(mockOp).getTracingContextSuffix(); + tracingContext.constructHeader(mockOp, null, null); + String[] idList = tracingContext.getHeader().split(COLON, SPLIT_NO_LIMIT); + Assertions.assertThat(idList).describedAs(\"Client Request Id should have all fields\").hasSize( + TracingHeaderVersion.getCurrentVersion().getFieldCount()); + Assertions.assertThat(tracingContext.getHeader()).describedAs(\"Operation Type Should Be Read\") + .contains(FSOperationType.READ.toString()); + Assertions.assertThat(tracingContext.getHeader()).describedAs(\"Read type in tracing context header should match\") + .contains(readType.toString()); + } + +// private testReadTypeInTracingContextHeaderInternal(ReadType readType) throws Exception { Review Comment: Removed anujmodi2021 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2250351378 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java: ########## @@ -77,8 +81,7 @@ public class TracingContext { * this field shall not be set. */ private String primaryRequestIdForRetry; - - private Integer operatedBlobCount = null; + private Integer operatedBlobCount = 1; // Only relevant for rename-delete over blob endpoint where it will be explicitly set. Review Comment: Because it was coming out as null in ClientReqId. Having a null value does not looks good and can be prone to NPE if someone used this value anywhere. Since this is set only in rename/delete other ops are prone to NPE. As to why set to 1, I thought for every operation this has to be 1. I am open to suggestions for a better default value but strongly feel null should be avoided. Thoughts? anujmodi2021 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2250351858 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java: ########## @@ -0,0 +1,50 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.utils; + +public enum TracingHeaderVersion { + + V0(\"\", 8), + V1(\"v1\", 13); + + private final String version; + private final int fieldCount; + + TracingHeaderVersion(String version, int fieldCount) { + this.version = version; + this.fieldCount = fieldCount; + } + + @Override + public String toString() { + return version; + } + + public static TracingHeaderVersion getCurrentVersion() { + return V1; Review Comment: We need to update it to the latest version every time we do a version upgrade. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java: ########## @@ -193,32 +213,33 @@ public void setListener(Listener listener) { public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) { clientRequestId = UUID.randomUUID().toString(); switch (format) { - case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty - header = - clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\" - + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID - + \":\" + opType + \":\" + retryCount; - header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation); - if (!(ingressHandler.equals(EMPTY_STRING))) { - header += \":\" + ingressHandler; - } - if (!(position.equals(EMPTY_STRING))) { - header += \":\" + position; - } - if (operatedBlobCount != null) { - header += (\":\" + operatedBlobCount); - } - header += (\":\" + httpOperation.getTracingContextSuffix()); - metricHeader += !(metricResults.trim().isEmpty()) ? metricResults : \"\"; + case ALL_ID_FORMAT: + header = TracingHeaderVersion.V1.getVersion() + COLON Review Comment: Fixed anujmodi2021 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2250353075 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java: ########## @@ -193,32 +213,33 @@ public void setListener(Listener listener) { public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) { clientRequestId = UUID.randomUUID().toString(); switch (format) { - case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty - header = - clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\" - + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID - + \":\" + opType + \":\" + retryCount; - header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation); - if (!(ingressHandler.equals(EMPTY_STRING))) { - header += \":\" + ingressHandler; - } - if (!(position.equals(EMPTY_STRING))) { - header += \":\" + position; - } - if (operatedBlobCount != null) { - header += (\":\" + operatedBlobCount); - } - header += (\":\" + httpOperation.getTracingContextSuffix()); - metricHeader += !(metricResults.trim().isEmpty()) ? metricResults : \"\"; + case ALL_ID_FORMAT: + header = TracingHeaderVersion.V1.getVersion() + COLON + + clientCorrelationID + COLON + + clientRequestId + COLON + + fileSystemID + COLON + + getPrimaryRequestIdForHeader(retryCount > 0) + COLON + + streamID + COLON + + opType + COLON + + getRetryHeader(previousFailure, retryPolicyAbbreviation) + COLON + + ingressHandler + COLON Review Comment: With empty checks we cannot have a fixed schema. We need the proper defined schema where each position after split is fixed for all the headers and analysis can be done easily without worrying about the position of info we need to analyse. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java: ########## @@ -193,32 +213,33 @@ public void setListener(Listener listener) { public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) { clientRequestId = UUID.randomUUID().toString(); switch (format) { - case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty - header = - clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\" - + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID - + \":\" + opType + \":\" + retryCount; - header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation); - if (!(ingressHandler.equals(EMPTY_STRING))) { - header += \":\" + ingressHandler; - } - if (!(position.equals(EMPTY_STRING))) { - header += \":\" + position; - } - if (operatedBlobCount != null) { - header += (\":\" + operatedBlobCount); - } - header += (\":\" + httpOperation.getTracingContextSuffix()); - metricHeader += !(metricResults.trim().isEmpty()) ? metricResults : \"\"; + case ALL_ID_FORMAT: + header = TracingHeaderVersion.V1.getVersion() + COLON + + clientCorrelationID + COLON + + clientRequestId + COLON + + fileSystemID + COLON + + getPrimaryRequestIdForHeader(retryCount > 0) + COLON + + streamID + COLON + + opType + COLON + + getRetryHeader(previousFailure, retryPolicyAbbreviation) + COLON + + ingressHandler + COLON + + position + COLON + + operatedBlobCount + COLON + + httpOperation.getTracingContextSuffix() + COLON + + getOperationSpecificHeader(opType); + + metricHeader += !(metricResults.trim().isEmpty()) ? metricResults : EMPTY_STRING; break; case TWO_ID_FORMAT: - header = clientCorrelationID + \":\" + clientRequestId; - metricHeader += !(metricResults.trim().isEmpty()) ? metricResults : \"\"; + header = TracingHeaderVersion.V1.getVersion() + COLON Review Comment: Fixed anujmodi2021 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2250353287 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestTracingContext.java: ########## @@ -326,8 +329,8 @@ fileSystemId, FSOperationType.CREATE_FILESYSTEM, tracingHeaderFormat, new Tracin } private void checkHeaderForRetryPolicyAbbreviation(String header, String expectedFailureReason, String expectedRetryPolicyAbbreviation) { - String[] headerContents = header.split(\":\"); - String previousReqContext = headerContents[6]; + String[] headerContents = header.split(\":\", SPLIT_NO_LIMIT); Review Comment: Taken ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java: ########## @@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception { in.close(); } + @Test + public void testReadTypeInTracingContextHeader() throws Exception { + AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem()); + AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore()); + AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration()); + AbfsClient spiedClient = Mockito.spy(spiedStore.getClient()); + Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize(); + Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize(); + Mockito.doReturn(spiedClient).when(spiedStore).getClient(); + Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore(); + Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration(); + int numOfReadCalls = 0; + int fileSize = 0; + + /* + * Test to verify Normal Read Type. + * Disabling read ahead ensures that read type is normal read. + */ + fileSize = 3 * ONE_MB; // To make sure multiple blocks are read. + numOfReadCalls += 3; // 3 blocks of 1MB each. + doReturn(false).when(spiedConfig).isReadAheadV2Enabled(); + doReturn(false).when(spiedConfig).isReadAheadEnabled(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls); Review Comment: Taken anujmodi2021 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2250353532 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java: ########## @@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception { in.close(); } + @Test + public void testReadTypeInTracingContextHeader() throws Exception { + AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem()); + AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore()); + AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration()); + AbfsClient spiedClient = Mockito.spy(spiedStore.getClient()); + Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize(); + Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize(); + Mockito.doReturn(spiedClient).when(spiedStore).getClient(); + Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore(); + Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration(); + int numOfReadCalls = 0; + int fileSize = 0; + + /* + * Test to verify Normal Read Type. + * Disabling read ahead ensures that read type is normal read. + */ + fileSize = 3 * ONE_MB; // To make sure multiple blocks are read. + numOfReadCalls += 3; // 3 blocks of 1MB each. + doReturn(false).when(spiedConfig).isReadAheadV2Enabled(); + doReturn(false).when(spiedConfig).isReadAheadEnabled(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls); + + /* + * Test to verify Missed Cache Read Type. + * Setting read ahead depth to 0 ensure that nothing can be got from prefetch. + * In such a case Input Stream will do a sequential read with missed cache read type. + */ + fileSize = ONE_MB; // To make sure only one block is read. + numOfReadCalls += 1; // 1 block of 1MB. + Mockito.doReturn(0).when(spiedConfig).getReadAheadQueueDepth(); + doReturn(true).when(spiedConfig).isReadAheadEnabled(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, MISSEDCACHE_READ, numOfReadCalls); + + /* + * Test to verify Prefetch Read Type. + * Setting read ahead depth to 2 with prefetch enabled ensures that prefetch is done. + * First read here might be Normal or Missed Cache but the rest 2 should be Prefetched Read. + */ + fileSize = 3 * ONE_MB; // To make sure multiple blocks are read. + numOfReadCalls += 3; + doReturn(true).when(spiedConfig).isReadAheadEnabled(); + Mockito.doReturn(3).when(spiedConfig).getReadAheadQueueDepth(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, PREFETCH_READ, numOfReadCalls); Review Comment: Taken ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java: ########## @@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception { in.close(); } + @Test + public void testReadTypeInTracingContextHeader() throws Exception { + AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem()); + AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore()); + AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration()); + AbfsClient spiedClient = Mockito.spy(spiedStore.getClient()); + Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize(); + Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize(); + Mockito.doReturn(spiedClient).when(spiedStore).getClient(); + Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore(); + Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration(); + int numOfReadCalls = 0; + int fileSize = 0; + + /* + * Test to verify Normal Read Type. + * Disabling read ahead ensures that read type is normal read. + */ + fileSize = 3 * ONE_MB; // To make sure multiple blocks are read. + numOfReadCalls += 3; // 3 blocks of 1MB each. + doReturn(false).when(spiedConfig).isReadAheadV2Enabled(); + doReturn(false).when(spiedConfig).isReadAheadEnabled(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls); + + /* + * Test to verify Missed Cache Read Type. + * Setting read ahead depth to 0 ensure that nothing can be got from prefetch. + * In such a case Input Stream will do a sequential read with missed cache read type. + */ + fileSize = ONE_MB; // To make sure only one block is read. + numOfReadCalls += 1; // 1 block of 1MB. + Mockito.doReturn(0).when(spiedConfig).getReadAheadQueueDepth(); + doReturn(true).when(spiedConfig).isReadAheadEnabled(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, MISSEDCACHE_READ, numOfReadCalls); + + /* + * Test to verify Prefetch Read Type. + * Setting read ahead depth to 2 with prefetch enabled ensures that prefetch is done. + * First read here might be Normal or Missed Cache but the rest 2 should be Prefetched Read. + */ + fileSize = 3 * ONE_MB; // To make sure multiple blocks are read. + numOfReadCalls += 3; + doReturn(true).when(spiedConfig).isReadAheadEnabled(); + Mockito.doReturn(3).when(spiedConfig).getReadAheadQueueDepth(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, PREFETCH_READ, numOfReadCalls); + + /* + * Test to verify Footer Read Type. + * Having file size less than footer read size and disabling small file opt + */ + fileSize = 8 * ONE_KB; + numOfReadCalls += 1; // Full file will be read along with footer. + doReturn(false).when(spiedConfig).readSmallFilesCompletely(); + doReturn(true).when(spiedConfig).optimizeFooterRead(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, FOOTER_READ, numOfReadCalls); + + /* + * Test to verify Small File Read Type. + * Having file size less than footer read size and disabling small file opt + */ + fileSize = 8 * ONE_KB; + numOfReadCalls += 1; // Full file will be read along with footer. + doReturn(true).when(spiedConfig).readSmallFilesCompletely(); + doReturn(false).when(spiedConfig).optimizeFooterRead(); + testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, SMALLFILE_READ, numOfReadCalls); + } Review Comment: Added anujmodi2021 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2250353998 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java: ########## @@ -578,6 +591,7 @@ int readRemote(long position, byte[] b, int offset, int length, TracingContext t streamStatistics.remoteReadOperation(); } LOG.trace(\"Trigger client.read for path={} position={} offset={} length={}\", path, position, offset, length); + tracingContext.setPosition(String.valueOf(position)); Review Comment: Thanks for the suggestion. I updated the current test to assert on position as well. hadoop-yetus commented on PR #7837: URL: https://github.com/apache/hadoop/pull/7837#issuecomment-3149195033 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 7 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 26m 49s | | trunk passed | | +1 :green_heart: | compile | 0m 26s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 23s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 30s | | trunk passed | | +1 :green_heart: | javadoc | 0m 28s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 22s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 47s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 6s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 14s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 22s | | the patch passed | | +1 :green_heart: | javadoc | 0m 17s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 44s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 52s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 25s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 26s | | The patch does not generate ASF License warnings. | | | | 79m 1s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/7/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7837 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 5a4dce188fdc 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 6345ec99eeb23cd09b5d7197e777b1dec23a35e0 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/7/testReport/ | | Max. process+thread count | 559 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/7/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. anmolanmol1234 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2250858558 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java: ########## @@ -77,8 +81,7 @@ public class TracingContext { * this field shall not be set. */ private String primaryRequestIdForRetry; - - private Integer operatedBlobCount = null; + private Integer operatedBlobCount = 1; // Only relevant for rename-delete over blob endpoint where it will be explicitly set. Review Comment: But there was a null check before it was added to the header which would avoid the NPE anmolanmol1234 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2250885278 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java: ########## @@ -886,40 +928,54 @@ private void testReadTypeInTracingContextHeaderInternal(AzureBlobFileSystem fs, ArgumentCaptor<ContextEncryptionAdapter> captor8 = ArgumentCaptor.forClass(ContextEncryptionAdapter.class); ArgumentCaptor<TracingContext> captor9 = ArgumentCaptor.forClass(TracingContext.class); - verify(fs.getAbfsStore().getClient(), times(numOfReadCalls)).read( + verify(fs.getAbfsStore().getClient(), times(totalReadCalls)).read( captor1.capture(), captor2.capture(), captor3.capture(), captor4.capture(), captor5.capture(), captor6.capture(), captor7.capture(), captor8.capture(), captor9.capture()); - TracingContext tracingContext = captor9.getAllValues().get(numOfReadCalls - 1); - verifyHeaderForReadTypeInTracingContextHeader(tracingContext, readType); + List<TracingContext> tracingContextList = captor9.getAllValues(); + if (readType == PREFETCH_READ) { + /* + * For Prefetch Enabled, first read can be Normal or Missed Cache Read. + * Sow e will assert only for last 2 calls which should be Prefetched Read. Review Comment: nit typo: so anmolanmol1234 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2250972747 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderValidator.java: ########## @@ -206,7 +207,7 @@ public void updateReadType(ReadType readType) { } /** - * Sets the value of the number of blobs operated on + * Sets the value of the number of blobs operated on976345 Review Comment: typo issue ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java: ########## @@ -886,40 +928,54 @@ private void testReadTypeInTracingContextHeaderInternal(AzureBlobFileSystem fs, ArgumentCaptor<ContextEncryptionAdapter> captor8 = ArgumentCaptor.forClass(ContextEncryptionAdapter.class); ArgumentCaptor<TracingContext> captor9 = ArgumentCaptor.forClass(TracingContext.class); - verify(fs.getAbfsStore().getClient(), times(numOfReadCalls)).read( + verify(fs.getAbfsStore().getClient(), times(totalReadCalls)).read( captor1.capture(), captor2.capture(), captor3.capture(), captor4.capture(), captor5.capture(), captor6.capture(), captor7.capture(), captor8.capture(), captor9.capture()); - TracingContext tracingContext = captor9.getAllValues().get(numOfReadCalls - 1); - verifyHeaderForReadTypeInTracingContextHeader(tracingContext, readType); + List<TracingContext> tracingContextList = captor9.getAllValues(); + if (readType == PREFETCH_READ) { + /* + * For Prefetch Enabled, first read can be Normal or Missed Cache Read. + * Sow e will assert only for last 2 calls which should be Prefetched Read. + * Since calls are asynchronous, we can not guarantee the order of calls. + * Therefore, we cannot assert on exact position here. + */ + for (int i = tracingContextList.size() - (numOfReadCalls - 1); i < tracingContextList.size(); i++) { + verifyHeaderForReadTypeInTracingContextHeader(tracingContextList.get(i), readType, -1); + } + } else if (readType == DIRECT_READ) { + int expectedReadPos = ONE_MB/3; Review Comment: comment for why are we starting with this position will help in clarity ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestApacheHttpClientFallback.java: ########## @@ -61,15 +61,15 @@ private TracingContext getSampleTracingContext(int[] jdkCallsRegister, answer.callRealMethod(); AbfsHttpOperation op = answer.getArgument(0); if (op instanceof AbfsAHCHttpOperation) { - Assertions.assertThat(tc.getHeader()).contains(APACHE_IMPL); + Assertions.assertThat(tc.getHeader()).endsWith(APACHE_IMPL); apacheCallsRegister[0]++; } if (op instanceof AbfsJdkHttpOperation) { jdkCallsRegister[0]++; if (AbfsApacheHttpClient.usable()) { - Assertions.assertThat(tc.getHeader()).contains(JDK_IMPL); + Assertions.assertThat(tc.getHeader()).endsWith(JDK_IMPL); Review Comment: this might fail if we add new header where the network library is not maintained as the last header, so contains looks better to me bhattmanish98 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2251145110 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java: ########## @@ -64,9 +67,10 @@ public class TracingContext { //final concatenated ID list set into x-ms-client-request-id header private String header = EMPTY_STRING; private String ingressHandler = EMPTY_STRING; - private String position = EMPTY_STRING; + private String position = String.valueOf(0); // position of read/write in remote file Review Comment: Any reason we are changing this default value? anujmodi2021 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2251498573 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java: ########## @@ -77,8 +81,7 @@ public class TracingContext { * this field shall not be set. */ private String primaryRequestIdForRetry; - - private Integer operatedBlobCount = null; + private Integer operatedBlobCount = 1; // Only relevant for rename-delete over blob endpoint where it will be explicitly set. Review Comment: Yes but we decided to keep the header schema fix and publishing this value as null does not look good in Client Request Id as it can be exposed to user. anujmodi2021 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2251499536 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java: ########## @@ -64,9 +67,10 @@ public class TracingContext { //final concatenated ID list set into x-ms-client-request-id header private String header = EMPTY_STRING; private String ingressHandler = EMPTY_STRING; - private String position = EMPTY_STRING; + private String position = String.valueOf(0); // position of read/write in remote file Review Comment: No reason, will revert. anujmodi2021 commented on code in PR #7837: URL: https://github.com/apache/hadoop/pull/7837#discussion_r2251505718 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java: ########## @@ -886,40 +928,54 @@ private void testReadTypeInTracingContextHeaderInternal(AzureBlobFileSystem fs, ArgumentCaptor<ContextEncryptionAdapter> captor8 = ArgumentCaptor.forClass(ContextEncryptionAdapter.class); ArgumentCaptor<TracingContext> captor9 = ArgumentCaptor.forClass(TracingContext.class); - verify(fs.getAbfsStore().getClient(), times(numOfReadCalls)).read( + verify(fs.getAbfsStore().getClient(), times(totalReadCalls)).read( captor1.capture(), captor2.capture(), captor3.capture(), captor4.capture(), captor5.capture(), captor6.capture(), captor7.capture(), captor8.capture(), captor9.capture()); - TracingContext tracingContext = captor9.getAllValues().get(numOfReadCalls - 1); - verifyHeaderForReadTypeInTracingContextHeader(tracingContext, readType); + List<TracingContext> tracingContextList = captor9.getAllValues(); + if (readType == PREFETCH_READ) { + /* + * For Prefetch Enabled, first read can be Normal or Missed Cache Read. + * Sow e will assert only for last 2 calls which should be Prefetched Read. + * Since calls are asynchronous, we can not guarantee the order of calls. + * Therefore, we cannot assert on exact position here. + */ + for (int i = tracingContextList.size() - (numOfReadCalls - 1); i < tracingContextList.size(); i++) { + verifyHeaderForReadTypeInTracingContextHeader(tracingContextList.get(i), readType, -1); + } + } else if (readType == DIRECT_READ) { + int expectedReadPos = ONE_MB/3; Review Comment: Already added in comment above. hadoop-yetus commented on PR #7837: URL: https://github.com/apache/hadoop/pull/7837#issuecomment-3151118542 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 7 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 27m 44s | | trunk passed | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 24s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 27s | | trunk passed | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 23s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 47s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 7s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 15s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 15s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 13s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 19s | | the patch passed | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 44s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 36s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 25s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 25s | | The patch does not generate ASF License warnings. | | | | 79m 32s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/8/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7837 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 2f9b7814e558 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 9059784765d6d3519f9649968b07fd94ef9798e8 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/8/testReport/ | | Max. process+thread count | 554 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/8/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7837: URL: https://github.com/apache/hadoop/pull/7837#issuecomment-3152193026 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 49s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 7 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 45m 11s | | trunk passed | | +1 :green_heart: | compile | 0m 44s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 37s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 42s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 9s | | trunk passed | | +1 :green_heart: | shadedclient | 40m 38s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 1s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 20s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 30s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 40m 51s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 56s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 142m 5s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/9/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7837 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 3280cde505cb 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c787d3b39af6bacd3d9a003fe3b6f81ccc10e194 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/9/testReport/ | | Max. process+thread count | 536 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/9/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. anujmodi2021 merged PR #7837: URL: https://github.com/apache/hadoop/pull/7837 anujmodi2021 opened a new pull request, #7881: URL: https://github.com/apache/hadoop/pull/7881 ### Description of PR Backport for 3.4 Jira: https://issues.apache.org/jira/browse/HADOOP-19645 anujmodi2021 commented on PR #7881: URL: https://github.com/apache/hadoop/pull/7881#issuecomment-3197856944 ------------------------------ :::: AGGREGATED TEST RESULT :::: ============================================================ HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 876, Failures: 0, Errors: 0, Skipped: 223 [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 32 [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 23 ============================================================ HNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 876, Failures: 0, Errors: 0, Skipped: 172 [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 32 [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 10 ============================================================ NonHNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 395 [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 33 [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 11 ============================================================ AppendBlob-HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 876, Failures: 0, Errors: 0, Skipped: 234 [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 56 [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-SharedKey-Blob ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 285 [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 27 [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 11 ============================================================ NonHNS-OAuth-DFS ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 400 [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 33 [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 24 ============================================================ NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 301 [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 27 [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 24 ============================================================ AppendBlob-NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 346 [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 51 [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 24 ============================================================ HNS-Oauth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 876, Failures: 0, Errors: 0, Skipped: 356 [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 32 [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-OAuth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 397 [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 33 [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 24 hadoop-yetus commented on PR #7881: URL: https://github.com/apache/hadoop/pull/7881#issuecomment-3198211215 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 12m 11s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 7 new or modified test files. | |||| _ branch-3.4 Compile Tests _ | | +1 :green_heart: | mvninstall | 36m 33s | | branch-3.4 passed | | +1 :green_heart: | compile | 0m 41s | | branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 39s | | branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 34s | | branch-3.4 passed | | +1 :green_heart: | mvnsite | 0m 43s | | branch-3.4 passed | | +1 :green_heart: | javadoc | 0m 41s | | branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 36s | | branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 10s | | branch-3.4 passed | | +1 :green_heart: | shadedclient | 32m 30s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 21s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 31s | | the patch passed | | +1 :green_heart: | javadoc | 0m 27s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 27s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 32m 13s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 27s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 127m 32s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7881/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7881 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux da6887b03eda 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | branch-3.4 / dc1821731086534899f15dc2abcc2d9bf2c61ae4 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7881/1/testReport/ | | Max. process+thread count | 728 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7881/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. anujmodi2021 merged PR #7881: URL: https://github.com/apache/hadoop/pull/7881", "created": "2025-07-29T12:45:56.000+0000", "updated": "2025-08-26T03:58:21.000+0000", "derived": {"summary_task": "Summarize this issue: There are a number of ways in which ABFS driver can trigger a network call to read data. We need a way to identify what type of read call was made from client. Plan is to add an indication for this in already present ClientRequestId header. Following are types of read we want to identify: # Direct Read: Read from a given position in remote file. This will be synchronous read # Normal Read: Read fr", "classification_task": "Classify the issue priority and type: There are a number of ways in which ABFS driver can trigger a network call to read data. We need a way to identify what type of read call was made from client. Plan is to add an indication for this in already present ClientRequestId header. Following are types of read we want to identify: # Direct Read: Read from a given position in remote file. This will be synchronous read # Normal Read: Read fr", "qna_task": "Question: What is this issue about?\nAnswer: ABFS: [ReadAheadV2] Improve Metrics for Read Calls to identify type of read done."}}
{"id": "13624880", "key": "HADOOP-19644", "project": "HADOOP", "summary": "ABFS: [ReadAheadV2] Negative tests for Read Buffer Manager V2 and dynamic scaling", "description": "", "comments": "", "created": "2025-07-29T10:52:56.000+0000", "updated": "2025-07-29T10:53:52.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: ABFS: [ReadAheadV2] Negative tests for Read Buffer Manager V2 and dynamic scaling"}}
{"id": "13624878", "key": "HADOOP-19643", "project": "HADOOP", "summary": "upgrade gson due to security fixes", "description": "not sure why https://github.com/google/gson/pull/2588 didn't attract a CVE linked to https://issues.apache.org/jira/browse/HADOOP-19632 - nimbus-jose-jwt uses gson", "comments": "pjfanning opened a new pull request, #7833: URL: https://github.com/apache/hadoop/pull/7833 Update LICENSE-binary <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR https://issues.apache.org/jira/browse/HADOOP-19643 ### How was this patch tested? ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #7833: URL: https://github.com/apache/hadoop/pull/7833#issuecomment-3134038336 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 34s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 23s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 33m 2s | | trunk passed | | +1 :green_heart: | compile | 16m 49s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 35s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 22m 44s | | trunk passed | | +1 :green_heart: | javadoc | 9m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 49s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 51m 52s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 37s | | Maven dependency ordering for patch | | -1 :x: | mvninstall | 30m 48s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7833/1/artifact/out/patch-mvninstall-root.txt) | root in the patch failed. | | +1 :green_heart: | compile | 15m 17s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 17s | | the patch passed | | +1 :green_heart: | compile | 13m 51s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 51s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 19m 40s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 9m 36s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 8m 22s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 56m 3s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 339m 51s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7833/1/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 16s | | The patch does not generate ASF License warnings. | | | | 629m 8s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.federation.policies.amrmproxy.TestLocalityMulticastAMRMProxyPolicy | | | hadoop.crypto.key.kms.server.TestKMSAudit | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7833/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7833 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux 3d08bdf50c29 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 04292f85b2cde016fe4e4c12bfe4c8cd1c040dd4 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7833/1/testReport/ | | Max. process+thread count | 3066 (vs. ulimit of 5500) | | modules | C: hadoop-project . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7833/1/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-07-29T10:11:17.000+0000", "updated": "2025-07-29T20:51:34.000+0000", "derived": {"summary_task": "Summarize this issue: not sure why https://github.com/google/gson/pull/2588 didn't attract a CVE linked to https://issues.apache.org/jira/browse/HADOOP-19632 - nimbus-jose-jwt uses gson", "classification_task": "Classify the issue priority and type: not sure why https://github.com/google/gson/pull/2588 didn't attract a CVE linked to https://issues.apache.org/jira/browse/HADOOP-19632 - nimbus-jose-jwt uses gson", "qna_task": "Question: What is this issue about?\nAnswer: upgrade gson due to security fixes"}}
{"id": "13624874", "key": "HADOOP-19642", "project": "HADOOP", "summary": "upgrade nimbus-jose-jwt due to CVE-2025-53864", "description": "https://www.cve.org/CVERecord?id=CVE-2025-53864", "comments": "Hi [~fanningpj] , working on a patch for this. I had raised Jira for the same https://issues.apache.org/jira/browse/HADOOP-19632.", "created": "2025-07-29T09:37:38.000+0000", "updated": "2025-07-29T09:54:35.000+0000", "derived": {"summary_task": "Summarize this issue: https://www.cve.org/CVERecord?id=CVE-2025-53864", "classification_task": "Classify the issue priority and type: https://www.cve.org/CVERecord?id=CVE-2025-53864", "qna_task": "Question: What is this issue about?\nAnswer: upgrade nimbus-jose-jwt due to CVE-2025-53864"}}
{"id": "13624851", "key": "HADOOP-19641", "project": "HADOOP", "summary": "ABFS: [ReadAheadV2] First Read should bypass ReadBufferManager", "description": "We have observed this across multiple workload runs that when we start reading data from input stream. The first read which came to input stream has to be read synchronously even if we trigger prefetch request for that particular offset. Most of the times we end up doing extra work of checking if the prefetch is trigerred, removing prefetch from the pending queue and go ahead to do a direct remote read in workload thread itself. To avoid all this overhead, we will always bypass read ahead for the very first read of each input stream and trigger read aheads for second read onwards.", "comments": "are you using the openFile seek policy as suggested? parquet will tell you when its a parquet file and its read policy is common: 8 byte footer, reall footer, rowgroups. Thanks for the feedback steve. We will definitely incorporate that. I will hold onto this PR and will make this change with the Read Policy suggested by user taken into consideration. Will work diligently on all the read policies and have reads happening in way optimal for each one of them.", "created": "2025-07-29T07:01:30.000+0000", "updated": "2025-08-07T07:55:08.000+0000", "derived": {"summary_task": "Summarize this issue: We have observed this across multiple workload runs that when we start reading data from input stream. The first read which came to input stream has to be read synchronously even if we trigger prefetch request for that particular offset. Most of the times we end up doing extra work of checking if the prefetch is trigerred, removing prefetch from the pending queue and go ahead to do a direct remote", "classification_task": "Classify the issue priority and type: We have observed this across multiple workload runs that when we start reading data from input stream. The first read which came to input stream has to be read synchronously even if we trigger prefetch request for that particular offset. Most of the times we end up doing extra work of checking if the prefetch is trigerred, removing prefetch from the pending queue and go ahead to do a direct remote", "qna_task": "Question: What is this issue about?\nAnswer: ABFS: [ReadAheadV2] First Read should bypass ReadBufferManager"}}
{"id": "13624477", "key": "HADOOP-19640", "project": "HADOOP", "summary": "Resource leak in AssumedRoleCredentialProvider", "description": "When `org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider` is used in a Hadoop Configuration object, it will attempt to `resolveCredentials()` inside the constructor. (lines 165-167) {code:java} // and force in a fail-fast check just to keep the stack traces less // convoluted resolveCredentials();{code} If this method fails, because the current identity is not able to assume role, the constructor will throw an exception, and fail to close the `stsClient`, `stsProvider` and any other resources that are created in the constructor, leaking threads and other resources. In a long running application, that handles Hadoop S3 file systems, where the user can dynamically change to configured role to assume, and external id, this will lead to eventually the system running out of resources due to the leaked threads created by the AWS SDK clients that are not closed when a wrong role or external id is used. There are two potential fixes for this problem: - Don't attempt to `resolveCredentials()` inside the constructor - Wrap the `resolveCredentials()` in the constructor in a try/catch block, that cleans the resources and rethrows the exception in the catch block.", "comments": "", "created": "2025-07-24T08:01:58.000+0000", "updated": "2025-07-24T08:02:55.000+0000", "derived": {"summary_task": "Summarize this issue: When `org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider` is used in a Hadoop Configuration object, it will attempt to `resolveCredentials()` inside the constructor. (lines 165-167) {code:java} // and force in a fail-fast check just to keep the stack traces less // convoluted resolveCredentials();{code} If this method fails, because the current identity is not able to assume role, the con", "classification_task": "Classify the issue priority and type: When `org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider` is used in a Hadoop Configuration object, it will attempt to `resolveCredentials()` inside the constructor. (lines 165-167) {code:java} // and force in a fail-fast check just to keep the stack traces less // convoluted resolveCredentials();{code} If this method fails, because the current identity is not able to assume role, the con", "qna_task": "Question: What is this issue about?\nAnswer: Resource leak in AssumedRoleCredentialProvider"}}
{"id": "13624415", "key": "HADOOP-19639", "project": "HADOOP", "summary": "SecretManager configuration at runtime", "description": "In case of TEZ *DAGAppMaster* the Hadoop *SecretManager* code can not read yarn config xml file, therefore the SELECTED_ALGORITHM and SELECTED_LENGTH variables in SecretManager can not be set at runtime. This can results with the following exception in FIPS environment: {code:java} java.security.InvalidParameterException: Key size for HMAC must be at least 112 bits in approved mode: SHA-1/HMAC at com.safelogic.cryptocomply.fips.core/com.safelogic.cryptocomply.jcajce.provider.BaseKeyGenerator.engineInit(Unknown Source) at java.base/javax.crypto.KeyGenerator.init(KeyGenerator.java:540) at java.base/javax.crypto.KeyGenerator.init(KeyGenerator.java:517) at org.apache.hadoop.security.token.SecretManager.<init>(SecretManager.java:157) at org.apache.hadoop.yarn.security.client.BaseClientToAMTokenSecretManager.<init>(BaseClientToAMTokenSecretManager.java:38) at org.apache.hadoop.yarn.security.client.ClientToAMTokenSecretManager.<init>(ClientToAMTokenSecretManager.java:46) at org.apache.tez.common.security.TezClientToAMTokenSecretManager.<init>(TezClientToAMTokenSecretManager.java:33) at org.apache.tez.dag.app.DAGAppMaster.serviceInit(DAGAppMaster.java:493) at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164) at org.apache.tez.dag.app.DAGAppMaster$9.run(DAGAppMaster.java:2649) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1910) at org.apache.tez.dag.app.DAGAppMaster.initAndStartAppMaster(DAGAppMaster.java:2646) at org.apache.tez.dag.app.DAGAppMaster.main(DAGAppMaster.java:2440) {code} To mitigate the problem we should provide some ability for the component to be able to modify the configuration without corresponding config files on class path.", "comments": "K0K0V0K opened a new pull request, #7827: URL: https://github.com/apache/hadoop/pull/7827 ### Description of PR - static configuration of SecretManager is required because it has some static method what use the selected algorithm - in case if class path not contains the config values (for example TEZ DAGAppMaster run) the default values will be loaded at runtime - the default values can cause failers in modern environments (they are not FIPS compliant) - new SecretManagerConfig created to be able to modify the SecretManager config without core-site.xml present on class path ### How was this patch tested? - Unit tests were run ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? let me add the motivation from Tez side as a context, so as [~bkosztolnik] mentioned we faced a problem after YARN-11738, because Tez is supposed to work by runtime payloads (containing all the hadoop config), but in the hadoop layer, the options are initialized in the static initializer, so tez cannot help with that exception, because new Configuration() depends on the core-site.xml config which is *maybe* on the classpath, otherwise, everything turns to default first I tried to make the TezClientToAMTokenSecretManager extend a custom Tez implementation of SecretManager, but it led to compilation issues, as Hadoop layers expect a Hadoop SecretManager (see RPC), so eventually we cannot pass a Tez SecretManager that doesn\u2019t inherit Hadoop\u2019s SecretManager, but as long as we extend the Hadoop one, the static initializer and then the field initializer of keyGen keyGen.init(SELECTED_LENGTH) will kick in immediately so this cannot be fixed from Tez, we have 2 options: 1. rework YARN-11738 (as this problem was also implied in this comment upstream) 2. make the core-site.xml localized to tez containers to have it picked up <- this is against tez design, so I would prefer 1) optimal way would be completely eliminate static fields from SecretManager, but I'm afraid they are there for a reason, so basically, anything could work for us which makes Tez able to intercept, and configure the SecretManager from a Configuration object, which is different than the default one (which is instantiated by new Configuration()) so this cannot be fixed from Tez, we have 2 options: 1. rework YARN-11738 (as this problem was also implied in this comment upstream) 2. make the core-site.xml localized to tez containers to have it picked up I\u2019m a bit against 2), because it\u2019s also a design decision to make file config resources available to tez containers instead of payload, so I would definitely be in favor of 1), here is where I need the opinion of Hadoop folks abstractdog commented on code in PR #7827: URL: https://github.com/apache/hadoop/pull/7827#discussion_r2228219825 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManagerConfig.java: ########## @@ -0,0 +1,123 @@ + +package org.apache.hadoop.security.token; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.CommonConfigurationKeysPublic; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import javax.crypto.KeyGenerator; +import javax.crypto.Mac; +import javax.crypto.SecretKey; +import java.security.NoSuchAlgorithmException; + +/** + * Provides configuration and utility methods for managing cryptographic key generation + * and message authentication code (MAC) generation using specified algorithms and key lengths. + * <p> + * This class supports static access to the selected cryptographic algorithm and key length, + * and provides methods to create configured {@link javax.crypto.KeyGenerator} and {@link javax.crypto.Mac} instances. + * The configuration is initialized statically from a provided {@link Configuration} object. + * <p> + * The {@link SecretManager} has some static method, so static configuration is required + */ +public class SecretManagerConfig { + private static final Logger LOG = LoggerFactory.getLogger(SecretManagerConfig.class); + private static String SELECTED_ALGORITHM; + private static int SELECTED_LENGTH; + + static { + update(new Configuration()); + } + + /** + * Updates the selected cryptographic algorithm and key length using the provided + * Hadoop {@link Configuration}. This method reads the values for + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_KEY} and + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_KEY}, or uses default values if not set. + * + * @param conf the configuration object containing cryptographic settings + */ + public static void update(Configuration conf) { + SELECTED_ALGORITHM = conf.get( + CommonConfigurationKeysPublic.HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_KEY, + CommonConfigurationKeysPublic.HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_DEFAULT); + LOG.debug(\"Selected hash algorithm: {}\", SELECTED_ALGORITHM); + SELECTED_LENGTH = conf.getInt( + CommonConfigurationKeysPublic.HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_KEY, + CommonConfigurationKeysPublic.HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_DEFAULT); + LOG.debug(\"Selected hash key length: {}\", SELECTED_LENGTH); + } + + /** + * Returns the currently selected cryptographic algorithm. + * + * @return the name of the selected algorithm + */ + public static String getSelectedAlgorithm() { + return SELECTED_ALGORITHM; + } + + /** + * Returns the currently selected key length in bits. + * + * @return the selected key length + */ + public static int getSelectedLength() { + return SELECTED_LENGTH; + } + + /** + * Sets the cryptographic algorithm to use. + * + * @param algorithm the algorithm name (e.g., \"HmacSHA256\", \"AES\") + */ + public static void setSelectedAlgorithm(String algorithm) { + SELECTED_ALGORITHM = algorithm; + LOG.debug(\"Selected hash algorithm set to {}\", algorithm); + } + + /** + * Sets the cryptographic key length to use (in bits). + * + * @param length the key length + */ + public static void setSelectedLength(int length) { + SELECTED_LENGTH = length; + LOG.debug(\"Selected hash key length set to{}\", length); Review Comment: nit: 1 space before the length abstractdog commented on code in PR #7827: URL: https://github.com/apache/hadoop/pull/7827#discussion_r2228222651 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManagerConfig.java: ########## @@ -0,0 +1,123 @@ + +package org.apache.hadoop.security.token; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.CommonConfigurationKeysPublic; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import javax.crypto.KeyGenerator; +import javax.crypto.Mac; +import javax.crypto.SecretKey; +import java.security.NoSuchAlgorithmException; + +/** + * Provides configuration and utility methods for managing cryptographic key generation + * and message authentication code (MAC) generation using specified algorithms and key lengths. + * <p> + * This class supports static access to the selected cryptographic algorithm and key length, + * and provides methods to create configured {@link javax.crypto.KeyGenerator} and {@link javax.crypto.Mac} instances. + * The configuration is initialized statically from a provided {@link Configuration} object. + * <p> + * The {@link SecretManager} has some static method, so static configuration is required + */ +public class SecretManagerConfig { + private static final Logger LOG = LoggerFactory.getLogger(SecretManagerConfig.class); + private static String SELECTED_ALGORITHM; + private static int SELECTED_LENGTH; + + static { + update(new Configuration()); + } + + /** + * Updates the selected cryptographic algorithm and key length using the provided + * Hadoop {@link Configuration}. This method reads the values for + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_KEY} and + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_KEY}, or uses default values if not set. + * + * @param conf the configuration object containing cryptographic settings + */ + public static void update(Configuration conf) { + SELECTED_ALGORITHM = conf.get( + CommonConfigurationKeysPublic.HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_KEY, + CommonConfigurationKeysPublic.HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_DEFAULT); + LOG.debug(\"Selected hash algorithm: {}\", SELECTED_ALGORITHM); + SELECTED_LENGTH = conf.getInt( + CommonConfigurationKeysPublic.HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_KEY, + CommonConfigurationKeysPublic.HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_DEFAULT); + LOG.debug(\"Selected hash key length: {}\", SELECTED_LENGTH); + } + + /** + * Returns the currently selected cryptographic algorithm. + * + * @return the name of the selected algorithm + */ + public static String getSelectedAlgorithm() { + return SELECTED_ALGORITHM; + } + + /** + * Returns the currently selected key length in bits. + * + * @return the selected key length + */ + public static int getSelectedLength() { + return SELECTED_LENGTH; + } + + /** + * Sets the cryptographic algorithm to use. + * + * @param algorithm the algorithm name (e.g., \"HmacSHA256\", \"AES\") + */ + public static void setSelectedAlgorithm(String algorithm) { Review Comment: we need to decide what other components will use, which is I guess update(conf), in which case this should rather become package protected with @VisibleForTesting abstractdog commented on code in PR #7827: URL: https://github.com/apache/hadoop/pull/7827#discussion_r2228223190 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManagerConfig.java: ########## @@ -0,0 +1,123 @@ + +package org.apache.hadoop.security.token; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.CommonConfigurationKeysPublic; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import javax.crypto.KeyGenerator; +import javax.crypto.Mac; +import javax.crypto.SecretKey; +import java.security.NoSuchAlgorithmException; + +/** + * Provides configuration and utility methods for managing cryptographic key generation + * and message authentication code (MAC) generation using specified algorithms and key lengths. + * <p> + * This class supports static access to the selected cryptographic algorithm and key length, + * and provides methods to create configured {@link javax.crypto.KeyGenerator} and {@link javax.crypto.Mac} instances. + * The configuration is initialized statically from a provided {@link Configuration} object. + * <p> + * The {@link SecretManager} has some static method, so static configuration is required + */ +public class SecretManagerConfig { + private static final Logger LOG = LoggerFactory.getLogger(SecretManagerConfig.class); + private static String SELECTED_ALGORITHM; + private static int SELECTED_LENGTH; + + static { + update(new Configuration()); + } + + /** + * Updates the selected cryptographic algorithm and key length using the provided + * Hadoop {@link Configuration}. This method reads the values for + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_KEY} and + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_KEY}, or uses default values if not set. + * + * @param conf the configuration object containing cryptographic settings + */ + public static void update(Configuration conf) { + SELECTED_ALGORITHM = conf.get( + CommonConfigurationKeysPublic.HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_KEY, + CommonConfigurationKeysPublic.HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_DEFAULT); + LOG.debug(\"Selected hash algorithm: {}\", SELECTED_ALGORITHM); + SELECTED_LENGTH = conf.getInt( + CommonConfigurationKeysPublic.HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_KEY, + CommonConfigurationKeysPublic.HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_DEFAULT); + LOG.debug(\"Selected hash key length: {}\", SELECTED_LENGTH); + } + + /** + * Returns the currently selected cryptographic algorithm. + * + * @return the name of the selected algorithm + */ + public static String getSelectedAlgorithm() { + return SELECTED_ALGORITHM; + } + + /** + * Returns the currently selected key length in bits. + * + * @return the selected key length + */ + public static int getSelectedLength() { + return SELECTED_LENGTH; + } + + /** + * Sets the cryptographic algorithm to use. + * + * @param algorithm the algorithm name (e.g., \"HmacSHA256\", \"AES\") + */ + public static void setSelectedAlgorithm(String algorithm) { + SELECTED_ALGORITHM = algorithm; + LOG.debug(\"Selected hash algorithm set to {}\", algorithm); + } + + /** + * Sets the cryptographic key length to use (in bits). + * + * @param length the key length + */ + public static void setSelectedLength(int length) { Review Comment: @VisibleForTesting for the same reason as setSelectedAlgorithm K0K0V0K commented on code in PR #7827: URL: https://github.com/apache/hadoop/pull/7827#discussion_r2228235542 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManagerConfig.java: ########## @@ -0,0 +1,123 @@ + +package org.apache.hadoop.security.token; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.CommonConfigurationKeysPublic; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import javax.crypto.KeyGenerator; +import javax.crypto.Mac; +import javax.crypto.SecretKey; +import java.security.NoSuchAlgorithmException; + +/** + * Provides configuration and utility methods for managing cryptographic key generation + * and message authentication code (MAC) generation using specified algorithms and key lengths. + * <p> + * This class supports static access to the selected cryptographic algorithm and key length, + * and provides methods to create configured {@link javax.crypto.KeyGenerator} and {@link javax.crypto.Mac} instances. + * The configuration is initialized statically from a provided {@link Configuration} object. + * <p> + * The {@link SecretManager} has some static method, so static configuration is required + */ +public class SecretManagerConfig { + private static final Logger LOG = LoggerFactory.getLogger(SecretManagerConfig.class); + private static String SELECTED_ALGORITHM; + private static int SELECTED_LENGTH; + + static { + update(new Configuration()); + } + + /** + * Updates the selected cryptographic algorithm and key length using the provided + * Hadoop {@link Configuration}. This method reads the values for + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_KEY} and + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_KEY}, or uses default values if not set. + * + * @param conf the configuration object containing cryptographic settings + */ + public static void update(Configuration conf) { + SELECTED_ALGORITHM = conf.get( + CommonConfigurationKeysPublic.HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_KEY, + CommonConfigurationKeysPublic.HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_DEFAULT); + LOG.debug(\"Selected hash algorithm: {}\", SELECTED_ALGORITHM); + SELECTED_LENGTH = conf.getInt( + CommonConfigurationKeysPublic.HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_KEY, + CommonConfigurationKeysPublic.HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_DEFAULT); + LOG.debug(\"Selected hash key length: {}\", SELECTED_LENGTH); + } + + /** + * Returns the currently selected cryptographic algorithm. + * + * @return the name of the selected algorithm + */ + public static String getSelectedAlgorithm() { + return SELECTED_ALGORITHM; + } + + /** + * Returns the currently selected key length in bits. + * + * @return the selected key length + */ + public static int getSelectedLength() { + return SELECTED_LENGTH; + } + + /** + * Sets the cryptographic algorithm to use. + * + * @param algorithm the algorithm name (e.g., \"HmacSHA256\", \"AES\") + */ + public static void setSelectedAlgorithm(String algorithm) { Review Comment: Hi @abstractdog ! Thanks for the review. I was thinking we can provide 2 method and components can decide what they prefer, but maybe you right and that will over complicate the things. I will delete these setters. abstractdog commented on code in PR #7827: URL: https://github.com/apache/hadoop/pull/7827#discussion_r2228243815 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManagerConfig.java: ########## @@ -0,0 +1,123 @@ + +package org.apache.hadoop.security.token; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.CommonConfigurationKeysPublic; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import javax.crypto.KeyGenerator; +import javax.crypto.Mac; +import javax.crypto.SecretKey; +import java.security.NoSuchAlgorithmException; + +/** + * Provides configuration and utility methods for managing cryptographic key generation + * and message authentication code (MAC) generation using specified algorithms and key lengths. + * <p> + * This class supports static access to the selected cryptographic algorithm and key length, + * and provides methods to create configured {@link javax.crypto.KeyGenerator} and {@link javax.crypto.Mac} instances. + * The configuration is initialized statically from a provided {@link Configuration} object. + * <p> + * The {@link SecretManager} has some static method, so static configuration is required + */ +public class SecretManagerConfig { + private static final Logger LOG = LoggerFactory.getLogger(SecretManagerConfig.class); + private static String SELECTED_ALGORITHM; + private static int SELECTED_LENGTH; + + static { + update(new Configuration()); + } + + /** + * Updates the selected cryptographic algorithm and key length using the provided + * Hadoop {@link Configuration}. This method reads the values for + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_KEY} and + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_KEY}, or uses default values if not set. + * + * @param conf the configuration object containing cryptographic settings + */ + public static void update(Configuration conf) { Review Comment: while this method is crucial for Tez as explain on [HADOOP-19639](https://issues.apache.org/jira/browse/HADOOP-19639), it might also bring confusion, which is due to the fact that we try to lazy init static things, so scenario I'm worried about a wrong usage: 1. keyGen is initialized by createKeyGenerator 2. update is called 3. update is not effective as the keyGen is already initialized need to make this more robust by giving a warning or even throwing an exception if the update(Configuration) is called after initialization, making the user aware that the settings won't be applied abstractdog commented on code in PR #7827: URL: https://github.com/apache/hadoop/pull/7827#discussion_r2228243815 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManagerConfig.java: ########## @@ -0,0 +1,123 @@ + +package org.apache.hadoop.security.token; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.CommonConfigurationKeysPublic; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import javax.crypto.KeyGenerator; +import javax.crypto.Mac; +import javax.crypto.SecretKey; +import java.security.NoSuchAlgorithmException; + +/** + * Provides configuration and utility methods for managing cryptographic key generation + * and message authentication code (MAC) generation using specified algorithms and key lengths. + * <p> + * This class supports static access to the selected cryptographic algorithm and key length, + * and provides methods to create configured {@link javax.crypto.KeyGenerator} and {@link javax.crypto.Mac} instances. + * The configuration is initialized statically from a provided {@link Configuration} object. + * <p> + * The {@link SecretManager} has some static method, so static configuration is required + */ +public class SecretManagerConfig { + private static final Logger LOG = LoggerFactory.getLogger(SecretManagerConfig.class); + private static String SELECTED_ALGORITHM; + private static int SELECTED_LENGTH; + + static { + update(new Configuration()); + } + + /** + * Updates the selected cryptographic algorithm and key length using the provided + * Hadoop {@link Configuration}. This method reads the values for + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_KEY} and + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_KEY}, or uses default values if not set. + * + * @param conf the configuration object containing cryptographic settings + */ + public static void update(Configuration conf) { Review Comment: while this method is crucial for Tez as explained on [HADOOP-19639](https://issues.apache.org/jira/browse/HADOOP-19639), it might also bring confusion, which is due to the fact that we try to lazy init static things, so scenario I'm worried about a wrong usage: 1. keyGen is initialized by createKeyGenerator 2. update is called 3. update is not effective as the keyGen is already initialized need to make this more robust by giving a warning or even throwing an exception if the update(Configuration) is called after initialization, making the user aware that the settings won't be applied abstractdog commented on code in PR #7827: URL: https://github.com/apache/hadoop/pull/7827#discussion_r2228243815 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManagerConfig.java: ########## @@ -0,0 +1,123 @@ + +package org.apache.hadoop.security.token; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.CommonConfigurationKeysPublic; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import javax.crypto.KeyGenerator; +import javax.crypto.Mac; +import javax.crypto.SecretKey; +import java.security.NoSuchAlgorithmException; + +/** + * Provides configuration and utility methods for managing cryptographic key generation + * and message authentication code (MAC) generation using specified algorithms and key lengths. + * <p> + * This class supports static access to the selected cryptographic algorithm and key length, + * and provides methods to create configured {@link javax.crypto.KeyGenerator} and {@link javax.crypto.Mac} instances. + * The configuration is initialized statically from a provided {@link Configuration} object. + * <p> + * The {@link SecretManager} has some static method, so static configuration is required + */ +public class SecretManagerConfig { + private static final Logger LOG = LoggerFactory.getLogger(SecretManagerConfig.class); + private static String SELECTED_ALGORITHM; + private static int SELECTED_LENGTH; + + static { + update(new Configuration()); + } + + /** + * Updates the selected cryptographic algorithm and key length using the provided + * Hadoop {@link Configuration}. This method reads the values for + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_KEY} and + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_KEY}, or uses default values if not set. + * + * @param conf the configuration object containing cryptographic settings + */ + public static void update(Configuration conf) { Review Comment: while this method is crucial for Tez as explained on [HADOOP-19639](https://issues.apache.org/jira/browse/HADOOP-19639), it might also bring confusion, which is due to the fact that we try to lazy init static things, so a possible scenario I'm worried about is this below: 1. keyGen is initialized by createKeyGenerator 2. update is called 3. update is not effective as the keyGen is already initialized need to make this more robust by giving a warning or even throwing an exception if the update(Configuration) is called after initialization, making the user aware that the settings won't be applied hadoop-yetus commented on PR #7827: URL: https://github.com/apache/hadoop/pull/7827#issuecomment-3113583720 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 44s | | trunk passed | | +1 :green_heart: | compile | 18m 30s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 14m 51s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 13s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 40s | | trunk passed | | +1 :green_heart: | javadoc | 1m 14s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 52s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 33s | | trunk passed | | +1 :green_heart: | shadedclient | 39m 22s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 58s | | the patch passed | | +1 :green_heart: | compile | 16m 36s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 16m 36s | | the patch passed | | +1 :green_heart: | compile | 14m 0s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 14m 0s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 9s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/1/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) | hadoop-common-project/hadoop-common: The patch generated 105 new + 3 unchanged - 5 fixed = 108 total (was 8) | | +1 :green_heart: | mvnsite | 1m 35s | | the patch passed | | +1 :green_heart: | javadoc | 1m 9s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 52s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 38s | | the patch passed | | +1 :green_heart: | shadedclient | 42m 48s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 23m 47s | | hadoop-common in the patch passed. | | -1 :x: | asflicense | 1m 1s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/1/artifact/out/results-asflicense.txt) | The patch generated 2 ASF License warnings. | | | | 227m 13s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7827 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 8cb7def7bf45 5.15.0-140-generic #150-Ubuntu SMP Sat Apr 12 06:00:09 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 917e1a914ed3e869376961a0ff2a2678b3d851db | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/1/testReport/ | | Max. process+thread count | 3151 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7827: URL: https://github.com/apache/hadoop/pull/7827#issuecomment-3113584468 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 28m 8s | | trunk passed | | +1 :green_heart: | compile | 8m 35s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 7m 29s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 43s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 56s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 29s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 31s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 8m 9s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 9s | | the patch passed | | +1 :green_heart: | compile | 7m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 7m 26s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 42s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/6/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) | hadoop-common-project/hadoop-common: The patch generated 99 new + 3 unchanged - 5 fixed = 102 total (was 8) | | +1 :green_heart: | mvnsite | 0m 50s | | the patch passed | | +1 :green_heart: | javadoc | 0m 43s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 37s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 34s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 49s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 47s | | hadoop-common in the patch passed. | | -1 :x: | asflicense | 0m 42s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/6/artifact/out/results-asflicense.txt) | The patch generated 2 ASF License warnings. | | | | 134m 7s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/6/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7827 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 15839c6479be 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f9f28d6d5d32f97017b839cd542001e5abfd01a6 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/6/testReport/ | | Max. process+thread count | 1290 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/6/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7827: URL: https://github.com/apache/hadoop/pull/7827#issuecomment-3113598769 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 34m 6s | | trunk passed | | +1 :green_heart: | compile | 10m 21s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 8m 48s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 39s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 52s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 29s | | trunk passed | | +1 :green_heart: | shadedclient | 27m 57s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 9m 9s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 9m 9s | | the patch passed | | +1 :green_heart: | compile | 8m 11s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 8m 11s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 38s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/5/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) | hadoop-common-project/hadoop-common: The patch generated 85 new + 3 unchanged - 5 fixed = 88 total (was 8) | | +1 :green_heart: | mvnsite | 0m 57s | | the patch passed | | +1 :green_heart: | javadoc | 0m 39s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 33s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 29s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 32s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 18s | | hadoop-common in the patch passed. | | -1 :x: | asflicense | 0m 34s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/5/artifact/out/results-asflicense.txt) | The patch generated 2 ASF License warnings. | | | | 151m 13s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7827 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux a6c2fd546b0d 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c12d388317dbaa35f3a7ccdeb195a863defa7cea | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/5/testReport/ | | Max. process+thread count | 3151 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/5/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7827: URL: https://github.com/apache/hadoop/pull/7827#issuecomment-3113633207 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 0s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 51s | | trunk passed | | +1 :green_heart: | compile | 16m 35s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 55s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 13s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 37s | | trunk passed | | +1 :green_heart: | javadoc | 1m 11s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 52s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 37s | | trunk passed | | +1 :green_heart: | shadedclient | 41m 1s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 59s | | the patch passed | | +1 :green_heart: | compile | 17m 7s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 17m 7s | | the patch passed | | +1 :green_heart: | compile | 14m 5s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 14m 5s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 8s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/2/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) | hadoop-common-project/hadoop-common: The patch generated 101 new + 3 unchanged - 5 fixed = 104 total (was 8) | | +1 :green_heart: | mvnsite | 1m 31s | | the patch passed | | +1 :green_heart: | javadoc | 1m 5s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 48s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 33s | | the patch passed | | +1 :green_heart: | shadedclient | 41m 16s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 22m 57s | | hadoop-common in the patch passed. | | -1 :x: | asflicense | 1m 1s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/2/artifact/out/results-asflicense.txt) | The patch generated 2 ASF License warnings. | | | | 225m 26s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7827 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux d930993a819a 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / fecb7de4f374c95b0e0597b513b5417c77f57429 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/2/testReport/ | | Max. process+thread count | 1267 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7827: URL: https://github.com/apache/hadoop/pull/7827#issuecomment-3113695501 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 14m 23s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 40m 48s | | trunk passed | | +1 :green_heart: | compile | 15m 50s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 14m 44s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 15s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 39s | | trunk passed | | +1 :green_heart: | javadoc | 1m 13s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 52s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 38s | | trunk passed | | +1 :green_heart: | shadedclient | 42m 4s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 51s | | the patch passed | | +1 :green_heart: | compile | 15m 28s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 28s | | the patch passed | | +1 :green_heart: | compile | 15m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 17s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 9s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/3/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) | hadoop-common-project/hadoop-common: The patch generated 101 new + 3 unchanged - 5 fixed = 104 total (was 8) | | +1 :green_heart: | mvnsite | 1m 34s | | the patch passed | | +1 :green_heart: | javadoc | 1m 10s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 48s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 48s | | the patch passed | | +1 :green_heart: | shadedclient | 39m 40s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 22m 50s | | hadoop-common in the patch passed. | | -1 :x: | asflicense | 1m 2s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/3/artifact/out/results-asflicense.txt) | The patch generated 2 ASF License warnings. | | | | 239m 6s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7827 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 8277f4c6fcb7 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / fecb7de4f374c95b0e0597b513b5417c77f57429 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/3/testReport/ | | Max. process+thread count | 1378 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/3/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7827: URL: https://github.com/apache/hadoop/pull/7827#issuecomment-3113751006 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 31m 30s | | trunk passed | | +1 :green_heart: | compile | 9m 37s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 8m 15s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 41s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 1s | | trunk passed | | +1 :green_heart: | javadoc | 0m 44s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 32s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 23s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 10m 3s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 10m 3s | | the patch passed | | +1 :green_heart: | compile | 9m 12s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 9m 12s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 39s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/8/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) | hadoop-common-project/hadoop-common: The patch generated 99 new + 3 unchanged - 5 fixed = 102 total (was 8) | | +1 :green_heart: | mvnsite | 0m 55s | | the patch passed | | +1 :green_heart: | javadoc | 0m 38s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 30s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 34s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 53s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 25s | | hadoop-common in the patch passed. | | -1 :x: | asflicense | 0m 40s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/8/artifact/out/results-asflicense.txt) | The patch generated 2 ASF License warnings. | | | | 143m 35s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/8/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7827 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 137a54bc5cde 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / dc8b28237db668b0b3f7ccbaf43f3f023e679158 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/8/testReport/ | | Max. process+thread count | 3151 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/8/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7827: URL: https://github.com/apache/hadoop/pull/7827#issuecomment-3113778631 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 33s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 41m 4s | | trunk passed | | +1 :green_heart: | compile | 16m 59s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 51s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 9s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 31s | | trunk passed | | +1 :green_heart: | javadoc | 1m 9s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 47s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 28s | | trunk passed | | +1 :green_heart: | shadedclient | 38m 13s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 1m 23s | | the patch passed | | +1 :green_heart: | compile | 16m 3s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 16m 3s | | the patch passed | | +1 :green_heart: | compile | 14m 11s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 14m 11s | | the patch passed | | +1 :green_heart: | blanks | 0m 1s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 9s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/4/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) | hadoop-common-project/hadoop-common: The patch generated 101 new + 3 unchanged - 5 fixed = 104 total (was 8) | | +1 :green_heart: | mvnsite | 1m 32s | | the patch passed | | +1 :green_heart: | javadoc | 1m 7s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 49s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 37s | | the patch passed | | +1 :green_heart: | shadedclient | 40m 43s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 23m 41s | | hadoop-common in the patch passed. | | -1 :x: | asflicense | 1m 6s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/4/artifact/out/results-asflicense.txt) | The patch generated 2 ASF License warnings. | | | | 223m 5s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7827 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux cb505e4e8d20 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 43d1e7af3a8259b87e1b9c3bc11f888ebec47e8a | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/4/testReport/ | | Max. process+thread count | 1378 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/4/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7827: URL: https://github.com/apache/hadoop/pull/7827#issuecomment-3114088366 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 25m 55s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 6s | | trunk passed | | +1 :green_heart: | compile | 20m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 18m 20s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 19s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 51s | | trunk passed | | +1 :green_heart: | javadoc | 1m 20s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 55s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 49s | | trunk passed | | +1 :green_heart: | shadedclient | 43m 6s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 1m 4s | | the patch passed | | +1 :green_heart: | compile | 18m 0s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 18m 0s | | the patch passed | | +1 :green_heart: | compile | 16m 41s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 16m 41s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 11s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/7/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) | hadoop-common-project/hadoop-common: The patch generated 99 new + 3 unchanged - 5 fixed = 102 total (was 8) | | +1 :green_heart: | mvnsite | 1m 47s | | the patch passed | | +1 :green_heart: | javadoc | 1m 14s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 58s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 49s | | the patch passed | | +1 :green_heart: | shadedclient | 39m 24s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 23m 7s | | hadoop-common in the patch passed. | | -1 :x: | asflicense | 1m 7s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/7/artifact/out/results-asflicense.txt) | The patch generated 2 ASF License warnings. | | | | 267m 40s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/7/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7827 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 573d4710b3df 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / dc8b28237db668b0b3f7ccbaf43f3f023e679158 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/7/testReport/ | | Max. process+thread count | 1254 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/7/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7827: URL: https://github.com/apache/hadoop/pull/7827#issuecomment-3114142782 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 28m 15s | | trunk passed | | +1 :green_heart: | compile | 8m 23s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 7m 31s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 41s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 57s | | trunk passed | | +1 :green_heart: | javadoc | 0m 48s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 29s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 15s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 7m 59s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 7m 59s | | the patch passed | | +1 :green_heart: | compile | 7m 25s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 7m 25s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 32s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/10/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) | hadoop-common-project/hadoop-common: The patch generated 99 new + 3 unchanged - 5 fixed = 102 total (was 8) | | +1 :green_heart: | mvnsite | 0m 51s | | the patch passed | | +1 :green_heart: | javadoc | 0m 43s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 35s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 32s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 51s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 25s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 0m 42s | | The patch does not generate ASF License warnings. | | | | 133m 45s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/10/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7827 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 35bc6361dd7d 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / b4c125bf4bf93c83bab494febd0a631651ae8236 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/10/testReport/ | | Max. process+thread count | 3151 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/10/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7827: URL: https://github.com/apache/hadoop/pull/7827#issuecomment-3114376961 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 38s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 45m 40s | | trunk passed | | +1 :green_heart: | compile | 15m 55s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 49s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 19s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 39s | | trunk passed | | +1 :green_heart: | javadoc | 1m 20s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 55s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 35s | | trunk passed | | +1 :green_heart: | shadedclient | 37m 22s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 56s | | the patch passed | | +1 :green_heart: | compile | 15m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 19s | | the patch passed | | +1 :green_heart: | compile | 13m 43s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 43s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 11s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/9/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) | hadoop-common-project/hadoop-common: The patch generated 99 new + 3 unchanged - 5 fixed = 102 total (was 8) | | +1 :green_heart: | mvnsite | 1m 42s | | the patch passed | | +1 :green_heart: | javadoc | 1m 13s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 52s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 40s | | the patch passed | | +1 :green_heart: | shadedclient | 37m 49s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 23m 55s | | hadoop-common in the patch passed. | | -1 :x: | asflicense | 1m 2s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/9/artifact/out/results-asflicense.txt) | The patch generated 1 ASF License warnings. | | | | 222m 47s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/9/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7827 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 04f8fbe20f81 5.15.0-140-generic #150-Ubuntu SMP Sat Apr 12 06:00:09 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / d7b4811a130b9a0e5df41c71b0b57e0439dcffe6 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/9/testReport/ | | Max. process+thread count | 1288 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/9/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7827: URL: https://github.com/apache/hadoop/pull/7827#issuecomment-3114850985 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 34s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 40m 18s | | trunk passed | | +1 :green_heart: | compile | 17m 59s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 7s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 18s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 39s | | trunk passed | | +1 :green_heart: | javadoc | 1m 18s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 56s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 38s | | trunk passed | | +1 :green_heart: | shadedclient | 37m 22s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 58s | | the patch passed | | +1 :green_heart: | compile | 15m 10s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 10s | | the patch passed | | +1 :green_heart: | compile | 13m 58s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 58s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 13s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/11/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) | hadoop-common-project/hadoop-common: The patch generated 7 new + 3 unchanged - 5 fixed = 10 total (was 8) | | +1 :green_heart: | mvnsite | 1m 39s | | the patch passed | | +1 :green_heart: | javadoc | 1m 12s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 52s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 39s | | the patch passed | | +1 :green_heart: | shadedclient | 39m 23s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 22m 50s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 7s | | The patch does not generate ASF License warnings. | | | | 221m 25s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/11/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7827 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 9029f853de3e 5.15.0-140-generic #150-Ubuntu SMP Sat Apr 12 06:00:09 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 62cb8b55bf58188f56016b180713ab7a2acd498c | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/11/testReport/ | | Max. process+thread count | 3152 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/11/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on PR #7827: URL: https://github.com/apache/hadoop/pull/7827#issuecomment-3116949549 @K0K0V0K LGTM +1. abstractdog commented on code in PR #7827: URL: https://github.com/apache/hadoop/pull/7827#discussion_r2230571393 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManagerConfig.java: ########## @@ -0,0 +1,133 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * <p> + * http://www.apache.org/licenses/LICENSE-2.0 + * <p> + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ +package org.apache.hadoop.security.token; + +import org.apache.hadoop.classification.InterfaceAudience; +import org.apache.hadoop.classification.InterfaceStability; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.CommonConfigurationKeysPublic; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import javax.crypto.KeyGenerator; +import javax.crypto.Mac; +import java.security.NoSuchAlgorithmException; + +/** + * Provides configuration and utility methods for managing cryptographic key generation + * and message authentication code (MAC) generation using specified algorithms and key lengths. + * <p> + * This class supports static access to the selected cryptographic algorithm and key length, + * and provides methods to create configured {@link javax.crypto.KeyGenerator} + * and {@link javax.crypto.Mac} instances. + * The configuration is initialized statically from a provided {@link Configuration} object. + * <p> + * The {@link SecretManager} has some static method, so static configuration is required + */ +@InterfaceAudience.Public +@InterfaceStability.Evolving +public class SecretManagerConfig { + private static final Logger LOG = LoggerFactory.getLogger(SecretManagerConfig.class); + private static String selectedAlgorithm; + private static int selectedLength; + private static boolean initialized; + + static { + update(new Configuration()); + } + + private SecretManagerConfig() { + } + + /** + * Updates the selected cryptographic algorithm and key length using the provided + * Hadoop {@link Configuration}. This method reads the values for + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_KEY} and + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_KEY}, or uses default values if not set. + * + * @param conf the configuration object containing cryptographic settings + */ + public static synchronized void update(Configuration conf) { + if (initialized) { + LOG.warn( + \"Keygen or Mac was already initialized with older config, those will not be updated\"); Review Comment: I'm afraid this message in its current form is confusing: 1. we don't know if keygen or mac 2. mac is threadlocal, so it can also further complicate this situation what about including Keygen and Mac instance here (their initialization was on debug level, so we don't know what's happening here), something like: ``` \"Keygen ({}) or Mac ({}, thread: {}) was already initialized with older config, those will not be updated\", keyGen, macForThisThread, threadId); ``` I know this complicates the code because there are not necessarily available here, we can also consider moving this class as a static inner class to SecretManager, as it's already tightly coupled, we won't won anything by having it a separate class abstractdog commented on code in PR #7827: URL: https://github.com/apache/hadoop/pull/7827#discussion_r2230573481 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManagerConfig.java: ########## @@ -0,0 +1,133 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * <p> + * http://www.apache.org/licenses/LICENSE-2.0 + * <p> + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ +package org.apache.hadoop.security.token; + +import org.apache.hadoop.classification.InterfaceAudience; +import org.apache.hadoop.classification.InterfaceStability; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.CommonConfigurationKeysPublic; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import javax.crypto.KeyGenerator; +import javax.crypto.Mac; +import java.security.NoSuchAlgorithmException; + +/** + * Provides configuration and utility methods for managing cryptographic key generation + * and message authentication code (MAC) generation using specified algorithms and key lengths. + * <p> + * This class supports static access to the selected cryptographic algorithm and key length, + * and provides methods to create configured {@link javax.crypto.KeyGenerator} + * and {@link javax.crypto.Mac} instances. + * The configuration is initialized statically from a provided {@link Configuration} object. + * <p> + * The {@link SecretManager} has some static method, so static configuration is required + */ +@InterfaceAudience.Public +@InterfaceStability.Evolving +public class SecretManagerConfig { + private static final Logger LOG = LoggerFactory.getLogger(SecretManagerConfig.class); + private static String selectedAlgorithm; + private static int selectedLength; + private static boolean initialized; + + static { + update(new Configuration()); + } + + private SecretManagerConfig() { + } + + /** + * Updates the selected cryptographic algorithm and key length using the provided + * Hadoop {@link Configuration}. This method reads the values for + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_KEY} and + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_KEY}, or uses default values if not set. + * + * @param conf the configuration object containing cryptographic settings + */ + public static synchronized void update(Configuration conf) { + if (initialized) { + LOG.warn( + \"Keygen or Mac was already initialized with older config, those will not be updated\"); + } + selectedAlgorithm = conf.get( + CommonConfigurationKeysPublic.HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_KEY, + CommonConfigurationKeysPublic.HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_DEFAULT); + LOG.debug(\"Selected hash algorithm: {}\", selectedAlgorithm); + selectedLength = conf.getInt( + CommonConfigurationKeysPublic.HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_KEY, + CommonConfigurationKeysPublic.HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_DEFAULT); + LOG.debug(\"Selected hash key length: {}\", selectedLength); + } + + /** + * Returns the currently selected cryptographic algorithm. + * + * @return the name of the selected algorithm + */ + public static synchronized String getSelectedAlgorithm() { + return selectedAlgorithm; + } + + /** + * Returns the currently selected key length in bits. + * + * @return the selected key length + */ + public static synchronized int getSelectedLength() { + return selectedLength; + } + + /** + * Creates a new {@link KeyGenerator} instance configured with the currently selected + * algorithm and key length. + * + * @return a new {@code KeyGenerator} instance + * @throws IllegalArgumentException if the specified algorithm is not available + */ + public static synchronized KeyGenerator createKeyGenerator() { + LOG.debug(\"Creating key generator instance {}, {}\", selectedAlgorithm, selectedLength); + initialized = true; + try { + KeyGenerator keyGen = KeyGenerator.getInstance(selectedAlgorithm); + keyGen.init(selectedLength); + return keyGen; + } catch (NoSuchAlgorithmException nsa) { + throw new IllegalArgumentException(\"Can't find \" + selectedAlgorithm, nsa); + } + } + + /** + * Creates a new {@link Mac} instance using the currently selected algorithm. + * + * @return a new {@code Mac} instance + * @throws IllegalArgumentException if the specified algorithm is not available + */ + public static synchronized Mac createMac() { + LOG.debug(\"Creating mac instance {}\", selectedAlgorithm); Review Comment: include thread id in this message (regardless of what the logging library does, we might want to have this information) abstractdog commented on PR #7827: URL: https://github.com/apache/hadoop/pull/7827#issuecomment-3116992779 @K0K0V0K thanks for the patch so far! please consider the 2 comments I left before proceeding, I believe it would make this patch cleaner and more user-friendly abstractdog commented on code in PR #7827: URL: https://github.com/apache/hadoop/pull/7827#discussion_r2230571393 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManagerConfig.java: ########## @@ -0,0 +1,133 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * <p> + * http://www.apache.org/licenses/LICENSE-2.0 + * <p> + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ +package org.apache.hadoop.security.token; + +import org.apache.hadoop.classification.InterfaceAudience; +import org.apache.hadoop.classification.InterfaceStability; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.CommonConfigurationKeysPublic; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import javax.crypto.KeyGenerator; +import javax.crypto.Mac; +import java.security.NoSuchAlgorithmException; + +/** + * Provides configuration and utility methods for managing cryptographic key generation + * and message authentication code (MAC) generation using specified algorithms and key lengths. + * <p> + * This class supports static access to the selected cryptographic algorithm and key length, + * and provides methods to create configured {@link javax.crypto.KeyGenerator} + * and {@link javax.crypto.Mac} instances. + * The configuration is initialized statically from a provided {@link Configuration} object. + * <p> + * The {@link SecretManager} has some static method, so static configuration is required + */ +@InterfaceAudience.Public +@InterfaceStability.Evolving +public class SecretManagerConfig { + private static final Logger LOG = LoggerFactory.getLogger(SecretManagerConfig.class); + private static String selectedAlgorithm; + private static int selectedLength; + private static boolean initialized; + + static { + update(new Configuration()); + } + + private SecretManagerConfig() { + } + + /** + * Updates the selected cryptographic algorithm and key length using the provided + * Hadoop {@link Configuration}. This method reads the values for + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_KEY} and + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_KEY}, or uses default values if not set. + * + * @param conf the configuration object containing cryptographic settings + */ + public static synchronized void update(Configuration conf) { + if (initialized) { + LOG.warn( + \"Keygen or Mac was already initialized with older config, those will not be updated\"); Review Comment: I'm afraid this message in its current form is confusing: 1. we don't know if keygen or mac 2. mac is threadlocal, so it can also further complicate this situation what about including Keygen and Mac instance here (their initialization was on debug level, so we don't know what's happening here), something like: ``` \"Keygen ({}) or Mac ({}, thread: {}) was already initialized with older config, those will not be updated\", keyGen, macForThisThread, threadId); ``` I know this complicates the code because those variables are not necessarily available here, we can also consider moving this class as a static inner class to SecretManager, as it's already tightly coupled, we won't won anything by having it a separate class hadoop-yetus commented on PR #7827: URL: https://github.com/apache/hadoop/pull/7827#issuecomment-3117058957 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 25s | | trunk passed | | +1 :green_heart: | compile | 16m 14s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 43s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 17s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 42s | | trunk passed | | +1 :green_heart: | javadoc | 1m 14s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 53s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 33s | | trunk passed | | +1 :green_heart: | shadedclient | 37m 6s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 55s | | the patch passed | | +1 :green_heart: | compile | 15m 7s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 7s | | the patch passed | | +1 :green_heart: | compile | 13m 51s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 51s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 13s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/12/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) | hadoop-common-project/hadoop-common: The patch generated 2 new + 3 unchanged - 5 fixed = 5 total (was 8) | | +1 :green_heart: | mvnsite | 1m 40s | | the patch passed | | +1 :green_heart: | javadoc | 1m 13s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 56s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 44s | | the patch passed | | +1 :green_heart: | shadedclient | 42m 4s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 22m 43s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 19s | | The patch does not generate ASF License warnings. | | | | 219m 35s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/12/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7827 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 8468afa4e21a 5.15.0-140-generic #150-Ubuntu SMP Sat Apr 12 06:00:09 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / b02c20254ed02b0802bb849fda7299e25ef3eb88 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/12/testReport/ | | Max. process+thread count | 1288 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/12/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. K0K0V0K commented on code in PR #7827: URL: https://github.com/apache/hadoop/pull/7827#discussion_r2230728221 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManagerConfig.java: ########## @@ -0,0 +1,133 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * <p> + * http://www.apache.org/licenses/LICENSE-2.0 + * <p> + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ +package org.apache.hadoop.security.token; + +import org.apache.hadoop.classification.InterfaceAudience; +import org.apache.hadoop.classification.InterfaceStability; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.CommonConfigurationKeysPublic; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import javax.crypto.KeyGenerator; +import javax.crypto.Mac; +import java.security.NoSuchAlgorithmException; + +/** + * Provides configuration and utility methods for managing cryptographic key generation + * and message authentication code (MAC) generation using specified algorithms and key lengths. + * <p> + * This class supports static access to the selected cryptographic algorithm and key length, + * and provides methods to create configured {@link javax.crypto.KeyGenerator} + * and {@link javax.crypto.Mac} instances. + * The configuration is initialized statically from a provided {@link Configuration} object. + * <p> + * The {@link SecretManager} has some static method, so static configuration is required + */ +@InterfaceAudience.Public +@InterfaceStability.Evolving +public class SecretManagerConfig { + private static final Logger LOG = LoggerFactory.getLogger(SecretManagerConfig.class); + private static String selectedAlgorithm; + private static int selectedLength; + private static boolean initialized; + + static { + update(new Configuration()); + } + + private SecretManagerConfig() { + } + + /** + * Updates the selected cryptographic algorithm and key length using the provided + * Hadoop {@link Configuration}. This method reads the values for + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_KEY} and + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_KEY}, or uses default values if not set. + * + * @param conf the configuration object containing cryptographic settings + */ + public static synchronized void update(Configuration conf) { + if (initialized) { + LOG.warn( + \"Keygen or Mac was already initialized with older config, those will not be updated\"); Review Comment: Thanks @abstractdog for the hints! I am a bit troubled with the macs ... As you mentioned these are thread locals, so not just one can be present of them in the java process. If I keep reference for them i believe the GC will never clean them (not sure). Maybe i can try with some WeakReference if you think that can be acceptable. K0K0V0K commented on code in PR #7827: URL: https://github.com/apache/hadoop/pull/7827#discussion_r2230768478 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManagerConfig.java: ########## @@ -0,0 +1,133 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * <p> + * http://www.apache.org/licenses/LICENSE-2.0 + * <p> + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ +package org.apache.hadoop.security.token; + +import org.apache.hadoop.classification.InterfaceAudience; +import org.apache.hadoop.classification.InterfaceStability; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.CommonConfigurationKeysPublic; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import javax.crypto.KeyGenerator; +import javax.crypto.Mac; +import java.security.NoSuchAlgorithmException; + +/** + * Provides configuration and utility methods for managing cryptographic key generation + * and message authentication code (MAC) generation using specified algorithms and key lengths. + * <p> + * This class supports static access to the selected cryptographic algorithm and key length, + * and provides methods to create configured {@link javax.crypto.KeyGenerator} + * and {@link javax.crypto.Mac} instances. + * The configuration is initialized statically from a provided {@link Configuration} object. + * <p> + * The {@link SecretManager} has some static method, so static configuration is required + */ +@InterfaceAudience.Public +@InterfaceStability.Evolving +public class SecretManagerConfig { + private static final Logger LOG = LoggerFactory.getLogger(SecretManagerConfig.class); + private static String selectedAlgorithm; + private static int selectedLength; + private static boolean initialized; + + static { + update(new Configuration()); + } + + private SecretManagerConfig() { + } + + /** + * Updates the selected cryptographic algorithm and key length using the provided + * Hadoop {@link Configuration}. This method reads the values for + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_KEY} and + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_KEY}, or uses default values if not set. + * + * @param conf the configuration object containing cryptographic settings + */ + public static synchronized void update(Configuration conf) { + if (initialized) { + LOG.warn( + \"Keygen or Mac was already initialized with older config, those will not be updated\"); Review Comment: The current solution seems not leaking ``` @Test public void tmpTest() throws InterruptedException { new Thread(\"tmpTest-1\") { @Override public void run() { Mac mac = SecretManagerConfig.createMac(); try { Thread.sleep(2000); } catch (InterruptedException e) { throw new RuntimeException(e); } } }.start(); new Thread(\"tmpTest-2\") { @Override public void run() { Mac mac = SecretManagerConfig.createMac(); try { Thread.sleep(2000); } catch (InterruptedException e) { throw new RuntimeException(e); } } }.start(); Thread.sleep(500); System.err.println(\"MACS:\" + SecretManagerConfig.MACS); Thread.sleep(2000); System.gc(); Thread.sleep(500); System.err.println(\"MACS:\" + SecretManagerConfig.MACS); } ``` OUTPUT: ``` MACS:{Thread[tmpTest-1,5,main]=javax.crypto.Mac@52aa2946, Thread[tmpTest-2,5,main]=javax.crypto.Mac@4de5031f} MACS:{} ``` Feel free to ask modification abstractdog commented on code in PR #7827: URL: https://github.com/apache/hadoop/pull/7827#discussion_r2230777430 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManagerConfig.java: ########## @@ -0,0 +1,143 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * <p> + * http://www.apache.org/licenses/LICENSE-2.0 + * <p> + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ +package org.apache.hadoop.security.token; + +import org.apache.hadoop.classification.InterfaceAudience; +import org.apache.hadoop.classification.InterfaceStability; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.CommonConfigurationKeysPublic; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import javax.crypto.KeyGenerator; +import javax.crypto.Mac; +import java.security.NoSuchAlgorithmException; +import java.util.Map; +import java.util.WeakHashMap; + +/** + * Provides configuration and utility methods for managing cryptographic key generation + * and message authentication code (MAC) generation using specified algorithms and key lengths. + * <p> + * This class supports static access to the selected cryptographic algorithm and key length, + * and provides methods to create configured {@link javax.crypto.KeyGenerator} + * and {@link javax.crypto.Mac} instances. + * The configuration is initialized statically from a provided {@link Configuration} object. + * <p> + * The {@link SecretManager} has some static method, so static configuration is required + */ +@InterfaceAudience.Public +@InterfaceStability.Evolving +public final class SecretManagerConfig { + private static final Logger LOG = LoggerFactory.getLogger(SecretManagerConfig.class); + private static String selectedAlgorithm; + private static int selectedLength; + + private static final Map<Thread, KeyGenerator> KEYGENS = new WeakHashMap<>(); Review Comment: KeyGenerator is not threadlocal, there will be a single global instance, right? K0K0V0K commented on code in PR #7827: URL: https://github.com/apache/hadoop/pull/7827#discussion_r2230781641 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManagerConfig.java: ########## @@ -0,0 +1,143 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * <p> + * http://www.apache.org/licenses/LICENSE-2.0 + * <p> + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ +package org.apache.hadoop.security.token; + +import org.apache.hadoop.classification.InterfaceAudience; +import org.apache.hadoop.classification.InterfaceStability; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.CommonConfigurationKeysPublic; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import javax.crypto.KeyGenerator; +import javax.crypto.Mac; +import java.security.NoSuchAlgorithmException; +import java.util.Map; +import java.util.WeakHashMap; + +/** + * Provides configuration and utility methods for managing cryptographic key generation + * and message authentication code (MAC) generation using specified algorithms and key lengths. + * <p> + * This class supports static access to the selected cryptographic algorithm and key length, + * and provides methods to create configured {@link javax.crypto.KeyGenerator} + * and {@link javax.crypto.Mac} instances. + * The configuration is initialized statically from a provided {@link Configuration} object. + * <p> + * The {@link SecretManager} has some static method, so static configuration is required + */ +@InterfaceAudience.Public +@InterfaceStability.Evolving +public final class SecretManagerConfig { + private static final Logger LOG = LoggerFactory.getLogger(SecretManagerConfig.class); + private static String selectedAlgorithm; + private static int selectedLength; + + private static final Map<Thread, KeyGenerator> KEYGENS = new WeakHashMap<>(); Review Comment: If everything works as expected yes, but no there is not granted some one will not call this method again ... you right this should be in SecretManager not in an other class abstractdog commented on code in PR #7827: URL: https://github.com/apache/hadoop/pull/7827#discussion_r2230784229 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManagerConfig.java: ########## @@ -0,0 +1,143 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * <p> + * http://www.apache.org/licenses/LICENSE-2.0 + * <p> + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ +package org.apache.hadoop.security.token; + +import org.apache.hadoop.classification.InterfaceAudience; +import org.apache.hadoop.classification.InterfaceStability; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.CommonConfigurationKeysPublic; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import javax.crypto.KeyGenerator; +import javax.crypto.Mac; +import java.security.NoSuchAlgorithmException; +import java.util.Map; +import java.util.WeakHashMap; + +/** + * Provides configuration and utility methods for managing cryptographic key generation + * and message authentication code (MAC) generation using specified algorithms and key lengths. + * <p> + * This class supports static access to the selected cryptographic algorithm and key length, + * and provides methods to create configured {@link javax.crypto.KeyGenerator} + * and {@link javax.crypto.Mac} instances. + * The configuration is initialized statically from a provided {@link Configuration} object. + * <p> + * The {@link SecretManager} has some static method, so static configuration is required + */ +@InterfaceAudience.Public +@InterfaceStability.Evolving +public final class SecretManagerConfig { + private static final Logger LOG = LoggerFactory.getLogger(SecretManagerConfig.class); + private static String selectedAlgorithm; + private static int selectedLength; + + private static final Map<Thread, KeyGenerator> KEYGENS = new WeakHashMap<>(); + public static final Map<Thread, Mac> MACS = new WeakHashMap<>(); + + static { + update(new Configuration()); + } + + private SecretManagerConfig() { + } + + /** + * Updates the selected cryptographic algorithm and key length using the provided + * Hadoop {@link Configuration}. This method reads the values for + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_KEY} and + * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_KEY}, or uses default values if not set. + * + * @param conf the configuration object containing cryptographic settings + */ + public static synchronized void update(Configuration conf) { + if (!KEYGENS.isEmpty()) { + LOG.warn(\"Keygen was already initialized with older config, those will not be updated.\" + + \"Hint: If you turn on debug log you can see when it happened. Keygens: {}\", KEYGENS); + } + if (!MACS.isEmpty()) { + LOG.warn(\"Mac was already initialized with older config, those will not be updated.\" + Review Comment: I don't think we need to store and log all the macs, I would be satisfied with logging if there is any in the current thread, acquired by: ``` threadLocalMac.get() ``` assuming that the update() happens on the same thread as the usage, this can be a useful logging message (without logging a whole MACS collection) hadoop-yetus commented on PR #7827: URL: https://github.com/apache/hadoop/pull/7827#issuecomment-3118018889 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 0s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 46s | | trunk passed | | +1 :green_heart: | compile | 15m 57s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 49s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 17s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 38s | | trunk passed | | +1 :green_heart: | javadoc | 1m 20s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 56s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 37s | | trunk passed | | +1 :green_heart: | shadedclient | 37m 39s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 55s | | the patch passed | | +1 :green_heart: | compile | 15m 14s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 14s | | the patch passed | | +1 :green_heart: | compile | 13m 38s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 38s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 14s | | hadoop-common-project/hadoop-common: The patch generated 0 new + 3 unchanged - 5 fixed = 3 total (was 8) | | +1 :green_heart: | mvnsite | 1m 39s | | the patch passed | | +1 :green_heart: | javadoc | 1m 14s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 56s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 43s | | the patch passed | | +1 :green_heart: | shadedclient | 37m 38s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 22m 43s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 7s | | The patch does not generate ASF License warnings. | | | | 215m 10s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/14/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7827 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux a551e66581bc 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / ef05d1708c289152368751b72c6d237812c5a84c | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/14/testReport/ | | Max. process+thread count | 3152 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/14/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7827: URL: https://github.com/apache/hadoop/pull/7827#issuecomment-3118085178 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 28s | | trunk passed | | +1 :green_heart: | compile | 18m 30s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 14m 51s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 13s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 36s | | trunk passed | | +1 :green_heart: | javadoc | 1m 15s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 51s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 32s | | trunk passed | | +1 :green_heart: | shadedclient | 39m 35s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 58s | | the patch passed | | +1 :green_heart: | compile | 16m 41s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 16m 41s | | the patch passed | | +1 :green_heart: | compile | 13m 55s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 55s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 8s | | hadoop-common-project/hadoop-common: The patch generated 0 new + 3 unchanged - 5 fixed = 3 total (was 8) | | +1 :green_heart: | mvnsite | 1m 36s | | the patch passed | | +1 :green_heart: | javadoc | 1m 9s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 51s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 43s | | the patch passed | | +1 :green_heart: | shadedclient | 44m 2s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 23m 30s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 2s | | The patch does not generate ASF License warnings. | | | | 229m 12s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/13/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7827 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux f6ec6f660895 5.15.0-140-generic #150-Ubuntu SMP Sat Apr 12 06:00:09 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / e56c0213ef2d44f90b6cd6a570878ca3ba78c4ae | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/13/testReport/ | | Max. process+thread count | 3152 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/13/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. K0K0V0K commented on code in PR #7827: URL: https://github.com/apache/hadoop/pull/7827#discussion_r2234948795 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManagerConfig.java: ########## @@ -0,0 +1,143 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * <p> + * http://www.apache.org/licenses/LICENSE-2.0 + * <p> + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ +package org.apache.hadoop.security.token; + +import org.apache.hadoop.classification.InterfaceAudience; +import org.apache.hadoop.classification.InterfaceStability; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.CommonConfigurationKeysPublic; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import javax.crypto.KeyGenerator; +import javax.crypto.Mac; +import java.security.NoSuchAlgorithmException; +import java.util.Map; +import java.util.WeakHashMap; + +/** + * Provides configuration and utility methods for managing cryptographic key generation + * and message authentication code (MAC) generation using specified algorithms and key lengths. + * <p> + * This class supports static access to the selected cryptographic algorithm and key length, + * and provides methods to create configured {@link javax.crypto.KeyGenerator} + * and {@link javax.crypto.Mac} instances. + * The configuration is initialized statically from a provided {@link Configuration} object. + * <p> + * The {@link SecretManager} has some static method, so static configuration is required + */ +@InterfaceAudience.Public +@InterfaceStability.Evolving +public final class SecretManagerConfig { + private static final Logger LOG = LoggerFactory.getLogger(SecretManagerConfig.class); + private static String selectedAlgorithm; + private static int selectedLength; + + private static final Map<Thread, KeyGenerator> KEYGENS = new WeakHashMap<>(); Review Comment: I just rechecked the code, seems like Keygenerator is a local variable for every SecretManager instance. Maybe this could be thread local, and maybe could improve the performance, but i would rather not touch this for sake of the stability. hadoop-yetus commented on PR #7827: URL: https://github.com/apache/hadoop/pull/7827#issuecomment-3126108509 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 2s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 13m 19s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/15/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | -1 :x: | compile | 0m 16s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/15/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 31s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/15/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -0 :warning: | checkstyle | 0m 28s | [/buildtool-branch-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/15/artifact/out/buildtool-branch-checkstyle-hadoop-common-project_hadoop-common.txt) | The patch fails to run checkstyle in hadoop-common | | -1 :x: | mvnsite | 0m 27s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/15/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in trunk failed. | | -1 :x: | javadoc | 0m 16s | [/branch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/15/artifact/out/branch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-common in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 21s | [/branch-javadoc-hadoop-common-project_hadoop-common-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/15/artifact/out/branch-javadoc-hadoop-common-project_hadoop-common-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-common in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | spotbugs | 2m 50s | | trunk passed | | -1 :x: | shadedclient | 8m 38s | | branch has errors when building and testing our client artifacts. | | -0 :warning: | patch | 9m 2s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 1m 17s | | the patch passed | | -1 :x: | compile | 5m 27s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/15/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 5m 27s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/15/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 25s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/15/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 0m 25s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/15/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 23s | [/buildtool-patch-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/15/artifact/out/buildtool-patch-checkstyle-hadoop-common-project_hadoop-common.txt) | The patch fails to run checkstyle in hadoop-common | | -1 :x: | mvnsite | 0m 26s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/15/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch failed. | | -1 :x: | javadoc | 0m 26s | [/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/15/artifact/out/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-common in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 26s | [/patch-javadoc-hadoop-common-project_hadoop-common-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/15/artifact/out/patch-javadoc-hadoop-common-project_hadoop-common-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-common in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | spotbugs | 0m 25s | [/patch-spotbugs-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/15/artifact/out/patch-spotbugs-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch failed. | | -1 :x: | shadedclient | 5m 7s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 25s | [/patch-unit-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/15/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch failed. | | +0 :ok: | asflicense | 0m 25s | | ASF License check generated no output? | | | | 40m 51s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/15/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7827 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux d2b0f5829ad5 5.15.0-140-generic #150-Ubuntu SMP Sat Apr 12 06:00:09 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / dc7eb8ae892c32c9080f2e9333ee611ac326c794 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/15/testReport/ | | Max. process+thread count | 152 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/15/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7827: URL: https://github.com/apache/hadoop/pull/7827#issuecomment-3126847633 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 14m 24s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 17s | | trunk passed | | +1 :green_heart: | compile | 15m 51s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 58s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 18s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 37s | | trunk passed | | +1 :green_heart: | javadoc | 1m 17s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 55s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 34s | | trunk passed | | +1 :green_heart: | shadedclient | 37m 30s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 37m 57s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 56s | | the patch passed | | +1 :green_heart: | compile | 15m 4s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 4s | | the patch passed | | +1 :green_heart: | compile | 13m 57s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 57s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 12s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/16/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) | hadoop-common-project/hadoop-common: The patch generated 1 new + 3 unchanged - 5 fixed = 4 total (was 8) | | +1 :green_heart: | mvnsite | 1m 38s | | the patch passed | | +1 :green_heart: | javadoc | 1m 15s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 53s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 42s | | the patch passed | | +1 :green_heart: | shadedclient | 37m 51s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 22m 42s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 8s | | The patch does not generate ASF License warnings. | | | | 229m 10s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/16/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7827 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux d37cb15a00f9 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / abbb7a3ee3423fdb67688a63b3ffc2e9c4ac31d5 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/16/testReport/ | | Max. process+thread count | 2152 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7827/16/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. K0K0V0K commented on PR #7827: URL: https://github.com/apache/hadoop/pull/7827#issuecomment-3127087300 Hi @abstractdog @slfan1989, I ended up deleting the SecretManagerConfig file and moved the relevant code into SecretManager. This ensures that only SecretManager has access to key generation and MAC creation, reducing the risk of other components using that logic unintentionally. Thank you for your previous reviews and suggestions\u2014they were very helpful in improving this PR. When you have a moment, could you kindly take another look? Thanks again! abstractdog commented on PR #7827: URL: https://github.com/apache/hadoop/pull/7827#issuecomment-3135574684 > Hi @abstractdog @slfan1989, > > I ended up deleting the SecretManagerConfig file and moved the relevant code into SecretManager. This ensures that only SecretManager has access to key generation and MAC creation, reducing the risk of other components using that logic unintentionally. > > Thank you for your previous reviews and suggestions\u2014they were very helpful in improving this PR. When you have a moment, could you kindly take another look? > > Thanks again! thanks a lot @K0K0V0K for taking care of this, looks good to me! I know not everything you\u2019ve had to do here has been the most exciting\u2014it is what it is, especially as long as we\u2019re dealing with static stuff around SecretManager let me defer the final decision to hadoop folks slfan1989 commented on PR #7827: URL: https://github.com/apache/hadoop/pull/7827#issuecomment-3138597566 @K0K0V0K Thank you for your contribution, LGTM +1. We will wait for 1-2 days to see if there are any other comments. If not, we will merge this PR. cc: @abstractdog slfan1989 merged PR #7827: URL: https://github.com/apache/hadoop/pull/7827 slfan1989 commented on PR #7827: URL: https://github.com/apache/hadoop/pull/7827#issuecomment-3146149735 @K0K0V0K Thanks for the contribution! Merged into trunk. @abstractdog @Hean-Chhinling Thanks for the review!", "created": "2025-07-23T16:17:56.000+0000", "updated": "2025-08-02T02:39:45.000+0000", "derived": {"summary_task": "Summarize this issue: In case of TEZ *DAGAppMaster* the Hadoop *SecretManager* code can not read yarn config xml file, therefore the SELECTED_ALGORITHM and SELECTED_LENGTH variables in SecretManager can not be set at runtime. This can results with the following exception in FIPS environment: {code:java} java.security.InvalidParameterException: Key size for HMAC must be at least 112 bits in approved mode: SHA-1/HMAC at ", "classification_task": "Classify the issue priority and type: In case of TEZ *DAGAppMaster* the Hadoop *SecretManager* code can not read yarn config xml file, therefore the SELECTED_ALGORITHM and SELECTED_LENGTH variables in SecretManager can not be set at runtime. This can results with the following exception in FIPS environment: {code:java} java.security.InvalidParameterException: Key size for HMAC must be at least 112 bits in approved mode: SHA-1/HMAC at ", "qna_task": "Question: What is this issue about?\nAnswer: SecretManager configuration at runtime"}}
{"id": "13624403", "key": "HADOOP-19638", "project": "HADOOP", "summary": "[JDK17] Set Up CI Support JDK17 & JDK21", "description": "Plan to establish a new build pipeline using JDK 17 for Apache Hadoop, as part of the ongoing effort to upgrade the project\u2019s Java compatibility. This pipeline will serve as the foundation for testing and validating future JDK 17 support and ensuring smooth integration with existing CI/CD processes.", "comments": "slfan1989 opened a new pull request, #7831: URL: https://github.com/apache/hadoop/pull/7831 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR JIRA: HADOOP-19638. [JDK17] Set Up CI Support JDK17. ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? slfan1989 commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3121008019 @GauthamBanasandra I\u2019m working on adding a new pipeline in Trunk to support JDK17. While reviewing the history, I noticed you've done in-depth work on this area, particularly around the upgrade to `jenkins.sh` As part of this change, I reviewed the implementation in HADOOP-16888(#2012) to better understand how JDK11 support was introduced. From what I can tell, it seems that enabling JDK17 unit test support only requires configuring the appropriate JDK17-related variables in `jenkins.sh`, similar to how JDK11 was handled. Could you please confirm if that's sufficient? Let me know if there are any additional compatibility steps or considerations I should be aware of. Thank you very much! slfan1989 commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3121011939 @ayushtkn @aajisaka Could you kindly provide some help and guidance? I would really appreciate it. hadoop-yetus commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3121183709 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 0s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 46s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 30m 21s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 0s | | trunk passed | | +1 :green_heart: | shadedclient | 30m 37s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 29s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 0s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 0s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | shadedclient | 30m 30s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 33s | | The patch does not generate ASF License warnings. | | | | 105m 45s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7831/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7831 | | Optional Tests | dupname asflicense mvnsite unit codespell detsecrets shellcheck shelldocs | | uname | Linux b2225bbcc657 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0024f51e6ba35cbe31db45afab54cb9a1521708d | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7831/1/testReport/ | | Max. process+thread count | 548 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7831/1/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. GauthamBanasandra commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3122378936 @slfan1989 yes, what you've done in this PR should be sufficient to get the multi-JDK tests running. The multi-JDK unit tests are only run as part of the nightly CI and not for PRs. Could you please trigger on nightly CI run on ci-hadoop.apache.org? Let me know if you need me to do this. slfan1989 commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3123880810 > @slfan1989 yes, what you've done in this PR should be sufficient to get the multi-JDK tests running. The multi-JDK unit tests are only run as part of the nightly CI and not for PRs. Could you please trigger on nightly CI run on ci-hadoop.apache.org? > > Let me know if you need me to do this. > @slfan1989 yes, what you've done in this PR should be sufficient to get the multi-JDK tests running. The multi-JDK unit tests are only run as part of the nightly CI and not for PRs. Could you please trigger on nightly CI run on ci-hadoop.apache.org? > > Let me know if you need me to do this. Thank you very much for your reply! I'm not very familiar with how to trigger a JDK17 build. If you happen to know how, would you mind helping trigger a JDK17 build for this PR? Also, if we want the PR builds to run under the JDK17 environment as well, do you know how we can configure that? slfan1989 commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3123896957 @steveloughran @ayushtkn @Hexiaoqiao Is it possible to remove JDK 8 compilation support from the trunk? We\u2019d appreciate your input. GauthamBanasandra commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3127566790 @slfan1989 I just checked the nightly pipeline for Linux and it looks like it's isn't using `jenkins.sh` anymore. The `jenkins.sh` is currently used by the nightly pipeline for Windows and the pre-commit pipeline for Linux. Anyway, I just created a new pipeline that runs on JDK 17 - https://ci-hadoop.apache.org/view/Hadoop/job/hadoop-qbt-trunk-java17-linux-x86_64/. You should be having the permission to trigger the builds. I've currently configured it to checkout your repo (I've mentioned the same in the pipeline's description page). I'll reconfigure it to github.com/apache/hadoop once you're done with the testing. GauthamBanasandra commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3127664208 I've started a run with JDK 17 on that pipeline - https://ci-hadoop.apache.org/view/Hadoop/job/hadoop-qbt-trunk-java17-linux-x86_64/1/console. slfan1989 commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3130322461 @GauthamBanasandra Thank you very much for your kind help in configuring this pipeline! GauthamBanasandra commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3131923151 Looks like the Jenkins CI had some trouble with the docker container when it was running the unit tests - ``` cd /home/jenkins/jenkins-home/workspace/hadoop-qbt-trunk-java17-linux-x86_64/sourcedir/hadoop-common-project/hadoop-auth /usr/bin/mvn --batch-mode -Dmaven.repo.local=/home/jenkins/jenkins-home/workspace/hadoop-qbt-trunk-java17-linux-x86_64/yetus-m2/hadoop-HADOOP-19638-full-2 -DskipTests test-compile spotbugs:spotbugs -DskipTests=true > /home/jenkins/jenkins-home/workspace/hadoop-qbt-trunk-java17-linux-x86_64/out/branch-spotbugs-hadoop-common-project_hadoop-auth.txt 2>&1 Cleaning up docker image used for testing. Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? ============================================================================ ============================================================================ Finished build. ============================================================================ ============================================================================ ``` I've started another run - https://ci-hadoop.apache.org/view/Hadoop/job/hadoop-qbt-trunk-java17-linux-x86_64/2/console GauthamBanasandra commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3133728324 > @steveloughran @ayushtkn @Hexiaoqiao @GauthamBanasandra Is it possible to remove JDK 8 compilation support from the trunk? We\u2019d appreciate your input. @slfan1989 I don't think we should remove JDK 8 support yet. ayushtkn commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3135759641 I don't think we can maintain so many JDK versions. There would be some thirdparty dependencies which we might upgrade to & they would be compiled on higher JDK versions. So, the moment that happens, JDK-8 compilation will break. & Not upgrading to those dependencies or not using higher JDK features doesn't make sense. Hadoop in branch-3 dropped JDK-7 support as well, branch-2 only has JDK-7 IIRC. We haven't marked the lower branches which are JDK-8 compliant as EOL, so, it isn't a deal breaker for someone who wants to stick to JDK-8, Moreover we can't run tests & all on all JDK versions, We need to pick one only. I think most of the projects are dropping the JDK-8 support and chasing for higher versions. I know in Hive it is like 4.0 line -JDK-8, 4.1.0 line - JDK-17 & now master branch min JDK-21 compile time, same with Tez the master is on JDK-21 compile time support. I don't think sticking to any legacy stuff is worth it if it adds any technical debt for us, Should chase for a higher version. Just my opinion, other might feel differently steveloughran commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3137400811 @GauthamBanasandra 3.5 is java17+ only. there are things we depend on which are java11; there are places where we can't make use of the JDK features, let alone language changes. we have stayed on java8 for longer than we've ever been on any older java version. Time to move on. If you want to keep java8 support alive, then backport new features to branch-3.4. Me? I'm looking at bug fixes only there. steveloughran commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3137407351 ...assuming we switch now to java17 on trunk, all backports to branch-3.4 must be retested on that branch in case something in the JDK itself changed. With the JUnit move that's needed anyway, IMO slfan1989 commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3138524338 @GauthamBanasandra @ayushtkn @steveloughran Thank you all for the feedback! From my perspective, it makes sense to drop support for JDK 8, and I also agree with using JDK 17 for the trunk branch. However, there are still some issues with the JDK 17 pipeline compilation, and I will work on fixing the related unit tests. slfan1989 commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3146156723 I have adjusted the trunk branch's build to default to JDK 17 support. hadoop-yetus commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3146204334 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 53s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 11m 29s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 29m 12s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 0s | | trunk passed | | +1 :green_heart: | shadedclient | 30m 4s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 27s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 0s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 0s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 1s | | No new issues. | | +1 :green_heart: | shadedclient | 30m 14s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 34s | | The patch does not generate ASF License warnings. | | | | 105m 11s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7831/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7831 | | Optional Tests | dupname asflicense mvnsite unit codespell detsecrets shellcheck shelldocs | | uname | Linux 459b670a40fa 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 577479ba84422a763054b1f4684b0a8fa23b3362 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7831/2/testReport/ | | Max. process+thread count | 547 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7831/2/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. GauthamBanasandra commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3192520796 @slfan1989 could you please let us know at what point does the compilation with JDK 8 breaks? hadoop-yetus commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3236498588 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 21m 44s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 7m 58s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 29m 42s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 0s | | trunk passed | | +1 :green_heart: | shadedclient | 29m 41s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 28s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 0s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 0s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 1s | | No new issues. | | +1 :green_heart: | shadedclient | 29m 57s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 122m 28s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7831/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7831 | | Optional Tests | dupname asflicense mvnsite unit codespell detsecrets shellcheck shelldocs | | uname | Linux 97acd1950f16 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f8e749693777c3816e6262928a717ccbf70921c4 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7831/3/testReport/ | | Max. process+thread count | 611 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7831/3/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. pan3793 commented on code in PR #7831: URL: https://github.com/apache/hadoop/pull/7831#discussion_r2314272985 ########## dev-support/jenkins.sh: ########## @@ -124,9 +124,8 @@ function run_ci() { YETUS_ARGS+=(\"--mvn-custom-repos\") YETUS_ARGS+=(\"--dockermemlimit=22g\") - # test with Java 8 and 11 - YETUS_ARGS+=(\"--java-home=/usr/lib/jvm/java-8-openjdk-amd64\") - YETUS_ARGS+=(\"--multijdkdirs=/usr/lib/jvm/java-11-openjdk-amd64\") + # test with Java 17 + YETUS_ARGS+=(\"--multijdkdirs=/usr/lib/jvm/java-17-openjdk-amd64\") Review Comment: now, all platforms on the trunk branch have `java-17-openjdk-amd64` installed in the docker image, it's time to move forward hadoop-yetus commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3340964462 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 59s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 51s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 35m 25s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 0s | | trunk passed | | +1 :green_heart: | shadedclient | 30m 33s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 46s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 0s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 0s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 1s | | No new issues. | | +1 :green_heart: | shadedclient | 30m 5s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 34s | | The patch does not generate ASF License warnings. | | | | 109m 37s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7831/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7831 | | Optional Tests | dupname asflicense mvnsite unit codespell detsecrets shellcheck shelldocs | | uname | Linux 8e7dd49e956b 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / db6a4cb61b785de00ed82cd3e9957acab0746cc4 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7831/5/testReport/ | | Max. process+thread count | 546 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7831/5/console | | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3393051785 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 21m 59s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 20s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 35m 7s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 0s | | trunk passed | | +1 :green_heart: | shadedclient | 29m 56s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 29s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 0s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 0s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 1s | | No new issues. | | +1 :green_heart: | shadedclient | 29m 35s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 129m 16s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7831/6/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7831 | | Optional Tests | dupname asflicense mvnsite unit codespell detsecrets shellcheck shelldocs | | uname | Linux aefc94cb1d97 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 3b2a01d785cffcf73c9ecca6a48caa45f9a101ae | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7831/6/testReport/ | | Max. process+thread count | 585 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7831/6/console | | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 merged PR #7831: URL: https://github.com/apache/hadoop/pull/7831 slfan1989 commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3393790285 @ayushtkn @steveloughran @GauthamBanasandra @cnauroth @pan3793 Merge this PR into the trunk branch to support pipeline compilation with JDK 17. slfan1989 opened a new pull request, #8030: URL: https://github.com/apache/hadoop/pull/8030 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR JIRA: HADOOP-19638. [Addendum] [JDK17] Set Up CI Support JDK17. ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #8030: URL: https://github.com/apache/hadoop/pull/8030#issuecomment-3395710867 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 9m 10s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 52s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 15m 51s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8030/2/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | +1 :green_heart: | mvnsite | 0m 0s | | trunk passed | | +1 :green_heart: | shadedclient | 13m 23s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 19s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 0s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 0s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 1s | | No new issues. | | +1 :green_heart: | shadedclient | 13m 34s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 18s | | The patch does not generate ASF License warnings. | | | | 62m 58s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8030/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8030 | | Optional Tests | dupname asflicense mvnsite unit codespell detsecrets shellcheck shelldocs | | uname | Linux 8828e8951baa 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / ac73fe197ca9f8418c9af6397d08903a130f4465 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8030/2/testReport/ | | Max. process+thread count | 573 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8030/2/console | | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #8030: URL: https://github.com/apache/hadoop/pull/8030#issuecomment-3395798651 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 22m 5s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 52s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 37m 36s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 0s | | trunk passed | | +1 :green_heart: | shadedclient | 32m 23s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 32s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 0s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 0s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | shadedclient | 32m 35s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 136m 57s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8030/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8030 | | Optional Tests | dupname asflicense mvnsite unit codespell detsecrets shellcheck shelldocs | | uname | Linux d7c4c36f06f4 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / ac73fe197ca9f8418c9af6397d08903a130f4465 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8030/1/testReport/ | | Max. process+thread count | 529 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8030/1/console | | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #8030: URL: https://github.com/apache/hadoop/pull/8030#issuecomment-3395810701 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 38s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 15m 43s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 0s | | trunk passed | | +1 :green_heart: | shadedclient | 13m 38s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 19s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 0s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 0s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | shadedclient | 13m 20s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 21s | | The patch does not generate ASF License warnings. | | | | 54m 52s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8030/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8030 | | Optional Tests | dupname asflicense mvnsite unit codespell detsecrets shellcheck shelldocs | | uname | Linux 0bd5ffd3e04f 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 2c677aca06d2cab666b3cdc4fc5031e5c794f12c | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8030/3/testReport/ | | Max. process+thread count | 729 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8030/3/console | | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on PR #8030: URL: https://github.com/apache/hadoop/pull/8030#issuecomment-3397990368 @cnauroth @szetszwo Could you please help review this PR? Thank you very much! We have confirmed that the current Hadoop version can be successfully compiled with both JDK 17 and JDK 21. I plan to add support for these two JDK versions. slfan1989 commented on PR #8030: URL: https://github.com/apache/hadoop/pull/8030#issuecomment-3401292933 @steveloughran Could you please review this PR? Thank you very much! slfan1989 commented on PR #8030: URL: https://github.com/apache/hadoop/pull/8030#issuecomment-3405488556 @Hexiaoqiao Could you please review this PR? Thank you very much! szetszwo commented on code in PR #8030: URL: https://github.com/apache/hadoop/pull/8030#discussion_r2433036912 ########## dev-support/jenkins.sh: ########## @@ -124,8 +124,9 @@ function run_ci() { YETUS_ARGS+=(\"--mvn-custom-repos\") YETUS_ARGS+=(\"--dockermemlimit=22g\") - # test with Java 17 - YETUS_ARGS+=(\"--multijdkdirs=/usr/lib/jvm/java-17-openjdk-amd64\") + # test with Java 17 & Java 21 + YETUS_ARGS+=(\"--java-home=/usr/lib/jvm/java-17-openjdk-amd64\") + YETUS_ARGS+=(\"--multijdkdirs=/usr/lib/jvm/java-21-openjdk-amd64\") Review Comment: Do we need two jdk dirs? ``` --multijdkdirs=/usr/lib/jvm/java-21-openjdk-amd64,/usr/lib/jvm/java-17-openjdk-amd64 ``` https://yetus.apache.org/documentation/0.15.1/precommit/usage-intro/#multijdk slfan1989 commented on code in PR #8030: URL: https://github.com/apache/hadoop/pull/8030#discussion_r2433097240 ########## dev-support/jenkins.sh: ########## @@ -124,8 +124,9 @@ function run_ci() { YETUS_ARGS+=(\"--mvn-custom-repos\") YETUS_ARGS+=(\"--dockermemlimit=22g\") - # test with Java 17 - YETUS_ARGS+=(\"--multijdkdirs=/usr/lib/jvm/java-17-openjdk-amd64\") + # test with Java 17 & Java 21 + YETUS_ARGS+=(\"--java-home=/usr/lib/jvm/java-17-openjdk-amd64\") + YETUS_ARGS+=(\"--multijdkdirs=/usr/lib/jvm/java-21-openjdk-amd64\") Review Comment: Thank you very much for helping with the review! Since I\u2019m not yet very familiar with Yetus, the current configuration was created by referring to and simulating the original JDK 8 and JDK 11 configurations. ``` # test with Java 8 and 11 YETUS_ARGS+=(\"--java-home=/usr/lib/jvm/java-8-openjdk-amd64\") YETUS_ARGS+=(\"--multijdkdirs=/usr/lib/jvm/java-11-openjdk-amd64\") YETUS_ARGS+=(\"--multijdktests=compile\") ``` szetszwo commented on code in PR #8030: URL: https://github.com/apache/hadoop/pull/8030#discussion_r2433168273 ########## dev-support/jenkins.sh: ########## @@ -124,8 +124,9 @@ function run_ci() { YETUS_ARGS+=(\"--mvn-custom-repos\") YETUS_ARGS+=(\"--dockermemlimit=22g\") - # test with Java 17 - YETUS_ARGS+=(\"--multijdkdirs=/usr/lib/jvm/java-17-openjdk-amd64\") + # test with Java 17 & Java 21 + YETUS_ARGS+=(\"--java-home=/usr/lib/jvm/java-17-openjdk-amd64\") + YETUS_ARGS+=(\"--multijdkdirs=/usr/lib/jvm/java-21-openjdk-amd64\") Review Comment: I see. Let's keep using it then. slfan1989 merged PR #8030: URL: https://github.com/apache/hadoop/pull/8030 slfan1989 commented on PR #8030: URL: https://github.com/apache/hadoop/pull/8030#issuecomment-3411088399 @szetszwo @Hexiaoqiao @zhtttylz Thank you very much for reviewing the code!", "created": "2025-07-23T13:49:51.000+0000", "updated": "2025-10-23T00:11:53.000+0000", "derived": {"summary_task": "Summarize this issue: Plan to establish a new build pipeline using JDK 17 for Apache Hadoop, as part of the ongoing effort to upgrade the project\u2019s Java compatibility. This pipeline will serve as the foundation for testing and validating future JDK 17 support and ensuring smooth integration with existing CI/CD processes.", "classification_task": "Classify the issue priority and type: Plan to establish a new build pipeline using JDK 17 for Apache Hadoop, as part of the ongoing effort to upgrade the project\u2019s Java compatibility. This pipeline will serve as the foundation for testing and validating future JDK 17 support and ensuring smooth integration with existing CI/CD processes.", "qna_task": "Question: What is this issue about?\nAnswer: [JDK17] Set Up CI Support JDK17 & JDK21"}}
{"id": "13624401", "key": "HADOOP-19637", "project": "HADOOP", "summary": "[JDK17] Attempted to build on CentOS Stream 9.", "description": "We aim to successfully compile Apache Hadoop on CentOS Stream 9 to verify the feasibility of using this platform in future development and deployment environments. This task may involve addressing potential compatibility issues related to GCC and libc versions. Initial attempts will focus on resolving compilation challenges and ensuring the platform meets the necessary requirements for Hadoop builds.", "comments": "", "created": "2025-07-23T13:37:59.000+0000", "updated": "2025-07-23T13:38:21.000+0000", "derived": {"summary_task": "Summarize this issue: We aim to successfully compile Apache Hadoop on CentOS Stream 9 to verify the feasibility of using this platform in future development and deployment environments. This task may involve addressing potential compatibility issues related to GCC and libc versions. Initial attempts will focus on resolving compilation challenges and ensuring the platform meets the necessary requirements for Hadoop buil", "classification_task": "Classify the issue priority and type: We aim to successfully compile Apache Hadoop on CentOS Stream 9 to verify the feasibility of using this platform in future development and deployment environments. This task may involve addressing potential compatibility issues related to GCC and libc versions. Initial attempts will focus on resolving compilation challenges and ensuring the platform meets the necessary requirements for Hadoop buil", "qna_task": "Question: What is this issue about?\nAnswer: [JDK17] Attempted to build on CentOS Stream 9."}}
{"id": "13624396", "key": "HADOOP-19636", "project": "HADOOP", "summary": "[JDK17] Remove CentOS 7 Support and Clean Up Dockerfile.", "description": "Removed build support for the following EOL (End-of-Life) operating system versions (e.g., CentOS 7, Debian 9, etc.). Cleaned up redundant code and dependency configurations in the Dockerfile related to these operating systems. Optimized the build logic to ensure that currently supported OS versions build successfully.", "comments": "slfan1989 opened a new pull request, #7822: URL: https://github.com/apache/hadoop/pull/7822 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR JIRA: HADOOP-19636. [JDK17] Remove EOL OS Support and Clean Up Dockerfile. ### How was this patch tested? test. ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3108826927 (!) A patch to the testing environment has been detected. Re-executing against the patched versions to perform further tests. The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/1/console in case of problems. hadoop-yetus commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3109175364 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 2m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 1s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 42s | | Maven dependency ordering for branch | | +1 :green_heart: | shadedclient | 40m 51s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 32s | | Maven dependency ordering for patch | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | hadolint | 0m 2s | | No new issues. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | shadedclient | 40m 8s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 95m 21s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7822 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint | | uname | Linux 6fb52aab7b2a 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / d4650c0d79570cf96fd5c62db6e8a00157b647d5 | | Max. process+thread count | 564 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/1/console | | versions | git=2.25.1 maven=3.6.3 hadolint=1.11.1-0-g0e692dd shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3111531611 @ayushtkn @GauthamBanasandra Could you please help review this PR? Thank you very much! cc: @pan3793 pan3793 commented on code in PR #7822: URL: https://github.com/apache/hadoop/pull/7822#discussion_r2227179274 ########## dev-support/docker/Dockerfile_centos_8: ########## @@ -1,118 +0,0 @@ -# Licensed to the Apache Software Foundation (ASF) under one -# or more contributor license agreements. See the NOTICE file -# distributed with this work for additional information -# regarding copyright ownership. The ASF licenses this file -# to you under the Apache License, Version 2.0 (the -# \"License\"); you may not use this file except in compliance -# with the License. You may obtain a copy of the License at -# -# http://www.apache.org/licenses/LICENSE-2.0 -# -# Unless required by applicable law or agreed to in writing, software -# distributed under the License is distributed on an \"AS IS\" BASIS, -# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. -# See the License for the specific language governing permissions and -# limitations under the License. - -# Dockerfile for installing the necessary dependencies for building Hadoop. -# See BUILDING.txt. - -FROM centos:8 Review Comment: I don't think we should remove centos8, instead, we should migrate it Rocky Linux 8 (or other RHEL-like OS) in place, then 9 or 10 ########## dev-support/Jenkinsfile: ########## @@ -113,144 +113,6 @@ pipeline { } } - // This is an optional stage which runs only when there's a change in - // C++/C++ build/platform. - // This stage serves as a means of cross platform validation, which is - // really needed to ensure that any C++ related/platform change doesn't - // break the Hadoop build on Centos 7. - stage ('precommit-run Centos 7') { Review Comment: there's leftover ceontos 7 stuff at line 86 ########## dev-support/docker/Dockerfile_debian_10: ########## @@ -1,102 +0,0 @@ -# Licensed to the Apache Software Foundation (ASF) under one -# or more contributor license agreements. See the NOTICE file -# distributed with this work for additional information -# regarding copyright ownership. The ASF licenses this file -# to you under the Apache License, Version 2.0 (the -# \"License\"); you may not use this file except in compliance -# with the License. You may obtain a copy of the License at -# -# http://www.apache.org/licenses/LICENSE-2.0 -# -# Unless required by applicable law or agreed to in writing, software -# distributed under the License is distributed on an \"AS IS\" BASIS, -# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. -# See the License for the specific language governing permissions and -# limitations under the License. - -# Dockerfile for installing the necessary dependencies for building Hadoop. -# See BUILDING.txt. - -FROM debian:10 Review Comment: same here, we should upgrade it to debian 12 or 13 in place pan3793 commented on code in PR #7822: URL: https://github.com/apache/hadoop/pull/7822#discussion_r2227178384 ########## dev-support/Jenkinsfile: ########## @@ -113,144 +113,6 @@ pipeline { } } - // This is an optional stage which runs only when there's a change in - // C++/C++ build/platform. - // This stage serves as a means of cross platform validation, which is - // really needed to ensure that any C++ related/platform change doesn't - // break the Hadoop build on Centos 7. - stage ('precommit-run Centos 7') { Review Comment: there's leftover centos 7 stuff at line 86 slfan1989 commented on code in PR #7822: URL: https://github.com/apache/hadoop/pull/7822#discussion_r2227281222 ########## dev-support/docker/Dockerfile_centos_8: ########## @@ -1,118 +0,0 @@ -# Licensed to the Apache Software Foundation (ASF) under one -# or more contributor license agreements. See the NOTICE file -# distributed with this work for additional information -# regarding copyright ownership. The ASF licenses this file -# to you under the Apache License, Version 2.0 (the -# \"License\"); you may not use this file except in compliance -# with the License. You may obtain a copy of the License at -# -# http://www.apache.org/licenses/LICENSE-2.0 -# -# Unless required by applicable law or agreed to in writing, software -# distributed under the License is distributed on an \"AS IS\" BASIS, -# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. -# See the License for the specific language governing permissions and -# limitations under the License. - -# Dockerfile for installing the necessary dependencies for building Hadoop. -# See BUILDING.txt. - -FROM centos:8 Review Comment: From a personal perspective, I don't agree with your suggestion. I believe we should completely remove operating systems that have reached their End of Life (EOL). If we need to support CentOS 9 or Debian 12 in the future, it should be done by submitting a new PR for a thorough evaluation. Rather than maintaining multiple Dockerfiles, I prefer a more lightweight approach, such as providing support through documentation. As the number of supported operating systems increases, if we have to maintain Dockerfiles for each one, we could end up managing dozens, which is neither cost-effective nor sustainable. pan3793 commented on code in PR #7822: URL: https://github.com/apache/hadoop/pull/7822#discussion_r2227342002 ########## dev-support/docker/Dockerfile_centos_8: ########## @@ -1,118 +0,0 @@ -# Licensed to the Apache Software Foundation (ASF) under one -# or more contributor license agreements. See the NOTICE file -# distributed with this work for additional information -# regarding copyright ownership. The ASF licenses this file -# to you under the Apache License, Version 2.0 (the -# \"License\"); you may not use this file except in compliance -# with the License. You may obtain a copy of the License at -# -# http://www.apache.org/licenses/LICENSE-2.0 -# -# Unless required by applicable law or agreed to in writing, software -# distributed under the License is distributed on an \"AS IS\" BASIS, -# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. -# See the License for the specific language governing permissions and -# limitations under the License. - -# Dockerfile for installing the necessary dependencies for building Hadoop. -# See BUILDING.txt. - -FROM centos:8 Review Comment: > I believe we should completely remove operating systems that have reached their End of Life (EOL). If we need to support CentOS 9 or Debian 12 in the future, it should be done by submitting a new PR for a thorough evaluation. I don't see much benefit in your proposal, I suppose upgrading in place is straightforward, and can leave clear diff in the commit history to guide users to understand what they should change for planning Hadoop cluster OS upgrading. > Rather than maintaining multiple Dockerfiles, I prefer a more lightweight approach, such as providing support through documentation. The documentation can easily become outdated (you can try `Building on macOS (without Docker)` in `BUILDING.txt`). As I replied here, I think the `Dockerfile` itself is the best documentation for setup the building env. https://lists.apache.org/thread/2ypqcrnsth3jk21rpjvjv53tntz21ht8 slfan1989 commented on code in PR #7822: URL: https://github.com/apache/hadoop/pull/7822#discussion_r2227448044 ########## dev-support/docker/Dockerfile_centos_8: ########## @@ -1,118 +0,0 @@ -# Licensed to the Apache Software Foundation (ASF) under one -# or more contributor license agreements. See the NOTICE file -# distributed with this work for additional information -# regarding copyright ownership. The ASF licenses this file -# to you under the Apache License, Version 2.0 (the -# \"License\"); you may not use this file except in compliance -# with the License. You may obtain a copy of the License at -# -# http://www.apache.org/licenses/LICENSE-2.0 -# -# Unless required by applicable law or agreed to in writing, software -# distributed under the License is distributed on an \"AS IS\" BASIS, -# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. -# See the License for the specific language governing permissions and -# limitations under the License. - -# Dockerfile for installing the necessary dependencies for building Hadoop. -# See BUILDING.txt. - -FROM centos:8 Review Comment: The choice of operating system should be made by the user, and therefore, the resolution of compilation issues should also be handled by the user. Take CentOS 7 as an example, which has multiple versions (such as 7.2, 7.3, 7.9, etc.). Different versions may have configuration or dependency differences (e.g., glibc, gcc versions), which can lead to compilation issues, such as with protobuf or native package compilation. For these issues, we should not add extra workarounds, as that would make the project redundant. If we were to upgrade to `CentOS 9`, we would change the Dockerfile name from `Dockerfile_centos_8` to `Dockerfile_centos_9`. Users comparing the diff would see that `Dockerfile_centos_8` has been deleted and replaced with `Dockerfile_centos_9`, which contains entirely new content. pan3793 commented on code in PR #7822: URL: https://github.com/apache/hadoop/pull/7822#discussion_r2227687926 ########## dev-support/docker/Dockerfile_centos_8: ########## @@ -1,118 +0,0 @@ -# Licensed to the Apache Software Foundation (ASF) under one -# or more contributor license agreements. See the NOTICE file -# distributed with this work for additional information -# regarding copyright ownership. The ASF licenses this file -# to you under the Apache License, Version 2.0 (the -# \"License\"); you may not use this file except in compliance -# with the License. You may obtain a copy of the License at -# -# http://www.apache.org/licenses/LICENSE-2.0 -# -# Unless required by applicable law or agreed to in writing, software -# distributed under the License is distributed on an \"AS IS\" BASIS, -# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. -# See the License for the specific language governing permissions and -# limitations under the License. - -# Dockerfile for installing the necessary dependencies for building Hadoop. -# See BUILDING.txt. - -FROM centos:8 Review Comment: I understand that we can not enumerate all Linux distributions and versions. I believe most enterprises use Debian/RHEL family of Linux distributions to run Hadoop. Given the limitation of developer resources in the Hadoop community, how about keeping only 2 OS Dockerfiles and CI pipelines - the latest(or sub-latest) version of Ubuntu(the default env for building, testing, releasing) and Rocky Linux(only verify the compilation)? They will serve as reference for users who want to set up a building environment based on their preferred Linux distribution. Hexiaoqiao commented on code in PR #7822: URL: https://github.com/apache/hadoop/pull/7822#discussion_r2230528916 ########## dev-support/docker/Dockerfile_centos_8: ########## @@ -1,118 +0,0 @@ -# Licensed to the Apache Software Foundation (ASF) under one -# or more contributor license agreements. See the NOTICE file -# distributed with this work for additional information -# regarding copyright ownership. The ASF licenses this file -# to you under the Apache License, Version 2.0 (the -# \"License\"); you may not use this file except in compliance -# with the License. You may obtain a copy of the License at -# -# http://www.apache.org/licenses/LICENSE-2.0 -# -# Unless required by applicable law or agreed to in writing, software -# distributed under the License is distributed on an \"AS IS\" BASIS, -# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. -# See the License for the specific language governing permissions and -# limitations under the License. - -# Dockerfile for installing the necessary dependencies for building Hadoop. -# See BUILDING.txt. - -FROM centos:8 Review Comment: My point is that we should remove some dependencies which are EOL, just like some other module. Back to here , CentOS 8 has reached its EOL and the packages re no longer available on mirror.centos.org site.(https://www.centos.org/centos-linux-eol/), So +1 to Shilun's comments from my side. cc @pan3793 What do you think about. Thanks. ########## dev-support/Jenkinsfile: ########## @@ -113,144 +113,6 @@ pipeline { } } - // This is an optional stage which runs only when there's a change in - // C++/C++ build/platform. - // This stage serves as a means of cross platform validation, which is - // really needed to ensure that any C++ related/platform change doesn't - // break the Hadoop build on Centos 7. - stage ('precommit-run Centos 7') { Review Comment: +1 pan3793 commented on code in PR #7822: URL: https://github.com/apache/hadoop/pull/7822#discussion_r2232440829 ########## dev-support/docker/Dockerfile_centos_8: ########## @@ -1,118 +0,0 @@ -# Licensed to the Apache Software Foundation (ASF) under one -# or more contributor license agreements. See the NOTICE file -# distributed with this work for additional information -# regarding copyright ownership. The ASF licenses this file -# to you under the Apache License, Version 2.0 (the -# \"License\"); you may not use this file except in compliance -# with the License. You may obtain a copy of the License at -# -# http://www.apache.org/licenses/LICENSE-2.0 -# -# Unless required by applicable law or agreed to in writing, software -# distributed under the License is distributed on an \"AS IS\" BASIS, -# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. -# See the License for the specific language governing permissions and -# limitations under the License. - -# Dockerfile for installing the necessary dependencies for building Hadoop. -# See BUILDING.txt. - -FROM centos:8 Review Comment: @Hexiaoqiao If you agree to retain at least one RHEL-family OS Dockerfile for Hadoop building, I suggest keeping CentOS 8, because CentOS 8 works well(the `mirror.centos.org` site was replaced by `vault.centos.org`, see `dev-support/docker/pkg-resolver/set-vault-as-baseurl-centos.sh`) for the Hadoop project build as of today, I plan to migrate it to Rocky Linux 8 soon. https://endoflife.date/rocky-linux stoty commented on code in PR #7822: URL: https://github.com/apache/hadoop/pull/7822#discussion_r2234892718 ########## dev-support/Jenkinsfile: ########## @@ -113,144 +113,6 @@ pipeline { } } - // This is an optional stage which runs only when there's a change in - // C++/C++ build/platform. - // This stage serves as a means of cross platform validation, which is - // really needed to ensure that any C++ related/platform change doesn't - // break the Hadoop build on Centos 7. - stage ('precommit-run Centos 7') { - environment { - SOURCEDIR = \"${WORKSPACE}/centos-7/src\" - PATCHDIR = \"${WORKSPACE}/centos-7/out\" - DOCKERFILE = \"${SOURCEDIR}/dev-support/docker/Dockerfile_centos_7\" - IS_OPTIONAL = 1 - } - - steps { - withCredentials(getGithubCreds()) { - sh '''#!/usr/bin/env bash - - chmod u+x \"${SOURCEDIR}/dev-support/jenkins.sh\" - \"${SOURCEDIR}/dev-support/jenkins.sh\" run_ci - ''' - } - } - - post { - // Since this is an optional platform, we want to copy the artifacts - // and archive it only if the build fails, to help with debugging. - failure { - sh '''#!/usr/bin/env bash - - cp -Rp \"${WORKSPACE}/centos-7/out\" \"${WORKSPACE}\" - ''' - archiveArtifacts \"out/**\" - } - - cleanup() { - script { - sh '''#!/usr/bin/env bash - - chmod u+x \"${SOURCEDIR}/dev-support/jenkins.sh\" - \"${SOURCEDIR}/dev-support/jenkins.sh\" cleanup_ci_proc - ''' - } - } - } - } - - // This is an optional stage which runs only when there's a change in - // C++/C++ build/platform. - // This stage serves as a means of cross platform validation, which is - // really needed to ensure that any C++ related/platform change doesn't - // break the Hadoop build on Centos 8. - stage ('precommit-run Centos 8') { Review Comment: Instead of removing CentOS 8, I would suggest replacing it with another supported RHEL8 clone, like Rocky Linux. slfan1989 commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3125789130 > I think we should replace CentOs 8 instead of dropping it outright. Thank you for your feedback. Feel free to continue sharing your thoughts in this email thread. So far, I\u2019ve received comments from @ayushtkn , @Hexiaoqiao , @cnauroth, @pan3793. We are still in the discussion phase, and a final decision will be made based on the collective input. https://lists.apache.org/thread/2ypqcrnsth3jk21rpjvjv53tntz21ht8 slfan1989 commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3125804239 @GauthamBanasandra Thank you, and I look forward to hearing your thoughts on this issue. stoty commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3125812503 Can you please forward the last email to stoty@apache.org @slfan1989 so that I can reply? I am not subscribed to commons-dev yet. slfan1989 commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3125839533 > Can you please forward the last email to [stoty@apache.org](mailto:stoty@apache.org) @slfan1989 so that I can reply? > I am not subscribed to commons-dev yet. I\u2019ve cc\u2019d you on the email\u2014please have a look when it\u2019s convenient for you. steveloughran commented on code in PR #7822: URL: https://github.com/apache/hadoop/pull/7822#discussion_r2237553419 ########## dev-support/docker/Dockerfile_centos_8: ########## @@ -1,118 +0,0 @@ -# Licensed to the Apache Software Foundation (ASF) under one -# or more contributor license agreements. See the NOTICE file -# distributed with this work for additional information -# regarding copyright ownership. The ASF licenses this file -# to you under the Apache License, Version 2.0 (the -# \"License\"); you may not use this file except in compliance -# with the License. You may obtain a copy of the License at -# -# http://www.apache.org/licenses/LICENSE-2.0 -# -# Unless required by applicable law or agreed to in writing, software -# distributed under the License is distributed on an \"AS IS\" BASIS, -# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. -# See the License for the specific language governing permissions and -# limitations under the License. - -# Dockerfile for installing the necessary dependencies for building Hadoop. -# See BUILDING.txt. - -FROM centos:8 Review Comment: I'm supportive of a RHEL variant. There's also the option of an amazon linux container image, which uses yum. slfan1989 commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3146234748 @Hexiaoqiao @steveloughran @stoty @pan3793 Thank you all for your participation and valuable feedback! After careful consideration, I have decided to adopt your suggestions in the upcoming JDK 17 upgrade. Regarding the removal of support for EOL operating systems, I plan to discontinue the Docker and Jenkins build commands on CentOS 7, while continuing to support image builds on CentOS 8 and Debian 10. Moving forward, we will also focus on upgrading to CentOS 9 and higher versions of Debian. cc: @GauthamBanasandra @ayushtkn @cnauroth hadoop-yetus commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3146238349 (!) A patch to the testing environment has been detected. Re-executing against the patched versions to perform further tests. The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/2/console in case of problems. hadoop-yetus commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3146239802 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 3m 15s | | Docker failed to build run-specific yetus/hadoop:tp-32546}. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7822 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/2/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3146256846 (!) A patch to the testing environment has been detected. Re-executing against the patched versions to perform further tests. The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/3/console in case of problems. hadoop-yetus commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3146256976 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 0m 17s | | Docker failed to build run-specific yetus/hadoop:tp-288}. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7822 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/3/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3146852693 (!) A patch to the testing environment has been detected. Re-executing against the patched versions to perform further tests. The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/4/console in case of problems. hadoop-yetus commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3146853778 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 3m 16s | | Docker failed to build run-specific yetus/hadoop:tp-11920}. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7822 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/4/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3146940448 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 28m 19s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | hadolint | 0m 0s | | hadolint was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 6s | | Maven dependency ordering for branch | | +1 :green_heart: | shadedclient | 49m 10s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 45s | | Maven dependency ordering for patch | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 40m 4s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 53s | | The patch does not generate ASF License warnings. | | | | 121m 4s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7822 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs hadolint jsonlint | | uname | Linux 6d7f8de11c86 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 4d2cd528c8c45ef0ad654fbf841e3d21d49519e9 | | Max. process+thread count | 604 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/5/console | | versions | git=2.27.0 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3146975557 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 10s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | hadolint | 0m 0s | | hadolint was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 0m 44s | | Maven dependency ordering for branch | | +1 :green_heart: | shadedclient | 40m 48s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 44s | | Maven dependency ordering for patch | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 40m 16s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 51s | | The patch does not generate ASF License warnings. | | | | 85m 41s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7822 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs hadolint jsonlint | | uname | Linux 3163bdea0f05 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 4d2cd528c8c45ef0ad654fbf841e3d21d49519e9 | | Max. process+thread count | 590 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/5/console | | versions | git=2.27.0 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3146976128 (!) A patch to the testing environment has been detected. Re-executing against the patched versions to perform further tests. The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/5/console in case of problems. hadoop-yetus commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3146976620 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 1m 18s | | Docker failed to build run-specific yetus/hadoop:tp-25352}. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7822 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/5/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. stoty commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3149167804 This looks good to me, but this no longer matches the JIRA/commit description, as only Centos7 is removed now, but ubuntu 10 and Centos 8 is kept. hadoop-yetus commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3194118724 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 29m 3s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | hadolint | 0m 0s | | hadolint was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 3s | | Maven dependency ordering for branch | | +1 :green_heart: | shadedclient | 49m 37s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 46s | | Maven dependency ordering for patch | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 39m 31s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 56s | | The patch does not generate ASF License warnings. | | | | 121m 50s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/6/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7822 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs hadolint jsonlint | | uname | Linux 59705cfced91 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a789f690dfe0df422d093f743e65fe648ad8d048 | | Max. process+thread count | 533 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/6/console | | versions | git=2.27.0 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3194151066 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 12s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | hadolint | 0m 0s | | hadolint was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 0m 45s | | Maven dependency ordering for branch | | +1 :green_heart: | shadedclient | 40m 19s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 47s | | Maven dependency ordering for patch | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 39m 23s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 56s | | The patch does not generate ASF License warnings. | | | | 84m 32s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/6/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7822 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs hadolint jsonlint | | uname | Linux b38d1747439b 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a789f690dfe0df422d093f743e65fe648ad8d048 | | Max. process+thread count | 583 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/6/console | | versions | git=2.27.0 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3194151639 (!) A patch to the testing environment has been detected. Re-executing against the patched versions to perform further tests. The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/6/console in case of problems. hadoop-yetus commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3194152152 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 1m 21s | | Docker failed to build run-specific yetus/hadoop:tp-21562}. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7822 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/6/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3217036390 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 28m 59s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | hadolint | 0m 0s | | hadolint was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 50s | | Maven dependency ordering for branch | | +1 :green_heart: | shadedclient | 50m 20s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 46s | | Maven dependency ordering for patch | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 39m 24s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 57s | | The patch does not generate ASF License warnings. | | | | 122m 26s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/7/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7822 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs hadolint jsonlint | | uname | Linux 1f22f16ee8ab 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 8925c4af7dc0ad369ecf37361333c5f2375fe54a | | Max. process+thread count | 529 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/7/console | | versions | git=2.27.0 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3217123391 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 15s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | hadolint | 0m 0s | | hadolint was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 0m 46s | | Maven dependency ordering for branch | | +1 :green_heart: | shadedclient | 41m 29s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 44s | | Maven dependency ordering for patch | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 39m 37s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 55s | | The patch does not generate ASF License warnings. | | | | 85m 52s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/7/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7822 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs hadolint jsonlint | | uname | Linux ea126888a24e 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 8925c4af7dc0ad369ecf37361333c5f2375fe54a | | Max. process+thread count | 534 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/7/console | | versions | git=2.27.0 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3217125222 (!) A patch to the testing environment has been detected. Re-executing against the patched versions to perform further tests. The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/7/console in case of problems. hadoop-yetus commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3217126419 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 1m 6s | | Docker failed to build run-specific yetus/hadoop:tp-13190}. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7822 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/7/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. pan3793 commented on code in PR #7822: URL: https://github.com/apache/hadoop/pull/7822#discussion_r2296430883 ########## dev-support/docker/pkg-resolver/packages.json: ########## @@ -35,7 +30,6 @@ \"ubuntu:focal\": \"build-essential\", \"ubuntu:noble\": \"build-essential\", \"ubuntu:focal::arch64\": \"build-essential\", Review Comment: ```suggestion \"ubuntu:focal::arch64\": \"build-essential\" ``` hadoop-yetus commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3217977054 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 53s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | hadolint | 0m 0s | | hadolint was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 30s | | Maven dependency ordering for branch | | +1 :green_heart: | shadedclient | 48m 35s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 44s | | Maven dependency ordering for patch | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 41m 30s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 58s | | The patch does not generate ASF License warnings. | | | | 94m 37s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/8/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7822 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs hadolint jsonlint | | uname | Linux 08cb2ba2ac54 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 793b2b17e1235a80af4553c8d5a804dcc5bc63d0 | | Max. process+thread count | 524 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/8/console | | versions | git=2.27.0 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3218022287 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 11s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | hadolint | 0m 0s | | hadolint was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 0m 46s | | Maven dependency ordering for branch | | +1 :green_heart: | shadedclient | 40m 7s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 45s | | Maven dependency ordering for patch | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 38m 39s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 55s | | The patch does not generate ASF License warnings. | | | | 83m 36s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/8/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7822 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs hadolint jsonlint | | uname | Linux 6b761ad73cbb 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 793b2b17e1235a80af4553c8d5a804dcc5bc63d0 | | Max. process+thread count | 580 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/8/console | | versions | git=2.27.0 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3218023225 (!) A patch to the testing environment has been detected. Re-executing against the patched versions to perform further tests. The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/8/console in case of problems. hadoop-yetus commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3218085442 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 21m 51s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 1s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 0m 32s | | Maven dependency ordering for branch | | +1 :green_heart: | shadedclient | 39m 53s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 33s | | Maven dependency ordering for patch | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | hadolint | 0m 1s | | No new issues. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | shadedclient | 40m 8s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 105m 34s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/8/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7822 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint | | uname | Linux fa4a74a316d6 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 793b2b17e1235a80af4553c8d5a804dcc5bc63d0 | | Max. process+thread count | 568 (vs. ulimit of 5500) | | modules | C: U: | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/8/console | | versions | git=2.25.1 maven=3.6.3 hadolint=1.11.1-0-g0e692dd shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3218962358 @steveloughran @Hexiaoqiao Could you please take another look at this PR? I've updated the description. If everything looks good, I\u2019ll go ahead and merge it. The co-authors are Stoty and Pan. cc: @stoty @pan3793 pan3793 commented on PR #7822: URL: https://github.com/apache/hadoop/pull/7822#issuecomment-3218968914 LGTM. I can submit PRs to 1) upgrade Debian from 10 to 11, and 2) migrate CentOS 8 to Rocky Linux 8 after this gets in. stoty commented on code in PR #7822: URL: https://github.com/apache/hadoop/pull/7822#discussion_r2297232121 ########## dev-support/Jenkinsfile: ########## @@ -113,144 +113,6 @@ pipeline { } } - // This is an optional stage which runs only when there's a change in - // C++/C++ build/platform. - // This stage serves as a means of cross platform validation, which is - // really needed to ensure that any C++ related/platform change doesn't - // break the Hadoop build on Centos 7. - stage ('precommit-run Centos 7') { - environment { - SOURCEDIR = \"${WORKSPACE}/centos-7/src\" - PATCHDIR = \"${WORKSPACE}/centos-7/out\" - DOCKERFILE = \"${SOURCEDIR}/dev-support/docker/Dockerfile_centos_7\" - IS_OPTIONAL = 1 - } - - steps { - withCredentials(getGithubCreds()) { - sh '''#!/usr/bin/env bash - - chmod u+x \"${SOURCEDIR}/dev-support/jenkins.sh\" - \"${SOURCEDIR}/dev-support/jenkins.sh\" run_ci - ''' - } - } - - post { - // Since this is an optional platform, we want to copy the artifacts - // and archive it only if the build fails, to help with debugging. - failure { - sh '''#!/usr/bin/env bash - - cp -Rp \"${WORKSPACE}/centos-7/out\" \"${WORKSPACE}\" - ''' - archiveArtifacts \"out/**\" - } - - cleanup() { - script { - sh '''#!/usr/bin/env bash - - chmod u+x \"${SOURCEDIR}/dev-support/jenkins.sh\" - \"${SOURCEDIR}/dev-support/jenkins.sh\" cleanup_ci_proc - ''' - } - } - } - } - - // This is an optional stage which runs only when there's a change in - // C++/C++ build/platform. - // This stage serves as a means of cross platform validation, which is - // really needed to ensure that any C++ related/platform change doesn't - // break the Hadoop build on Centos 8. - stage ('precommit-run Centos 8') { Review Comment: (This was already suggested earlier...) slfan1989 commented on code in PR #7822: URL: https://github.com/apache/hadoop/pull/7822#discussion_r2297261130 ########## dev-support/Jenkinsfile: ########## @@ -113,144 +113,6 @@ pipeline { } } - // This is an optional stage which runs only when there's a change in - // C++/C++ build/platform. - // This stage serves as a means of cross platform validation, which is - // really needed to ensure that any C++ related/platform change doesn't - // break the Hadoop build on Centos 7. - stage ('precommit-run Centos 7') { - environment { - SOURCEDIR = \"${WORKSPACE}/centos-7/src\" - PATCHDIR = \"${WORKSPACE}/centos-7/out\" - DOCKERFILE = \"${SOURCEDIR}/dev-support/docker/Dockerfile_centos_7\" - IS_OPTIONAL = 1 - } - - steps { - withCredentials(getGithubCreds()) { - sh '''#!/usr/bin/env bash - - chmod u+x \"${SOURCEDIR}/dev-support/jenkins.sh\" - \"${SOURCEDIR}/dev-support/jenkins.sh\" run_ci - ''' - } - } - - post { - // Since this is an optional platform, we want to copy the artifacts - // and archive it only if the build fails, to help with debugging. - failure { - sh '''#!/usr/bin/env bash - - cp -Rp \"${WORKSPACE}/centos-7/out\" \"${WORKSPACE}\" - ''' - archiveArtifacts \"out/**\" - } - - cleanup() { - script { - sh '''#!/usr/bin/env bash - - chmod u+x \"${SOURCEDIR}/dev-support/jenkins.sh\" - \"${SOURCEDIR}/dev-support/jenkins.sh\" cleanup_ci_proc - ''' - } - } - } - } - - // This is an optional stage which runs only when there's a change in - // C++/C++ build/platform. - // This stage serves as a means of cross platform validation, which is - // really needed to ensure that any C++ related/platform change doesn't - // break the Hadoop build on Centos 8. - stage ('precommit-run Centos 8') { Review Comment: Thank you for your suggestion! We will continue to retain CentOS 8 and plan to upgrade to Rocky Linux 8 in the future. slfan1989 merged PR #7822: URL: https://github.com/apache/hadoop/pull/7822", "created": "2025-07-23T13:31:22.000+0000", "updated": "2025-09-18T06:26:42.000+0000", "derived": {"summary_task": "Summarize this issue: Removed build support for the following EOL (End-of-Life) operating system versions (e.g., CentOS 7, Debian 9, etc.). Cleaned up redundant code and dependency configurations in the Dockerfile related to these operating systems. Optimized the build logic to ensure that currently supported OS versions build successfully.", "classification_task": "Classify the issue priority and type: Removed build support for the following EOL (End-of-Life) operating system versions (e.g., CentOS 7, Debian 9, etc.). Cleaned up redundant code and dependency configurations in the Dockerfile related to these operating systems. Optimized the build logic to ensure that currently supported OS versions build successfully.", "qna_task": "Question: What is this issue about?\nAnswer: [JDK17] Remove CentOS 7 Support and Clean Up Dockerfile. "}}
{"id": "13624383", "key": "HADOOP-19635", "project": "HADOOP", "summary": "ABFS: [FNS Over Blob] Marker creation fail exception should not be propagated", "description": "Marker creation is attempted during certain operations such as {*}create{*}, {*}getPathStatus{*}, {*}setPathProperties{*}, and {*}rename{*}, in order to add a 0-byte file that signifies the existence of a folder at that path. However, if marker creation fails, the failure should not be propagated to the user.", "comments": "anmolanmol1234 opened a new pull request, #7825: URL: https://github.com/apache/hadoop/pull/7825 Marker creation is a best-effort operation performed during folder-related actions such as create, getPathStatus, setPathProperties, and rename. It involves writing a 0-byte file to indicate the presence of a folder. However, marker creation is not critical to the success of the primary operation. This change ensures that failures encountered during marker creation (e.g., due to transient issues or permission errors) are not propagated back to the user, preserving expected behavior and preventing unnecessary operation failures. hadoop-yetus commented on PR #7825: URL: https://github.com/apache/hadoop/pull/7825#issuecomment-3110072912 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 59s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 40m 7s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | +1 :green_heart: | javadoc | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 37s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 11s | | trunk passed | | +1 :green_heart: | shadedclient | 35m 27s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 32s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 22s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | +1 :green_heart: | javadoc | 0m 29s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 27s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 7s | | the patch passed | | +1 :green_heart: | shadedclient | 38m 32s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 0s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 40s | | The patch does not generate ASF License warnings. | | | | 129m 46s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7825 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux ecc3a3683189 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / e7f2bc3c1192045d0e6d24f754f7bbee8f9bea9f | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/1/testReport/ | | Max. process+thread count | 632 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. anmolanmol1234 commented on PR #7825: URL: https://github.com/apache/hadoop/pull/7825#issuecomment-3114250873 ============================================================ HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 209 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23 ============================================================ HNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 863, Failures: 0, Errors: 0, Skipped: 161 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 10 ============================================================ NonHNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 702, Failures: 0, Errors: 0, Skipped: 242 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 11 ============================================================ AppendBlob-HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 220 [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-SharedKey-Blob ============================================================ [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 12 [WARNING] Tests run: 707, Failures: 0, Errors: 0, Skipped: 159 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8 [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 11 ============================================================ NonHNS-OAuth-DFS ============================================================ [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 699, Failures: 0, Errors: 0, Skipped: 244 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24 ============================================================ NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 12 [WARNING] Tests run: 704, Failures: 0, Errors: 0, Skipped: 171 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8 [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24 ============================================================ AppendBlob-NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 12 [WARNING] Tests run: 701, Failures: 0, Errors: 0, Skipped: 212 [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 9 [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24 ============================================================ HNS-Oauth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 734, Failures: 0, Errors: 0, Skipped: 216 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-OAuth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 699, Failures: 0, Errors: 0, Skipped: 241 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24 Copilot commented on code in PR #7825: URL: https://github.com/apache/hadoop/pull/7825#discussion_r2235023374 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java: ########## @@ -1160,8 +1160,12 @@ public AbfsRestOperation setPathProperties(final String path, // This path could be present as an implicit directory in FNS. if (op.getResult().getStatusCode() == HTTP_NOT_FOUND && isNonEmptyDirectory(path, tracingContext)) { // Implicit path found, create a marker blob at this path and set properties. - this.createPathRestOp(path, false, false, false, null, - contextEncryptionAdapter, tracingContext); + try { + this.createPathRestOp(path, false, false, false, null, + contextEncryptionAdapter, tracingContext); + } catch (AbfsRestOperationException exception) { + LOG.debug(\"Marker creation failed for path {} during setPathProperties\", path); Review Comment: Consider including the exception details in the debug log message to aid in troubleshooting. For example: `LOG.debug(\"Marker creation failed for path {} during setPathProperties: {}\", path, exception.getMessage());` ```suggestion LOG.debug(\"Marker creation failed for path {} during setPathProperties: {}\", path, exception.getMessage()); ``` ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java: ########## @@ -1242,7 +1246,12 @@ public AbfsRestOperation getPathStatus(final String path, && isImplicitCheckRequired && isNonEmptyDirectory(path, tracingContext)) { // Implicit path found. // Create a marker blob at this path. - this.createMarkerAtPath(path, null, contextEncryptionAdapter, tracingContext); + try { + this.createMarkerAtPath(path, null, contextEncryptionAdapter, + tracingContext); + } catch (AbfsRestOperationException exception) { + LOG.debug(\"Marker creation failed for path {} during getPathStatus \", path); Review Comment: Consider including the exception details in the debug log message to aid in troubleshooting. For example: `LOG.debug(\"Marker creation failed for path {} during getPathStatus: {}\", path, exception.getMessage());` ```suggestion LOG.debug(\"Marker creation failed for path {} during getPathStatus: {}\", path, exception.getMessage()); ``` ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemOauth.java: ########## @@ -167,6 +168,30 @@ public void testBlobDataReader() throws Exception { } + /* + * BLOB DATA READER should have only READ access to the container and blobs in the container. + * */ + @Test + public void testGetPathStatusWithReader() throws Exception { + String clientId = this.getConfiguration().get(TestConfigurationKeys.FS_AZURE_BLOB_DATA_READER_CLIENT_ID); + Assume.assumeTrue(\"Reader client id not provided\", clientId != null); + String secret = this.getConfiguration().get(TestConfigurationKeys.FS_AZURE_BLOB_DATA_READER_CLIENT_SECRET); + Assume.assumeTrue(\"Reader client secret not provided\", secret != null); + + Path existedFolderPath = path(EXISTED_FOLDER_PATH); + createAzCopyFolder(existedFolderPath); + final AzureBlobFileSystem fs = getBlobReader(); + + // Use abfsStore in this test to verify the ERROR code in AbfsRestOperationException + AzureBlobFileSystemStore abfsStore = fs.getAbfsStore(); + TracingContext tracingContext = getTestTracingContext(fs, true); + + // GETPATHSTATUS marker creation fail should not be propagated to the caller. Review Comment: There's an extra space at the beginning of this comment line. Remove the leading space for consistent formatting. ```suggestion // GETPATHSTATUS marker creation fail should not be propagated to the caller. ``` bhattmanish98 commented on code in PR #7825: URL: https://github.com/apache/hadoop/pull/7825#discussion_r2235073847 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java: ########## @@ -1160,8 +1160,12 @@ public AbfsRestOperation setPathProperties(final String path, // This path could be present as an implicit directory in FNS. if (op.getResult().getStatusCode() == HTTP_NOT_FOUND && isNonEmptyDirectory(path, tracingContext)) { // Implicit path found, create a marker blob at this path and set properties. - this.createPathRestOp(path, false, false, false, null, - contextEncryptionAdapter, tracingContext); + try { + this.createPathRestOp(path, false, false, false, null, + contextEncryptionAdapter, tracingContext); + } catch (AbfsRestOperationException exception) { + LOG.debug(\"Marker creation failed for path {} during setPathProperties\", path); Review Comment: It would be better we can log exception status code as well or some details of exception just to know what the exact reason of the failure is ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java: ########## @@ -1242,7 +1246,12 @@ public AbfsRestOperation getPathStatus(final String path, && isImplicitCheckRequired && isNonEmptyDirectory(path, tracingContext)) { // Implicit path found. // Create a marker blob at this path. - this.createMarkerAtPath(path, null, contextEncryptionAdapter, tracingContext); + try { + this.createMarkerAtPath(path, null, contextEncryptionAdapter, + tracingContext); + } catch (AbfsRestOperationException exception) { + LOG.debug(\"Marker creation failed for path {} during getPathStatus \", path); Review Comment: Same as above. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java: ########## @@ -1160,8 +1160,12 @@ public AbfsRestOperation setPathProperties(final String path, // This path could be present as an implicit directory in FNS. if (op.getResult().getStatusCode() == HTTP_NOT_FOUND && isNonEmptyDirectory(path, tracingContext)) { // Implicit path found, create a marker blob at this path and set properties. - this.createPathRestOp(path, false, false, false, null, - contextEncryptionAdapter, tracingContext); + try { + this.createPathRestOp(path, false, false, false, null, Review Comment: Test case to check this change is not added, should we add that? anujmodi2021 commented on code in PR #7825: URL: https://github.com/apache/hadoop/pull/7825#discussion_r2235533414 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java: ########## @@ -1160,8 +1160,12 @@ public AbfsRestOperation setPathProperties(final String path, // This path could be present as an implicit directory in FNS. if (op.getResult().getStatusCode() == HTTP_NOT_FOUND && isNonEmptyDirectory(path, tracingContext)) { // Implicit path found, create a marker blob at this path and set properties. - this.createPathRestOp(path, false, false, false, null, - contextEncryptionAdapter, tracingContext); + try { + this.createPathRestOp(path, false, false, false, null, + contextEncryptionAdapter, tracingContext); + } catch (AbfsRestOperationException exception) { + LOG.debug(\"Marker creation failed for path {} during setPathProperties\", path); Review Comment: +1 on this. At least status code and storage error code (Enum like PATHNOTFOUND) we should print in debug log. Also a comment here on why we are swallowing the error for future reference will be better. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java: ########## @@ -1242,7 +1246,12 @@ public AbfsRestOperation getPathStatus(final String path, && isImplicitCheckRequired && isNonEmptyDirectory(path, tracingContext)) { // Implicit path found. // Create a marker blob at this path. - this.createMarkerAtPath(path, null, contextEncryptionAdapter, tracingContext); + try { + this.createMarkerAtPath(path, null, contextEncryptionAdapter, + tracingContext); + } catch (AbfsRestOperationException exception) { Review Comment: Was keeping this behind a config not part of the plan? anmolanmol1234 commented on code in PR #7825: URL: https://github.com/apache/hadoop/pull/7825#discussion_r2235546667 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java: ########## @@ -1242,7 +1246,12 @@ public AbfsRestOperation getPathStatus(final String path, && isImplicitCheckRequired && isNonEmptyDirectory(path, tracingContext)) { // Implicit path found. // Create a marker blob at this path. - this.createMarkerAtPath(path, null, contextEncryptionAdapter, tracingContext); + try { + this.createMarkerAtPath(path, null, contextEncryptionAdapter, + tracingContext); + } catch (AbfsRestOperationException exception) { Review Comment: No we planned to just swallow exceptions instead of config changes manika137 commented on code in PR #7825: URL: https://github.com/apache/hadoop/pull/7825#discussion_r2235613773 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java: ########## @@ -1242,7 +1246,12 @@ public AbfsRestOperation getPathStatus(final String path, && isImplicitCheckRequired && isNonEmptyDirectory(path, tracingContext)) { // Implicit path found. // Create a marker blob at this path. - this.createMarkerAtPath(path, null, contextEncryptionAdapter, tracingContext); + try { Review Comment: For confirmation- we want to swallow exceptions only for metadata related operations and allow permission exceptions for all write ones (createDirectory, createFile)? manika137 commented on code in PR #7825: URL: https://github.com/apache/hadoop/pull/7825#discussion_r2235613773 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java: ########## @@ -1242,7 +1246,12 @@ public AbfsRestOperation getPathStatus(final String path, && isImplicitCheckRequired && isNonEmptyDirectory(path, tracingContext)) { // Implicit path found. // Create a marker blob at this path. - this.createMarkerAtPath(path, null, contextEncryptionAdapter, tracingContext); + try { Review Comment: For confirmation- we want to swallow exceptions only for metadata related operations and allow permission exceptions for all write ones (createDirectory, createFile), correct? anmolanmol1234 commented on code in PR #7825: URL: https://github.com/apache/hadoop/pull/7825#discussion_r2235721787 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java: ########## @@ -1242,7 +1246,12 @@ public AbfsRestOperation getPathStatus(final String path, && isImplicitCheckRequired && isNonEmptyDirectory(path, tracingContext)) { // Implicit path found. // Create a marker blob at this path. - this.createMarkerAtPath(path, null, contextEncryptionAdapter, tracingContext); + try { Review Comment: marker creation is best effort for us, we don't want to fail the actual operation called if marker creation fails, so only for marker creation we are swallowing exception and not blocking the actual call anmolanmol1234 commented on code in PR #7825: URL: https://github.com/apache/hadoop/pull/7825#discussion_r2235940559 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java: ########## @@ -1160,8 +1160,12 @@ public AbfsRestOperation setPathProperties(final String path, // This path could be present as an implicit directory in FNS. if (op.getResult().getStatusCode() == HTTP_NOT_FOUND && isNonEmptyDirectory(path, tracingContext)) { // Implicit path found, create a marker blob at this path and set properties. - this.createPathRestOp(path, false, false, false, null, - contextEncryptionAdapter, tracingContext); + try { + this.createPathRestOp(path, false, false, false, null, Review Comment: This change is no longer needed anmolanmol1234 commented on code in PR #7825: URL: https://github.com/apache/hadoop/pull/7825#discussion_r2235958717 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java: ########## @@ -1160,8 +1160,12 @@ public AbfsRestOperation setPathProperties(final String path, // This path could be present as an implicit directory in FNS. if (op.getResult().getStatusCode() == HTTP_NOT_FOUND && isNonEmptyDirectory(path, tracingContext)) { // Implicit path found, create a marker blob at this path and set properties. - this.createPathRestOp(path, false, false, false, null, - contextEncryptionAdapter, tracingContext); + try { + this.createPathRestOp(path, false, false, false, null, + contextEncryptionAdapter, tracingContext); + } catch (AbfsRestOperationException exception) { + LOG.debug(\"Marker creation failed for path {} during setPathProperties\", path); Review Comment: taken hadoop-yetus commented on PR #7825: URL: https://github.com/apache/hadoop/pull/7825#issuecomment-3126864078 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 29s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 2m 55s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/3/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | -1 :x: | compile | 0m 25s | [/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/3/artifact/out/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 24s | [/branch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/3/artifact/out/branch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -0 :warning: | checkstyle | 0m 23s | [/buildtool-branch-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/3/artifact/out/buildtool-branch-checkstyle-hadoop-tools_hadoop-azure.txt) | The patch fails to run checkstyle in hadoop-azure | | -1 :x: | mvnsite | 0m 25s | [/branch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/3/artifact/out/branch-mvnsite-hadoop-tools_hadoop-azure.txt) | hadoop-azure in trunk failed. | | -1 :x: | javadoc | 0m 24s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/3/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 26s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/3/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | spotbugs | 0m 25s | [/branch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/3/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure.txt) | hadoop-azure in trunk failed. | | +1 :green_heart: | shadedclient | 2m 59s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 25s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/3/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | compile | 0m 24s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/3/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 24s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/3/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 1m 27s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/3/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 1m 27s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/3/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 22s | [/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/3/artifact/out/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt) | The patch fails to run checkstyle in hadoop-azure | | -1 :x: | mvnsite | 0m 23s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/3/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | javadoc | 0m 18s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/3/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 24s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/3/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | spotbugs | 0m 23s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/3/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | +1 :green_heart: | shadedclient | 4m 31s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 1m 55s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/3/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | asflicense | 0m 39s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/3/artifact/out/results-asflicense.txt) | The patch generated 1 ASF License warnings. | | | | 19m 45s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7825 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux dc108aee355c 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f28f92259789882b7b2455500a3d1765de294bd5 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/3/testReport/ | | Max. process+thread count | 129 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/3/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7825: URL: https://github.com/apache/hadoop/pull/7825#issuecomment-3127003915 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 3s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 0m 25s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/4/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | -1 :x: | compile | 0m 26s | [/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/4/artifact/out/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 26s | [/branch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/4/artifact/out/branch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -0 :warning: | checkstyle | 0m 22s | [/buildtool-branch-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/4/artifact/out/buildtool-branch-checkstyle-hadoop-tools_hadoop-azure.txt) | The patch fails to run checkstyle in hadoop-azure | | -1 :x: | mvnsite | 0m 28s | [/branch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/4/artifact/out/branch-mvnsite-hadoop-tools_hadoop-azure.txt) | hadoop-azure in trunk failed. | | -1 :x: | javadoc | 0m 12s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/4/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 2m 47s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/4/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | spotbugs | 0m 28s | [/branch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/4/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure.txt) | hadoop-azure in trunk failed. | | +1 :green_heart: | shadedclient | 5m 20s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 26s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/4/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | compile | 0m 26s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/4/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 26s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/4/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 26s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/4/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 0m 26s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/4/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 25s | [/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/4/artifact/out/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt) | The patch fails to run checkstyle in hadoop-azure | | -1 :x: | mvnsite | 0m 26s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/4/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | javadoc | 0m 27s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/4/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 51s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/4/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | spotbugs | 2m 20s | | the patch passed | | +1 :green_heart: | shadedclient | 3m 8s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 24s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/4/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | +0 :ok: | asflicense | 0m 10s | | ASF License check generated no output? | | | | 19m 43s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7825 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 39f7d777211b 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / e8472eb4cee9abd4ebe5e349175bb90082c06c61 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/4/testReport/ | | Max. process+thread count | 78 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/4/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7825: URL: https://github.com/apache/hadoop/pull/7825#issuecomment-3127345361 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 20m 55s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 4s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 37s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 49s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 10s | | trunk passed | | +1 :green_heart: | shadedclient | 35m 18s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 32s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 22s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 34s | | the patch passed | | +1 :green_heart: | javadoc | 0m 29s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 27s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 35m 10s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 52s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 144m 58s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7825 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 1446327a7baa 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f28f92259789882b7b2455500a3d1765de294bd5 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/2/testReport/ | | Max. process+thread count | 547 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. manika137 commented on code in PR #7825: URL: https://github.com/apache/hadoop/pull/7825#discussion_r2238281684 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemOauth.java: ########## @@ -167,6 +168,30 @@ public void testBlobDataReader() throws Exception { } + /* + * GetPathStatus with Blob Data Reader role should not throw an exception when marker creation fails due to permission issues. + * */ + @Test + public void testGetPathStatusWithReader() throws Exception { + String clientId = this.getConfiguration().get(TestConfigurationKeys.FS_AZURE_BLOB_DATA_READER_CLIENT_ID); Review Comment: Nit: since we have already imported TestConfigurationKeys constants we can use FS_AZURE_BLOB_DATA_READER_CLIENT_ID and FS_AZURE_BLOB_DATA_READER_CLIENT_SECRET directly anmolanmol1234 commented on code in PR #7825: URL: https://github.com/apache/hadoop/pull/7825#discussion_r2238320094 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemOauth.java: ########## @@ -167,6 +168,30 @@ public void testBlobDataReader() throws Exception { } + /* + * GetPathStatus with Blob Data Reader role should not throw an exception when marker creation fails due to permission issues. + * */ + @Test + public void testGetPathStatusWithReader() throws Exception { + String clientId = this.getConfiguration().get(TestConfigurationKeys.FS_AZURE_BLOB_DATA_READER_CLIENT_ID); Review Comment: taken hadoop-yetus commented on PR #7825: URL: https://github.com/apache/hadoop/pull/7825#issuecomment-3130529507 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 8s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 0m 31s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/5/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | -1 :x: | compile | 0m 25s | [/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/5/artifact/out/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 25s | [/branch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/5/artifact/out/branch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -0 :warning: | checkstyle | 0m 23s | [/buildtool-branch-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/5/artifact/out/buildtool-branch-checkstyle-hadoop-tools_hadoop-azure.txt) | The patch fails to run checkstyle in hadoop-azure | | -1 :x: | mvnsite | 3m 6s | [/branch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/5/artifact/out/branch-mvnsite-hadoop-tools_hadoop-azure.txt) | hadoop-azure in trunk failed. | | -1 :x: | javadoc | 0m 25s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/5/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 25s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/5/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | spotbugs | 0m 25s | [/branch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/5/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure.txt) | hadoop-azure in trunk failed. | | +1 :green_heart: | shadedclient | 5m 41s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 24s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/5/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | compile | 0m 24s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/5/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 24s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/5/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 24s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/5/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 0m 24s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/5/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 8s | [/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/5/artifact/out/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt) | The patch fails to run checkstyle in hadoop-azure | | -1 :x: | mvnsite | 0m 24s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/5/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | javadoc | 0m 24s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/5/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 25s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/5/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | spotbugs | 0m 25s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/5/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | +1 :green_heart: | shadedclient | 4m 6s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 25s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/5/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | +0 :ok: | asflicense | 0m 24s | | ASF License check generated no output? | | | | 16m 12s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7825 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux f2985c2e055a 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 4640893a4e42f4ea3249fb4974e3142b72e80e12 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/5/testReport/ | | Max. process+thread count | 41 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/5/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7825: URL: https://github.com/apache/hadoop/pull/7825#issuecomment-3135939508 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 1s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 45m 26s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 41s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 10s | | trunk passed | | +1 :green_heart: | shadedclient | 42m 37s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/7/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | +1 :green_heart: | javadoc | 0m 29s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 7s | | the patch passed | | +1 :green_heart: | shadedclient | 40m 43s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 57s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 144m 28s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/7/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7825 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 25cf764a5bf9 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 8dc9cdc11db577814800d19f5dce3a6ed3d2a5d9 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/7/testReport/ | | Max. process+thread count | 530 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/7/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7825: URL: https://github.com/apache/hadoop/pull/7825#issuecomment-3136596005 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 50s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 45m 18s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 37s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 31s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 19s | | trunk passed | | +1 :green_heart: | shadedclient | 43m 32s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 0m 35s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 20s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 30s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 40m 30s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 57s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 144m 48s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/8/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7825 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux ef313b1a3b7c 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / bef0a6c2b1e17cbdaa7612bb823f500c193d8a2c | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/8/testReport/ | | Max. process+thread count | 533 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/8/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. anujmodi2021 merged PR #7825: URL: https://github.com/apache/hadoop/pull/7825", "created": "2025-07-23T12:14:47.000+0000", "updated": "2025-09-30T10:09:13.000+0000", "derived": {"summary_task": "Summarize this issue: Marker creation is attempted during certain operations such as {*}create{*}, {*}getPathStatus{*}, {*}setPathProperties{*}, and {*}rename{*}, in order to add a 0-byte file that signifies the existence of a folder at that path. However, if marker creation fails, the failure should not be propagated to the user.", "classification_task": "Classify the issue priority and type: Marker creation is attempted during certain operations such as {*}create{*}, {*}getPathStatus{*}, {*}setPathProperties{*}, and {*}rename{*}, in order to add a 0-byte file that signifies the existence of a folder at that path. However, if marker creation fails, the failure should not be propagated to the user.", "qna_task": "Question: What is this issue about?\nAnswer: ABFS: [FNS Over Blob] Marker creation fail exception should not be propagated"}}
{"id": "13624380", "key": "HADOOP-19634", "project": "HADOOP", "summary": "acknowledge Guava license on LimitInputStream", "description": "When ASF projects copy 3rd party code into their code bases, they are meant to: * check the orginal license is Category A - https://www.apache.org/legal/resolved.html * keep the original source code headers * add something to their LICENSE that mentions the source file and what license is on it * if the 3rd party project is Apache licensed but has a NOTICE file, we need to copy its contents to our NOTICE * these requirements are only negated if the original code is submitted to the ASF project by the code's copyright holder (the individual or company that wrote the original code). * Hadoop copy https://github.com/apache/hadoop/blob/c357e435fd691b4c184b82325bef6d7c65e5f32b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LimitInputStream.java#L39 * Original code has a Guava copyright that we probably need to keep https://github.com/google/guava/blob/master/guava/src/com/google/common/io/ByteStreams.java", "comments": "pjfanning opened a new pull request, #7821: URL: https://github.com/apache/hadoop/pull/7821 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR See https://issues.apache.org/jira/browse/HADOOP-19634 ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? steveloughran merged PR #7821: URL: https://github.com/apache/hadoop/pull/7821", "created": "2025-07-23T11:57:14.000+0000", "updated": "2025-07-23T16:42:06.000+0000", "derived": {"summary_task": "Summarize this issue: When ASF projects copy 3rd party code into their code bases, they are meant to: * check the orginal license is Category A - https://www.apache.org/legal/resolved.html * keep the original source code headers * add something to their LICENSE that mentions the source file and what license is on it * if the 3rd party project is Apache licensed but has a NOTICE file, we need to copy its contents to our", "classification_task": "Classify the issue priority and type: When ASF projects copy 3rd party code into their code bases, they are meant to: * check the orginal license is Category A - https://www.apache.org/legal/resolved.html * keep the original source code headers * add something to their LICENSE that mentions the source file and what license is on it * if the 3rd party project is Apache licensed but has a NOTICE file, we need to copy its contents to our", "qna_task": "Question: What is this issue about?\nAnswer: acknowledge Guava license on LimitInputStream"}}
{"id": "13624358", "key": "HADOOP-19633", "project": "HADOOP", "summary": "upgrade commons-beanutils to 1.11.0", "description": "", "comments": "", "created": "2025-07-23T09:01:52.000+0000", "updated": "2025-07-23T09:01:52.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: upgrade commons-beanutils to 1.11.0"}}
{"id": "13624357", "key": "HADOOP-19632", "project": "HADOOP", "summary": "Upgrade nimbusds to 10.0.2", "description": "Includes fix for CVE-2025-53864", "comments": "[~ananysin] the version is wrong - we need at least 10.0.2 to fix https://www.cve.org/CVERecord?id=CVE-2025-53864 I would suggest using the latest - 10.4 Sure [~fanningpj] AnanyaSingh2121 opened a new pull request, #7836: URL: https://github.com/apache/hadoop/pull/7836 (no comment) hadoop-yetus commented on PR #7836: URL: https://github.com/apache/hadoop/pull/7836#issuecomment-3135017183 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 55s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 44m 35s | | trunk passed | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 21s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 0m 25s | | trunk passed | | +1 :green_heart: | javadoc | 0m 26s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 22s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 88m 13s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 14s | | the patch passed | | +1 :green_heart: | compile | 0m 13s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 13s | | the patch passed | | +1 :green_heart: | compile | 0m 13s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 13s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 16s | | the patch passed | | +1 :green_heart: | javadoc | 0m 13s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 12s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | shadedclient | 10m 27s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 15s | | hadoop-project in the patch passed. | | +1 :green_heart: | asflicense | 0m 34s | | The patch does not generate ASF License warnings. | | | | 102m 53s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7836/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7836 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux a9ee212ce67e 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / ebe2cfe49dfcbd6d3d42732a2a0f11cb476d7556 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7836/1/testReport/ | | Max. process+thread count | 525 (vs. ulimit of 5500) | | modules | C: hadoop-project U: hadoop-project | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7836/1/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. pjfanning opened a new pull request, #7870: URL: https://github.com/apache/hadoop/pull/7870 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR CVE-2025-53864 fix has now appeared in 9.37.4 ### How was this patch tested? ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? hadoop-yetus commented on PR #7870: URL: https://github.com/apache/hadoop/pull/7870#issuecomment-3187924858 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 31s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 10m 45s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 32m 36s | | trunk passed | | +1 :green_heart: | compile | 16m 48s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 47s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 22m 12s | | trunk passed | | +1 :green_heart: | javadoc | 10m 14s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 46s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 53m 10s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 34s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 31m 35s | | the patch passed | | +1 :green_heart: | compile | 15m 20s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 20s | | the patch passed | | +1 :green_heart: | compile | 14m 4s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 14m 4s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 19m 28s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 9m 30s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 47s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 53m 36s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 359m 25s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7870/1/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 13s | | The patch does not generate ASF License warnings. | | | | 648m 35s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7870/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7870 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux 78fd5943d0a3 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 5428389323ee3d1a20cc1bca948d55556634be91 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7870/1/testReport/ | | Max. process+thread count | 3329 (vs. ulimit of 5500) | | modules | C: hadoop-project . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7870/1/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7870: URL: https://github.com/apache/hadoop/pull/7870#issuecomment-3193392751 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 32s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 47s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 32m 11s | | trunk passed | | +1 :green_heart: | compile | 15m 53s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 49s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 22m 27s | | trunk passed | | +1 :green_heart: | javadoc | 9m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 51s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 52m 1s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 36s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 31m 45s | | the patch passed | | +1 :green_heart: | compile | 15m 30s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 30s | | the patch passed | | +1 :green_heart: | compile | 13m 50s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 50s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 19m 28s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 9m 38s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 44s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 53m 31s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 357m 15s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7870/2/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 10s | | The patch does not generate ASF License warnings. | | | | 643m 11s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7870/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7870 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux e1aa30c14bc2 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / b71c33b81f0ddc3c6b5380c9c38b01b9fcef73f4 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7870/2/testReport/ | | Max. process+thread count | 2991 (vs. ulimit of 5500) | | modules | C: hadoop-project . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7870/2/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. ayushtkn commented on code in PR #7870: URL: https://github.com/apache/hadoop/pull/7870#discussion_r2299062286 ########## hadoop-project/pom.xml: ########## @@ -244,7 +244,7 @@ <openssl-wildfly.version>2.1.4.Final</openssl-wildfly.version> <jsonschema2pojo.version>1.0.2</jsonschema2pojo.version> <woodstox.version>5.4.0</woodstox.version> - <nimbus-jose-jwt.version>9.37.2</nimbus-jose-jwt.version> + <nimbus-jose-jwt.version>9.37.4</nimbus-jose-jwt.version> Review Comment: why don't we move to 9.48? https://mvnrepository.com/artifact/com.nimbusds/nimbus-jose-jwt/9.48 pjfanning commented on code in PR #7870: URL: https://github.com/apache/hadoop/pull/7870#discussion_r2299121430 ########## hadoop-project/pom.xml: ########## @@ -244,7 +244,7 @@ <openssl-wildfly.version>2.1.4.Final</openssl-wildfly.version> <jsonschema2pojo.version>1.0.2</jsonschema2pojo.version> <woodstox.version>5.4.0</woodstox.version> - <nimbus-jose-jwt.version>9.37.2</nimbus-jose-jwt.version> + <nimbus-jose-jwt.version>9.37.4</nimbus-jose-jwt.version> Review Comment: @ayushtkn 9.48 is affected by CVE-2025-8916 while 9.37.4 was specifically patched with the fix. pjfanning commented on code in PR #7870: URL: https://github.com/apache/hadoop/pull/7870#discussion_r2299121430 ########## hadoop-project/pom.xml: ########## @@ -244,7 +244,7 @@ <openssl-wildfly.version>2.1.4.Final</openssl-wildfly.version> <jsonschema2pojo.version>1.0.2</jsonschema2pojo.version> <woodstox.version>5.4.0</woodstox.version> - <nimbus-jose-jwt.version>9.37.2</nimbus-jose-jwt.version> + <nimbus-jose-jwt.version>9.37.4</nimbus-jose-jwt.version> Review Comment: @ayushtkn 9.48 is affected by CVE-2025-53864 while 9.37.4 was specifically patched with the fix. AnanyaSingh2121 commented on PR #7836: URL: https://github.com/apache/hadoop/pull/7836#issuecomment-3222609894 Added the changes for License-binary hadoop-yetus commented on PR #7836: URL: https://github.com/apache/hadoop/pull/7836#issuecomment-3223593915 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 8m 10s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 45s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 20m 3s | | trunk passed | | +1 :green_heart: | compile | 8m 23s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 7m 26s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 15m 8s | | trunk passed | | +1 :green_heart: | javadoc | 5m 36s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 45s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 30m 53s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 6m 44s | | Maven dependency ordering for patch | | -1 :x: | mvninstall | 2m 32s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7836/2/artifact/out/patch-mvninstall-root.txt) | root in the patch failed. | | -1 :x: | compile | 3m 56s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7836/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 3m 56s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7836/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 3m 45s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7836/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 3m 45s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7836/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 9m 26s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7836/2/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 5m 14s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 49s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | shadedclient | 15m 57s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 199m 10s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7836/2/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 0m 43s | | The patch does not generate ASF License warnings. | | | | 342m 58s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7836/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7836 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux ab7be46c111d 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / bf57741c9f666feb5859857cc65d3ee40545044f | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7836/2/testReport/ | | Max. process+thread count | 3926 (vs. ulimit of 5500) | | modules | C: hadoop-project . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7836/2/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7870: URL: https://github.com/apache/hadoop/pull/7870#issuecomment-3275915284 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 33s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 7s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 33m 2s | | trunk passed | | +1 :green_heart: | compile | 15m 46s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 37s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 21m 43s | | trunk passed | | +1 :green_heart: | javadoc | 10m 7s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 50s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 52m 20s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 37s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 31m 13s | | the patch passed | | +1 :green_heart: | compile | 15m 10s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 10s | | the patch passed | | +1 :green_heart: | compile | 13m 44s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 44s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 18m 13s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 9m 37s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 51s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 53m 29s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 803m 41s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7870/4/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 49s | | The patch does not generate ASF License warnings. | | | | 1087m 43s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.mapreduce.v2.TestUberAM | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7870/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7870 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux f41a9b0d025f 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / d15e31597255bf5bb43434b3c493900f315096fa | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7870/4/testReport/ | | Max. process+thread count | 3705 (vs. ulimit of 5500) | | modules | C: hadoop-project . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7870/4/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. steveloughran commented on PR #7836: URL: https://github.com/apache/hadoop/pull/7836#issuecomment-3293226585 build failed. @AnanyaSingh2121 can you rebase and force push to see what happens against trunk now. thanks pjfanning commented on PR #7836: URL: https://github.com/apache/hadoop/pull/7836#issuecomment-3293253190 @steveloughran https://github.com/apache/hadoop/pull/7965 seems to be a more accurate attempt at this. I also have https://github.com/apache/hadoop/pull/7870 which also fixes the CVE by upgrading a 9.x patch that was specially released with the CVE fix. hadoop-yetus commented on PR #7836: URL: https://github.com/apache/hadoop/pull/7836#issuecomment-3302359120 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 10m 42s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 24m 33s | | trunk passed | | +1 :green_heart: | compile | 8m 33s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 7m 34s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 14m 31s | | trunk passed | | +1 :green_heart: | javadoc | 5m 31s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 53s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 31m 4s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 25s | | Maven dependency ordering for patch | | -1 :x: | mvninstall | 7m 31s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7836/3/artifact/out/patch-mvninstall-root.txt) | root in the patch failed. | | -1 :x: | compile | 4m 1s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7836/3/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 4m 1s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7836/3/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 3m 43s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7836/3/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 3m 43s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7836/3/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 9m 30s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7836/3/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 5m 23s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 53s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | shadedclient | 16m 5s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 265m 23s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7836/3/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 0m 41s | | The patch does not generate ASF License warnings. | | | | 406m 50s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.server.balancer.TestBalancerLongRunningTasks | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7836/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7836 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux 8e30f3250d87 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c7db3f77b8f6ed3c5f30e632a247d93af2ebbac0 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7836/3/testReport/ | | Max. process+thread count | 4186 (vs. ulimit of 5500) | | modules | C: hadoop-project . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7836/3/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7965: URL: https://github.com/apache/hadoop/pull/7965#issuecomment-3304801731 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 34s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 51s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 40m 52s | | trunk passed | | +1 :green_heart: | compile | 15m 57s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 49s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 21m 8s | | trunk passed | | +1 :green_heart: | javadoc | 9m 44s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 44s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 53m 6s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 36s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 40m 44s | | the patch passed | | +1 :green_heart: | compile | 15m 21s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 21s | | the patch passed | | +1 :green_heart: | compile | 13m 42s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 42s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 19m 48s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 1s | | No new issues. | | +1 :green_heart: | javadoc | 9m 38s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 47s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 53m 12s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 450m 48s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7965/3/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 14s | | The patch does not generate ASF License warnings. | | | | 754m 15s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7965/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7965 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux e199fa50a2f0 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / b51e3a0ed589cdad9206e767d8ff6344ce088c3d | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7965/3/testReport/ | | Max. process+thread count | 3299 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-tools/hadoop-sls . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7965/3/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7965: URL: https://github.com/apache/hadoop/pull/7965#issuecomment-3304976023 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 53s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 34s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 47m 12s | | trunk passed | | +1 :green_heart: | compile | 17m 59s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 33s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 22m 58s | | trunk passed | | +1 :green_heart: | javadoc | 10m 22s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 55s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 59m 12s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 36s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 49m 21s | | the patch passed | | +1 :green_heart: | compile | 17m 52s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 17m 52s | | the patch passed | | +1 :green_heart: | compile | 16m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 16m 26s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 19m 58s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 10m 51s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 8m 10s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 62m 23s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 503m 12s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7965/2/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 18s | | The patch does not generate ASF License warnings. | | | | 848m 20s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.TestReconstructStripedFileWithRandomECPolicy | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7965/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7965 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux 903ae8a8a984 5.15.0-151-generic #161-Ubuntu SMP Tue Jul 22 14:25:40 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 03d76d18b073b38de6c01c1d2c724f9feba7d793 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7965/2/testReport/ | | Max. process+thread count | 3484 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-tools/hadoop-sls . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7965/2/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. rohit-kb commented on PR #7965: URL: https://github.com/apache/hadoop/pull/7965#issuecomment-3306891395 @pjfanning could you please take a look at this again whenever you have time? Thanks pjfanning commented on PR #7965: URL: https://github.com/apache/hadoop/pull/7965#issuecomment-3306924014 the test build crashed rohit-kb commented on PR #7965: URL: https://github.com/apache/hadoop/pull/7965#issuecomment-3307020292 Could you please help understand how the test build crashed as I am a bit confused about the CI process here? As per this execution: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7965/2/testReport/history/ The attempt #3 has no test failures with 19K+ passes and 300+ skips. Thanks pjfanning commented on PR #7965: URL: https://github.com/apache/hadoop/pull/7965#issuecomment-3311743722 I'll let one of the Hadoop committers review this. The CI jobs can be flaky and when I want to rerun them, I rebase the PR to cause a new run. steveloughran commented on PR #7965: URL: https://github.com/apache/hadoop/pull/7965#issuecomment-3312539483 @rohit-kb do a rebase and a force push to trigger a new yetus build. thanks hadoop-yetus commented on PR #7965: URL: https://github.com/apache/hadoop/pull/7965#issuecomment-3314493477 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 34s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 11m 40s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 40m 35s | | trunk passed | | +1 :green_heart: | compile | 15m 59s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 42s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 22m 9s | | trunk passed | | +1 :green_heart: | javadoc | 9m 44s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 56s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 52m 7s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 37s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 40m 45s | | the patch passed | | +1 :green_heart: | compile | 15m 17s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 17s | | the patch passed | | +1 :green_heart: | compile | 13m 36s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 36s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 18m 34s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 9m 39s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 52s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 53m 51s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 460m 19s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7965/4/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 17s | | The patch does not generate ASF License warnings. | | | | 765m 28s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7965/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7965 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux 4dc676b6b738 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 13d0d0419323bc818420a9321e593f886eeda4c0 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7965/4/testReport/ | | Max. process+thread count | 3396 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-tools/hadoop-sls . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7965/4/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. hadoop-yetus commented on PR #7965: URL: https://github.com/apache/hadoop/pull/7965#issuecomment-3321392221 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 22m 11s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 10m 27s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 47m 23s | | trunk passed | | +1 :green_heart: | compile | 18m 8s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 16m 52s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 22m 57s | | trunk passed | | +1 :green_heart: | javadoc | 10m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 8m 5s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 57m 45s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 36s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 47m 52s | | the patch passed | | +1 :green_heart: | compile | 17m 35s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 17m 35s | | the patch passed | | +1 :green_heart: | compile | 15m 46s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 46s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 19m 58s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 1s | | No new issues. | | +1 :green_heart: | javadoc | 10m 17s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 58s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 61m 4s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 505m 36s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7965/5/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 22s | | The patch does not generate ASF License warnings. | | | | 869m 23s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7965/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7965 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux 7a0cad96adcb 5.15.0-151-generic #161-Ubuntu SMP Tue Jul 22 14:25:40 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 4aee820e7dff92a6b86afde61f29d1391e284302 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7965/5/testReport/ | | Max. process+thread count | 3598 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-tools/hadoop-sls . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7965/5/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. rohit-kb commented on PR #7965: URL: https://github.com/apache/hadoop/pull/7965#issuecomment-3322038031 I am able to run the crashed tests locally: `[INFO] Running org.apache.hadoop.yarn.server.resourcemanager.TestRMHAForAsyncScheduler [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 33.61 s steveloughran merged PR #7965: URL: https://github.com/apache/hadoop/pull/7965 steveloughran commented on PR #7965: URL: https://github.com/apache/hadoop/pull/7965#issuecomment-3324549237 @rohit-kb - merged. will take a backport to branch-3.4 rohit-kb commented on PR #7965: URL: https://github.com/apache/hadoop/pull/7965#issuecomment-3324573362 Thanks @slfan1989, @steveloughran for the review and the merge! Will upload to branch-3.4 soon hadoop-yetus commented on PR #7965: URL: https://github.com/apache/hadoop/pull/7965#issuecomment-3324609056 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 51s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 12m 33s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 46m 52s | | trunk passed | | +1 :green_heart: | compile | 17m 56s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 13s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 23m 33s | | trunk passed | | +1 :green_heart: | javadoc | 10m 24s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 47s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 59m 0s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 35s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 49m 48s | | the patch passed | | +1 :green_heart: | compile | 18m 12s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 18m 12s | | the patch passed | | +1 :green_heart: | compile | 15m 58s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 58s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 19m 47s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 1s | | No new issues. | | +1 :green_heart: | javadoc | 11m 15s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 9m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 65m 15s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 529m 32s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7965/6/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 33s | | The patch does not generate ASF License warnings. | | | | 880m 49s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7965/6/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7965 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux 073c181e5730 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 54195011a5d3bdb01704484607b66a24d854a2b3 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7965/6/testReport/ | | Max. process+thread count | 2509 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-tools/hadoop-sls . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7965/6/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. rohit-kb opened a new pull request, #7993: URL: https://github.com/apache/hadoop/pull/7993 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files? pjfanning commented on PR #7836: URL: https://github.com/apache/hadoop/pull/7836#issuecomment-3325673729 #7965 was merged making this obsolete? pjfanning closed pull request #7870: HADOOP-19632. Upgrade to nimbus-jose-jwt 9.37.4 due to CVE-2025-53864 URL: https://github.com/apache/hadoop/pull/7870 pjfanning commented on PR #7870: URL: https://github.com/apache/hadoop/pull/7870#issuecomment-3325674481 https://github.com/apache/hadoop/pull/7965 was merged making this obsolete hadoop-yetus commented on PR #7993: URL: https://github.com/apache/hadoop/pull/7993#issuecomment-3327115927 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 12m 8s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ branch-3.4 Compile Tests _ | | +0 :ok: | mvndep | 3m 1s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 34m 38s | | branch-3.4 passed | | +1 :green_heart: | compile | 16m 54s | | branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 56s | | branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 22m 6s | | branch-3.4 passed | | +1 :green_heart: | javadoc | 8m 24s | | branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 36s | | branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 48m 5s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 33s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 30m 2s | | the patch passed | | +1 :green_heart: | compile | 16m 6s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 16m 6s | | the patch passed | | +1 :green_heart: | compile | 15m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 18s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 18m 11s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 8m 16s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 39s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 49m 6s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 622m 40s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7993/1/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 9s | | The patch does not generate ASF License warnings. | | | | 909m 22s | | | | Reason | Tests | |-------:|:------| | Unreaped Processes | root:2 | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7993/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7993 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux 56e26a8a857e 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | branch-3.4 / b0c85b602202ff0f371a6ead734a230805a248a0 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Unreaped Processes Log | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7993/1/artifact/out/patch-unit-root-reaper.txt | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7993/1/testReport/ | | Max. process+thread count | 4564 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-tools/hadoop-sls . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7993/1/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated. slfan1989 merged PR #7993: URL: https://github.com/apache/hadoop/pull/7993 slfan1989 commented on PR #7993: URL: https://github.com/apache/hadoop/pull/7993#issuecomment-3327295427 @rohit-kb Thanks for the contribution! Merged into branch-3.4. rohit-kb commented on PR #7993: URL: https://github.com/apache/hadoop/pull/7993#issuecomment-3327346430 Thanks @slfan1989 for the review and the merge steveloughran closed pull request #7836: HADOOP-19632 : Upgrade nimbusds to 10.4 URL: https://github.com/apache/hadoop/pull/7836 steveloughran commented on PR #7836: URL: https://github.com/apache/hadoop/pull/7836#issuecomment-3329278859 yes it does; closing this", "created": "2025-07-23T09:00:38.000+0000", "updated": "2025-09-24T15:17:03.000+0000", "derived": {"summary_task": "Summarize this issue: Includes fix for CVE-2025-53864", "classification_task": "Classify the issue priority and type: Includes fix for CVE-2025-53864", "qna_task": "Question: What is this issue about?\nAnswer: Upgrade nimbusds to 10.0.2"}}
{"id": "13633006", "key": "HIVE-29301", "project": "HIVE", "summary": "Missing histogram info in DESCRIBE FORMATTED when executing another DESCRIBE FORMATTED before setting metastore.stats.fetch.kll", "description": "I had already observed several times an issue with DESCRIBE FORMATTED: sometimes the \"histogram\" info is empty, even when setting hive.stats.kll.enable and metastore.stats.fetch.kll. This time I managed to create a MRE (minimum reproducible example): The following qfile shows the wrong behavior ({{{}histogram {}}}): {code:java} CREATE TABLE tab1 AS (SELECT 1 as key); DESCRIBE FORMATTED tab1 key; set metastore.stats.fetch.kll=true; CREATE TABLE tab2 AS (SELECT 1 as key); set hive.stats.kll.enable=true; ANALYZE TABLE tab2 COMPUTE STATISTICS FOR COLUMNS; DESCRIBE FORMATTED tab2 key; {code} While the following qfile works as expected: {code:java} CREATE TABLE tab1 AS (SELECT 1 as key); set metastore.stats.fetch.kll=true; DESCRIBE FORMATTED tab1 key; CREATE TABLE tab2 AS (SELECT 1 as key); set hive.stats.kll.enable=true; ANALYZE TABLE tab2 COMPUTE STATISTICS FOR COLUMNS; DESCRIBE FORMATTED tab2 key; {code} resulting in a {{histogram Q1: 1, Q2: 1, Q3: 1}} The only change is the order of {{set metastore.stats.fetch.kll=true;}} and {{DESCRIBE FORMATTED tab1 key;}}. Please note that the interchanged DESCRIBE FORMATTED is a command on an unrelated table!", "comments": "", "created": "2025-10-31T13:24:16.000+0000", "updated": "2025-10-31T14:48:40.000+0000", "derived": {"summary_task": "Summarize this issue: I had already observed several times an issue with DESCRIBE FORMATTED: sometimes the \"histogram\" info is empty, even when setting hive.stats.kll.enable and metastore.stats.fetch.kll. This time I managed to create a MRE (minimum reproducible example): The following qfile shows the wrong behavior ({{{}histogram {}}}): {code:java} CREATE TABLE tab1 AS (SELECT 1 as key); DESCRIBE FORMATTED tab1 key; s", "classification_task": "Classify the issue priority and type: I had already observed several times an issue with DESCRIBE FORMATTED: sometimes the \"histogram\" info is empty, even when setting hive.stats.kll.enable and metastore.stats.fetch.kll. This time I managed to create a MRE (minimum reproducible example): The following qfile shows the wrong behavior ({{{}histogram {}}}): {code:java} CREATE TABLE tab1 AS (SELECT 1 as key); DESCRIBE FORMATTED tab1 key; s", "qna_task": "Question: What is this issue about?\nAnswer: Missing histogram info in DESCRIBE FORMATTED when executing another DESCRIBE FORMATTED before setting metastore.stats.fetch.kll"}}
{"id": "13632995", "key": "HIVE-29300", "project": "HIVE", "summary": "Wrong estimation for num rows in EXPLAIN with histogram statistics", "description": "Given a query {{{}SELECT 1 FROM sh2a WHERE k1 < 10 AND k2 < 250{}}}, with a selectivity of 0.02 for {{k1 < 10}} and 0.5 for {{{}k2 < 250{}}}, the combined selectivity should be 0.01 resulting in selecting 5 rows of the 500 rows of the table. If we activate histograms, the combined selectivity is estimated only with the last clause as approx 0.5, so estimating that 247 rows of 500 rows are selected. Reason: {{StatsRulesProcFactory.FilterStatsRule#evaluateComparatorWithHistogram}} needs to estimate the selectivity of the condition and multiply it by currNumRows.", "comments": "", "created": "2025-10-31T11:01:04.000+0000", "updated": "2025-10-31T14:48:28.000+0000", "derived": {"summary_task": "Summarize this issue: Given a query {{{}SELECT 1 FROM sh2a WHERE k1 < 10 AND k2 < 250{}}}, with a selectivity of 0.02 for {{k1 < 10}} and 0.5 for {{{}k2 < 250{}}}, the combined selectivity should be 0.01 resulting in selecting 5 rows of the 500 rows of the table. If we activate histograms, the combined selectivity is estimated only with the last clause as approx 0.5, so estimating that 247 rows of 500 rows are selected", "classification_task": "Classify the issue priority and type: Given a query {{{}SELECT 1 FROM sh2a WHERE k1 < 10 AND k2 < 250{}}}, with a selectivity of 0.02 for {{k1 < 10}} and 0.5 for {{{}k2 < 250{}}}, the combined selectivity should be 0.01 resulting in selecting 5 rows of the 500 rows of the table. If we activate histograms, the combined selectivity is estimated only with the last clause as approx 0.5, so estimating that 247 rows of 500 rows are selected", "qna_task": "Question: What is this issue about?\nAnswer: Wrong estimation for num rows in EXPLAIN with histogram statistics"}}
{"id": "13632973", "key": "HIVE-29299", "project": "HIVE", "summary": "Upgrade Spring to 6.2.12 and spring-ldap-core to 3.3.4 to resolve CVE-2025-41249", "description": "There is [CVE-2025-41249|http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2025-41249] for the current version of {*}spring-core 5.3.39{*}. Current version of *spring-ldap-core[2.4.4]* has spring-core-5.3.39 as dependency, therefore needs to be upgraded to *3.3.4* which is the next latest version not affected by this vulnerability ({*}depends on spring-core-6.2.12{*}).", "comments": "", "created": "2025-10-31T08:31:53.000+0000", "updated": "2025-10-31T10:13:44.000+0000", "derived": {"summary_task": "Summarize this issue: There is [CVE-2025-41249|http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2025-41249] for the current version of {*}spring-core 5.3.39{*}. Current version of *spring-ldap-core[2.4.4]* has spring-core-5.3.39 as dependency, therefore needs to be upgraded to *3.3.4* which is the next latest version not affected by this vulnerability ({*}depends on spring-core-6.2.12{*}).", "classification_task": "Classify the issue priority and type: There is [CVE-2025-41249|http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2025-41249] for the current version of {*}spring-core 5.3.39{*}. Current version of *spring-ldap-core[2.4.4]* has spring-core-5.3.39 as dependency, therefore needs to be upgraded to *3.3.4* which is the next latest version not affected by this vulnerability ({*}depends on spring-core-6.2.12{*}).", "qna_task": "Question: What is this issue about?\nAnswer: Upgrade Spring to 6.2.12 and spring-ldap-core to 3.3.4 to resolve CVE-2025-41249"}}
{"id": "13632959", "key": "HIVE-29298", "project": "HIVE", "summary": "Refactoring minor issues in profile output servlet", "description": "", "comments": "", "created": "2025-10-31T06:53:14.000+0000", "updated": "2025-10-31T08:59:28.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Refactoring minor issues in profile output servlet"}}
{"id": "13632851", "key": "HIVE-29297", "project": "HIVE", "summary": "The directory of the direct insert manifest files should be hidden from read queries", "description": "In HIVE-27536 the value of the \"tmpPrefix\" variable in the Utilities class was changed to \"-tmp.\" from \"_tmp.\". This prefix was used to create the temporary directory for the manifest files. As a side effect of this change, if there is an insert and the read query running at the same time on the same table, the manifest files are read by the the read query. This leads to exception like this {code:java} ERROR : Failed with exception java.io.IOException:java.lang.RuntimeException: ORC split generation failed with exception: org.apache.orc.FileFormatException: Malformed ORC file hdfs://ccycloud-1.kuczoram731.root.comops.site:8020/warehouse/tablespace/managed/hive/acid_test/-tmp.delta_0000002_0000002_0000/000000_0.manifest. Invalid postscript. java.io.IOException: java.lang.RuntimeException: ORC split generation failed with exception: org.apache.orc.FileFormatException: Malformed ORC file hdfs://ccycloud-1.kuczoram731.root.comops.site:8020/warehouse/tablespace/managed/hive/acid_test/-tmp.delta_0000002_0000002_0000/000000_0.manifest. Invalid postscript. at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:646) at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:553) at org.apache.hadoop.hive.ql.exec.FetchTask.executeInner(FetchTask.java:217) at org.apache.hadoop.hive.ql.exec.FetchTask.execute(FetchTask.java:114) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:819) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:547) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:541) at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:190) at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:236) at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:92) at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:341) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1910) at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:361) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: java.lang.RuntimeException: ORC split generation failed with exception: org.apache.orc.FileFormatException: Malformed ORC file hdfs://ccycloud-1.kuczoram731.root.comops.site:8020/warehouse/tablespace/managed/hive/acid_test/-tmp.delta_0000002_0000002_0000/000000_0.manifest. Invalid postscript. at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1891) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1980) at org.apache.hadoop.hive.ql.exec.FetchOperator.generateWrappedSplits(FetchOperator.java:457) at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextSplits(FetchOperator.java:424) at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:328) at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:584) ... 21 more Caused by: java.util.concurrent.ExecutionException: org.apache.orc.FileFormatException: Malformed ORC file hdfs://ccycloud-1.kuczoram731.root.comops.site:8020/warehouse/tablespace/managed/hive/acid_test/-tmp.delta_0000002_0000002_0000/000000_0.manifest. Invalid postscript. at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:192) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1885) ... 26 more Caused by: org.apache.orc.FileFormatException: Malformed ORC file hdfs://ccycloud-1.kuczoram731.root.comops.site:8020/warehouse/tablespace/managed/hive/acid_test/-tmp.delta_0000002_0000002_0000/000000_0.manifest. Invalid postscript. at org.apache.orc.impl.ReaderImpl.ensureOrcFooter(ReaderImpl.java:464) at org.apache.orc.impl.ReaderImpl.extractFileTail(ReaderImpl.java:812) at org.apache.orc.impl.ReaderImpl.<init>(ReaderImpl.java:567) at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:61) at org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:112) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.populateAndCacheStripeDetails(OrcInputFormat.java:1686) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.callInternal(OrcInputFormat.java:1574) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.access$2900(OrcInputFormat.java:1357) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator$1.run(OrcInputFormat.java:1546) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator$1.run(OrcInputFormat.java:1543) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1910) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:1543) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:1357) ... 4 more {code}", "comments": "Merged to master Thanks for the fix, [~kuczoram] !", "created": "2025-10-30T08:26:28.000+0000", "updated": "2025-10-31T20:57:44.000+0000", "derived": {"summary_task": "Summarize this issue: In HIVE-27536 the value of the \"tmpPrefix\" variable in the Utilities class was changed to \"-tmp.\" from \"_tmp.\". This prefix was used to create the temporary directory for the manifest files. As a side effect of this change, if there is an insert and the read query running at the same time on the same table, the manifest files are read by the the read query. This leads to exception like this {code:", "classification_task": "Classify the issue priority and type: In HIVE-27536 the value of the \"tmpPrefix\" variable in the Utilities class was changed to \"-tmp.\" from \"_tmp.\". This prefix was used to create the temporary directory for the manifest files. As a side effect of this change, if there is an insert and the read query running at the same time on the same table, the manifest files are read by the the read query. This leads to exception like this {code:", "qna_task": "Question: What is this issue about?\nAnswer: The directory of the direct insert manifest files should be hidden from read queries"}}
{"id": "13632825", "key": "HIVE-29296", "project": "HIVE", "summary": "Remove getPrimaryKeys codes from beeline module", "description": "If you send a select query from Beeline client with --color, you can see exception in HS2 logs: {code:java} ./apache-hive-beeline-4.2.0-SNAPSHOT/bin/beeline -u \"jdbc:hive2://127.0.0.1:10000/default hive\" --color{code} 2025-09-18T16:22:34,121 ERROR [HiveServer2-Handler-Pool: Thread-166] thrift.ThriftCLIService: Failed to get primary keys [request: TGetPrimaryKeysReq(sessionHandle:TSessionHandle(sessionId:THandleIdentifier(guid:56 22 37 10 00 2D 4B BB B6 FE 42 49 E2 25 75 1F, secret:C7 E6 BF 96 D1 82 4E F5 82 36 AC A7 A1 3E 7A A6)), catalogName:, tableName:test12)] org.apache.hive.service.cli.HiveSQLException: org.apache.thrift.protocol.TProtocolException: Required field 'db_name' is unset! Struct:PrimaryKeysRequest(db_name:null, tbl_name:test12, catName:hive) As i said in HIVE-29213 https://issues.apache.org/jira/browse/HIVE-29213?focusedCommentId=18033761&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-18033761 I believe retrieving the primary key of a table on the beeline side is meaningless (the original purpose was only for coloring the beeline output). {color:#172b4d}We do not need to investigate how to retrieve the correct db_name to ensure the method _HiveDatabaseMetaData::getPrimaryKeys(String catalog, String schema, String table)_ executes properly. {*}I believe removing this code logic would suffice{*}.{color}", "comments": "", "created": "2025-10-30T02:31:05.000+0000", "updated": "2025-10-31T10:30:14.000+0000", "derived": {"summary_task": "Summarize this issue: If you send a select query from Beeline client with --color, you can see exception in HS2 logs: {code:java} ./apache-hive-beeline-4.2.0-SNAPSHOT/bin/beeline -u \"jdbc:hive2://127.0.0.1:10000/default hive\" --color{code} 2025-09-18T16:22:34,121 ERROR [HiveServer2-Handler-Pool: Thread-166] thrift.ThriftCLIService: Failed to get primary keys [request: TGetPrimaryKeysReq(sessionHandle:TSessionHandle(ses", "classification_task": "Classify the issue priority and type: If you send a select query from Beeline client with --color, you can see exception in HS2 logs: {code:java} ./apache-hive-beeline-4.2.0-SNAPSHOT/bin/beeline -u \"jdbc:hive2://127.0.0.1:10000/default hive\" --color{code} 2025-09-18T16:22:34,121 ERROR [HiveServer2-Handler-Pool: Thread-166] thrift.ThriftCLIService: Failed to get primary keys [request: TGetPrimaryKeysReq(sessionHandle:TSessionHandle(ses", "qna_task": "Question: What is this issue about?\nAnswer: Remove getPrimaryKeys codes from beeline module"}}
{"id": "13632794", "key": "HIVE-29295", "project": "HIVE", "summary": "Improve SchemaTool features in HiveMetaStore", "description": "There are some improvements need to do, for Schema tool in the HiveMetaStore.", "comments": "", "created": "2025-10-29T16:29:06.000+0000", "updated": "2025-10-29T16:29:15.000+0000", "derived": {"summary_task": "Summarize this issue: There are some improvements need to do, for Schema tool in the HiveMetaStore.", "classification_task": "Classify the issue priority and type: There are some improvements need to do, for Schema tool in the HiveMetaStore.", "qna_task": "Question: What is this issue about?\nAnswer: Improve SchemaTool features in HiveMetaStore"}}
{"id": "13632748", "key": "HIVE-29294", "project": "HIVE", "summary": "Credential vending for external system access", "description": "_Credential vending_ issues *short-lived, scoped credentials* to the query engine for accessing table storage. Instead of requiring long-term cloud storage keys or broad access, the engine is \u201cvended\u201d a temporary credential just-in-time for the query.", "comments": "", "created": "2025-10-29T09:42:39.000+0000", "updated": "2025-10-29T09:52:06.000+0000", "derived": {"summary_task": "Summarize this issue: _Credential vending_ issues *short-lived, scoped credentials* to the query engine for accessing table storage. Instead of requiring long-term cloud storage keys or broad access, the engine is \u201cvended\u201d a temporary credential just-in-time for the query.", "classification_task": "Classify the issue priority and type: _Credential vending_ issues *short-lived, scoped credentials* to the query engine for accessing table storage. Instead of requiring long-term cloud storage keys or broad access, the engine is \u201cvended\u201d a temporary credential just-in-time for the query.", "qna_task": "Question: What is this issue about?\nAnswer: Credential vending for external system access"}}
{"id": "13632622", "key": "HIVE-29293", "project": "HIVE", "summary": "Restrict using mapreduce.job.queuename config at tez session level", "description": "Queries with config 'mapreduce.job.queuename' is allowing access to any queue if tez.queue.name is null (which is the default behavior). It is ideal to restrict 'mapreduce.job.queuename' from setting this at the Tez session.", "comments": "Hi [~hemanth619], can you please set affected version(s)?", "created": "2025-10-27T22:44:19.000+0000", "updated": "2025-10-30T00:01:28.000+0000", "derived": {"summary_task": "Summarize this issue: Queries with config 'mapreduce.job.queuename' is allowing access to any queue if tez.queue.name is null (which is the default behavior). It is ideal to restrict 'mapreduce.job.queuename' from setting this at the Tez session.", "classification_task": "Classify the issue priority and type: Queries with config 'mapreduce.job.queuename' is allowing access to any queue if tez.queue.name is null (which is the default behavior). It is ideal to restrict 'mapreduce.job.queuename' from setting this at the Tez session.", "qna_task": "Question: What is this issue about?\nAnswer: Restrict using mapreduce.job.queuename config at tez session level"}}
{"id": "13632559", "key": "HIVE-29292", "project": "HIVE", "summary": "Do not call getMSC from client itself, this can lead to deadlock", "description": "", "comments": "Merged to master Thanks [~difin] for the review!", "created": "2025-10-27T10:21:34.000+0000", "updated": "2025-10-28T08:59:30.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Do not call getMSC from client itself, this can lead to deadlock"}}
{"id": "13632556", "key": "HIVE-29291", "project": "HIVE", "summary": "Use minHistoryWriteId by default", "description": "", "comments": "", "created": "2025-10-27T10:10:48.000+0000", "updated": "2025-10-30T14:26:50.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Use minHistoryWriteId by default"}}
{"id": "13632540", "key": "HIVE-29290", "project": "HIVE", "summary": "Enabling \"hive.merge.nway.joins\" returns wrong resutls", "description": "Enabling CBO and \"nway.joins\" returns wrong results. set hive.merge.nway.joins=true; set hive.cbo.enable=true; create table taba(id string); INSERT INTO TABLE taba VALUES ('1'),('2'); create table tabb(id string); INSERT INTO TABLE tabb VALUES ('1'); create table tabc(id string); INSERT INTO TABLE tabc VALUES ('1'),('2'),('2'); //Full data select * from taba A left outer join tabb B on (A.id = B.id) left outer join tabc C on (C.id = A.id); {code} +-----+-------+-------+ | id | id_1 | id_2 | +-----+-------+-------+ | 1 | 1 | 1 | | 2 | NULL | 2 | | 2 | NULL | 2 | +-----+-------+-------+ {code} //ID is not null select * from taba A left outer join chinna.tabb B on (A.id = B.id) left outer join chinna.tabc C on (C.id = A.id) where B.id is not null; {code} +-----+-------+-------+ | id | id_1 | id_2 | +-----+-------+-------+ | 1 | 1 | 1 | +-----+-------+-------+ {code} //ID is null select * from taba A left outer join tabb B on (A.id = B.id) left outer join tabc C on (C.id = A.id) where B.id is null; {code} +-----+-------+-------+ | id | id_1 | id_2 | +-----+-------+-------+ | 1 | NULL | 1 | | 2 | NULL | 2 | | 2 | NULL | 2 | +-----+-------+-------+ {code} In this case B.id is not null for id=1, Correct output is {code} +-----+-------+-------+ | id | id_1 | id_2 | +-----+-------+-------+ | 2 | NULL | 2 | | 2 | NULL | 2 | +-----+-------+-------+ {code}", "comments": "[~chinnalalam] please set the affected version Haven't investigated enough but disabling anti-join is yielding expected result with the following set. {code:java} set hive.merge.nway.joins=true; set hive.auto.convert.anti.join=false;{code} cc [~thomas.rebele] , [~kkasa] I checked with the PR of HIVE-29176. I could reproduce it there as well, so it's a different problem.", "created": "2025-10-27T08:58:26.000+0000", "updated": "2025-10-31T09:44:17.000+0000", "derived": {"summary_task": "Summarize this issue: Enabling CBO and \"nway.joins\" returns wrong results. set hive.merge.nway.joins=true; set hive.cbo.enable=true; create table taba(id string); INSERT INTO TABLE taba VALUES ('1'),('2'); create table tabb(id string); INSERT INTO TABLE tabb VALUES ('1'); create table tabc(id string); INSERT INTO TABLE tabc VALUES ('1'),('2'),('2'); //Full data select * from taba A left outer join tabb B on (A.id = B.i", "classification_task": "Classify the issue priority and type: Enabling CBO and \"nway.joins\" returns wrong results. set hive.merge.nway.joins=true; set hive.cbo.enable=true; create table taba(id string); INSERT INTO TABLE taba VALUES ('1'),('2'); create table tabb(id string); INSERT INTO TABLE tabb VALUES ('1'); create table tabc(id string); INSERT INTO TABLE tabc VALUES ('1'),('2'),('2'); //Full data select * from taba A left outer join tabb B on (A.id = B.i", "qna_task": "Question: What is this issue about?\nAnswer: Enabling \"hive.merge.nway.joins\" returns wrong resutls"}}
{"id": "13632519", "key": "HIVE-29289", "project": "HIVE", "summary": "Remove Deprecated Metastore configuration from HiveConf", "description": "Goal is to remove the redundant configuration present in HiveConf and MetaStoreConf which are deprecated.", "comments": "", "created": "2025-10-27T04:21:06.000+0000", "updated": "2025-10-27T04:21:06.000+0000", "derived": {"summary_task": "Summarize this issue: Goal is to remove the redundant configuration present in HiveConf and MetaStoreConf which are deprecated.", "classification_task": "Classify the issue priority and type: Goal is to remove the redundant configuration present in HiveConf and MetaStoreConf which are deprecated.", "qna_task": "Question: What is this issue about?\nAnswer: Remove Deprecated Metastore configuration from HiveConf"}}
{"id": "13632512", "key": "HIVE-29288", "project": "HIVE", "summary": "Support alter table command for Z-ordering", "description": "", "comments": "", "created": "2025-10-27T01:01:53.000+0000", "updated": "2025-10-27T01:02:00.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Support alter table command for Z-ordering"}}
{"id": "13632309", "key": "HIVE-29287", "project": "HIVE", "summary": "Variant Shredding", "description": "", "comments": "", "created": "2025-10-23T14:02:51.000+0000", "updated": "2025-11-01T12:30:03.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Variant Shredding"}}
{"id": "13632190", "key": "HIVE-29286", "project": "HIVE", "summary": "Session close from different clients/threads leaks resources", "description": "Consider the following scenario where three clients/threads interact with HiveServer2 (HS2) using directly (not via JDBC/ODBC) the Thrift APIs. To showcase the problem we assume that the following properties are set. {code:sql} set hive.driver.parallel.compilation=true; set hive.driver.parallel.compilation.global.limit=1; set hive.server2.compile.lock.timeout=0s; {code} * C1: Connect with HS2 and start session with handle S1 * C1: Send Q1, a slow-compilation query, using S1 * HS2: Obtain the compilation lock and start compiling Q1 * C2: Connect with HS2 and start session with handle S2 * C2: Send Q2, a fast-compilation query, using S2 * HS2: Q2 blocks, waiting for the compilation lock to become available. * C3: Connect with HS2 and close session with handle S2 C1, C2, C3 are different Thrift connections so they are handled by separate HS2 threads. C3 will successfully close/kill/stop the session. However, since Q2 was blocked in compilation it can't be stopped immediately. When C2 finally obtains the compilation lock and finishes, the operation will error out since various session related entries have been cleared out. Thread-152 handles the requests from C2, and it reaches the end of compilation it throws the following errors. {noformat} 2025-10-22T03:05:32,929 ERROR [87a053ce-f5d8-4baa-bdbc-8fd7bc1eef31 HiveServer2-Handler-Pool: Thread-152] thrift.ThriftCLIService: Failed to execute statement [request: TExecuteStatementReq(sessionHandle:TSessionHandle(sessionId:THandleIdentifier(guid:87 A0 53 CE F5 D8 4B AA BD BC 8F D7 BC 1E EF 31, secret:C5 A3 62 BB 51 0B 4A 76 B6 7A 04 0B 38 5C 0A 5A)), statement:EXPLAIN SELECT AVG(age) FROM person GROUP BY name, confOverlay:{}, runAsync:false, queryTimeout:0)] java.lang.RuntimeException: java.lang.NullPointerException: Cannot invoke \"org.apache.hadoop.hive.ql.session.SessionState.getSessionId()\" because the return value of \"org.apache.hive.service.cli.session.HiveSession.getSessionState()\" is null at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:89) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at java.base/java.security.AccessController.doPrivileged(AccessController.java:714) ~[?:?] at java.base/javax.security.auth.Subject.doAs(Subject.java:525) ~[?:?] at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953) ~[hadoop-common-3.4.1.jar:?] at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at jdk.proxy2/jdk.proxy2.$Proxy43.executeStatement(Unknown Source) ~[?:?] at org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:281) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:651) ~[hive-service-4.2.0-SNAPSHOT.jar:?] at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1670) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1650) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:250) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?] at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?] at java.base/java.lang.Thread.run(Thread.java:1583) [?:?] Caused by: java.lang.NullPointerException: Cannot invoke \"org.apache.hadoop.hive.ql.session.SessionState.getSessionId()\" because the return value of \"org.apache.hive.service.cli.session.HiveSession.getSessionState()\" is null at org.apache.hive.service.cli.operation.Operation.afterRun(Operation.java:270) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.cli.operation.Operation.run(Operation.java:288) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:558) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:532) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) ~[?:?] at java.base/java.lang.reflect.Method.invoke(Method.java:580) ~[?:?] at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] ... 17 more 2025-10-22T03:05:32,930 ERROR [87a053ce-f5d8-4baa-bdbc-8fd7bc1eef31 HiveServer2-Handler-Pool: Thread-152] thrift.ThriftCLIService: Failed to close the session org.apache.hive.service.cli.HiveSQLException: Session does not exist: SessionHandle [87a053ce-f5d8-4baa-bdbc-8fd7bc1eef31] at org.apache.hive.service.cli.session.SessionManager.closeSession(SessionManager.java:625) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.cli.CLIService.closeSession(CLIService.java:240) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.cli.thrift.ThriftCLIService.CloseSession(ThriftCLIService.java:611) ~[hive-service-4.2.0-SNAPSHOT.jar:?] at org.apache.hive.service.rpc.thrift.TCLIService$Processor$CloseSession.getResult(TCLIService.java:1620) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.rpc.thrift.TCLIService$Processor$CloseSession.getResult(TCLIService.java:1600) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:250) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?] at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?] at java.base/java.lang.Thread.run(Thread.java:1583) [?:?] {noformat} The fact that C3 closes the session that is open by another client leads to race conditions and may also cause resource leaks. The error may not always be the same with the one reported above since it really depends at which stage of compilation/execution a session is closed/killed. When the queries are over ACID tables the abrupt session termination of S2 will fail to close the transaction that is opened for Q2 leading to a transaction leak. The scenario is inspired from [Hue|https://gethue.com/] workflows where session close can be initiated by user request or [configuration settings|https://docs.gethue.com/administrator/configuration/server/#idle-session-timeout].", "comments": "The [^session-close-txn-leak.patch] contains a unit test that reproduces the problem on current master (commit ad55d58eadd6c1aabc7fed51e25dfffe158a44f6). As explained also under HIVE-11402 the objects that are handling the sessions in HS2 are not really thread safe and there are many assumptions that only one thread is making use of the session at every point in time. The scenario described above violates these assumptions since we have many threads/clients operating on the same session thus the use-case is not officially supported by HS2. HIVE-14227, is very similar to this issue reporting issues when sessions are used by different Thrift connections. The respective patch was never merged since there was no consensus to move forward. Overall, there is a general agreement that clients should avoid using sessions from different connections since the results in most cases are unpredictable.", "created": "2025-10-22T10:41:03.000+0000", "updated": "2025-10-22T11:11:12.000+0000", "derived": {"summary_task": "Summarize this issue: Consider the following scenario where three clients/threads interact with HiveServer2 (HS2) using directly (not via JDBC/ODBC) the Thrift APIs. To showcase the problem we assume that the following properties are set. {code:sql} set hive.driver.parallel.compilation=true; set hive.driver.parallel.compilation.global.limit=1; set hive.server2.compile.lock.timeout=0s; {code} * C1: Connect with HS2 and ", "classification_task": "Classify the issue priority and type: Consider the following scenario where three clients/threads interact with HiveServer2 (HS2) using directly (not via JDBC/ODBC) the Thrift APIs. To showcase the problem we assume that the following properties are set. {code:sql} set hive.driver.parallel.compilation=true; set hive.driver.parallel.compilation.global.limit=1; set hive.server2.compile.lock.timeout=0s; {code} * C1: Connect with HS2 and ", "qna_task": "Question: What is this issue about?\nAnswer: Session close from different clients/threads leaks resources"}}
{"id": "13632147", "key": "HIVE-29285", "project": "HIVE", "summary": "Iceberg: Add docker-compose setups for REST Catalog integrations with Gravitino and Polaris", "description": "", "comments": "What's the motivation for adding and maintaining these examples in the Hive repository? It would be perfectly possible to keep them in a repository apart or contribute them to the Gravitino and Polaris repo respectively. I am raising this question since now Gravitino and Polaris specificities become part of our release cycle and Hive developers should take care of maintaining those keeping them up to date and working. Do we want this extra burden? I don\u2019t see an issue with keeping a Hive docker-compose setup along with Gravitino or Polaris services. I\u2019ve seen the same approach used in Trino \u2014 it simplifies local deployment and provides a practical user guide for integrating with external catalogs. PS: It\u2019s not part of the official Hive release Docker image. cc [~okumin], [~zhangbutao] The motivation for adding REST Catalog integration examples to Hive repo: * Supporting community interest in REST Catalog integrations in Hive: My recent LinkedIn post discussing the Medium article on the Hive REST Catalog client got a significant engagement: 136 likes and 9 reposts, many from data engineers across various companies. This is strong signal that indicates a real-world demand for understanding and using Hive with external REST catalogs. Providing examples directly addresses this expressed interest and helps to maintain interest in Apache Hive. * Precedent in other Database and Data Warehousing Open-Source Projects: ** Apache Doris: this repository includes a docker/thirdparties/docker-compose directory containing over 20 docker-compose-based integration examples with systems like Elasticsearch, Spark and even Hive. Link: [https://github.com/apache/doris/tree/master/docker/thirdparties/docker-compose] ** ClickHouse: they maintain numerous Docker integration setups within their ci/docker/integration directory, used for testing integrations with HMS Catalog, Unitity Catalog, Hive, PostgreSQL, MySQL, etc. Link: [https://github.com/ClickHouse/ClickHouse/tree/master/ci/docker/integration] I never object to integrating other open-source components into Hive, as long as such integrations benefit the Hive community. The only point I would emphasize is that any new feature integration must be accompanied by community-friendly documentation to help Hive users understand and test the functionality. This will assist the Hive community in gathering positive feedback from users. Otherwise, newly integrated features may go unnoticed and unused, and the related code blocks could become dead code. I noticed that this integration includes some README.md files to help users get started quickly. However, I believe it would be more appropriate to incorporate the usage documentation into https://hive.apache.org/, which should be regarded as the community's sole entry point for user documentation. Just to clarify my previous comment. I am not questioning the usefulness of having such user-guides/examples. They are definitely very helpful for end-users and can expand Hive popularity. I am mainly trying to figure out if they should be part of the Hive repository. If the motivation is to build HMS integration tests with Gravitino, Polaris, etc., then it makes perfect sense to have such files in the Hive repository. However, as it stands right now the docker-compose files and everything else are not used for testing. Their main role seems to be to document how the integration works and what configurations users should set in order to get things working. If the motivation is to document how to setup HMS with other projects then maybe a better place for all these would be the Hive Website ([https://github.com/apache/hive-site]). The site will be indexed by search engines and the information will be easier to find there. Note that almost everything that gets into [https://github.com/apache/hive] becomes part of the (source or binary) release. The files may not be in Docker hub but they still have to fulfill all the other release criteria such as licensing, cves, etc. Anyways, I am not actively involved in the development of the REST catalog so my comments are mostly advisory and not blocking. Feel free to continue with whatever path seems to be the best for the project and its end-users. > If the motivation is to build HMS integration tests with Gravitino, Polaris, etc. Just to clarify, this setup fully replaces HMS with alternative metadata providers such as Gravitino or Polaris, and HS2 communicates with them via REST. From my perspective, the motivation is to provide a simple one-liner script (docker compose) to initialize clusters with Gravitino or Polaris catalog integration in the local environment. From the conversations, I assume Hive developers and users are expected customers. Additionally, I expect the Hive project to demonstrate how to set up HiveServer2 for external Iceberg ecosystems. It means the Hive project is not intended to define best practices for setting up Apache Gravitino or Polaris. Having docker-compose.yaml or not: I agree that having docker-compose.yaml could be a good example or useful tool to develop related features. In that sense, I am slightly biased to start not with Gravitino/Polaris but with HMS REST with authentication=none. The REST catalog implementation is opaque to HS2, and we may not need to maintain docker-compose.yaml for every external project. We can add a variant when we need to develop, for instance, Gravitino-specific features aggressively. apache/hive vs apache/hive-site: In my opinion, it would be convenient for Hive developers if we had docker-compose.yaml in apache/hive. As docker-compose settings can express how to build a Docker image, it might make local dev easier. For Hive users, apache/hive or apache/hive-site might not make a difference. I agree that we should have a document to integrate HiveServer2 with an external catalog for Hive users anyway. I don't mind whether it is part of HIVE-29285 or a separate ticket, but we probably don't have a strong reason to disagree.", "created": "2025-10-21T20:40:54.000+0000", "updated": "2025-11-01T01:03:29.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Iceberg: Add docker-compose setups for REST Catalog integrations with Gravitino and Polaris"}}
{"id": "13632099", "key": "HIVE-29284", "project": "HIVE", "summary": "Decouple hard hadoop-yarn-registry dependency from hive-exec", "description": "I am tracking down some transitive dependency issues in {{spark}}, and I have found a potential tighter-than-necessary coupling between {{hive-exec}} and {{hadoop-yarn-registry}}. {{hadoop-yarn-registry}} is definitely used from the LLAP components, but {{hive-exec}} seems to depend on it only at a single point, in {noformat} ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java {noformat} {noformat} if (llapMode) { // localize llap client jars addJarLRByClass(LlapTaskSchedulerService.class, commonLocalResources); addJarLRByClass(LlapProtocolClientImpl.class, commonLocalResources); addJarLRByClass(LlapProtocolClientProxy.class, commonLocalResources); addJarLRByClass(RegistryOperations.class, commonLocalResources); } {noformat} Since spark uses hive-exec but does not use LLAP, my spark side dependency issues would go away, if the reference to this RegistryOperations.class was not resolved at compile time, but only at runtime. (== removing the compile time dependency to registry). I'll provide a simple patch to demonstrate the idea.", "comments": "(proposed patch linked above)", "created": "2025-10-21T12:20:16.000+0000", "updated": "2025-10-22T11:52:42.000+0000", "derived": {"summary_task": "Summarize this issue: I am tracking down some transitive dependency issues in {{spark}}, and I have found a potential tighter-than-necessary coupling between {{hive-exec}} and {{hadoop-yarn-registry}}. {{hadoop-yarn-registry}} is definitely used from the LLAP components, but {{hive-exec}} seems to depend on it only at a single point, in {noformat} ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java {nof", "classification_task": "Classify the issue priority and type: I am tracking down some transitive dependency issues in {{spark}}, and I have found a potential tighter-than-necessary coupling between {{hive-exec}} and {{hadoop-yarn-registry}}. {{hadoop-yarn-registry}} is definitely used from the LLAP components, but {{hive-exec}} seems to depend on it only at a single point, in {noformat} ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java {nof", "qna_task": "Question: What is this issue about?\nAnswer: Decouple hard hadoop-yarn-registry dependency from hive-exec"}}
{"id": "13632074", "key": "HIVE-29283", "project": "HIVE", "summary": "Display reason/txn id that is causing ready for cleaning state of a compaction in SHOW COMPACTIONS output", "description": "*Problem Statement:* We see compaction requests are showing 'ready for cleaning' state. And sometimes, it will not finish and stuck there. There could be multiple reasons like a {*}long running transaction{*}, Obsolete directories were already cleaned, Cleaner stopped the attempt to clean for this specific compaction request etc. So the specific reason about why a compaction is in ready for cleaning can only be known after looking at the logs which is very difficult and would take a lot of time to get to a conclusion on the RCA Another case is where Initiator not able to initiate a compaction request for some reason like file not found etc.. We can capture this one as well and display *Proposal:* Display the reason for 'ready for cleaning/failed to initiate' state of a compaction request. If it is due to long running txn, we should display that txn id, so that if that txn happens to be stale/invalid/unwanted etc, then that can be aborted so that compaction can continue", "comments": "", "created": "2025-10-21T08:24:13.000+0000", "updated": "2025-10-21T08:24:50.000+0000", "derived": {"summary_task": "Summarize this issue: *Problem Statement:* We see compaction requests are showing 'ready for cleaning' state. And sometimes, it will not finish and stuck there. There could be multiple reasons like a {*}long running transaction{*}, Obsolete directories were already cleaned, Cleaner stopped the attempt to clean for this specific compaction request etc. So the specific reason about why a compaction is in ready for cleani", "classification_task": "Classify the issue priority and type: *Problem Statement:* We see compaction requests are showing 'ready for cleaning' state. And sometimes, it will not finish and stuck there. There could be multiple reasons like a {*}long running transaction{*}, Obsolete directories were already cleaned, Cleaner stopped the attempt to clean for this specific compaction request etc. So the specific reason about why a compaction is in ready for cleani", "qna_task": "Question: What is this issue about?\nAnswer: Display reason/txn id that is causing ready for cleaning state of a compaction in SHOW COMPACTIONS output"}}
{"id": "13632073", "key": "HIVE-29282", "project": "HIVE", "summary": "Make function ddl task work with catalog", "description": "The code related to function DDL does not have catalog attributes. We need to add catalog attributes to function-related interfaces so that functions can work simultaneously across multiple catalogs. [https://github.com/apache/hive/tree/master/ql/src/java/org/apache/hadoop/hive/ql/ddl/function]", "comments": "", "created": "2025-10-21T08:18:19.000+0000", "updated": "2025-10-21T08:18:52.000+0000", "derived": {"summary_task": "Summarize this issue: The code related to function DDL does not have catalog attributes. We need to add catalog attributes to function-related interfaces so that functions can work simultaneously across multiple catalogs. [https://github.com/apache/hive/tree/master/ql/src/java/org/apache/hadoop/hive/ql/ddl/function]", "classification_task": "Classify the issue priority and type: The code related to function DDL does not have catalog attributes. We need to add catalog attributes to function-related interfaces so that functions can work simultaneously across multiple catalogs. [https://github.com/apache/hive/tree/master/ql/src/java/org/apache/hadoop/hive/ql/ddl/function]", "qna_task": "Question: What is this issue about?\nAnswer: Make function ddl task work with catalog"}}
{"id": "13632072", "key": "HIVE-29281", "project": "HIVE", "summary": "Make proactive cache eviction work with catalog", "description": "Proactive LLAP cache eviction was introduced in HIVE-22820. But this feature didn't add catalog field. We need to consider to add catalog field to it to make it work in case of there are multiple catalogs. Proactive LLAP cache eviction was used several places. such as [https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropDatabaseHandler.java] {code:java} if (LlapHiveUtils.isLlapMode(context.getConf())) { ProactiveEviction.Request.Builder llapEvictRequestBuilder = ProactiveEviction.Request.Builder.create(); llapEvictRequestBuilder.addDb(dbName); ProactiveEviction.evict(context.getConf(), llapEvictRequestBuilder.build()); } {code}", "comments": "", "created": "2025-10-21T08:06:42.000+0000", "updated": "2025-10-21T08:06:42.000+0000", "derived": {"summary_task": "Summarize this issue: Proactive LLAP cache eviction was introduced in HIVE-22820. But this feature didn't add catalog field. We need to consider to add catalog field to it to make it work in case of there are multiple catalogs. Proactive LLAP cache eviction was used several places. such as [https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropDatabaseHandler.java]", "classification_task": "Classify the issue priority and type: Proactive LLAP cache eviction was introduced in HIVE-22820. But this feature didn't add catalog field. We need to consider to add catalog field to it to make it work in case of there are multiple catalogs. Proactive LLAP cache eviction was used several places. such as [https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropDatabaseHandler.java]", "qna_task": "Question: What is this issue about?\nAnswer: Make proactive cache eviction work with catalog"}}
{"id": "13632068", "key": "HIVE-29280", "project": "HIVE", "summary": "Drop deprecated methods from Metastore", "description": "In RawStore, AlterHandler and IHMSHandler, we have many methods marked as deprecated even since 2.2.0. It's good to drop them off in the next major release to keep the code clean and maintainable.", "comments": "", "created": "2025-10-21T07:31:31.000+0000", "updated": "2025-10-29T14:33:26.000+0000", "derived": {"summary_task": "Summarize this issue: In RawStore, AlterHandler and IHMSHandler, we have many methods marked as deprecated even since 2.2.0. It's good to drop them off in the next major release to keep the code clean and maintainable.", "classification_task": "Classify the issue priority and type: In RawStore, AlterHandler and IHMSHandler, we have many methods marked as deprecated even since 2.2.0. It's good to drop them off in the next major release to keep the code clean and maintainable.", "qna_task": "Question: What is this issue about?\nAnswer: Drop deprecated methods from Metastore"}}
{"id": "13632065", "key": "HIVE-29279", "project": "HIVE", "summary": "Make table ddl task work with catalog", "description": "Table related ddl task should respect catalog. Such as these syntax: {code:java} create table cat.db.tbl(id int); alter table cat.db.tbl set .. {code}", "comments": "", "created": "2025-10-21T06:46:34.000+0000", "updated": "2025-10-21T06:46:34.000+0000", "derived": {"summary_task": "Summarize this issue: Table related ddl task should respect catalog. Such as these syntax: {code:java} create table cat.db.tbl(id int); alter table cat.db.tbl set .. {code}", "classification_task": "Classify the issue priority and type: Table related ddl task should respect catalog. Such as these syntax: {code:java} create table cat.db.tbl(id int); alter table cat.db.tbl set .. {code}", "qna_task": "Question: What is this issue about?\nAnswer: Make table ddl task work with catalog"}}
{"id": "13632055", "key": "HIVE-29278", "project": "HIVE", "summary": "Make Replicate functions work with catalog", "description": "*Replicate functions* was added in HIVE-16145. Its some interfaces can not work with catalog. Such as: [https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropDatabaseHandler.java] [https://github.com/apache/hive/blob/|https://github.com/apache/hive/blob/rel/release-4.1.0/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AlterDatabaseHandler.java] [master|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropDatabaseHandler.java] [/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AlterDatabaseHandler.java|https://github.com/apache/hive/blob/rel/release-4.1.0/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AlterDatabaseHandler.java] [https://github.com/apache/hive/blob/|https://github.com/apache/hive/blob/rel/release-4.1.0/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/CreateDatabaseHandler.java] [master|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropDatabaseHandler.java] [/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/CreateDatabaseHandler.java|https://github.com/apache/hive/blob/rel/release-4.1.0/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/CreateDatabaseHandler.java] [https://github.com/apache/hive/blob/|https://github.com/apache/hive/blob/rel/release-4.1.0/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/incremental/IncrementalLoadTasksBuilder.java] [master|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropDatabaseHandler.java] [/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/incremental/IncrementalLoadTasksBuilder.java|https://github.com/apache/hive/blob/rel/release-4.1.0/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/incremental/IncrementalLoadTasksBuilder.java] [https://github.com/apache/hive/blob/|https://github.com/apache/hive/blob/rel/release-4.1.0/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplLoadWork.java] [master|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropDatabaseHandler.java] [/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplLoadWork.java|https://github.com/apache/hive/blob/rel/release-4.1.0/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplLoadWork.java] These interfaces were found in [HIVE-29177|https://github.com/apache/hive/pull/6088#top] , and you can check the code blocks which need to add catalog by searching keyword {*}TODO catalog{*}. We need to add catalog to these interfaces to make this repl function work with catalog.", "comments": "", "created": "2025-10-21T03:14:44.000+0000", "updated": "2025-10-21T06:13:01.000+0000", "derived": {"summary_task": "Summarize this issue: *Replicate functions* was added in HIVE-16145. Its some interfaces can not work with catalog. Such as: [https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropDatabaseHandler.java] [https://github.com/apache/hive/blob/|https://github.com/apache/hive/blob/rel/release-4.1.0/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AlterDataba", "classification_task": "Classify the issue priority and type: *Replicate functions* was added in HIVE-16145. Its some interfaces can not work with catalog. Such as: [https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropDatabaseHandler.java] [https://github.com/apache/hive/blob/|https://github.com/apache/hive/blob/rel/release-4.1.0/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AlterDataba", "qna_task": "Question: What is this issue about?\nAnswer: Make Replicate functions work with catalog"}}
{"id": "13632017", "key": "HIVE-29277", "project": "HIVE", "summary": "Invalid index should return null for get_json_object", "description": "{{get_json_object}} index is numeric, but current implement will return the entire array string for invalid index like {*}$[1 ]{*}. {code:sql} hive> select get_json_object('[1,2,3]', \"$[1 ]\"); [1,2,3] {code}", "comments": "Hi [~wechar], can you please set affected version(s)?", "created": "2025-10-20T16:15:28.000+0000", "updated": "2025-10-29T04:05:24.000+0000", "derived": {"summary_task": "Summarize this issue: {{get_json_object}} index is numeric, but current implement will return the entire array string for invalid index like {*}$[1 ]{*}. {code:sql} hive> select get_json_object('[1,2,3]', \"$[1 ]\"); [1,2,3] {code}", "classification_task": "Classify the issue priority and type: {{get_json_object}} index is numeric, but current implement will return the entire array string for invalid index like {*}$[1 ]{*}. {code:sql} hive> select get_json_object('[1,2,3]', \"$[1 ]\"); [1,2,3] {code}", "qna_task": "Question: What is this issue about?\nAnswer: Invalid index should return null for get_json_object "}}
{"id": "13631961", "key": "HIVE-29276", "project": "HIVE", "summary": "Support major OAuth 2 Authorization Server implementation", "description": "HIVE-29020 added OAuth 2 support based on the latest best practice, such as \"RFC 9068: JSON Web Token (JWT) Profile for OAuth 2.0 Access Tokens\". Some real IdPs don't follow some of the new best practices. In this ticket, we will update the OAuth 2 configurations so that we can support major identity platforms, such as Microsoft Entra ID or Okta. For example, RFC 9068 recommends including `typ: at+jwt` in a JWT header so that a malicious user can't inject a JWT that is not for OAuth 2 protected resources. However, Okta's access tokens don't always have the type parameter and there is no tuning knob to add it. https://developer.okta.com/docs/api/openapi/okta-oauth/guides/overview/#id-token-header", "comments": "I'm learning Microsoft Entra ID, and its OAuth 2 access token is unique... This is a sample payload. {code:java} { \"typ\": \"JWT\", \"alg\": \"RS256\", \"kid\": \"{kid}\" } { \"aud\": \"{App ID, UUID}\", \"iss\": \"https://login.microsoftonline.com/{tenant id}/v2.0\", \"iat\": 1761535310, \"nbf\": 1761535310, \"exp\": 1761539210, \"aio\": \"...\", \"azp\": \"...\", \"azpacr\": \"1\", \"oid\": \"...\", \"rh\": \"...\", \"roles\": [ \"catalog2\" ], \"sub\": \"{UUID}\", \"tid\": \"{tenant ID}\", \"uti\": \"...\", \"ver\": \"2.0\", \"xms_ftd\": \"...\" }{code} It uses \"JWT\" for \"typ\" and service accounts(= machine user) use \"roles\" instead of \"scope\"", "created": "2025-10-20T06:18:52.000+0000", "updated": "2025-10-27T03:57:05.000+0000", "derived": {"summary_task": "Summarize this issue: HIVE-29020 added OAuth 2 support based on the latest best practice, such as \"RFC 9068: JSON Web Token (JWT) Profile for OAuth 2.0 Access Tokens\". Some real IdPs don't follow some of the new best practices. In this ticket, we will update the OAuth 2 configurations so that we can support major identity platforms, such as Microsoft Entra ID or Okta. For example, RFC 9068 recommends including `typ: at", "classification_task": "Classify the issue priority and type: HIVE-29020 added OAuth 2 support based on the latest best practice, such as \"RFC 9068: JSON Web Token (JWT) Profile for OAuth 2.0 Access Tokens\". Some real IdPs don't follow some of the new best practices. In this ticket, we will update the OAuth 2 configurations so that we can support major identity platforms, such as Microsoft Entra ID or Okta. For example, RFC 9068 recommends including `typ: at", "qna_task": "Question: What is this issue about?\nAnswer: Support major OAuth 2 Authorization Server implementation"}}
{"id": "13631797", "key": "HIVE-29275", "project": "HIVE", "summary": "Stats autogather calculates the min statistic incorrectly", "description": "In stats_histogram.q autogather gets enabled and then rows are inserted into a newly created table. The minimum value for column e is [-123.2|https://github.com/apache/hive/blob/55d9ab7d6b00fa510be791b9de55974f61c90519/ql/src/test/queries/clientpositive/stats_histogram.q#L20]. However, {{DESCRIBE FORMATTED test_stats e}} shows [-10.2|https://github.com/apache/hive/blob/55d9ab7d6b00fa510be791b9de55974f61c90519/ql/src/test/results/clientpositive/llap/stats_histogram.q.out#L364] as the minimum value. When executing {{ANALYZE TABLE test_stats COMPUTE STATISTICS FOR COLUMNS;}} before the {{DESCRIBE FORMATTED test_stats e}} command, the [min value is -123.2|https://github.com/thomasrebele/hive/commit/2be9bef2851028678fa6752f7482080b3d201a51#diff-436ceeced7ea88c3ad4d931cfbf3d09feb838eef368a74ca8106d378209b1209L262-L364] as expected.", "comments": "[~krisztiankasa] suggested to try it without vectorization to track down the bug. [~thomas.rebele] please set the affected version", "created": "2025-10-17T07:37:55.000+0000", "updated": "2025-10-30T19:26:40.000+0000", "derived": {"summary_task": "Summarize this issue: In stats_histogram.q autogather gets enabled and then rows are inserted into a newly created table. The minimum value for column e is [-123.2|https://github.com/apache/hive/blob/55d9ab7d6b00fa510be791b9de55974f61c90519/ql/src/test/queries/clientpositive/stats_histogram.q#L20]. However, {{DESCRIBE FORMATTED test_stats e}} shows [-10.2|https://github.com/apache/hive/blob/55d9ab7d6b00fa510be791b9de55", "classification_task": "Classify the issue priority and type: In stats_histogram.q autogather gets enabled and then rows are inserted into a newly created table. The minimum value for column e is [-123.2|https://github.com/apache/hive/blob/55d9ab7d6b00fa510be791b9de55974f61c90519/ql/src/test/queries/clientpositive/stats_histogram.q#L20]. However, {{DESCRIBE FORMATTED test_stats e}} shows [-10.2|https://github.com/apache/hive/blob/55d9ab7d6b00fa510be791b9de55", "qna_task": "Question: What is this issue about?\nAnswer: Stats autogather calculates the min statistic incorrectly"}}
{"id": "13631740", "key": "HIVE-29274", "project": "HIVE", "summary": "Flaky TestMetastoreLeaseLeader#testHouseKeepingThreads", "description": "[https://ci.hive.apache.org/job/hive-flaky-check/906/] master: [https://ci.hive.apache.org/job/hive-precommit/job/master/2706/testReport/junit/org.apache.hadoop.hive.metastore/TestMetastoreLeaseNonLeader/Testing___split_04___PostProcess___testHouseKeepingThreads/] github PR: [https://ci.hive.apache.org/job/hive-precommit/job/PR-6135/1/testReport/junit/org.apache.hadoop.hive.metastore/TestMetastoreLeaseLeader/Testing___split_01___PostProcess___testHouseKeepingThreads/]", "comments": "[~zabetak] should we disable it right away, wdyt? cc [~dengzh] it might be a code issue as well: {code:java} javax.jdo.JDOUserException: Transaction is still active. You should always close your transactions correctly using commit() or rollback(). FailedObject:org.datanucleus.api.jdo.JDOPersistenceManager@2d5b55be at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.close(JDOPersistenceManagerFactory.java:644) at org.apache.hadoop.hive.metastore.PersistenceManagerProvider.closePmfInternal(PersistenceManagerProvider.java:151){code} From my perspective if it is too flaky we can disable ASAP and search for the root cause a bit later. Fix has been pushed to master. Thank you [~dkuzmenko] for the review! Flaky check: https://ci.hive.apache.org/job/hive-flaky-check/927/", "created": "2025-10-16T15:37:00.000+0000", "updated": "2025-10-28T03:08:33.000+0000", "derived": {"summary_task": "Summarize this issue: [https://ci.hive.apache.org/job/hive-flaky-check/906/] master: [https://ci.hive.apache.org/job/hive-precommit/job/master/2706/testReport/junit/org.apache.hadoop.hive.metastore/TestMetastoreLeaseNonLeader/Testing___split_04___PostProcess___testHouseKeepingThreads/] github PR: [https://ci.hive.apache.org/job/hive-precommit/job/PR-6135/1/testReport/junit/org.apache.hadoop.hive.metastore/TestMetastore", "classification_task": "Classify the issue priority and type: [https://ci.hive.apache.org/job/hive-flaky-check/906/] master: [https://ci.hive.apache.org/job/hive-precommit/job/master/2706/testReport/junit/org.apache.hadoop.hive.metastore/TestMetastoreLeaseNonLeader/Testing___split_04___PostProcess___testHouseKeepingThreads/] github PR: [https://ci.hive.apache.org/job/hive-precommit/job/PR-6135/1/testReport/junit/org.apache.hadoop.hive.metastore/TestMetastore", "qna_task": "Question: What is this issue about?\nAnswer: Flaky TestMetastoreLeaseLeader#testHouseKeepingThreads"}}
{"id": "13631713", "key": "HIVE-29273", "project": "HIVE", "summary": "Default port appended should respect transport mode when host is specified without port", "description": "As of now, when the port is not specified in the JDBC URI, 1000 gets appended by default irrespective of transport mode being binary/http. When HS2 is running in http mode and user tries to connect without specifying the port explicitly, it tries to connect to port 10000, running into error Sample error: {code:java} Error: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:10000/default;transportMode=http;httpPath=cliservice: Could not establish connection to jdbc:hive2://localhost:10000/default;transportMode=http;httpPath=cliservice: org.apache.http.conn.HttpHostConnectException: Connect to localhost:10000 {code}", "comments": "Merged. [~tanishqchugh] Thanks for your contribution! https://github.com/apache/hive/pull/6137", "created": "2025-10-16T09:53:04.000+0000", "updated": "2025-10-20T05:44:48.000+0000", "derived": {"summary_task": "Summarize this issue: As of now, when the port is not specified in the JDBC URI, 1000 gets appended by default irrespective of transport mode being binary/http. When HS2 is running in http mode and user tries to connect without specifying the port explicitly, it tries to connect to port 10000, running into error Sample error: {code:java} Error: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:10000", "classification_task": "Classify the issue priority and type: As of now, when the port is not specified in the JDBC URI, 1000 gets appended by default irrespective of transport mode being binary/http. When HS2 is running in http mode and user tries to connect without specifying the port explicitly, it tries to connect to port 10000, running into error Sample error: {code:java} Error: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:10000", "qna_task": "Question: What is this issue about?\nAnswer: Default port appended should respect transport mode when host is specified without port"}}
{"id": "13631693", "key": "HIVE-29272", "project": "HIVE", "summary": "Query-based MINOR compaction should not consider minOpenWriteId", "description": "In certain scenarios the query-based MINOR compaction produces empty delta file. In ACID tables it will be automatically cleaned up like no compaction happened, but on insert-only tables it causes data loss. This issue happens if there is an aborted and an open transaction on the compacted table. Let\u2019s see an example: * Run an insert which creates delta_0000001_0000001 (writeId=1) * Start an insert and abort the transaction (writeId=2) * Run an insert which creates delta_0000003_0000003 (writeId=3) * Run an insert which creates delta_0000004_0000004 (writeId=4), but before it finishes, start the MINOR compaction * When the compaction is finished the table will contain the following files: delta_0000001_0000003 delta_0000004_0000004 delta_0000004_0000004/000000_0 delta_0000001_0000001 delta_0000001_0000001/000000_0 delta_0000003_0000003 delta_0000001_0000001/000000_0 * It can be seen that the delta_0000001_0000003 directory (which was produced by the compactor) is empty. * When the Cleaner runs, it will remove delta_0000001_0000001 and delta_0000003_0000003, so the data in them will be lost. This happens because of this check in the MINOR compaction: {code:java} long minWriteID = validWriteIdList.getMinOpenWriteId() == null ? 1 : validWriteIdList.getMinOpenWriteId(); long highWatermark = validWriteIdList.getHighWatermark(); List<AcidUtils.ParsedDelta> deltas = dir.getCurrentDirectories().stream().filter( delta -> delta.isDeleteDelta() == isDeleteDelta && delta.getMaxWriteId() <= highWatermark && delta.getMinWriteId() >= minWriteID) .collect(Collectors.toList()); if (deltas.isEmpty()) { query.setLength(0); // no alter query needed; clear StringBuilder return; } {code} If the table has aborted and open transactions, the minOpenWriteId will be set. In the example it will be 4. When the ValidCompactorWriteIdList is created in the TxnUtils.createValidCompactWriteIdList the highWaterMark will be set to minOpenWriteId-1, so this will ensure that the compaction range is below the minOpenWriteId. But in the minor compaction's code the minOpenWriteId is considered as the lower limit, so it wants to compact deltas which are above this values. This is not correct, it seems like a misunderstanding this minOpenWriteId values. In the example the compaction should consider delta_1 and delta_3, but none of them fulfills the conditions \"delta.getMinWriteId() >= minWriteID\" as the minWriteID=minOpenWriteId=4 here. This check in the MINOR compaction code is not correct, I think it is safe to leave out checking against the minOpenWriteId as the highWatermark already adjusted to it.", "comments": "Hi [~kuczoram], can you please set affected version(s)? Merged to master Thanks [~kuczoram] for the fix! Thanks a lot [~dkuzmenko] for the review and for merging the fix.", "created": "2025-10-16T08:14:50.000+0000", "updated": "2025-10-31T16:48:15.000+0000", "derived": {"summary_task": "Summarize this issue: In certain scenarios the query-based MINOR compaction produces empty delta file. In ACID tables it will be automatically cleaned up like no compaction happened, but on insert-only tables it causes data loss. This issue happens if there is an aborted and an open transaction on the compacted table. Let\u2019s see an example: * Run an insert which creates delta_0000001_0000001 (writeId=1) * Start an inser", "classification_task": "Classify the issue priority and type: In certain scenarios the query-based MINOR compaction produces empty delta file. In ACID tables it will be automatically cleaned up like no compaction happened, but on insert-only tables it causes data loss. This issue happens if there is an aborted and an open transaction on the compacted table. Let\u2019s see an example: * Run an insert which creates delta_0000001_0000001 (writeId=1) * Start an inser", "qna_task": "Question: What is this issue about?\nAnswer: Query-based MINOR compaction should not consider minOpenWriteId"}}
{"id": "13631618", "key": "HIVE-29271", "project": "HIVE", "summary": "Skip corrupted files while reading an Orc table", "description": "*Scenario:* There are large number of corrupted files scattered across multiple partitions. They were created by some external tools. Now when we query the table, exceptions like below are thrown {noformat} Caused by: com.google.protobuf.InvalidProtocolBufferException: Protocol message contained an invalid tag (zero). at com.google.protobuf.InvalidProtocolBufferException.invalidTag(InvalidProtocolBufferException.java:89) at com.google.protobuf.CodedInputStream.readTag(CodedInputStream.java:108) at org.apache.orc.OrcProto$PostScript.<init>(OrcProto.java:30246) at org.apache.orc.OrcProto$PostScript.<init>(OrcProto.java:30210) at org.apache.orc.OrcProto$PostScript$1.parsePartialFrom(OrcProto.java:30353) at org.apache.orc.OrcProto$PostScript$1.parsePartialFrom(OrcProto.java:30348) at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:89) at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:95) at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:49) at org.apache.orc.OrcProto$PostScript.parseFrom(OrcProto.java:30791) at org.apache.orc.impl.ReaderImpl.extractPostScript(ReaderImpl.java:644) at org.apache.orc.impl.ReaderImpl.extractFileTail(ReaderImpl.java:814) at org.apache.orc.impl.ReaderImpl.<init>(ReaderImpl.java:567) at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:61){noformat} So, it is not possible to query the data from good files. The only way available today is to identify corrupted files from the table and remove them. Orc-tools is taking a lot of time to find out the corrupt files as it will traverse each file sequentially and show errors for corrupt file. *Proposal:* In spark we have a config, *ignoreCorruptFiles* using which we can read data from rest of the files skipping corrupt files. Can we also implement something like this in Hive as well? We can have a flag to enable this feature which is disabled by default. *Issues:* If we do not fail the queries, corrupt files may accumulate and may cause issues later like size of the table, incorrect results etc.. The reason behind requesting this feature is that it is very difficult to identify faulty/corrupt files easily in a large table/s. So it is also good if we can list all the corrupt files using a simple Hive query, so that they can be deleted without disturbing the actual Hive query flow to skip them", "comments": "", "created": "2025-10-15T13:04:28.000+0000", "updated": "2025-10-15T13:04:28.000+0000", "derived": {"summary_task": "Summarize this issue: *Scenario:* There are large number of corrupted files scattered across multiple partitions. They were created by some external tools. Now when we query the table, exceptions like below are thrown {noformat} Caused by: com.google.protobuf.InvalidProtocolBufferException: Protocol message contained an invalid tag (zero). at com.google.protobuf.InvalidProtocolBufferException.invalidTag(InvalidProtocol", "classification_task": "Classify the issue priority and type: *Scenario:* There are large number of corrupted files scattered across multiple partitions. They were created by some external tools. Now when we query the table, exceptions like below are thrown {noformat} Caused by: com.google.protobuf.InvalidProtocolBufferException: Protocol message contained an invalid tag (zero). at com.google.protobuf.InvalidProtocolBufferException.invalidTag(InvalidProtocol", "qna_task": "Question: What is this issue about?\nAnswer: Skip corrupted files while reading an Orc table"}}
{"id": "13631600", "key": "HIVE-29270", "project": "HIVE", "summary": "Remove HMS compactor workers deprecated functionality", "description": "The metastore compactor workers functionality was deprecated as part of [HIVE-26443|https://issues.apache.org/jira/browse/HIVE-26443]", "comments": "Merged to master Thanks for the patch [~tanishqchugh] and [~dengzh] for the review!", "created": "2025-10-15T10:06:35.000+0000", "updated": "2025-10-20T08:11:06.000+0000", "derived": {"summary_task": "Summarize this issue: The metastore compactor workers functionality was deprecated as part of [HIVE-26443|https://issues.apache.org/jira/browse/HIVE-26443]", "classification_task": "Classify the issue priority and type: The metastore compactor workers functionality was deprecated as part of [HIVE-26443|https://issues.apache.org/jira/browse/HIVE-26443]", "qna_task": "Question: What is this issue about?\nAnswer: Remove HMS compactor workers deprecated functionality"}}
{"id": "13631572", "key": "HIVE-29268", "project": "HIVE", "summary": "Iceberg: Close catalog in Catalogs.loadTable() to fix resource leak from open ResolvingFileIO", "description": "`Catalogs.loadTable()` loads an Iceberg catalog and then loads a table from that catalog by calling the`loadTable()` method on the loaded catalog. It doesn't close the catalog before existing the function which causes resource leaks with exceptions like following in the hive.log: {code:java} 2025-10-14 16:52:12 2025-10-14T20:52:12,131 INFO [HiveServer2-Background-Pool: Thread-150] tez.TezSessionState: Opening new Tez Session (id: c8064268-e392-40ee-a062-008ade6a3b8e, scratch dir: file:/tmp/hive/hive/_tez_session_dir/c8064268-e392-40ee-a062-008ade6a3b8e) 2025-10-14 16:52:12 2025-10-14T20:52:12,142 INFO [HiveServer2-Background-Pool: Thread-150] client.TezClient: Session mode. Starting session. 2025-10-14 16:52:12 2025-10-14T20:52:12,171 INFO [HiveServer2-Background-Pool: Thread-150] client.TezClientUtils: Ignoring 'tez.lib.uris' since 'tez.ignore.lib.uris' is set to true 2025-10-14 16:52:12 2025-10-14T20:52:12,184 INFO [HiveServer2-Background-Pool: Thread-150] client.TezClient: Tez system stage directory file:/tmp/hive/hive/_tez_session_dir/c8064268-e392-40ee-a062-008ade6a3b8e/.tez/application_1760475132141_0001 doesn't exist and is created 2025-10-14 16:52:12 2025-10-14T20:52:12,222 INFO [HiveServer2-Background-Pool: Thread-150] conf.Configuration: resource-types.xml not found 2025-10-14 16:52:12 2025-10-14T20:52:12,222 INFO [HiveServer2-Background-Pool: Thread-150] resource.ResourceUtils: Unable to find 'resource-types.xml'. 2025-10-14 16:52:12 2025-10-14T20:52:12,236 INFO [HiveServer2-Background-Pool: Thread-150] common.TezYARNUtils: Ignoring 'tez.lib.uris' since 'tez.ignore.lib.uris' is set to true 2025-10-14 16:52:12 2025-10-14T20:52:12,243 INFO [HiveServer2-Background-Pool: Thread-150] Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled 2025-10-14 16:52:12 2025-10-14T20:52:12,323 INFO [HiveServer2-Background-Pool: Thread-150] client.LocalClient: DAGAppMaster thread has been created 2025-10-14 16:52:12 2025-10-14T20:52:12,323 INFO [HiveServer2-Background-Pool: Thread-150] client.LocalClient: DAGAppMaster is not created wait for 100ms... 2025-10-14 16:52:12 2025-10-14T20:52:12,323 INFO [DAGAppMaster Thread] client.LocalClient: Using working directory: /tmp/hive/hive/_tez_session_dir/c8064268-e392-40ee-a062-008ade6a3b8e/.tez/application_1760475132141_0001_wd 2025-10-14 16:52:12 2025-10-14T20:52:12,364 WARN [Finalizer] io.ResolvingFileIO: Unclosed ResolvingFileIO instance created by: 2025-10-14 16:52:12 org.apache.iceberg.io.ResolvingFileIO.<init>(ResolvingFileIO.java:84) 2025-10-14 16:52:12 java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(Unknown Source) 2025-10-14 16:52:12 java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source) 2025-10-14 16:52:12 java.base/java.lang.reflect.Constructor.newInstance(Unknown Source) 2025-10-14 16:52:12 org.apache.iceberg.common.DynConstructors$Ctor.newInstanceChecked(DynConstructors.java:51) 2025-10-14 16:52:12 org.apache.iceberg.common.DynConstructors$Ctor.newInstance(DynConstructors.java:64) 2025-10-14 16:52:12 org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:391) 2025-10-14 16:52:12 org.apache.iceberg.rest.RESTSessionCatalog.newFileIO(RESTSessionCatalog.java:983) 2025-10-14 16:52:12 org.apache.iceberg.rest.RESTSessionCatalog.newFileIO(RESTSessionCatalog.java:974) 2025-10-14 16:52:12 org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:233) 2025-10-14 16:52:12 org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:82) 2025-10-14 16:52:12 org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:277) 2025-10-14 16:52:12 org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:331) 2025-10-14 16:52:12 org.apache.iceberg.mr.Catalogs.loadCatalog(Catalogs.java:237) 2025-10-14 16:52:12 org.apache.iceberg.mr.Catalogs.loadTable(Catalogs.java:110) 2025-10-14 16:52:12 org.apache.iceberg.mr.Catalogs.loadTable(Catalogs.java:104) 2025-10-14 16:52:12 org.apache.iceberg.mr.hive.IcebergTableUtil.lambda$getTable$1(IcebergTableUtil.java:178) 2025-10-14 16:52:12 org.apache.iceberg.mr.hive.IcebergTableUtil.getTable(IcebergTableUtil.java:184) 2025-10-14 16:52:12 org.apache.iceberg.mr.hive.BaseHiveIcebergMetaHook.preCreateTable(BaseHiveIcebergMetaHook.java:140) 2025-10-14 16:52:12 org.apache.hadoop.hive.metastore.client.HookEnabledMetaStoreClient.createTable(HookEnabledMetaStoreClient.java:113) 2025-10-14 16:52:12 java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(Unknown Source) 2025-10-14 16:52:12 java.base/java.lang.reflect.Method.invoke(Unknown Source) 2025-10-14 16:52:12 org.apache.hadoop.hive.metastore.client.SynchronizedMetaStoreClient$SynchronizedHandler.invoke(SynchronizedMetaStoreClient.java:69) 2025-10-14 16:52:12 jdk.proxy2/jdk.proxy2.$Proxy24.createTable(Unknown Source) 2025-10-14 16:52:12 org.apache.hadoop.hive.metastore.client.MetaStoreClientWrapper.createTable(MetaStoreClientWrapper.java:445) 2025-10-14 16:52:12 java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(Unknown Source) 2025-10-14 16:52:12 java.base/java.lang.reflect.Method.invoke(Unknown Source) 2025-10-14 16:52:12 org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:232) 2025-10-14 16:52:12 jdk.proxy2/jdk.proxy2.$Proxy24.createTable(Unknown Source) 2025-10-14 16:52:12 org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1419) 2025-10-14 16:52:12 org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1431) 2025-10-14 16:52:12 org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:158) 2025-10-14 16:52:12 org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:116) 2025-10-14 16:52:12 org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) 2025-10-14 16:52:12 org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214) 2025-10-14 16:52:12 org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) 2025-10-14 16:52:12 org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:354) 2025-10-14 16:52:12 org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:327) 2025-10-14 16:52:12 org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:244) 2025-10-14 16:52:12 org.apache.hadoop.hive.ql.Executor.execute(Executor.java:105) 2025-10-14 16:52:12 org.apache.hadoop.hive.ql.Driver.execute(Driver.java:345) 2025-10-14 16:52:12 org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:189) 2025-10-14 16:52:12 org.apache.hadoop.hive.ql.Driver.run(Driver.java:142) 2025-10-14 16:52:12 org.apache.hadoop.hive.ql.Driver.run(Driver.java:137) 2025-10-14 16:52:12 org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:190) 2025-10-14 16:52:12 org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:234) 2025-10-14 16:52:12 org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:334) 2025-10-14 16:52:12 java.base/java.security.AccessController.doPrivileged(Unknown Source) 2025-10-14 16:52:12 java.base/javax.security.auth.Subject.doAs(Unknown Source) 2025-10-14 16:52:12 org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953) 2025-10-14 16:52:12 org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:354) 2025-10-14 16:52:12 java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) 2025-10-14 16:52:12 java.base/java.util.concurrent.FutureTask.run(Unknown Source) 2025-10-14 16:52:12 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) 2025-10-14 16:52:12 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) 2025-10-14 16:52:12 java.base/java.lang.Thread.run(Unknown Source) {code}", "comments": "", "created": "2025-10-15T00:47:19.000+0000", "updated": "2025-10-29T14:46:25.000+0000", "derived": {"summary_task": "Summarize this issue: `Catalogs.loadTable()` loads an Iceberg catalog and then loads a table from that catalog by calling the`loadTable()` method on the loaded catalog. It doesn't close the catalog before existing the function which causes resource leaks with exceptions like following in the hive.log: {code:java} 2025-10-14 16:52:12 2025-10-14T20:52:12,131 INFO [HiveServer2-Background-Pool: Thread-150] tez.TezSessionSt", "classification_task": "Classify the issue priority and type: `Catalogs.loadTable()` loads an Iceberg catalog and then loads a table from that catalog by calling the`loadTable()` method on the loaded catalog. It doesn't close the catalog before existing the function which causes resource leaks with exceptions like following in the hive.log: {code:java} 2025-10-14 16:52:12 2025-10-14T20:52:12,131 INFO [HiveServer2-Background-Pool: Thread-150] tez.TezSessionSt", "qna_task": "Question: What is this issue about?\nAnswer: Iceberg: Close catalog in Catalogs.loadTable() to fix resource leak from open ResolvingFileIO"}}
{"id": "13631545", "key": "HIVE-29267", "project": "HIVE", "summary": "Fix NPE on Grouping Sets Optimizer for UNION ALL Queries", "description": "Steps to reproduce create table grp_set_test (key string, value string, col0 int, col1 int, col2 int, col3 int); set hive.optimize.grouping.set.threshold=1; with ssr as (select col2 as sales from grp_set_test) select channel , sum(sales) as sales from (select 'store channel' as channel, sales from ssr union all select 'catalog channel' as channel, sales from ssr ) x group by channel with rollup; *Actual Error:* Caused by: java.lang.NullPointerException: Cannot invoke \"java.util.Map.containsKey(Object)\" because the return value of \"org.apache.hadoop.hive.ql.exec.Operator.getColumnExprMap()\" is null at org.apache.hadoop.hive.ql.optimizer.GroupingSetOptimizer$GroupingSetProcessor.selectPartitionColumn(GroupingSetOptimizer.java:230) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4] at org.apache.hadoop.hive.ql.optimizer.GroupingSetOptimizer$GroupingSetProcessor.process(GroupingSetOptimizer.java:100) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4] at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4] at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4] at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4] at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:158) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4] at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:120) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4] at org.apache.hadoop.hive.ql.optimizer.GroupingSetOptimizer.transform(GroupingSetOptimizer.java:374) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4] at org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsDependentOptimizations(TezCompiler.java:497) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4] at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:227) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4] at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:182) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4] at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.compilePlan(SemanticAnalyzer.java:13150) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4] at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:13378) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4] at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:481) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4] at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:332) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4] at org.apache.hadoop.hive.ql.Compiler.analyze(Compiler.java:224) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4] at org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:109) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4] at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:508) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4] at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:460) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4] at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:424) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4] at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:418) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4] at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:121) ~[hive-exec-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4] at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:207) ~[hive-service-4.0.1.3.4.1.0-4.jar:4.0.1.3.4.1.0-4] ... 26 more", "comments": "Merged. [https://github.com/apache/hive/pull/6128] [~Indhumathi27] Thanks for your contribution, and [~ngsg] thanks for your review!", "created": "2025-10-14T17:58:20.000+0000", "updated": "2025-10-17T06:08:59.000+0000", "derived": {"summary_task": "Summarize this issue: Steps to reproduce create table grp_set_test (key string, value string, col0 int, col1 int, col2 int, col3 int); set hive.optimize.grouping.set.threshold=1; with ssr as (select col2 as sales from grp_set_test) select channel , sum(sales) as sales from (select 'store channel' as channel, sales from ssr union all select 'catalog channel' as channel, sales from ssr ) x group by channel with rollup; *", "classification_task": "Classify the issue priority and type: Steps to reproduce create table grp_set_test (key string, value string, col0 int, col1 int, col2 int, col3 int); set hive.optimize.grouping.set.threshold=1; with ssr as (select col2 as sales from grp_set_test) select channel , sum(sales) as sales from (select 'store channel' as channel, sales from ssr union all select 'catalog channel' as channel, sales from ssr ) x group by channel with rollup; *", "qna_task": "Question: What is this issue about?\nAnswer: Fix NPE on Grouping Sets Optimizer for UNION ALL Queries"}}
{"id": "13631485", "key": "HIVE-29266", "project": "HIVE", "summary": "Add hive authorization support for catalog", "description": "We need to add relevant authorization capabilities to the catalog. Authorization features exist on both the HiveServer2 and HMS sides, referring to HIVE-25214 and HIVE-26245. This ticket documents all authorization-related issues.", "comments": "", "created": "2025-10-14T09:03:40.000+0000", "updated": "2025-10-14T09:05:35.000+0000", "derived": {"summary_task": "Summarize this issue: We need to add relevant authorization capabilities to the catalog. Authorization features exist on both the HiveServer2 and HMS sides, referring to HIVE-25214 and HIVE-26245. This ticket documents all authorization-related issues.", "classification_task": "Classify the issue priority and type: We need to add relevant authorization capabilities to the catalog. Authorization features exist on both the HiveServer2 and HMS sides, referring to HIVE-25214 and HIVE-26245. This ticket documents all authorization-related issues.", "qna_task": "Question: What is this issue about?\nAnswer: Add hive authorization support for catalog"}}
{"id": "13631484", "key": "HIVE-29265", "project": "HIVE", "summary": "UnsupportedDoubleException could leave the stale column marker in COLUMN_STATS_ACCURATE", "description": "Take the schema_evol_orc_nonvec_part.q as an example, {code:java} CREATE TABLE part_change_lower_to_higher_numeric_group_decimal_to_float_n7(insert_num int, c1 decimal(38,18), c2 decimal(38,18), c3 float, b STRING) PARTITIONED BY(part INT); insert into table part_change_lower_to_higher_numeric_group_decimal_to_float_n7 partition(part=1) SELECT insert_num, decimal1, decimal1, float1, 'original' FROM schema_evolution_data_n25; {code} for column c3, the above query will throw UnsupportedDoubleException on gathering the column stats, as a result this column stats is ignored, we couldn't find the stats entry in part_col_stats. While in partition_params, the column stats c3 is marked as true: \\{\"BASIC_STATS\":\"true\",\"COLUMN_STATS\":{\"b\":\"true\",\"c1\":\"true\",\"c2\":\"true\",\"c3\":\"true\",\"insert_num\":\"true\"}} If a valid insert happens afterwards, the new column stats for c3 will take over, this would make the c3 stats incorrect.", "comments": "[~dengzh] please set the affected version", "created": "2025-10-14T09:02:45.000+0000", "updated": "2025-10-28T03:09:12.000+0000", "derived": {"summary_task": "Summarize this issue: Take the schema_evol_orc_nonvec_part.q as an example, {code:java} CREATE TABLE part_change_lower_to_higher_numeric_group_decimal_to_float_n7(insert_num int, c1 decimal(38,18), c2 decimal(38,18), c3 float, b STRING) PARTITIONED BY(part INT); insert into table part_change_lower_to_higher_numeric_group_decimal_to_float_n7 partition(part=1) SELECT insert_num, decimal1, decimal1, float1, 'original' FRO", "classification_task": "Classify the issue priority and type: Take the schema_evol_orc_nonvec_part.q as an example, {code:java} CREATE TABLE part_change_lower_to_higher_numeric_group_decimal_to_float_n7(insert_num int, c1 decimal(38,18), c2 decimal(38,18), c3 float, b STRING) PARTITIONED BY(part INT); insert into table part_change_lower_to_higher_numeric_group_decimal_to_float_n7 partition(part=1) SELECT insert_num, decimal1, decimal1, float1, 'original' FRO", "qna_task": "Question: What is this issue about?\nAnswer: UnsupportedDoubleException could leave the stale column marker in COLUMN_STATS_ACCURATE"}}
{"id": "13631429", "key": "HIVE-29264", "project": "HIVE", "summary": "Use metaconf prefix to honor metastore configurations for direct sql in q-files", "description": "In *alter_table_column_stats.q,* the _hive.metastore.try.direct.sql=false_ for partitioned table and following command is running: {code:java} analyze table testpart0 compute statistics for columns; {code} but the JDO implementation for _get_aggr_stats_for_ is to throw MetaException but the exception is not thrown. As soon I replace it with set metaconf:metastore.try.direct.sql=false then exception is observed in {code:java} itests/qtest/target/surefire-reports/org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver-output.txt {code} but q file is not failing. In short, # replace hivename with varname for metaconfs # investigate whether error be thrown if JDO fails. Attaching TestMiniLlapLocalCliDriver-output.txt showing {code:java} metastore.RetryingHMSHandler: MetaException(message:Jdo path is not implemented for stats aggr.) {code} *Stacktrace:* {code:java} 2025-10-10T00:04:54,397 ERROR [dc1b1868-d07a-4dcc-9cbd-6a0b7eca7e46 main] metastore.RetryingHMSHandler: MetaException(message:Jdo path is not implemented for stats aggr.) at org.apache.hadoop.hive.metastore.ObjectStore$28.getJdoResult(ObjectStore.java:10076) at org.apache.hadoop.hive.metastore.ObjectStore$28.getJdoResult(ObjectStore.java:10063) at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:4415) at org.apache.hadoop.hive.metastore.ObjectStore.get_aggr_stats_for(ObjectStore.java:10082) at org.apache.hadoop.hive.metastore.ObjectStore.get_aggr_stats_for(ObjectStore.java:10050) at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) at java.base/java.lang.reflect.Method.invoke(Method.java:580) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) at jdk.proxy2/jdk.proxy2.$Proxy55.get_aggr_stats_for(Unknown Source) at org.apache.hadoop.hive.metastore.HMSHandler.get_aggr_stats_for(HMSHandler.java:9409) at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) at java.base/java.lang.reflect.Method.invoke(Method.java:580) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:91) at org.apache.hadoop.hive.metastore.AbstractHMSHandlerProxy.invoke(AbstractHMSHandlerProxy.java:82) at jdk.proxy2/jdk.proxy2.$Proxy56.get_aggr_stats_for(Unknown Source) at org.apache.hadoop.hive.metastore.client.ThriftHiveMetaStoreClient.getAggrColStatsFor(ThriftHiveMetaStoreClient.java:1167) at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreClientWithLocalCache.getAggrColStatsFor(HiveMetaStoreClientWithLocalCache.java:400) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getAggrColStatsFor(SessionHiveMetaStoreClient.java:2383) at org.apache.hadoop.hive.metastore.client.MetaStoreClientWrapper.getAggrColStatsFor(MetaStoreClientWrapper.java:1032) at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) at java.base/java.lang.reflect.Method.invoke(Method.java:580) at org.apache.hadoop.hive.metastore.client.SynchronizedMetaStoreClient$SynchronizedHandler.invoke(SynchronizedMetaStoreClient.java:69) at jdk.proxy2/jdk.proxy2.$Proxy57.getAggrColStatsFor(Unknown Source) at org.apache.hadoop.hive.metastore.client.MetaStoreClientWrapper.getAggrColStatsFor(MetaStoreClientWrapper.java:1032) at org.apache.hadoop.hive.metastore.client.BaseMetaStoreClient.getAggrColStatsFor(BaseMetaStoreClient.java:769) at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) at java.base/java.lang.reflect.Method.invoke(Method.java:580) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:232) at jdk.proxy2/jdk.proxy2.$Proxy57.getAggrColStatsFor(Unknown Source) at org.apache.hadoop.hive.ql.metadata.Hive.getAggrColStatsFor(Hive.java:6244) at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:379) at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:194) at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:182) at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:177) at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89) at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:148) at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:125) at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:84) at org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsAnnotation(TezCompiler.java:472) at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:202) at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:182) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.compilePlan(SemanticAnalyzer.java:13150) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:13378) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12725) at org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.analyze(ColumnStatsSemanticAnalyzer.java:671) at org.apache.hadoop.hive.ql.Compiler.analyze(Compiler.java:224) at org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:109) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:498) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:450) at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:414) at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:408) at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126) at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:234) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259) at org.apache.hadoop.hive.cli.CliDriver.processCmd1(CliDriver.java:203) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:129) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:430) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:358) at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:760) at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:730) at org.apache.hadoop.hive.cli.control.CoreCliDriver.runTest(CoreCliDriver.java:115) at org.apache.hadoop.hive.cli.control.CliAdapter.runTest(CliAdapter.java:139) at org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver(TestMiniLlapLocalCliDriver.java:62) at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) at java.base/java.lang.reflect.Method.invoke(Method.java:580) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.apache.hadoop.hive.cli.control.CliAdapter$2$1.evaluate(CliAdapter.java:118) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.apache.hadoop.hive.cli.control.CliAdapter$1$1.evaluate(CliAdapter.java:89) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:316) at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:240) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:214) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:155) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:385) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162) at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:507) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:495) {code}", "comments": "To repro: Run the attached {_}alter_table_column_stats{_}.q with (modified line 137 with metaconf prefix) {code:java} mvn clean test -Dtest=TestMiniLlapLocalCliDriver -Dqfile=alter_table_column_stats.q -Drat.skip -Pitests -pl itests/qtest {code} and check the output of _itests/qtest/target/surefire-reports/org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver-output.txt_ Merged. [~Aggarwal_Raghav] Thanks for your contribution! [https://github.com/apache/hive/pull/6129]", "created": "2025-10-13T19:03:27.000+0000", "updated": "2025-10-19T03:08:14.000+0000", "derived": {"summary_task": "Summarize this issue: In *alter_table_column_stats.q,* the _hive.metastore.try.direct.sql=false_ for partitioned table and following command is running: {code:java} analyze table testpart0 compute statistics for columns; {code} but the JDO implementation for _get_aggr_stats_for_ is to throw MetaException but the exception is not thrown. As soon I replace it with set metaconf:metastore.try.direct.sql=false then exceptio", "classification_task": "Classify the issue priority and type: In *alter_table_column_stats.q,* the _hive.metastore.try.direct.sql=false_ for partitioned table and following command is running: {code:java} analyze table testpart0 compute statistics for columns; {code} but the JDO implementation for _get_aggr_stats_for_ is to throw MetaException but the exception is not thrown. As soon I replace it with set metaconf:metastore.try.direct.sql=false then exceptio", "qna_task": "Question: What is this issue about?\nAnswer: Use metaconf prefix to honor metastore configurations for direct sql in q-files"}}
{"id": "13631419", "key": "HIVE-29263", "project": "HIVE", "summary": "NPE in desc catalog command using CommandAuthorizerV1", "description": "Environment: Local setup without kerberos, Hive (master branch (2fa252a04748e97e6581b7685f8dcc93825eb7f9) Steps to repro: {code:java} create catalog test_new_catalog location '/tmp/test_new_catalog/'; desc catalog test_new_catalog; {code} Stacktrace: {code:java} 2025-10-13 23:26:07,359 ERROR thrift.ThriftCLIService: Failed to execute statement [request: TExecuteStatementReq(sessionHandle:TSessionHandle(sessionId:THandleIdentifier(guid:72 92 FC 4B 3F 5B 4A 7D 96 8E 33 77 72 F0 78 83, secret:47 79 DA 4B 80 D4 42 F1 80 6D 6B BE 25 E3 0D 07)), statement:desc catalog test_new_catalog, confOverlay:{}, runAsync:true, queryTimeout:0)]org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: NullPointerException Cannot invoke \"org.apache.hadoop.hive.ql.metadata.Table.isView()\" because \"tbl\" is null; Query ID: raghav_20251013232607_6ba0bd75-1c78-4ab7-8791-f125e57a7d74 at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:376) at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:212) at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:268) at org.apache.hive.service.cli.operation.Operation.run(Operation.java:286) at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:558) at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:543) at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:311) at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:650) at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1670) at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1650) at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38) at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38) at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56) at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:250) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) at java.base/java.lang.Thread.run(Thread.java:1583)Caused by: java.lang.NullPointerException: Cannot invoke \"org.apache.hadoop.hive.ql.metadata.Table.isView()\" because \"tbl\" is null at org.apache.hadoop.hive.ql.security.authorization.command.CommandAuthorizerV1.authorizeInputs(CommandAuthorizerV1.java:145) at org.apache.hadoop.hive.ql.security.authorization.command.CommandAuthorizerV1.doAuthorization(CommandAuthorizerV1.java:67) at org.apache.hadoop.hive.ql.security.authorization.command.CommandAuthorizer.doAuthorization(CommandAuthorizer.java:56) at org.apache.hadoop.hive.ql.Compiler.authorize(Compiler.java:440) at org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:122) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:498) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:450) at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:414) at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:408) at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126) at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:205) ... 15 more {code}", "comments": "In CommandAuthorizerV2 as well i.e. _org.apache.hadoop.hive.ql.security.authorization.command.CommandAuthorizerV2#addHivePrivObject()_ I don't see any CASE statement for CATALOG. We need to handle it from RANGER project as well or am I missing something? CC [~zhangbutao] [~okumin] [~dkuzmenko] When I ran _*catalog.q*_ with _*--! qt:authorizer*_ {code:java} [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 6.241 s <<< FAILURE! -- in org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver[ERROR] org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[catalog] -- Time elapsed: 1.568 s <<< FAILURE!java.lang.AssertionError: Unexpected object type at org.apache.hadoop.hive.ql.security.authorization.command.CommandAuthorizerV2.addHivePrivObject(CommandAuthorizerV2.java:252) at org.apache.hadoop.hive.ql.security.authorization.command.CommandAuthorizerV2.getHivePrivObjects(CommandAuthorizerV2.java:152) at org.apache.hadoop.hive.ql.security.authorization.command.CommandAuthorizerV2.doAuthorization(CommandAuthorizerV2.java:77) at org.apache.hadoop.hive.ql.security.authorization.command.CommandAuthorizer.doAuthorization(CommandAuthorizer.java:58) at org.apache.hadoop.hive.ql.Compiler.authorize(Compiler.java:440) at org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:122) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:498) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:450) at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:414) {code} Taking a glance at Ranger 2.7, I believe Ranger is not aware of Hive Metastore's catalog; it assumes any catalog is \"hive\". We might have a chance to update it. Yes, we need to add hive authorization support for catalog, like data connector HIVE-25214. Thanks for the reply [~okumin] [~zhangbutao] . I'll look into this. Hi [~Aggarwal_Raghav] , can you please set affected version(s)?", "created": "2025-10-13T17:57:57.000+0000", "updated": "2025-10-30T19:13:26.000+0000", "derived": {"summary_task": "Summarize this issue: Environment: Local setup without kerberos, Hive (master branch (2fa252a04748e97e6581b7685f8dcc93825eb7f9) Steps to repro: {code:java} create catalog test_new_catalog location '/tmp/test_new_catalog/'; desc catalog test_new_catalog; {code} Stacktrace: {code:java} 2025-10-13 23:26:07,359 ERROR thrift.ThriftCLIService: Failed to execute statement [request: TExecuteStatementReq(sessionHandle:TSessionH", "classification_task": "Classify the issue priority and type: Environment: Local setup without kerberos, Hive (master branch (2fa252a04748e97e6581b7685f8dcc93825eb7f9) Steps to repro: {code:java} create catalog test_new_catalog location '/tmp/test_new_catalog/'; desc catalog test_new_catalog; {code} Stacktrace: {code:java} 2025-10-13 23:26:07,359 ERROR thrift.ThriftCLIService: Failed to execute statement [request: TExecuteStatementReq(sessionHandle:TSessionH", "qna_task": "Question: What is this issue about?\nAnswer: NPE in desc catalog command using CommandAuthorizerV1"}}
{"id": "13631366", "key": "HIVE-29262", "project": "HIVE", "summary": "LEAD/LAG functions shows incorrect data when vectorization enabled", "description": "Please use this leadlag_Vectorization.q file to repro this scenario. This case happnes when table has more columns. mvn test -Dtest=TestMiniLlapLocalCliDriver -Dqfile=leadlag_Vectorization.q", "comments": "[~rkirtir] please set the affected version [~rkirtir] please provide more details - what data exactly is incorrect when running: {code:java} mvn test -Dtest=TestMiniLlapLocalCliDriver -Dqfile=leadlag_Vectorization.q {code} If you see below query SELECT *ws_bill_customer_sk,* *ws_item_sk,* ws_sold_date_sk, ws_sales_price, LEAD(ws_sales_price) OVER ( *PARTITION BY ws_item_sk, ws_bill_customer_sk* ORDER BY ws_sold_date_sk ) AS next_sales_price, LEAD(ws_sales_price) OVER ( PARTITION BY ws_bill_customer_sk, ws_item_sk ORDER BY ws_sold_date_sk ) - ws_sales_price AS sales_price_diff FROM web_sales_txt The order of columns in SELECT clause and PARTITION BY is different. In this case result gets messed up. Attached both output files [^leadlag_Vectorization_ve_false.q.out] happens in 4.0.1 also", "created": "2025-10-13T09:28:45.000+0000", "updated": "2025-10-31T17:50:23.000+0000", "derived": {"summary_task": "Summarize this issue: Please use this leadlag_Vectorization.q file to repro this scenario. This case happnes when table has more columns. mvn test -Dtest=TestMiniLlapLocalCliDriver -Dqfile=leadlag_Vectorization.q", "classification_task": "Classify the issue priority and type: Please use this leadlag_Vectorization.q file to repro this scenario. This case happnes when table has more columns. mvn test -Dtest=TestMiniLlapLocalCliDriver -Dqfile=leadlag_Vectorization.q", "qna_task": "Question: What is this issue about?\nAnswer: LEAD/LAG functions shows incorrect data when vectorization enabled"}}
{"id": "13631280", "key": "HIVE-29261", "project": "HIVE", "summary": "Clean up GitHub Actions for Docker Release", "description": "The current workflow has two jobs, `buildFromArchive` for manual triggers and `buildFromSource` for automation. Both have the most in common. [https://github.com/apache/hive/blob/master/.github/workflows/docker-GA-images.yml] Before adding new steps in HIVE-29260, we would make them converge so that we don't need to replicate new steps.", "comments": "Merged. [~simhadri-g] [~dkuzmenko] Thanks for your reviews! https://github.com/apache/hive/pull/6127", "created": "2025-10-11T01:32:22.000+0000", "updated": "2025-10-22T04:19:23.000+0000", "derived": {"summary_task": "Summarize this issue: The current workflow has two jobs, `buildFromArchive` for manual triggers and `buildFromSource` for automation. Both have the most in common. [https://github.com/apache/hive/blob/master/.github/workflows/docker-GA-images.yml] Before adding new steps in HIVE-29260, we would make them converge so that we don't need to replicate new steps.", "classification_task": "Classify the issue priority and type: The current workflow has two jobs, `buildFromArchive` for manual triggers and `buildFromSource` for automation. Both have the most in common. [https://github.com/apache/hive/blob/master/.github/workflows/docker-GA-images.yml] Before adding new steps in HIVE-29260, we would make them converge so that we don't need to replicate new steps.", "qna_task": "Question: What is this issue about?\nAnswer: Clean up GitHub Actions for Docker Release"}}
{"id": "13631279", "key": "HIVE-29260", "project": "HIVE", "summary": "Add smoke tests for Docker images", "description": "The current workflow immediately pushes images once the build phase succeeds. So, we may push images with a wrong hive-site.xml or wrong Docker setup. [https://github.com/apache/hive/blob/master/.github/workflows/docker-GA-images.yml] This ticket would add testing the minimal configurations and connectivity.", "comments": "It would be awesome to have some tests around the images. I think that we already build the images during the precommit run so ideally the tests should run as part of that run as well. In fact I believe that we could create some java test classes somewhere in itest module and have some logic there that spins up the necessary containers and runs some queries via a JDBC/ODBC connection. Possibly, we could even use some .q files as input.", "created": "2025-10-11T01:26:43.000+0000", "updated": "2025-10-22T04:19:33.000+0000", "derived": {"summary_task": "Summarize this issue: The current workflow immediately pushes images once the build phase succeeds. So, we may push images with a wrong hive-site.xml or wrong Docker setup. [https://github.com/apache/hive/blob/master/.github/workflows/docker-GA-images.yml] This ticket would add testing the minimal configurations and connectivity.", "classification_task": "Classify the issue priority and type: The current workflow immediately pushes images once the build phase succeeds. So, we may push images with a wrong hive-site.xml or wrong Docker setup. [https://github.com/apache/hive/blob/master/.github/workflows/docker-GA-images.yml] This ticket would add testing the minimal configurations and connectivity.", "qna_task": "Question: What is this issue about?\nAnswer: Add smoke tests for Docker images"}}
{"id": "13631220", "key": "HIVE-29259", "project": "HIVE", "summary": "The result format of the desc catalog is messy", "description": "At present, you can find the result format of desc catalog is messy & wrong, e.g: {code:java} 0: jdbc:hive2://127.0.0.1:10000/default> desc catalog hive; +------------------+---------------------------------------------------------+------------+ | cat_name | comment | location | +------------------+-------------------------------------------------------- +-------------+ | Catalog Name | hive | NULL | | Comment | Default catalog for Hive | NULL | | Location | hdfs://127.0.0.1:8028/user/hive/warehouse/hiveicetest | NULL | +------------------+---------------------------------------------------------+------------+{code} *It should output the result in this format:* {code:java} 0: jdbc:hive2://127.0.0.1:10000/default> desc catalog hive; +------------------+-------------------------------------------------------+-------------------------------------------------------------+ | cat_name | comment | location | +------------------+-------------------------------------------------------+-------------------------------------------------------------+ | hive | Default catalog for Hive | hdfs://127.0.0.1:8028/user/hive/warehouse/hiveicetest | +------------------+-------------------------------------------------------+-------------------------------------------------------------+{code}", "comments": "", "created": "2025-10-10T09:38:22.000+0000", "updated": "2025-10-10T09:50:51.000+0000", "derived": {"summary_task": "Summarize this issue: At present, you can find the result format of desc catalog is messy & wrong, e.g: {code:java} 0: jdbc:hive2://127.0.0.1:10000/default> desc catalog hive; +------------------+---------------------------------------------------------+------------+ | cat_name | comment | location | +------------------+-------------------------------------------------------- +-------------+ | Catalog Name | hive | NUL", "classification_task": "Classify the issue priority and type: At present, you can find the result format of desc catalog is messy & wrong, e.g: {code:java} 0: jdbc:hive2://127.0.0.1:10000/default> desc catalog hive; +------------------+---------------------------------------------------------+------------+ | cat_name | comment | location | +------------------+-------------------------------------------------------- +-------------+ | Catalog Name | hive | NUL", "qna_task": "Question: What is this issue about?\nAnswer: The result format of the desc catalog is messy"}}
{"id": "13631214", "key": "HIVE-29258", "project": "HIVE", "summary": "Remove Class.forName() for driver loading", "description": "For example in HiveSchemaHelper.java, Class.forName(driver) is explicitly used. https://github.com/apache/hive/blob/4f7140f0b324b5f2abde10acb671fd1f2ced9517/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/HiveSchemaHelper.java#L85 But with JDBC 4+ DriverManager should automatically take care of classloading.", "comments": "", "created": "2025-10-10T09:00:24.000+0000", "updated": "2025-10-10T09:07:01.000+0000", "derived": {"summary_task": "Summarize this issue: For example in HiveSchemaHelper.java, Class.forName(driver) is explicitly used. https://github.com/apache/hive/blob/4f7140f0b324b5f2abde10acb671fd1f2ced9517/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/HiveSchemaHelper.java#L85 But with JDBC 4+ DriverManager should automatically take care of classloading.", "classification_task": "Classify the issue priority and type: For example in HiveSchemaHelper.java, Class.forName(driver) is explicitly used. https://github.com/apache/hive/blob/4f7140f0b324b5f2abde10acb671fd1f2ced9517/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/HiveSchemaHelper.java#L85 But with JDBC 4+ DriverManager should automatically take care of classloading.", "qna_task": "Question: What is this issue about?\nAnswer: Remove Class.forName() for driver loading"}}
{"id": "13631133", "key": "HIVE-29257", "project": "HIVE", "summary": "Remove OPTIMIZED SQL entry from EXPLAIN EXTENDED", "description": "The {{EXPLAIN EXTENDED}} statement among other things prints a serialization of the CBO plan in the form of SQL (so called OPTIMIZED SQL). This feature was introduced in HIVE-19360 for diagnosability purposes to assist in the understanding of the DAG plan with respect to join ordering. However, in most cases when developers want to understand the join order they look into the CBO plan and not in the OPTIMIZED SQL output. Determining the join order from SQL becomes challenging for queries that contain more than a few tables and nested SQL constructs make the situation worse. The CBO RelNode tree is usually easier to navigate. In various cases the SQL serialization is an invalid SQL query. This led people to believe that the CBO plan is wrong. As a consequence, they attempt to fix the CBO plan while in fact the plan is perfectly valid. The bug usually lies only in the serialization part but the latter does not have any effect on the execution of the query. Users (and sometimes developers) believe that the OPTIMIZED SQL is the query that is actually run by Hive. There have been various cases where people copy-paste the OPTIMIZED SQL entry and try to run it when they have issues with the original query and when that fails as well it adds up to the confusion. In addition, since the serializer is buggy it sometimes raises exceptions and the OPTIMIZED SQL does not appear in the output. People will raise bug fixes and invest effort in a feature that is not very useful. Currently, there are 282 .q.out files that use {{EXPLAIN EXTENDED}} and thus contain an entry of OPTIMIZED SQL. Even minor changes in the CBO plan do affect the OPTIMIZED SQL necessitating updates and reviews on multiple files. Typically calcite upgrades trigger many changes in OPTIMIZED SQL. {noformat} $ find . -name \"*.q.out\" -exec grep -a \"OPTIMIZED SQL\" {} \\; | wc -l 820 $ find . -name \"*.q.out\" -exec grep -l \"OPTIMIZED SQL\" {} \\; | wc -l 282 {noformat} I propose to remove the {{OPTIMIZED SQL}} feature and related code to reduce: * maintenance overhead * plan/file (e.g., .q.out) size * users/dev confusion about its meaning/usage", "comments": "I agree with the reasoning, printing an invalid SQL query leads to confusion. Do you think it makes sense to replace OPTIMIZED SQL by something else, e.g., OPTIMIZED PLAN, printing the CBO plan? Not sure whether that's helpful, as the CBO plan can already be obtained by EXPLAIN CBO. [~thomas.rebele] Since we already have options to display the CBO plan (as text and json) via other EXPLAIN commands, I would refrain from adding more redundancy in EXPLAIN EXTENDED. By doing this we also avoid too much coupling between \"logical\" and \"physical\" plans. +1 for removing OPTIMIZED SQL entry, especially because {quote}In various cases the SQL serialization is an invalid SQL query{quote} and checking the join order is easier by the CBO plan.", "created": "2025-10-09T12:12:12.000+0000", "updated": "2025-10-13T12:26:33.000+0000", "derived": {"summary_task": "Summarize this issue: The {{EXPLAIN EXTENDED}} statement among other things prints a serialization of the CBO plan in the form of SQL (so called OPTIMIZED SQL). This feature was introduced in HIVE-19360 for diagnosability purposes to assist in the understanding of the DAG plan with respect to join ordering. However, in most cases when developers want to understand the join order they look into the CBO plan and not in t", "classification_task": "Classify the issue priority and type: The {{EXPLAIN EXTENDED}} statement among other things prints a serialization of the CBO plan in the form of SQL (so called OPTIMIZED SQL). This feature was introduced in HIVE-19360 for diagnosability purposes to assist in the understanding of the DAG plan with respect to join ordering. However, in most cases when developers want to understand the join order they look into the CBO plan and not in t", "qna_task": "Question: What is this issue about?\nAnswer: Remove OPTIMIZED SQL entry from EXPLAIN EXTENDED"}}
{"id": "13631112", "key": "HIVE-29256", "project": "HIVE", "summary": "[DOC] Discard invalid installation documentation", "description": "Currently, the installation documentation for HIVE is very disorganized, with multiple installation documents for HIVE on the homepage. I suggest we should use [Apache Hive : Manual Installation|https://hive.apache.org/docs/latest/admin/manual-installation/] as the standard and discard other invalid installation documents. Because this document has been rewritten since the release of HIVE version 4.0, we should follow it as the standard compared to other outdated and invalid installation documents. Or, at the very least, it should be placed in a prominent location within the documentation, rather than being buried in outdated and invalid documents.", "comments": "[~zhangbutao] [~zabetak] hello,sir,what do you think? Although we've made some optimizations to the web documentation, the overall structure still looks quite troublesome. I agree with your opinion \u2014 we should remove some invalid&outdated installation documents, such as [https://hive.apache.org/docs/latest/admin/adminmanual-installation/]. [~lisoda] If you have ideas on how to improve the documentation feel free to raise a pull request.", "created": "2025-10-09T09:39:36.000+0000", "updated": "2025-10-09T10:44:58.000+0000", "derived": {"summary_task": "Summarize this issue: Currently, the installation documentation for HIVE is very disorganized, with multiple installation documents for HIVE on the homepage. I suggest we should use [Apache Hive : Manual Installation|https://hive.apache.org/docs/latest/admin/manual-installation/] as the standard and discard other invalid installation documents. Because this document has been rewritten since the release of HIVE version ", "classification_task": "Classify the issue priority and type: Currently, the installation documentation for HIVE is very disorganized, with multiple installation documents for HIVE on the homepage. I suggest we should use [Apache Hive : Manual Installation|https://hive.apache.org/docs/latest/admin/manual-installation/] as the standard and discard other invalid installation documents. Because this document has been rewritten since the release of HIVE version ", "qna_task": "Question: What is this issue about?\nAnswer: [DOC] Discard invalid installation documentation"}}
{"id": "13631009", "key": "HIVE-29255", "project": "HIVE", "summary": "Document effects of JDK/tzdata on operations involving DATETIME and TIMESTAMP data types", "description": "Various functions and operations involving DATETIME and TIMESTAMP data types rely on JDK APIs in {{java.time}} package. The results of these APIs are strongly affected by changes in the [Time Zone Database|https://www.iana.org/time-zones](tzdata). Different JDK providers and versions package different tzdata versions thus changing the JDK and/or Timezone provider can lead to different results in SQL queries and elsewhere. Although, JDK/tzdata changes are outside the control of Hive we should document this potential behavior change somewhere so that users are aware.", "comments": "", "created": "2025-10-08T14:17:31.000+0000", "updated": "2025-10-08T14:18:11.000+0000", "derived": {"summary_task": "Summarize this issue: Various functions and operations involving DATETIME and TIMESTAMP data types rely on JDK APIs in {{java.time}} package. The results of these APIs are strongly affected by changes in the [Time Zone Database|https://www.iana.org/time-zones](tzdata). Different JDK providers and versions package different tzdata versions thus changing the JDK and/or Timezone provider can lead to different results in S", "classification_task": "Classify the issue priority and type: Various functions and operations involving DATETIME and TIMESTAMP data types rely on JDK APIs in {{java.time}} package. The results of these APIs are strongly affected by changes in the [Time Zone Database|https://www.iana.org/time-zones](tzdata). Different JDK providers and versions package different tzdata versions thus changing the JDK and/or Timezone provider can lead to different results in S", "qna_task": "Question: What is this issue about?\nAnswer: Document effects of JDK/tzdata on operations involving DATETIME and TIMESTAMP data types"}}
{"id": "13631007", "key": "HIVE-29254", "project": "HIVE", "summary": "Display TxnId associated with the query in show processlist command", "description": "Today we don't have a way to identify the query id along with the associated txn id. Show transactions provide {noformat} +-----------------+--------------------+----------------+----------------------+----------------------------------------------------+--------------------------------------------+ | txnid | state | startedtime | lastheartbeattime | user | host | +-----------------+--------------------+----------------+----------------------+----------------------------------------------------+--------------------------------------------+ | Transaction ID | Transaction State | Started Time | Last Heartbeat Time | User | Hostname | | 37 | OPEN | 1759478145278 | 1759478145278 | hive/ip-10-17-78-194.support.fuse.cloudera.com@SUPPORT.FUSE.CLOUDERA.COM | ip-10-17-78-194.support.fuse.cloudera.com | +-----------------+--------------------+----------------+----------------------+----------------------------------------------------+--------------------------------------------+{noformat} Here we don't know the query id and not sure if we retrieve it and display in the above command Show processlist gives below data {noformat} 0: jdbc:hive2://localhost:10000> show processlist; +------------+------------+-------------------+---------------------------------------+--------------------------+------------------------+----------------------------------------------------+-----------+-------------------+-------------------+---------------+-----------------+ | User Name | Ip Addr | Execution Engine | Session Id | Session Active Time (s) | Session Idle Time (s) | Query ID | State | Opened Timestamp | Elapsed Time (s) | Runtime (s) | Query | +------------+------------+-------------------+---------------------------------------+--------------------------+------------------------+----------------------------------------------------+-----------+-------------------+-------------------+---------------+-----------------+ | hive | 127.0.0.1 | mr | 6cdf027f-90e1-48af-b4d8-a11aac71ca1b | 154 | 19 | rtrivedi_20240624193743_fb4b2bf8-a02a-4b76-a92c-4e3ee4bb6e9e | FINISHED | 1719275863493 | 102 | 83 | show tables | | hive | 127.0.0.1 | mr | 43945f54-d65c-424d-b523-08e7675d8223 | 165 | 67 | rtrivedi_20240624193826_42bff3ed-fb8d-4478-9500-fc6ff2173041 | RUNNING | 1719275906721 | 59 | Not finished | show databases | +------------+------------+-------------------+---------------------------------------+--------------------------+------------------------+----------------------------------------------------+-----------+-------------------+-------------------+---------------+-----------------+ 2 rows selected (4.149 seconds){noformat} We can try to display transaction id in this command, so that we can map transaction details from the show transactions output Customer use case: Some transactions are hanging for long, but they don't know the query corresponding to that txn id before aborting that manually", "comments": "Having a way to correlate a query with a transaction is a good idea.", "created": "2025-10-08T14:11:41.000+0000", "updated": "2025-10-28T11:52:11.000+0000", "derived": {"summary_task": "Summarize this issue: Today we don't have a way to identify the query id along with the associated txn id. Show transactions provide {noformat} +-----------------+--------------------+----------------+----------------------+----------------------------------------------------+--------------------------------------------+ | txnid | state | startedtime | lastheartbeattime | user | host | +-----------------+--------------", "classification_task": "Classify the issue priority and type: Today we don't have a way to identify the query id along with the associated txn id. Show transactions provide {noformat} +-----------------+--------------------+----------------+----------------------+----------------------------------------------------+--------------------------------------------+ | txnid | state | startedtime | lastheartbeattime | user | host | +-----------------+--------------", "qna_task": "Question: What is this issue about?\nAnswer: Display TxnId associated with the query in show processlist command"}}
{"id": "13630955", "key": "HIVE-29253", "project": "HIVE", "summary": "Bump netty version to 4.1.127.Final due to CVE-2025-58057, CVE-2025-58056", "description": "", "comments": "Merged to master Thanks for the upgrade [~ramitg254] and [~Aggarwal_Raghav] , [~InvisibleProgrammer] for the review!", "created": "2025-10-08T06:26:23.000+0000", "updated": "2025-10-16T10:19:23.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Bump netty version to 4.1.127.Final due to CVE-2025-58057, CVE-2025-58056 "}}
{"id": "13630926", "key": "HIVE-29252", "project": "HIVE", "summary": "Iceberg: Add support for Column Defaults with Alter commands", "description": "Allow column defaults with Alter Table Commands", "comments": "Committed to master. Thanx [~difin] for the review!!!", "created": "2025-10-07T22:36:44.000+0000", "updated": "2025-10-18T14:05:59.000+0000", "derived": {"summary_task": "Summarize this issue: Allow column defaults with Alter Table Commands", "classification_task": "Classify the issue priority and type: Allow column defaults with Alter Table Commands", "qna_task": "Question: What is this issue about?\nAnswer: Iceberg: Add support for Column Defaults with Alter commands"}}
{"id": "13630899", "key": "HIVE-29251", "project": "HIVE", "summary": "Hive ACID: HiveConf object shouldn't be shared between multiple cleanup task threads", "description": "The {{HiveConf}} object is shared across multiple cleanup tasks. When the Cleaner runs with multiple threads, this can lead to a race condition. At the end of a cleanup task, the open transaction cap is lifted to check for any deltas that may become eligible for cleanup once the open transaction completes. If none are found, the cleanup request is marked complete; otherwise, it\u2019s returned to the queue with a \u201cready-for-cleaning\u201d status. {code:java} conf.set(ValidTxnList.VALID_TXNS_KEY, new ValidReadTxnList().toString()); dir = AcidUtils.getAcidState(fs, path, conf, new ValidReaderWriteIdList(info.getFullTableName(), new long[0], new BitSet(), info.highestWriteId, Long.MAX_VALUE), Ref.from(false), false, dirSnapshots); List<Path> remained = subtract(CompactorUtil.getObsoleteDirs(dir, isDynPartAbort), deleted); if (!remained.isEmpty()) { LOG.warn(\"Remained {} obsolete directories from {}. {}\", remained.size(), location, CompactorUtil.getDebugInfo(remained)); } else { LOG.debug(\"All cleared below the watermark: {} from {}\", info.highestWriteId, location); success = true; }{code} that might override the `{{{}ValidTxnList.VALID_TXNS_KEY`{}}} set in a concurrent task, leading to premature cleanup of data still in use by an active transaction. {code:java} private void cleanUsingAcidDir(CompactionInfo ci, String location, long minOpenTxn) throws Exception { ValidTxnList validTxnList = TxnUtils.createValidTxnListForCleaner(txnHandler.getOpenTxns(), minOpenTxn, false); //save it so that getAcidState() sees it conf.set(ValidTxnList.VALID_TXNS_KEY, validTxnList.writeToString());{code}", "comments": "Merged to master. Thank you v much for the review, [~sbadhya] !", "created": "2025-10-07T16:24:18.000+0000", "updated": "2025-10-13T08:29:02.000+0000", "derived": {"summary_task": "Summarize this issue: The {{HiveConf}} object is shared across multiple cleanup tasks. When the Cleaner runs with multiple threads, this can lead to a race condition. At the end of a cleanup task, the open transaction cap is lifted to check for any deltas that may become eligible for cleanup once the open transaction completes. If none are found, the cleanup request is marked complete; otherwise, it\u2019s returned to the q", "classification_task": "Classify the issue priority and type: The {{HiveConf}} object is shared across multiple cleanup tasks. When the Cleaner runs with multiple threads, this can lead to a race condition. At the end of a cleanup task, the open transaction cap is lifted to check for any deltas that may become eligible for cleanup once the open transaction completes. If none are found, the cleanup request is marked complete; otherwise, it\u2019s returned to the q", "qna_task": "Question: What is this issue about?\nAnswer: Hive ACID: HiveConf object shouldn't be shared between multiple cleanup task threads"}}
{"id": "13630871", "key": "HIVE-29250", "project": "HIVE", "summary": "Display effects of PlanModifierForASTConv in EXPLAIN CBO output", "description": "The EXPLAIN CBO clause outputs the plan generated by the cost-based (Calcite) optimizer. Although it is not explicitly defined which state of the CBO plan should be displayed to the end-users someone would expect that it is the final plan after applying all kind of transformations that are relevant and can affect the shape of the physical plan. However, at the moment the transformations that happen inside PlanModifierForASTConv are not *always* reflected to the plan. These transformations are important cause they can affect the physical plan both in terms of correctness and performance. Some transformations from PlanModifierForASTConv are already visible mostly as a side-effect of modifying directly the object reference that is passed as input.", "comments": "", "created": "2025-10-07T11:43:28.000+0000", "updated": "2025-10-13T12:14:35.000+0000", "derived": {"summary_task": "Summarize this issue: The EXPLAIN CBO clause outputs the plan generated by the cost-based (Calcite) optimizer. Although it is not explicitly defined which state of the CBO plan should be displayed to the end-users someone would expect that it is the final plan after applying all kind of transformations that are relevant and can affect the shape of the physical plan. However, at the moment the transformations that happe", "classification_task": "Classify the issue priority and type: The EXPLAIN CBO clause outputs the plan generated by the cost-based (Calcite) optimizer. Although it is not explicitly defined which state of the CBO plan should be displayed to the end-users someone would expect that it is the final plan after applying all kind of transformations that are relevant and can affect the shape of the physical plan. However, at the moment the transformations that happe", "qna_task": "Question: What is this issue about?\nAnswer: Display effects of PlanModifierForASTConv in EXPLAIN CBO output"}}
{"id": "13630852", "key": "HIVE-29249", "project": "HIVE", "summary": "RuntimeException in PlanModifierForASTConv.introduceDerivedTable for queries with self joins", "description": "Various queries containing more than 2 joins of the same relation (table, materialized view, CTE) fail at compile time. {code:sql} create table t1 (key int, value int); create table t2 (a string, b string); explain cbo with cte as (select key, value, BLOCK__OFFSET__INSIDE__FILE, INPUT__FILE__NAME, ROW__ID, ROW__IS__DELETED from t1) select * from cte a join t2 b join cte c {code} The query fails with the following stacktrace: {noformat} java.lang.RuntimeException: Couldn't find child node in parent's inputs at org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForASTConv.introduceDerivedTable(PlanModifierForASTConv.java:341) at org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForASTConv$SelfJoinHandler.visit(PlanModifierForASTConv.java:236) at org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveJoin.accept(HiveJoin.java:229) at org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelShuttleImpl.visitChild(HiveRelShuttleImpl.java:60) at org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelShuttleImpl.visit(HiveRelShuttleImpl.java:114) at org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForASTConv$SelfJoinHandler.visit(PlanModifierForASTConv.java:242) at org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveProject.accept(HiveProject.java:134) at org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForASTConv.convertOpTree(PlanModifierForASTConv.java:109) at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.convert(ASTConverter.java:136) at org.apache.hadoop.hive.ql.parse.CalcitePlanner.getOptimizedAST(CalcitePlanner.java:1405) at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:609) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:13218) at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:482) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:359) at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:187) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:359) at org.apache.hadoop.hive.ql.Compiler.analyze(Compiler.java:224) at org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:109) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:498) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:450) at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:414) at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:408) at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126) at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:234) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259) at org.apache.hadoop.hive.cli.CliDriver.processCmd1(CliDriver.java:203) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:129) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:430) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:358) at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:760) at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:730) at org.apache.hadoop.hive.cli.control.CoreCliDriver.runTest(CoreCliDriver.java:115) at org.apache.hadoop.hive.cli.control.CliAdapter.runTest(CliAdapter.java:139) at org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver(TestMiniLlapLocalCliDriver.java:62) {noformat} The repro above requires all technical columns (e.g., {{BLOCK__OFFSET__INSIDE__FILE}}) to be present in the SELECT clause which is rather rare in real SQL queries. However, the problem can still appear in other use-cases/scenarios when using the {{hive.optimize.cte}} features and/or materialized views and the same view or CTE is used multiple times in the query.", "comments": "This is caused by HIVE-28222 or rather a follow-up of the latter. Problem is reproducible using TestMiniLlapLocalCliDriver on current master branch (commit d46d900cba2b484f033772ec42e62e8f96e0c07a). Hi [~zabetak] , can you please set the affected version(s)?", "created": "2025-10-07T08:35:03.000+0000", "updated": "2025-10-30T19:20:18.000+0000", "derived": {"summary_task": "Summarize this issue: Various queries containing more than 2 joins of the same relation (table, materialized view, CTE) fail at compile time. {code:sql} create table t1 (key int, value int); create table t2 (a string, b string); explain cbo with cte as (select key, value, BLOCK__OFFSET__INSIDE__FILE, INPUT__FILE__NAME, ROW__ID, ROW__IS__DELETED from t1) select * from cte a join t2 b join cte c {code} The query fails wi", "classification_task": "Classify the issue priority and type: Various queries containing more than 2 joins of the same relation (table, materialized view, CTE) fail at compile time. {code:sql} create table t1 (key int, value int); create table t2 (a string, b string); explain cbo with cte as (select key, value, BLOCK__OFFSET__INSIDE__FILE, INPUT__FILE__NAME, ROW__ID, ROW__IS__DELETED from t1) select * from cte a join t2 b join cte c {code} The query fails wi", "qna_task": "Question: What is this issue about?\nAnswer: RuntimeException in PlanModifierForASTConv.introduceDerivedTable for queries with self joins"}}
{"id": "13630835", "key": "HIVE-29248", "project": "HIVE", "summary": "HMS Iceberg REST API should return 403 on HiveAccessControlException", "description": "The current implementation does not handle permission errors and returns a 500 error. This is the exception when I integrated HMS Iceberg REST Catalog with Apache Ranger. {code:java} 2025-10-07T02:26:57,248 ERROR [qtp100805003-49] rest.HMSCatalogServlet: Error processing REST request org.apache.iceberg.exceptions.RESTException: Unhandled error: ErrorResponse(code=500, type=RuntimeException, message=Failed to list namespace under namespace: default in Hive Metastore) java.lang.RuntimeException: Failed to list namespace under namespace: default in Hive Metastore at org.apache.iceberg.hive.HiveCatalog.loadNamespaceMetadata(HiveCatalog.java:632) at org.apache.iceberg.catalog.SupportsNamespaces.namespaceExists(SupportsNamespaces.java:159) at org.apache.iceberg.rest.CatalogHandlers.namespaceExists(CatalogHandlers.java:167) at org.apache.iceberg.rest.HMSCatalogAdapter.namespaceExists(HMSCatalogAdapter.java:249) at org.apache.iceberg.rest.HMSCatalogAdapter.handleRequest(HMSCatalogAdapter.java:441) at org.apache.iceberg.rest.HMSCatalogAdapter.execute(HMSCatalogAdapter.java:524) at org.apache.iceberg.rest.HMSCatalogServlet.service(HMSCatalogServlet.java:75) at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) ... Caused by: MetaException(message:Permission denied: user [trino] does not have [USE] privilege on [default]) at org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.newMetaException(MetaStoreUtils.java:229) at org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.newMetaException(MetaStoreUtils.java:219) at org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.HiveMetaStoreAuthorizer.onEvent(HiveMetaStoreAuthorizer.java:137) at org.apache.hadoop.hive.metastore.HMSHandler.firePreEvent(HMSHandler.java:4133) at org.apache.hadoop.hive.metastore.HMSHandler.get_database_req(HMSHandler.java:1475) ... Caused by: org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAccessControlException: Permission denied: user [trino] does not have [USE] privil ege on [default] at org.apache.ranger.authorization.hive.authorizer.RangerHiveAuthorizer.checkPrivileges(RangerHiveAuthorizer.java:1155) at org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.HiveMetaStoreAuthorizer.checkPrivileges(HiveMetaStoreAuthorizer.java:701) at org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.HiveMetaStoreAuthorizer.onEvent(HiveMetaStoreAuthorizer.java:133) ... 69 more at org.apache.iceberg.rest.HMSCatalogAdapter.execute(HMSCatalogAdapter.java:537) ~[hive-standalone-metastore-rest-catalog-4.2.0-SNAPSHOT.jar:4.2.0 at org.apache.iceberg.rest.HMSCatalogServlet.service(HMSCatalogServlet.java:75) ~[hive-standalone-metastore-rest-catalog-4.2.0-SNAPSHOT.jar:4.2.0- SNAPSHOT] {code}", "comments": "As HiveAccessControlException belongs to hive-exec(ql), it turned out that HiveCatalog cannot correctly classify the error category. This is not a REST problem but a HiveCatalog problem. I guess authorization-related modules should belong to storage-api or standalone-metastore-common because HMS or external modules are involved. I'm still checking the best way. A sample patch. This doesn't work unless `iceberg-catalog` depends on `hive-exec`, which is probably not a good idea. https://github.com/okumin/hive/commit/af242c85eff5aedd5716b4879c3e92d0500734ae", "created": "2025-10-07T02:34:30.000+0000", "updated": "2025-10-28T08:41:01.000+0000", "derived": {"summary_task": "Summarize this issue: The current implementation does not handle permission errors and returns a 500 error. This is the exception when I integrated HMS Iceberg REST Catalog with Apache Ranger. {code:java} 2025-10-07T02:26:57,248 ERROR [qtp100805003-49] rest.HMSCatalogServlet: Error processing REST request org.apache.iceberg.exceptions.RESTException: Unhandled error: ErrorResponse(code=500, type=RuntimeException, messag", "classification_task": "Classify the issue priority and type: The current implementation does not handle permission errors and returns a 500 error. This is the exception when I integrated HMS Iceberg REST Catalog with Apache Ranger. {code:java} 2025-10-07T02:26:57,248 ERROR [qtp100805003-49] rest.HMSCatalogServlet: Error processing REST request org.apache.iceberg.exceptions.RESTException: Unhandled error: ErrorResponse(code=500, type=RuntimeException, messag", "qna_task": "Question: What is this issue about?\nAnswer: HMS Iceberg REST API should return 403 on HiveAccessControlException"}}
{"id": "13630803", "key": "HIVE-29247", "project": "HIVE", "summary": "Iceberg: Add authentication support to HiveRESTCatalogClient", "description": "", "comments": "", "created": "2025-10-06T19:24:35.000+0000", "updated": "2025-10-07T14:48:31.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Iceberg: Add authentication support to HiveRESTCatalogClient"}}
{"id": "13630751", "key": "HIVE-29246", "project": "HIVE", "summary": "Upgrade derby version to 10.17.1.0", "description": "As hive master branch is on JDK 21, upgrade of derby to 10.17.1.0 is possible [https://db.apache.org/derby/derby_downloads.html] This will help with CVE's as well [CVE-2022-46337|https://www.cve.org/CVERecord?id=CVE-2022-46337] [https://mvnrepository.com/artifact/org.apache.derby/derby/10.14.2.0] Derby project has done refactoring: # org.apache.derby.jdbc.EmbeddedDriver is now part of *derbytools* jar # org.apache.derby.security.SystemPermission \u2192 org.apache.derby.shared.common.security.SystemPermission (this is part of derbyshared jar which is compile time dependency for org.apache.derby:derby) # org.apache.derby.jdbc.AutoloadedDriver \u2192 org.apache.derby.iapi.jdbc.AutoloadedDriver", "comments": "Fixed in [https://github.com/apache/hive/commit/009dea2b09eceba05bc96282842311b27e0d8c07] Thanks for the PR [~Aggarwal_Raghav] ! Documentation update in https://github.com/apache/hive-site/commit/07b3afd1f4aad11ff07565459ce9c49c64b09ded", "created": "2025-10-06T08:08:23.000+0000", "updated": "2025-10-15T10:15:06.000+0000", "derived": {"summary_task": "Summarize this issue: As hive master branch is on JDK 21, upgrade of derby to 10.17.1.0 is possible [https://db.apache.org/derby/derby_downloads.html] This will help with CVE's as well [CVE-2022-46337|https://www.cve.org/CVERecord?id=CVE-2022-46337] [https://mvnrepository.com/artifact/org.apache.derby/derby/10.14.2.0] Derby project has done refactoring: # org.apache.derby.jdbc.EmbeddedDriver is now part of *derbytools*", "classification_task": "Classify the issue priority and type: As hive master branch is on JDK 21, upgrade of derby to 10.17.1.0 is possible [https://db.apache.org/derby/derby_downloads.html] This will help with CVE's as well [CVE-2022-46337|https://www.cve.org/CVERecord?id=CVE-2022-46337] [https://mvnrepository.com/artifact/org.apache.derby/derby/10.14.2.0] Derby project has done refactoring: # org.apache.derby.jdbc.EmbeddedDriver is now part of *derbytools*", "qna_task": "Question: What is this issue about?\nAnswer: Upgrade derby version to 10.17.1.0"}}
{"id": "13630700", "key": "HIVE-29245", "project": "HIVE", "summary": "Normalize Java license headers", "description": "SonarQube often reports that license headers in new files don't have the expected format. It is probably because some existing files are not compliant with the asf.header, and contributors copy-and-paste the wrong format of headers. > Line does not match expected header line of ' * http://www.apache.org/licenses/LICENSE-2.0'. Header external_checkstyle:header.HeaderCheck [https://sonarcloud.io/project/issues?sinceLeakPeriod=true&issueStatuses=OPEN%2CCONFIRMED&pullRequest=6108&id=apache_hive&open=AZmX8jO00mHwehPdTX0p] We can locally check those violations with the following configuration and command. {code:java} $ cat checkstyle/checkstyle.xml <?xml version=\"1.0\"?> <!DOCTYPE module PUBLIC \"-//Puppy Crawl//DTD Check Configuration 1.2//EN\" \"http://www.puppycrawl.com/dtds/configuration_1_2.dtd\"> <module name=\"Checker\"> <module name=\"Header\"> <property name=\"headerFile\" value=\"${config_loc}/asf.header\"/> </module> </module> $ mvn checkstyle:check{code} In this ticket, we will rewrite incompatible license headers.", "comments": "", "created": "2025-10-05T04:58:22.000+0000", "updated": "2025-10-17T09:25:37.000+0000", "derived": {"summary_task": "Summarize this issue: SonarQube often reports that license headers in new files don't have the expected format. It is probably because some existing files are not compliant with the asf.header, and contributors copy-and-paste the wrong format of headers. > Line does not match expected header line of ' * http://www.apache.org/licenses/LICENSE-2.0'. Header external_checkstyle:header.HeaderCheck [https://sonarcloud.io/pro", "classification_task": "Classify the issue priority and type: SonarQube often reports that license headers in new files don't have the expected format. It is probably because some existing files are not compliant with the asf.header, and contributors copy-and-paste the wrong format of headers. > Line does not match expected header line of ' * http://www.apache.org/licenses/LICENSE-2.0'. Header external_checkstyle:header.HeaderCheck [https://sonarcloud.io/pro", "qna_task": "Question: What is this issue about?\nAnswer: Normalize Java license headers"}}
{"id": "13630638", "key": "HIVE-29244", "project": "HIVE", "summary": "Add catalog field into ShowLocksRequest", "description": "[https://github.com/apache/hive/blob/744a0d6869e3a93f5d1f9afc151677ab432709d1/standalone-metastore/metastore-common/src/main/thrift/hive_metastore.thrift#L1292-L1297] At present, {*}show locks dbName{*}could only get result from default hive catalog. We need to add catalog field into ShowLocksRequest, so that we can get locks info from different catalogs by new syntax {*}show locks catName.dbName{*}.", "comments": "", "created": "2025-10-03T15:16:48.000+0000", "updated": "2025-10-03T15:16:48.000+0000", "derived": {"summary_task": "Summarize this issue: [https://github.com/apache/hive/blob/744a0d6869e3a93f5d1f9afc151677ab432709d1/standalone-metastore/metastore-common/src/main/thrift/hive_metastore.thrift#L1292-L1297] At present, {*}show locks dbName{*}could only get result from default hive catalog. We need to add catalog field into ShowLocksRequest, so that we can get locks info from different catalogs by new syntax {*}show locks catName.dbName{", "classification_task": "Classify the issue priority and type: [https://github.com/apache/hive/blob/744a0d6869e3a93f5d1f9afc151677ab432709d1/standalone-metastore/metastore-common/src/main/thrift/hive_metastore.thrift#L1292-L1297] At present, {*}show locks dbName{*}could only get result from default hive catalog. We need to add catalog field into ShowLocksRequest, so that we can get locks info from different catalogs by new syntax {*}show locks catName.dbName{", "qna_task": "Question: What is this issue about?\nAnswer: Add catalog field into ShowLocksRequest"}}
{"id": "13630633", "key": "HIVE-29243", "project": "HIVE", "summary": "Evaluate whether certain functions can be enabled under non hive default catalogs", "description": "I have noticed that some features are currently limited to the default 'hive' catalog and are not available for newly created catalogs. Such as: HIVE-19898 and HIVE-24127 and HIVE-22291 Since we aim to fully develop the capabilities of catalog, we need to evaluate whether these features should also be enabled for non default 'hive' catalogs. This is to prevent potential difficulties in using newly created catalogs due to the absence of these functions in the future. BTW, Some task can only do in NATIVE catalog, like ReploadTask. As for external catalogs(e.g., JDBC, REST), we need to skip those tasks that are exclusive to native catalogs.", "comments": "", "created": "2025-10-03T15:01:05.000+0000", "updated": "2025-10-27T13:52:55.000+0000", "derived": {"summary_task": "Summarize this issue: I have noticed that some features are currently limited to the default 'hive' catalog and are not available for newly created catalogs. Such as: HIVE-19898 and HIVE-24127 and HIVE-22291 Since we aim to fully develop the capabilities of catalog, we need to evaluate whether these features should also be enabled for non default 'hive' catalogs. This is to prevent potential difficulties in using newly", "classification_task": "Classify the issue priority and type: I have noticed that some features are currently limited to the default 'hive' catalog and are not available for newly created catalogs. Such as: HIVE-19898 and HIVE-24127 and HIVE-22291 Since we aim to fully develop the capabilities of catalog, we need to evaluate whether these features should also be enabled for non default 'hive' catalogs. This is to prevent potential difficulties in using newly", "qna_task": "Question: What is this issue about?\nAnswer: Evaluate whether certain functions can be enabled under non hive default catalogs"}}
{"id": "13630631", "key": "HIVE-29242", "project": "HIVE", "summary": "Add catalog for transaction module", "description": "A previous WIP ticket could be referred to: https://issues.apache.org/jira/browse/HIVE-18973 _Make transaction system work with catalogs._ We may need to continue this work to make transaction module work with catalog.", "comments": "", "created": "2025-10-03T14:50:21.000+0000", "updated": "2025-10-03T14:50:56.000+0000", "derived": {"summary_task": "Summarize this issue: A previous WIP ticket could be referred to: https://issues.apache.org/jira/browse/HIVE-18973 _Make transaction system work with catalogs._ We may need to continue this work to make transaction module work with catalog.", "classification_task": "Classify the issue priority and type: A previous WIP ticket could be referred to: https://issues.apache.org/jira/browse/HIVE-18973 _Make transaction system work with catalogs._ We may need to continue this work to make transaction module work with catalog.", "qna_task": "Question: What is this issue about?\nAnswer: Add catalog for transaction module"}}
{"id": "13630630", "key": "HIVE-29241", "project": "HIVE", "summary": "Add managedLocationUri field for catalog", "description": "HIVE-22995 introduced the concept of managedLocationUri. This means that created databases will have two locations. Currently, when creating a catalog, there is only the location attribute, without managedLocationUri. We should also add the managedLocationUri field to the catalog, so that when users create a database from the new created catalog, it can inherit the two locations from the catalog. Additionally, to avoid disrupting the location of the current default catalog 'hive' (which is controlled by the properties *metastore.warehouse.dir* and {*}metastore.warehouse.external.dir{*}), the locations for newly created catalogs should be separated from the default 'hive' catalog. Two new parameters can be added, such as *metastore.warehouse.catalog.dir* and {*}metastore.warehouse.catalog.external.dir{*}. Moreover, the location for each newly created catalog should have the catalog name appended at the end. For example, if *metastore.warehouse.catalog.dir* is *hdfs://ns1/testdir,* then the location for a newly created catalog named testcat would be {*}hdfs://ns1/testdir/testcat{*}. Consequently, the default path for a database like testdb created under this catalog would be {*}hdfs://ns1/testdir/testcat/testdb{*}.", "comments": "", "created": "2025-10-03T14:39:04.000+0000", "updated": "2025-10-03T14:40:19.000+0000", "derived": {"summary_task": "Summarize this issue: HIVE-22995 introduced the concept of managedLocationUri. This means that created databases will have two locations. Currently, when creating a catalog, there is only the location attribute, without managedLocationUri. We should also add the managedLocationUri field to the catalog, so that when users create a database from the new created catalog, it can inherit the two locations from the catalog. ", "classification_task": "Classify the issue priority and type: HIVE-22995 introduced the concept of managedLocationUri. This means that created databases will have two locations. Currently, when creating a catalog, there is only the location attribute, without managedLocationUri. We should also add the managedLocationUri field to the catalog, so that when users create a database from the new created catalog, it can inherit the two locations from the catalog. ", "qna_task": "Question: What is this issue about?\nAnswer: Add managedLocationUri field for catalog"}}
{"id": "13630626", "key": "HIVE-29240", "project": "HIVE", "summary": "Increase GKE node pool capacity to run more PR jobs concurrently in CI", "description": "Currently the CI is using an auto-scalable node pool ([slave-pool9|https://console.cloud.google.com/kubernetes/nodepool/us-central1-c/hive-test-kube/slave-pool9]) that can span up to 8 nodes. Based on the current configuration 2 pull requests can run in parallel every ~3 hours. This ticket aims to increase the number of nodes for the slave-pool9 node pool to 16 essentially doubling the number of pull requests that can run concurrently in CI.", "comments": "There is a high chance that the change will also affect the monthly cost of the CI infrastructure. We will monitor the consumption and if cost remains at reasonable levels the changes will be retained otherwise we may rollback to the previous values. The changes were implemented via the GKE console. As of now the node pool can scale up to 16 nodes.", "created": "2025-10-03T14:06:58.000+0000", "updated": "2025-10-03T14:14:30.000+0000", "derived": {"summary_task": "Summarize this issue: Currently the CI is using an auto-scalable node pool ([slave-pool9|https://console.cloud.google.com/kubernetes/nodepool/us-central1-c/hive-test-kube/slave-pool9]) that can span up to 8 nodes. Based on the current configuration 2 pull requests can run in parallel every ~3 hours. This ticket aims to increase the number of nodes for the slave-pool9 node pool to 16 essentially doubling the number of p", "classification_task": "Classify the issue priority and type: Currently the CI is using an auto-scalable node pool ([slave-pool9|https://console.cloud.google.com/kubernetes/nodepool/us-central1-c/hive-test-kube/slave-pool9]) that can span up to 8 nodes. Based on the current configuration 2 pull requests can run in parallel every ~3 hours. This ticket aims to increase the number of nodes for the slave-pool9 node pool to 16 essentially doubling the number of p", "qna_task": "Question: What is this issue about?\nAnswer: Increase GKE node pool capacity to run more PR jobs concurrently in CI"}}
{"id": "13630578", "key": "HIVE-29239", "project": "HIVE", "summary": "Upgrade checkstyle to 11.1.0", "description": "Since we reintroduced checkstyle in HIVE-29196, I have seen checkstyle violations in PRs when using modern java features. We should upgrade checkstyle to the latest version of 11.1.0 for better language support.", "comments": "[~soumyakanti.das] Please provide some examples with the violations that you observed. I believe that the problems you encountered are not violations but rather crashes and bugs in the checkstyle library. Fixed in https://github.com/apache/hive/commit/0e8749ee85bf202e69f5b4d3e8d325a01a5a2033", "created": "2025-10-02T22:24:19.000+0000", "updated": "2025-10-06T09:07:25.000+0000", "derived": {"summary_task": "Summarize this issue: Since we reintroduced checkstyle in HIVE-29196, I have seen checkstyle violations in PRs when using modern java features. We should upgrade checkstyle to the latest version of 11.1.0 for better language support.", "classification_task": "Classify the issue priority and type: Since we reintroduced checkstyle in HIVE-29196, I have seen checkstyle violations in PRs when using modern java features. We should upgrade checkstyle to the latest version of 11.1.0 for better language support.", "qna_task": "Question: What is this issue about?\nAnswer: Upgrade checkstyle to 11.1.0"}}
{"id": "13630516", "key": "HIVE-29238", "project": "HIVE", "summary": "upgrade kafka version to fix CVE-2024-31141 and CVE-2021-38153", "description": "", "comments": "", "created": "2025-10-02T10:09:53.000+0000", "updated": "2025-10-27T12:11:47.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: upgrade kafka version to fix CVE-2024-31141 and CVE-2021-38153"}}
{"id": "13630510", "key": "HIVE-29237", "project": "HIVE", "summary": "TestHiveCommitLocks flakyness", "description": "TestHiveCommitLocks is flaky on downstream. I couldn't reproduce the issue on upstream but I'm pretty sure it can happen any time - as the test cases are reusing resources. Update: With the help of [~dkuzmenko] , the root cause is identified: It seems Hive: Refactor commit lock mechanism from HiveTableOperations (#6648) port from Iceberg had a mistake and left two doNothing().when(spyOps).doUnlock(any()); calls in the test class.", "comments": "Merged to master. Thanks [~InvisibleProgrammer] for handling that.", "created": "2025-10-02T08:19:35.000+0000", "updated": "2025-10-07T08:10:17.000+0000", "derived": {"summary_task": "Summarize this issue: TestHiveCommitLocks is flaky on downstream. I couldn't reproduce the issue on upstream but I'm pretty sure it can happen any time - as the test cases are reusing resources. Update: With the help of [~dkuzmenko] , the root cause is identified: It seems Hive: Refactor commit lock mechanism from HiveTableOperations (#6648) port from Iceberg had a mistake and left two doNothing().when(spyOps).doUnlock", "classification_task": "Classify the issue priority and type: TestHiveCommitLocks is flaky on downstream. I couldn't reproduce the issue on upstream but I'm pretty sure it can happen any time - as the test cases are reusing resources. Update: With the help of [~dkuzmenko] , the root cause is identified: It seems Hive: Refactor commit lock mechanism from HiveTableOperations (#6648) port from Iceberg had a mistake and left two doNothing().when(spyOps).doUnlock", "qna_task": "Question: What is this issue about?\nAnswer: TestHiveCommitLocks flakyness"}}
{"id": "13630458", "key": "HIVE-29236", "project": "HIVE", "summary": "hive-site: The link to the organization of the community member is invalid", "description": "[https://github.com/apache/hive-site/blob/b5d187552c34e8ffc7bf7ac4afaf985bb82bf3e2/content/community/people.md?plain=1#L39] The organization of the community member is invalid if you check the website: [https://hive.apache.org/community/people/]", "comments": "", "created": "2025-10-01T15:55:05.000+0000", "updated": "2025-10-01T15:55:05.000+0000", "derived": {"summary_task": "Summarize this issue: [https://github.com/apache/hive-site/blob/b5d187552c34e8ffc7bf7ac4afaf985bb82bf3e2/content/community/people.md?plain=1#L39] The organization of the community member is invalid if you check the website: [https://hive.apache.org/community/people/]", "classification_task": "Classify the issue priority and type: [https://github.com/apache/hive-site/blob/b5d187552c34e8ffc7bf7ac4afaf985bb82bf3e2/content/community/people.md?plain=1#L39] The organization of the community member is invalid if you check the website: [https://hive.apache.org/community/people/]", "qna_task": "Question: What is this issue about?\nAnswer: hive-site: The link to the organization of the community member is invalid"}}
{"id": "13630432", "key": "HIVE-29235", "project": "HIVE", "summary": "Iceberg returns incorrect count value", "description": "For iceberg table, Hive tries to read partitionStatistics. But if the table doesn't have them, Hive calculates using default statistics, which is incorrect. We are using Hive 4.1.0. SELECT count(*), log_date FROM db1.tbl1 GROUP BY 2; +----------+-------------+ | _c0 | log_date | +----------+-------------+ | 343662 | 2025-09-29 | | 2513459 | 2025-09-30 | SELECT count(*) FROM db1.tb1 WHERE log_date = '2025-09-29'; // 2857121 SELECT count(*) FROM db1.tb1 WHERE log_date = '2025-09-30'; // 2857121", "comments": "Hi [~jyoo94] . Thanks for reporting this issue! Could you please give a more specific test example to help others to reproduce this issue? For example, how to create a table and insert a small amount of data to reproduce this issue? [~zhangbutao] So we first used Flink to produced data in iceberg format (without the flink option of `write.metadata.statistics.enabled`), with 2 partitions (and about 30 other columns) with log_date as partition field (string type) Then, simply do `SELECT count(*) FROM db1.tb1 WHERE log_date = '2025-09-29'` > For iceberg table, Hive tries to read partitionStatistics. But if the table doesn't have them, Hive calculates using default statistics, which is incorrect. [~jyoo94] what do you mean by default statistics? If partitionStatistics is not available HiveIcebergStorageHandler#canComputeQueryUsingStats should returns false and trigger the query execution. [~dkuzmenko] It's basicStats. It estimates stats in absense of statistics. [https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java#L275] Also, when in optimizer, when it tries to get row count, it sums up all row count for each partitions. https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java#L933-L958 [~jyoo94] estimated stats are used for query planning, not query output. Do you see tasks being launched, or is the result available instantly? what is the value of `hive.iceberg.stats.source` in hive-site.xml ? Sry I was AFK due to Korean Thanksgiving day. [~dkuzmenko] We didn't set any value for the key `hive.iceberg.stats.source`", "created": "2025-10-01T09:51:46.000+0000", "updated": "2025-10-27T22:21:00.000+0000", "derived": {"summary_task": "Summarize this issue: For iceberg table, Hive tries to read partitionStatistics. But if the table doesn't have them, Hive calculates using default statistics, which is incorrect. We are using Hive 4.1.0. SELECT count(*), log_date FROM db1.tbl1 GROUP BY 2; +----------+-------------+ | _c0 | log_date | +----------+-------------+ | 343662 | 2025-09-29 | | 2513459 | 2025-09-30 | SELECT count(*) FROM db1.tb1 WHERE log_date ", "classification_task": "Classify the issue priority and type: For iceberg table, Hive tries to read partitionStatistics. But if the table doesn't have them, Hive calculates using default statistics, which is incorrect. We are using Hive 4.1.0. SELECT count(*), log_date FROM db1.tbl1 GROUP BY 2; +----------+-------------+ | _c0 | log_date | +----------+-------------+ | 343662 | 2025-09-29 | | 2513459 | 2025-09-30 | SELECT count(*) FROM db1.tb1 WHERE log_date ", "qna_task": "Question: What is this issue about?\nAnswer: Iceberg returns incorrect count value"}}
{"id": "13630342", "key": "HIVE-29234", "project": "HIVE", "summary": "Iceberg: Validate HMS REST Catalog Client with OAuth2", "description": "Add OAuth 2 support to HiveRESTCatalogClient, covering initial token acquisition and automatic refresh handling. OAuth2 authentication manager enhancement discussion: [https://lists.apache.org/thread/6fvfq2wx6joxqnc4qboq5wfv07hdlv0p] [https://github.com/dremio/iceberg-auth-manager]", "comments": "Thanks for reviewing this PR, [~okumin] !", "created": "2025-09-30T12:28:27.000+0000", "updated": "2025-10-14T15:35:05.000+0000", "derived": {"summary_task": "Summarize this issue: Add OAuth 2 support to HiveRESTCatalogClient, covering initial token acquisition and automatic refresh handling. OAuth2 authentication manager enhancement discussion: [https://lists.apache.org/thread/6fvfq2wx6joxqnc4qboq5wfv07hdlv0p] [https://github.com/dremio/iceberg-auth-manager]", "classification_task": "Classify the issue priority and type: Add OAuth 2 support to HiveRESTCatalogClient, covering initial token acquisition and automatic refresh handling. OAuth2 authentication manager enhancement discussion: [https://lists.apache.org/thread/6fvfq2wx6joxqnc4qboq5wfv07hdlv0p] [https://github.com/dremio/iceberg-auth-manager]", "qna_task": "Question: What is this issue about?\nAnswer: Iceberg: Validate HMS REST Catalog Client with OAuth2"}}
{"id": "13630275", "key": "HIVE-29233", "project": "HIVE", "summary": "Iceberg: Validate HiveRESTCatalogClient with external RESTCatalogs like Gravitino", "description": "", "comments": "Thanks for the code review, [~okumin] and [~dkuzmenko] !", "created": "2025-09-29T22:39:42.000+0000", "updated": "2025-10-06T19:14:06.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Iceberg: Validate HiveRESTCatalogClient with external RESTCatalogs like Gravitino"}}
{"id": "13630232", "key": "HIVE-29232", "project": "HIVE", "summary": "set hive.exec.parallel=true; \u5f15\u53d1\u540c\u4e00\u4e2asql\uff0c\u5e76\u884cstage\u7684map.xml\u6587\u4ef6\u8def\u5f84\u91cd\u590d\uff0c\u5bfc\u81f4\u4efb\u52a1\u5076\u73b0\u51fa\u9519", "description": "int pathId=10000 public Path getMRTmpPath() { return new Path(getMRScratchDir(), MR_PREFIX + nextPathId()); } // \u8fd9\u6bb5\u4ee3\u7801\u5728\u5e76\u53d1\u6267\u884c\u65f6\u53ef\u80fd\u51fa\u73b0\u8fd4\u56de\u503c\u4e00\u6837\u7684\u60c5\u51b5 private String nextPathId() { return Integer.toString(pathid++); }", "comments": "Hi [~liukailong123] . Thanks for your reporting this issue. Please note that JIRA descriptions must be written in English to ensure all developers can understand them. One question: are you using the MR engine or the Tez engine? The Hive community has deprecated the MR engine, and the current primary code maintenance is focused on the Tez engine module. Additionally, Apache Hive no longer uses patch files for code contributions. Please submit your code fixes via PR to [https://github.com/apache/hive/pulls] . Thanks.", "created": "2025-09-29T13:17:46.000+0000", "updated": "2025-10-01T10:35:46.000+0000", "derived": {"summary_task": "Summarize this issue: int pathId=10000 public Path getMRTmpPath() { return new Path(getMRScratchDir(), MR_PREFIX + nextPathId()); } // \u8fd9\u6bb5\u4ee3\u7801\u5728\u5e76\u53d1\u6267\u884c\u65f6\u53ef\u80fd\u51fa\u73b0\u8fd4\u56de\u503c\u4e00\u6837\u7684\u60c5\u51b5 private String nextPathId() { return Integer.toString(pathid++); }", "classification_task": "Classify the issue priority and type: int pathId=10000 public Path getMRTmpPath() { return new Path(getMRScratchDir(), MR_PREFIX + nextPathId()); } // \u8fd9\u6bb5\u4ee3\u7801\u5728\u5e76\u53d1\u6267\u884c\u65f6\u53ef\u80fd\u51fa\u73b0\u8fd4\u56de\u503c\u4e00\u6837\u7684\u60c5\u51b5 private String nextPathId() { return Integer.toString(pathid++); }", "qna_task": "Question: What is this issue about?\nAnswer: set hive.exec.parallel=true; \u5f15\u53d1\u540c\u4e00\u4e2asql\uff0c\u5e76\u884cstage\u7684map.xml\u6587\u4ef6\u8def\u5f84\u91cd\u590d\uff0c\u5bfc\u81f4\u4efb\u52a1\u5076\u73b0\u51fa\u9519"}}
{"id": "13630186", "key": "HIVE-29231", "project": "HIVE", "summary": "Fix flaky TestEmbeddedHiveMetaStore.testDatabase by enforcing deterministic database lookup", "description": "The test org.apache.hadoop.hive.metastore.TestEmbeddedHiveMetaStore.testDatabase(standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java) passed using normal maven-test, but showed Non-deterministic behavior under NonDex(https://github.com/TestingResearchIllinois/NonDex) and thus failed. NonDex is a tool for detecting hidden assumptions in code by exploring non-deterministic behaviors of specifications that allow multiple valid implementations. Some of the error messages are: {code:java} Build Failure observed: java.lang.AssertionError: first database is not testdb1 in list: []{code} Steps to reproduce: {code:java} mvn -pl standalone-metastore/metastore-server edu.illinois:nondex-maven-plugin:2.2.1:nondex -Dtest=org.apache.hadoop.hive.metastore.TestEmbeddedHiveMetaStore#testDatabase {code} Root cause: The test asserted database existence using getDatabases(\".*\") which relies on cached database lists that may return stale results when nondex randomizes test execution. This led to nondeterministic test outcomes where databases existed but were not found in the cached list. Fix: Replace getDatabases(\".*\") with direct getDatabase(name) calls in TestHiveMetaStore class's testDatabase method. This ensures consistent behavior regardless of test shuffling. Additionally, I believe this improve the harness of tests without altering and damaging the core database operations being tested(add, find, drop). Confirm Fixed: re-run {code:java} mvn -pl standalone-metastore/metastore-server edu.illinois:nondex-maven-plugin:2.2.1:nondex -Dtest=org.apache.hadoop.hive.metastore.TestEmbeddedHiveMetaStore#testDatabase {code} And the result is Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, BUILD SUCCESS", "comments": "", "created": "2025-09-29T00:17:01.000+0000", "updated": "2025-09-30T20:52:06.000+0000", "derived": {"summary_task": "Summarize this issue: The test org.apache.hadoop.hive.metastore.TestEmbeddedHiveMetaStore.testDatabase(standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java) passed using normal maven-test, but showed Non-deterministic behavior under NonDex(https://github.com/TestingResearchIllinois/NonDex) and thus failed. NonDex is a tool for detecting hidden assumptions in code b", "classification_task": "Classify the issue priority and type: The test org.apache.hadoop.hive.metastore.TestEmbeddedHiveMetaStore.testDatabase(standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java) passed using normal maven-test, but showed Non-deterministic behavior under NonDex(https://github.com/TestingResearchIllinois/NonDex) and thus failed. NonDex is a tool for detecting hidden assumptions in code b", "qna_task": "Question: What is this issue about?\nAnswer: Fix flaky TestEmbeddedHiveMetaStore.testDatabase by enforcing deterministic database lookup"}}
{"id": "13630139", "key": "HIVE-29230", "project": "HIVE", "summary": "Iceberg: Reads fails after Schema evolution with complex type columns", "description": "If we add a complex type column to an existing table with data the reads fails {noformat} Caused by: java.lang.ClassCastException: optional binary point is not a group at org.apache.parquet.schema.Type.asGroupType(Type.java:247) at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.buildVectorizedParquetReader(VectorizedParquetRecordReader.java:541) at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:455) at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:415) at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.next(VectorizedParquetRecordReader.java:353) at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.next(VectorizedParquetRecordReader.java:97) at org.apache.iceberg.mr.hive.vector.HiveBatchIterator.advance(HiveBatchIterator.java:75) at org.apache.iceberg.mr.hive.vector.HiveBatchIterator.hasNext(HiveBatchIterator.java:143) at org.apache.iceberg.mr.mapreduce.IcebergRecordReader.nextKeyValue(IcebergRecordReader.java:119) at org.apache.iceberg.mr.hive.vector.HiveIcebergVectorizedRecordReader.next(HiveIcebergVectorizedRecordReader.java:48) at org.apache.iceberg.mr.hive.vector.HiveIcebergVectorizedRecordReader.next(HiveIcebergVectorizedRecordReader.java:34) at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:373) at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:82) at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:118) at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:58) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:208) at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:75) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:414) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:293) {noformat}", "comments": "Committed to master. Thanx [~difin] & [~dkuzmenko] for the reviews!!!", "created": "2025-09-27T16:53:30.000+0000", "updated": "2025-10-01T06:01:49.000+0000", "derived": {"summary_task": "Summarize this issue: If we add a complex type column to an existing table with data the reads fails {noformat} Caused by: java.lang.ClassCastException: optional binary point is not a group at org.apache.parquet.schema.Type.asGroupType(Type.java:247) at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.buildVectorizedParquetReader(VectorizedParquetRecordReader.java:541) at org.apache.hadoop.hive", "classification_task": "Classify the issue priority and type: If we add a complex type column to an existing table with data the reads fails {noformat} Caused by: java.lang.ClassCastException: optional binary point is not a group at org.apache.parquet.schema.Type.asGroupType(Type.java:247) at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.buildVectorizedParquetReader(VectorizedParquetRecordReader.java:541) at org.apache.hadoop.hive", "qna_task": "Question: What is this issue about?\nAnswer: Iceberg: Reads fails after Schema evolution with complex type columns"}}
{"id": "13630116", "key": "HIVE-29229", "project": "HIVE", "summary": "Consider deprecating and removing the TestMiniLlapCliDriver", "description": "Tested against master branch, commit id: https://github.com/apache/hive/commit/fda96f8a0db51207bc3ea8f95a9cd19bc5d94260 {code:java} mvn test -Dtest=TestMiniLlapLocalCliDriver -Dqfile=catalog.q -Dtest.output.overwrite=true -pl itests/qtest -Pitests{code} {code:java} mvn test -Dtest=TestMiniLlapCliDriver -Dqfile=catalog.q -Dtest.output.overwrite=true -pl itests/qtest -Pitests{code} I encountered a confusing discrepancy between the results of running {{catalog.q}} with {{TestMiniLlapLocalCliDriver}} and {{{}TestMiniLlapCliDriver{}}}. I haven't investigated the root cause yet. Since our CI has been defaulting to {{TestMiniLlapLocalCliDriver}} since HIVE-23138, should we consider deprecating and removing {{{}TestMiniLlapCliDriver{}}}? This would prevent confusion for Hive developers and help streamline our test module code.", "comments": "TestMiniLlapLocalCliDriver helps to debug code executed on llap daemons. >> TestMiniLlapLocalCliDriver helps to debug code executed on llap daemons. TestMiniLlapLocalCliDriver is beneficial for debugging code and is also the current default test class. So, should we consider to remove TestMiniLlapCliDriver? I'm not sure what practical value TestMiniLlapCliDriver has.", "created": "2025-09-27T07:37:58.000+0000", "updated": "2025-10-02T15:54:18.000+0000", "derived": {"summary_task": "Summarize this issue: Tested against master branch, commit id: https://github.com/apache/hive/commit/fda96f8a0db51207bc3ea8f95a9cd19bc5d94260 {code:java} mvn test -Dtest=TestMiniLlapLocalCliDriver -Dqfile=catalog.q -Dtest.output.overwrite=true -pl itests/qtest -Pitests{code} {code:java} mvn test -Dtest=TestMiniLlapCliDriver -Dqfile=catalog.q -Dtest.output.overwrite=true -pl itests/qtest -Pitests{code} I encountered a c", "classification_task": "Classify the issue priority and type: Tested against master branch, commit id: https://github.com/apache/hive/commit/fda96f8a0db51207bc3ea8f95a9cd19bc5d94260 {code:java} mvn test -Dtest=TestMiniLlapLocalCliDriver -Dqfile=catalog.q -Dtest.output.overwrite=true -pl itests/qtest -Pitests{code} {code:java} mvn test -Dtest=TestMiniLlapCliDriver -Dqfile=catalog.q -Dtest.output.overwrite=true -pl itests/qtest -Pitests{code} I encountered a c", "qna_task": "Question: What is this issue about?\nAnswer: Consider deprecating and removing the TestMiniLlapCliDriver"}}
{"id": "13630114", "key": "HIVE-29228", "project": "HIVE", "summary": "Support Credential Vending in Iceberg REST Catalog", "description": "Iceberg REST API spec allows LoadTable or [/v1/\\{prefix}/namespaces/\\{namespace}/tables/\\{table}/credentials|https://github.com/apache/iceberg/blob/apache-iceberg-1.10.0/open-api/rest-catalog-open-api.yaml#L1184-L1220] to issue temporary credentials. Related PRs and discussions.. * [apache/iceberg#10722 OpenAPI: Standardize credentials in loadTable/loadView responses|https://github.com/apache/iceberg/pull/10722] * [apache/iceberg#11118 REST: Standardize vended credentials used in loadTable / loadView responses|https://github.com/apache/iceberg/issues/11118] * Iceberg dev ML: [DISCUSS] REST: Standardize vended credentials in Spec: [https://lists.apache.org/thread/jmklpnywnghg7qwmwr14zj2k6tnxmdo4] * Google Docs: [https://docs.google.com/document/d/1lySd_5hMZNtISLKsOvAq7xiNzdXU6TAoHF_yrOXWQvM/edit?tab=t.0#heading=h.hs6r9d26w1y2]", "comments": "", "created": "2025-09-27T04:32:57.000+0000", "updated": "2025-10-29T09:52:06.000+0000", "derived": {"summary_task": "Summarize this issue: Iceberg REST API spec allows LoadTable or [/v1/\\{prefix}/namespaces/\\{namespace}/tables/\\{table}/credentials|https://github.com/apache/iceberg/blob/apache-iceberg-1.10.0/open-api/rest-catalog-open-api.yaml#L1184-L1220] to issue temporary credentials. Related PRs and discussions.. * [apache/iceberg#10722 OpenAPI: Standardize credentials in loadTable/loadView responses|https://github.com/apache/iceb", "classification_task": "Classify the issue priority and type: Iceberg REST API spec allows LoadTable or [/v1/\\{prefix}/namespaces/\\{namespace}/tables/\\{table}/credentials|https://github.com/apache/iceberg/blob/apache-iceberg-1.10.0/open-api/rest-catalog-open-api.yaml#L1184-L1220] to issue temporary credentials. Related PRs and discussions.. * [apache/iceberg#10722 OpenAPI: Standardize credentials in loadTable/loadView responses|https://github.com/apache/iceb", "qna_task": "Question: What is this issue about?\nAnswer: Support Credential Vending in Iceberg REST Catalog"}}
{"id": "13630056", "key": "HIVE-29227", "project": "HIVE", "summary": "Apply masking in memory instead of rewriting the file in QOutProcessor#maskPatterns", "description": "After HIVE-29226 has been merged, the masking is applied in various places. Refactor it so that the masking is only applied once, in memory, before the SORT_QUERY_RESULTS is applied. QFile also defines some patterns for masking. It would be a good to have a single place for all these mask rules. Probably a good opportunity for refactoring.", "comments": "", "created": "2025-09-26T11:07:27.000+0000", "updated": "2025-09-26T11:07:27.000+0000", "derived": {"summary_task": "Summarize this issue: After HIVE-29226 has been merged, the masking is applied in various places. Refactor it so that the masking is only applied once, in memory, before the SORT_QUERY_RESULTS is applied. QFile also defines some patterns for masking. It would be a good to have a single place for all these mask rules. Probably a good opportunity for refactoring.", "classification_task": "Classify the issue priority and type: After HIVE-29226 has been merged, the masking is applied in various places. Refactor it so that the masking is only applied once, in memory, before the SORT_QUERY_RESULTS is applied. QFile also defines some patterns for masking. It would be a good to have a single place for all these mask rules. Probably a good opportunity for refactoring.", "qna_task": "Question: What is this issue about?\nAnswer: Apply masking in memory instead of rewriting the file in QOutProcessor#maskPatterns"}}
{"id": "13630055", "key": "HIVE-29226", "project": "HIVE", "summary": "Make order of qfile query results deterministic when masking and sorting", "description": "Apply the masking before sorting in QTestResultProcessor to make the query results deterministic.", "comments": "Merged to master. Thanks for the fix, [~thomas.rebele] !", "created": "2025-09-26T11:02:50.000+0000", "updated": "2025-10-08T13:09:37.000+0000", "derived": {"summary_task": "Summarize this issue: Apply the masking before sorting in QTestResultProcessor to make the query results deterministic.", "classification_task": "Classify the issue priority and type: Apply the masking before sorting in QTestResultProcessor to make the query results deterministic.", "qna_task": "Question: What is this issue about?\nAnswer: Make order of qfile query results deterministic when masking and sorting"}}
{"id": "13629768", "key": "HIVE-29225", "project": "HIVE", "summary": "Premature deletion of scratch directories during output streaming", "description": "Once a job or application finishes, the corresponding lock file is released, and YARN no longer reports any active jobs or applications. At this point, Hive assumes the associated scratch directory is no longer needed and proceeds to delete it upon *ClearDanglingScratchDir* service is invoked. However, in some cases, Hive may still be streaming output to the client after the application is marked as finished. This causes the scratch directory to be deleted prematurely, even though it is still required for ongoing output. As a result, queries can fail with *IOException* errors because the scratch directory is removed while Hive is still using it. {code:java} org.apache.hive.service.cli.HiveSQLException: java.io.IOException: java.io.IOException: 2049.323.265264 /user/mapr/tmp/hive/mapr/ecfdf7a4-91ba-4832-a408-8d459f90ac4b/hive_2025-07-23_12-57-08_793_7535333864129536266-1/-mr-10001/.hive-staging_hive_2025-07-23_12-57-08_793_7535333864129536266-1/-ext-10002/000008_0 (Input/output error) {code} --- *STEPS TO REPRODUCE:* 1. Add the following properties into hive-site.xml and restart Hive services: {code:java} <property> <name>hive.scratchdir.lock</name> <value>true</value> </property> <property> <name>tez.session.am.dag.submit.timeout.secs</name> <value>10</value> </property> {code} 2. Generate the data using the following commands: {code:java} for i in {1..1000000}; do echo \"\" >> /tmp/file1.txt; done for i in {1..8}; do cat /tmp/file1.txt >> /tmp/file2.txt; cat /tmp/file2.txt >> /tmp/file1.txt; done {code} 3. Create Hive table and load data into it: {code:java} CREATE TABLE i (id INT); LOAD DATA LOCAL INPATH '/tmp/file2.txt' INTO TABLE i; {code} 4. Connect to HiveServer2 using Beeline and execute the following queries: {code:java} SET hive.fetch.task.conversion=none; SELECT * FROM i; {code} 5. Open a new session to the same host. After the query from step 4 starts returning the results, wait 30 seconds and execute the command below: {code:java} hive --service cleardanglingscratchdir -v {code} 6. \u201dhive --service cleardanglingscratchdir -v\u201d deletes the scratchdir used by the query and the query fails during the next 20-30 seconds with the following error: {code:java} ... | NULL | org.apache.hive.service.cli.HiveSQLException: java.io.IOException: java.io.IOException: 2049.323.265264 /user/mapr/tmp/hive/mapr/ecfdf7a4-91ba-4832-a408-8d459f90ac4b/hive_2025-07-23_12-57-08_793_7535333864129536266-1/-mr-10001/.hive-staging_hive_2025-07-23_12-57-08_793_7535333864129536266-1/-ext-10002/000008_0 (Input/output error) 0: jdbc:hive2://node1:10000/> {code}", "comments": "Hi [~sercan.tekin], please set the affected version(s).", "created": "2025-09-24T02:07:06.000+0000", "updated": "2025-10-30T00:09:41.000+0000", "derived": {"summary_task": "Summarize this issue: Once a job or application finishes, the corresponding lock file is released, and YARN no longer reports any active jobs or applications. At this point, Hive assumes the associated scratch directory is no longer needed and proceeds to delete it upon *ClearDanglingScratchDir* service is invoked. However, in some cases, Hive may still be streaming output to the client after the application is marked ", "classification_task": "Classify the issue priority and type: Once a job or application finishes, the corresponding lock file is released, and YARN no longer reports any active jobs or applications. At this point, Hive assumes the associated scratch directory is no longer needed and proceeds to delete it upon *ClearDanglingScratchDir* service is invoked. However, in some cases, Hive may still be streaming output to the client after the application is marked ", "qna_task": "Question: What is this issue about?\nAnswer: Premature deletion of scratch directories during output streaming"}}
{"id": "13629678", "key": "HIVE-29224", "project": "HIVE", "summary": "Move TPC-DS CTE suggestion tests in new test suite", "description": "HIVE-28259 introduced a new experimental feature for finding and materializing common table expressions (CTEs) using CBO. The effect of the suggester on TPC-DS queries was tested using the TestTezTPCDS30TBPerfCliDriver. As it is right now we cannot easily fine-tune other CTE related configurations (e.g., hive.optimize.cte.materialize.threshold) without affecting existing query plans and doing so may decrease test coverage for certain parts of the engine. I propose to move the suggester based (\"hive.optimize.cte.suggester.class\") tests under a new test suite (new TestTPCDSCteCliDriver) relying on the TPC-DS 30TB stats focusing on CTE materialization/suggestion features. *Pros* * Increase in test coverage * No coupling of experimental features with main plan regression suite *Cons* * Increase in test runtime * Redundancy in golden files (*.q.out) Once the CTE feature(s) are stabilized and become part of the default configuration it will no longer be necessary to maintain a separate suite.", "comments": "Fixed in https://github.com/apache/hive/commit/008f4d11e230f30103dc156247f97bf4d18e0b33", "created": "2025-09-23T08:49:53.000+0000", "updated": "2025-10-03T07:15:04.000+0000", "derived": {"summary_task": "Summarize this issue: HIVE-28259 introduced a new experimental feature for finding and materializing common table expressions (CTEs) using CBO. The effect of the suggester on TPC-DS queries was tested using the TestTezTPCDS30TBPerfCliDriver. As it is right now we cannot easily fine-tune other CTE related configurations (e.g., hive.optimize.cte.materialize.threshold) without affecting existing query plans and doing so m", "classification_task": "Classify the issue priority and type: HIVE-28259 introduced a new experimental feature for finding and materializing common table expressions (CTEs) using CBO. The effect of the suggester on TPC-DS queries was tested using the TestTezTPCDS30TBPerfCliDriver. As it is right now we cannot easily fine-tune other CTE related configurations (e.g., hive.optimize.cte.materialize.threshold) without affecting existing query plans and doing so m", "qna_task": "Question: What is this issue about?\nAnswer: Move TPC-DS CTE suggestion tests in new test suite"}}
{"id": "13629673", "key": "HIVE-29223", "project": "HIVE", "summary": "The BUG is found in apache/hive:4.1.0", "description": "*The BUG is found in apache/hive:4.1.0* _Query 20250923_071049_00000_ht8pd failed: java.lang.NumberFormatException: For input string: \"60s\"_ _project with docker containers explains_ *_https://github.com/Repinoid/hive410bug_*", "comments": "hi [~naeel] , could you please add a cleaner bug description and repro steps. I tried to describe this bug on [https://github.com/Repinoid/hive410bug] readme [https://github.com/Repinoid/hive410bug/blob/main/README.md] It is necessary to clone & deploy With metastore apache/hive Version 4.0.1 - no errors When using latest Version - 4.1.0 the bug _Query 20250923_071049_00000_ht8pd failed: java.lang.NumberFormatException: For input string: \"60s\"_ {{occurs with Trino *SCREATE SCHEMA* command }} _trino> CREATE SCHEMA minio_catalog.mini WITH (location = 's3a://tiny/');_ [~naeel] Could you please post the error stacktrace of HMS 4.1.0? {code:java} 2025-10-27 20:06:53,824 ERROR [Metastore-Handler-Pool: Thread-59] metastore.RetryingHMSHandler (RetryingHMSHandler.java:invokeInternal(144)) - MetaException(message:java.lang.NumberFormatException: For input string: \"60s\") at org.apache.hadoop.hive.metastore.ExceptionHandler.newMetaException(ExceptionHandler.java:152) at org.apache.hadoop.hive.metastore.ExceptionHandler.defaultMetaException(ExceptionHandler.java:168) at org.apache.hadoop.hive.metastore.HMSHandler.create_database_req(HMSHandler.java:1429) at org.apache.hadoop.hive.metastore.HMSHandler.create_database(HMSHandler.java:1368) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) at java.base/java.lang.reflect.Method.invoke(Unknown Source) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:91) at org.apache.hadoop.hive.metastore.AbstractHMSHandlerProxy.invoke(AbstractHMSHandlerProxy.java:82) at jdk.proxy2/jdk.proxy2.$Proxy26.create_database(Unknown Source) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$create_database.getResult(ThriftHiveMetastore.java:19312) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$create_database.getResult(ThriftHiveMetastore.java:19291) at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38) at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:104) at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:250) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) at java.base/java.lang.Thread.run(Unknown Source) Caused by: java.lang.NumberFormatException: For input string: \"60s\" at java.base/java.lang.NumberFormatException.forInputString(Unknown Source) at java.base/java.lang.Long.parseLong(Unknown Source) at java.base/java.lang.Long.parseLong(Unknown Source) at org.apache.hadoop.conf.Configuration.getLong(Configuration.java:1607) at org.apache.hadoop.fs.s3a.S3AUtils.longOption(S3AUtils.java:985) at org.apache.hadoop.fs.s3a.S3AFileSystem.initThreadPools(S3AFileSystem.java:785) at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:555) at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3615) at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172) at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716) at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557) at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366) at org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.checkPermissions(StorageBasedAuthorizationProvider.java:409) at org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.authorize(StorageBasedAuthorizationProvider.java:381) at org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.authorize(StorageBasedAuthorizationProvider.java:176) at org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.authorizeCreateDatabase(AuthorizationPreEventListener.java:238) at org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.onEvent(AuthorizationPreEventListener.java:165) at org.apache.hadoop.hive.metastore.HMSHandler.firePreEvent(HMSHandler.java:4130) at org.apache.hadoop.hive.metastore.HMSHandler.create_database_core(HMSHandler.java:1217) at org.apache.hadoop.hive.metastore.HMSHandler.create_database_req(HMSHandler.java:1421){code} Has nothing todo with HMS. make sure to use proper version of Hadoop (3.4.1) as specified in RN: https://hive.apache.org/general/downloads/#31-july-2025--release-410-available wget -P ./jars [https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.4.1/hadoop-aws-3.4.1.jar]", "created": "2025-09-23T08:08:58.000+0000", "updated": "2025-10-27T17:23:35.000+0000", "derived": {"summary_task": "Summarize this issue: *The BUG is found in apache/hive:4.1.0* _Query 20250923_071049_00000_ht8pd failed: java.lang.NumberFormatException: For input string: \"60s\"_ _project with docker containers explains_ *_https://github.com/Repinoid/hive410bug_*", "classification_task": "Classify the issue priority and type: *The BUG is found in apache/hive:4.1.0* _Query 20250923_071049_00000_ht8pd failed: java.lang.NumberFormatException: For input string: \"60s\"_ _project with docker containers explains_ *_https://github.com/Repinoid/hive410bug_*", "qna_task": "Question: What is this issue about?\nAnswer:  The BUG is found in apache/hive:4.1.0"}}
{"id": "13629669", "key": "HIVE-29222", "project": "HIVE", "summary": "Compilation failure when running with thrift profile", "description": "Compilation failure post running with thrift profile This issue is happening post HIVE-29184: Iceberg: Basic Variant type support in Hive HIVE-29184 adds VARIANT_TYPE_NAME directly in serdeConstants.java, but it is thrift generated code. The same changes should have been made in serde.thrift. Compiling with thrift profile reverts this change causing compilation issue", "comments": "Committed to master. Thanx [~vikramahuja_] for the contribution!!!", "created": "2025-09-23T06:29:16.000+0000", "updated": "2025-09-27T16:49:51.000+0000", "derived": {"summary_task": "Summarize this issue: Compilation failure post running with thrift profile This issue is happening post HIVE-29184: Iceberg: Basic Variant type support in Hive HIVE-29184 adds VARIANT_TYPE_NAME directly in serdeConstants.java, but it is thrift generated code. The same changes should have been made in serde.thrift. Compiling with thrift profile reverts this change causing compilation issue", "classification_task": "Classify the issue priority and type: Compilation failure post running with thrift profile This issue is happening post HIVE-29184: Iceberg: Basic Variant type support in Hive HIVE-29184 adds VARIANT_TYPE_NAME directly in serdeConstants.java, but it is thrift generated code. The same changes should have been made in serde.thrift. Compiling with thrift profile reverts this change causing compilation issue", "qna_task": "Question: What is this issue about?\nAnswer: Compilation failure when running with thrift profile"}}
{"id": "13629624", "key": "HIVE-29221", "project": "HIVE", "summary": "Upgrade bouncycastle to 1.79", "description": "We should upgrade bouncycastle to 1.79 for [https://www.wiz.io/vulnerability-database/cve/cve-2025-8916].", "comments": "Close this since Apache Hive is already using 1.82.", "created": "2025-09-22T16:52:01.000+0000", "updated": "2025-09-22T17:07:17.000+0000", "derived": {"summary_task": "Summarize this issue: We should upgrade bouncycastle to 1.79 for [https://www.wiz.io/vulnerability-database/cve/cve-2025-8916].", "classification_task": "Classify the issue priority and type: We should upgrade bouncycastle to 1.79 for [https://www.wiz.io/vulnerability-database/cve/cve-2025-8916].", "qna_task": "Question: What is this issue about?\nAnswer: Upgrade bouncycastle to 1.79"}}
{"id": "13629606", "key": "HIVE-29220", "project": "HIVE", "summary": "Iceberg: Enable write operations with REST Catalog HMS Client", "description": "", "comments": "Thanks a lot for the code review and valuable comments, [~okumin] !", "created": "2025-09-22T13:53:59.000+0000", "updated": "2025-09-27T01:28:06.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Iceberg: Enable write operations with REST Catalog HMS Client"}}
{"id": "13629540", "key": "HIVE-29219", "project": "HIVE", "summary": "Upgrade Jersey to 2.x", "description": "", "comments": "", "created": "2025-09-22T06:01:23.000+0000", "updated": "2025-09-22T06:04:25.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Upgrade Jersey to 2.x"}}
{"id": "13629488", "key": "HIVE-29218", "project": "HIVE", "summary": "LOAD OVERWRITE PARTITION on muti-level partititoned external Iceberg table may unexpectedly delete other partitions", "description": "When using {{appendFile}} as the implementation of {{{}LOAD{}}}, there is an issue with handling {{LOAD OVERWRITE}} on multi-level partitioned Iceberg tables. The overwrite logic may incorrectly delete partitions that should not be affected. *Steps to Reproduce* 1. Create an Iceberg table partitioned by two columns, {{pcol1}} and {{{}pcol2{}}}. {code:java} create external table ice_parquet_multi_partitioned ( strcol string, intcol integer ) partitioned by (pcol1 string, pcol2 string) stored by iceberg; {code} 2. Insert data into partition {{{}pcol1=x/pcol2=y{}}}. {code:java} LOAD DATA LOCAL INPATH '/path/to/pcol1=x_pcol2=y.parquet' INTO TABLE ice_parquet_multi_partitioned PARTITION (pcol1='x', pcol2='y');{code} 3. Run a {{LOAD OVERWRITE}} into another partition, e.g., {{{}pcol1=x/pcol2=z{}}}. {code:java} LOAD DATA LOCAL INPATH '/path/to/pcol1=x_pcol2=z.parquet' OVERWRITE INTO TABLE ice_parquet_multi_partitioned PARTITION (pcol1='x', pcol2='z');{code} *Expected Behavior* Only the target partition ({{{}pcol1=x/pcol2=z{}}}) should be overwritten. Existing partitions ({{{}pcol1=x/pcol2=y{}}}) should remain intact. *Actual Behavior* The existing partition {{pcol1=x/pcol2=y}} is unexpectedly deleted when overwriting another partition with the same {{pcol1}} value but different {{{}pcol2{}}}.", "comments": "Merged to master Thanks for the fix and refactor, [~bravoqq] and [~ayushsaxena] for the review!", "created": "2025-09-20T15:06:49.000+0000", "updated": "2025-09-26T08:13:22.000+0000", "derived": {"summary_task": "Summarize this issue: When using {{appendFile}} as the implementation of {{{}LOAD{}}}, there is an issue with handling {{LOAD OVERWRITE}} on multi-level partitioned Iceberg tables. The overwrite logic may incorrectly delete partitions that should not be affected. *Steps to Reproduce* 1. Create an Iceberg table partitioned by two columns, {{pcol1}} and {{{}pcol2{}}}. {code:java} create external table ice_parquet_multi_p", "classification_task": "Classify the issue priority and type: When using {{appendFile}} as the implementation of {{{}LOAD{}}}, there is an issue with handling {{LOAD OVERWRITE}} on multi-level partitioned Iceberg tables. The overwrite logic may incorrectly delete partitions that should not be affected. *Steps to Reproduce* 1. Create an Iceberg table partitioned by two columns, {{pcol1}} and {{{}pcol2{}}}. {code:java} create external table ice_parquet_multi_p", "qna_task": "Question: What is this issue about?\nAnswer: LOAD OVERWRITE PARTITION on muti-level partititoned external Iceberg table may unexpectedly delete other partitions"}}
{"id": "13629447", "key": "HIVE-29217", "project": "HIVE", "summary": "Add configuration to choose materialization strategy for CTEs", "description": "Currently there are two ways for materializing common table expressions (CTEs) that are present in SQL queries. +SQL/AST based materialization+ It was introduced in HIVE-11752 and relies on the syntax of SQL queries specifically in presence of WITH clauses. It is performed early on during the semantic analysis and triggers before any kind of optimization. +CBO based materialization+ It was introduced in HIVE-28259 and relies on plan equivalences. It is performed during the cost based optimization phase and after the initial analysis. Currently the two strategies interfere with each other and in various cases if they are both enabled they can affect negatively the performance due to excessive materialization. Another drawback is that if a query contains a WITH clause we cannot bypass the AST materialization (and let the materialization decision entirely on CBO) so we are risking a non-optimal choice purely based on the syntax of the query. I propose adding a new configuration property for explicitly selecting between AST and CBO materialization allowing each materialization strategy to be applied independently of the other. In order to avoid changes in behavior and retain backward compatibility the default materialization strategy will use the old AST based approach.", "comments": "[~zabetak] Do we know if one of them is better than the other? If CBO based materialization is better then we could probably set it as default. [~soumyakanti.das] Without adding a new configuration it is not possible to set CBO materialization as default. The main goal of this change is to make the two materialization strategies independent of each other. Other than that the CBO strategy is still experimental so it cannot be the default at this point in time.", "created": "2025-09-19T15:19:30.000+0000", "updated": "2025-10-09T08:00:31.000+0000", "derived": {"summary_task": "Summarize this issue: Currently there are two ways for materializing common table expressions (CTEs) that are present in SQL queries. +SQL/AST based materialization+ It was introduced in HIVE-11752 and relies on the syntax of SQL queries specifically in presence of WITH clauses. It is performed early on during the semantic analysis and triggers before any kind of optimization. +CBO based materialization+ It was introdu", "classification_task": "Classify the issue priority and type: Currently there are two ways for materializing common table expressions (CTEs) that are present in SQL queries. +SQL/AST based materialization+ It was introduced in HIVE-11752 and relies on the syntax of SQL queries specifically in presence of WITH clauses. It is performed early on during the semantic analysis and triggers before any kind of optimization. +CBO based materialization+ It was introdu", "qna_task": "Question: What is this issue about?\nAnswer: Add configuration to choose materialization strategy for CTEs"}}
{"id": "13629389", "key": "HIVE-29216", "project": "HIVE", "summary": "DirectSQL disables strict checking in MySQL, allowing undroppable partitions", "description": "[https://github.com/apache/hive/blob/77d0d8d92c3257fb056337e5757f0f9bd8c34f02/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/DatabaseProduct.java#L314] When `SET @@session.sql_mode=ANSI_QUOTES` is written, it clears all other MySQL defaults, namely `STRICT_TRANS_TABLES`. When inserting a partition with a value > 256 characters, it is successfully inserted when it should be rejected in MySQL. This leads to a scenario where you can insert a partition, the `PARTITIONS` table has a `PART_NAME` with > 256 chars, but the `PART_KEY_VALS` is silently truncated to 256 chars. So when you attempt to drop a table/cleanup partitions, it will never succeed, as the partition returned to the client is the truncated one, which then 404s on attempted deletion (correctly). Ideally there is a validation check on partition values to ensure that it will insert into the DB, or we don't clear the default session mode (unless required)", "comments": "", "created": "2025-09-18T22:28:19.000+0000", "updated": "2025-09-18T22:28:36.000+0000", "derived": {"summary_task": "Summarize this issue: [https://github.com/apache/hive/blob/77d0d8d92c3257fb056337e5757f0f9bd8c34f02/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/DatabaseProduct.java#L314] When `SET @@session.sql_mode=ANSI_QUOTES` is written, it clears all other MySQL defaults, namely `STRICT_TRANS_TABLES`. When inserting a partition with a value > 256 characters, it is successfully inserted when", "classification_task": "Classify the issue priority and type: [https://github.com/apache/hive/blob/77d0d8d92c3257fb056337e5757f0f9bd8c34f02/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/DatabaseProduct.java#L314] When `SET @@session.sql_mode=ANSI_QUOTES` is written, it clears all other MySQL defaults, namely `STRICT_TRANS_TABLES`. When inserting a partition with a value > 256 characters, it is successfully inserted when", "qna_task": "Question: What is this issue about?\nAnswer: DirectSQL disables strict checking in MySQL, allowing undroppable partitions"}}
{"id": "13629363", "key": "HIVE-29215", "project": "HIVE", "summary": "Owner info for view is incorrect for alter view in the authorization events", "description": "Alter view query is sending in the wrong information regarding the 'owner', which can lead to incorrect authorization. More details to be added.", "comments": "Hi [~hemanth619] , please set the affected version(s) and repro steps.", "created": "2025-09-18T17:46:12.000+0000", "updated": "2025-10-31T16:22:49.000+0000", "derived": {"summary_task": "Summarize this issue: Alter view query is sending in the wrong information regarding the 'owner', which can lead to incorrect authorization. More details to be added.", "classification_task": "Classify the issue priority and type: Alter view query is sending in the wrong information regarding the 'owner', which can lead to incorrect authorization. More details to be added.", "qna_task": "Question: What is this issue about?\nAnswer: Owner info for view is incorrect for alter view in the authorization events"}}
{"id": "13629328", "key": "HIVE-29214", "project": "HIVE", "summary": "SELECT * from view failing with SemanticException when we created with column aliases.", "description": "SELECT * from view failing with SemanticException when we created with column aliases. Reproduction steps: {noformat} CREATE TABLE t0(c0 bigint ); INSERT INTO t0 VALUES(10),(20),(30); CREATE VIEW v0(col1) AS (SELECT * FROM t0); SELECT * FROM v0;{noformat} Exception Stacktrace: {noformat} org.apache.hadoop.hive.ql.parse.SemanticException: line 1:66 cannot recognize input near ')' 'v0' '<EOF>' in subquery source in definition of VIEW v0 [ SELECT `c0` AS `col1` FROM ((select `t0`.`c0` from `default`.`t0`)) `v0` ] used as v0 at Line 2:14 at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.replaceViewReferenceWithDefinition(SemanticAnalyzer.java:2909) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:2438) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:2328) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeAndResolveChildTree(SemanticAnalyzer.java:13052) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genResolvedParseTree(SemanticAnalyzer.java:13029) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:13159) at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:481) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:360) at org.apache.hadoop.hive.ql.Compiler.analyze(Compiler.java:224) at org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:109) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:498) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:450) at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:414) at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:408) at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126) at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:234) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259) at org.apache.hadoop.hive.cli.CliDriver.processCmd1(CliDriver.java:203) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:129) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:427) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:358) at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:746) at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:716) at org.apache.hadoop.hive.cli.control.CoreCliDriver.runTest(CoreCliDriver.java:115) at org.apache.hadoop.hive.cli.control.CliAdapter.runTest(CliAdapter.java:139) at org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver(TestMiniLlapLocalCliDriver.java:62) at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) at java.base/java.lang.reflect.Method.invoke(Method.java:580) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.apache.hadoop.hive.cli.control.CliAdapter$2$1.evaluate(CliAdapter.java:118) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.apache.hadoop.hive.cli.control.CliAdapter$1$1.evaluate(CliAdapter.java:89) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:316) at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:240) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:214) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:155) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:385) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162) at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:507) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:495) Caused by: org.apache.hadoop.hive.ql.parse.ParseException: line 1:66 cannot recognize input near ')' 'v0' '<EOF>' in subquery source at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:125) at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:101) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.replaceViewReferenceWithDefinition(SemanticAnalyzer.java:2879) ... 62 more Caused by: NoViableAltException(431@[]) at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.identifier(HiveParser_IdentifiersParser.java:15227) at org.apache.hadoop.hive.ql.parse.HiveParser.identifier(HiveParser.java:43054) at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.subQuerySource(HiveParser_FromClauseParser.java:5422) at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.atomjoinSource(HiveParser_FromClauseParser.java:1932) at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.joinSource(HiveParser_FromClauseParser.java:2186) at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.atomjoinSource(HiveParser_FromClauseParser.java:2121) at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.joinSource(HiveParser_FromClauseParser.java:2186) at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.fromSource(HiveParser_FromClauseParser.java:1761) at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.fromClause(HiveParser_FromClauseParser.java:1604) at org.apache.hadoop.hive.ql.parse.HiveParser.fromClause(HiveParser.java:43312) at org.apache.hadoop.hive.ql.parse.HiveParser.atomSelectStatement(HiveParser.java:36792) at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:37085) at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:36678) at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:35940) at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:35828) at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:2776) at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1655) at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:123) ... 64 more{noformat}", "comments": "Already an issue HIVE-26493 open for the same so its a duplicate issue.", "created": "2025-09-18T13:24:40.000+0000", "updated": "2025-09-20T17:53:08.000+0000", "derived": {"summary_task": "Summarize this issue: SELECT * from view failing with SemanticException when we created with column aliases. Reproduction steps: {noformat} CREATE TABLE t0(c0 bigint ); INSERT INTO t0 VALUES(10),(20),(30); CREATE VIEW v0(col1) AS (SELECT * FROM t0); SELECT * FROM v0;{noformat} Exception Stacktrace: {noformat} org.apache.hadoop.hive.ql.parse.SemanticException: line 1:66 cannot recognize input near ')' 'v0' '<EOF>' in su", "classification_task": "Classify the issue priority and type: SELECT * from view failing with SemanticException when we created with column aliases. Reproduction steps: {noformat} CREATE TABLE t0(c0 bigint ); INSERT INTO t0 VALUES(10),(20),(30); CREATE VIEW v0(col1) AS (SELECT * FROM t0); SELECT * FROM v0;{noformat} Exception Stacktrace: {noformat} org.apache.hadoop.hive.ql.parse.SemanticException: line 1:66 cannot recognize input near ')' 'v0' '<EOF>' in su", "qna_task": "Question: What is this issue about?\nAnswer: SELECT * from view failing with SemanticException when we created with column aliases."}}
{"id": "13629296", "key": "HIVE-29213", "project": "HIVE", "summary": "HS2 report 'Failed to get primary keys' if using old beeline client", "description": "HIVE-19996 fixed a performance issue related to beeline. However, this fix causes a warning exception \"Failed to get primary keys\" to appear in the HS2 logs when using a beeline client that does not include this patch (such as Hive 4.1.0) to submit SQL queries to Hive 4.2.0. Here is a simple test procedure: # Use the Hive 4.1.0 beeline client to connect to the latest version of Hive 4.2.0 (which includes HIVE-19996). // Create a test table create table testdb.test12(id int); {color:#de350b}*// Query this table, and you will notice the following exception: Failed to get primary keys ..*{color} select * from testdb.test12; {code:java} 2025-09-18T16:22:34,121 ERROR [HiveServer2-Handler-Pool: Thread-166] thrift.ThriftCLIService: Failed to get primary keys [request: TGetPrimaryKeysReq(sessionHandle:TSessionHandle(sessionId:THandleIdentifier(guid:56 22 37 10 00 2D 4B BB B6 FE 42 49 E2 25 75 1F, secret:C7 E6 BF 96 D1 82 4E F5 82 36 AC A7 A1 3E 7A A6)), catalogName:, tableName:test12)] org.apache.hive.service.cli.HiveSQLException: org.apache.thrift.protocol.TProtocolException: Required field 'db_name' is unset! Struct:PrimaryKeysRequest(db_name:null, tbl_name:test12, catName:hive) at org.apache.hive.service.cli.operation.GetPrimaryKeysOperation.runInternal(GetPrimaryKeysOperation.java:120) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.cli.operation.Operation.run(Operation.java:286) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.cli.session.HiveSessionImpl.getPrimaryKeys(HiveSessionImpl.java:998) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) ~[?:?] at java.base/java.lang.reflect.Method.invoke(Method.java:580) ~[?:?] at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at java.base/java.security.AccessController.doPrivileged(AccessController.java:714) ~[?:?] at java.base/javax.security.auth.Subject.doAs(Subject.java:525) ~[?:?] at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953) ~[hadoop-common-3.4.1.jar:?] at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at jdk.proxy2/jdk.proxy2.$Proxy41.getPrimaryKeys(Unknown Source) ~[?:?] at org.apache.hive.service.cli.CLIService.getPrimaryKeys(CLIService.java:416) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.cli.thrift.ThriftCLIService.GetPrimaryKeys(ThriftCLIService.java:919) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetPrimaryKeys.getResult(TCLIService.java:1870) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetPrimaryKeys.getResult(TCLIService.java:1850) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:250) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?] at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?] at java.base/java.lang.Thread.run(Thread.java:1583) [?:?] Caused by: org.apache.thrift.protocol.TProtocolException: Required field 'db_name' is unset! Struct:PrimaryKeysRequest(db_name:null, tbl_name:test12, catName:hive) at org.apache.hadoop.hive.metastore.api.PrimaryKeysRequest.validate(PrimaryKeysRequest.java:591) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_primary_keys_args.validate(ThriftHiveMetastore.java) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_primary_keys_args$get_primary_keys_argsStandardScheme.write(ThriftHiveMetastore.java) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_primary_keys_args$get_primary_keys_argsStandardScheme.write(ThriftHiveMetastore.java) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_primary_keys_args.write(ThriftHiveMetastore.java) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:71) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:62) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.send_get_primary_keys(ThriftHiveMetastore.java:4926) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_primary_keys(ThriftHiveMetastore.java:4918) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.client.ThriftHiveMetaStoreClient.getPrimaryKeys(ThriftHiveMetaStoreClient.java:2393) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.client.MetaStoreClientWrapper.getPrimaryKeys(MetaStoreClientWrapper.java:1081) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] {code} *{color:#de350b}// show tables\uff0c then you will find new exceptions\uff1aclient.ThriftHiveMetaStoreClient: Got error flushing the cache org.apache.thrift.TApplicationException: Unrecognized type -128{color}* show tables; {code:java} 2025-09-18T16:22:39,771 INFO [56223710-002d-4bbb-b6fe-4249e225751f HiveServer2-Handler-Pool: Thread-166] ql.Driver: Compiling command(queryId=hive_20250918162239_69ae0c6e-309f-4b87-a2c4-8093ae4700e8): show tables 2025-09-18T16:22:39,771 WARN [56223710-002d-4bbb-b6fe-4249e225751f HiveServer2-Handler-Pool: Thread-166] client.ThriftHiveMetaStoreClient: Got error flushing the cache org.apache.thrift.TApplicationException: Unrecognized type -128 at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:79) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_flushCache(ThriftHiveMetastore.java:7493) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.flushCache(ThriftHiveMetastore.java:7481) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.client.ThriftHiveMetaStoreClient.flushCache(ThriftHiveMetaStoreClient.java:2541) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.client.MetaStoreClientWrapper.flushCache(MetaStoreClientWrapper.java:1043) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.client.MetaStoreClientWrapper.flushCache(MetaStoreClientWrapper.java:1043) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.client.MetaStoreClientWrapper.flushCache(MetaStoreClientWrapper.java:1043) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) ~[?:?] at java.base/java.lang.reflect.Method.invoke(Method.java:580) ~[?:?] at org.apache.hadoop.hive.metastore.client.SynchronizedMetaStoreClient$SynchronizedHandler.invoke(SynchronizedMetaStoreClient.java:69) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at jdk.proxy2/jdk.proxy2.$Proxy32.flushCache(Unknown Source) ~[?:?] at org.apache.hadoop.hive.metastore.client.MetaStoreClientWrapper.flushCache(MetaStoreClientWrapper.java:1043) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) ~[?:?] at java.base/java.lang.reflect.Method.invoke(Method.java:580) ~[?:?] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:232) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at jdk.proxy2/jdk.proxy2.$Proxy32.flushCache(Unknown Source) ~[?:?] at org.apache.hadoop.hive.ql.Compiler.analyze(Compiler.java:198) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:109) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:498) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:450) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:414) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:408) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:205) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:268) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.cli.operation.Operation.run(Operation.java:286) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:558) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:543) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) ~[?:?] at java.base/java.lang.reflect.Method.invoke(Method.java:580) ~[?:?] at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at java.base/java.security.AccessController.doPrivileged(AccessController.java:714) ~[?:?] at java.base/javax.security.auth.Subject.doAs(Subject.java:525) ~[?:?] at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953) ~[hadoop-common-3.4.1.jar:?] at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at jdk.proxy2/jdk.proxy2.$Proxy41.executeStatementAsync(Unknown Source) ~[?:?] at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:315) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:652) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1670) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1650) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56) ~[hive-service-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:250) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?] at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?] at java.base/java.lang.Thread.run(Thread.java:1583) [?:?] 2025-09-18T16:22:39,773 WARN [56223710-002d-4bbb-b6fe-4249e225751f HiveServer2-Handler-Pool: Thread-166] metastore.RetryingMetaStoreClient: MetaStoreClient lost connection. Attempting to reconnect (1 of 1) after 1s. getDatabase org.apache.thrift.transport.TTransportException: Socket is closed by peer. at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:184) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.thrift.transport.TTransport.readAll(TTransport.java:109) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:464) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:362) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:245) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_database_req(ThriftHiveMetastore.java:1497) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_database_req(ThriftHiveMetastore.java:1484) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.client.ThriftHiveMetaStoreClient.getDatabase(ThriftHiveMetaStoreClient.java:1979) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.client.MetaStoreClientWrapper.getDatabase(MetaStoreClientWrapper.java:211) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getDatabase(SessionHiveMetaStoreClient.java:2281) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.client.MetaStoreClientWrapper.getDatabase(MetaStoreClientWrapper.java:211) ~[hive-exec-4.2.0-SNAPSHOT.jar:4.2.0-SNAPSHOT] at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) ~[?:?] {code} {color:#172b4d}HIVE-19996 may have broken the backward compatibility of beeline. Accessing Hive 4.2 through the Hive 4.1 beeline client should theoretically not present compatibility issues. We need to ensure backward compatibility as much as possible to provide users with a smooth experience.{color}", "comments": "[~zhangbutao] is that a backward incompatibility issue? older beeline clients are unable to perform any SELECT operations? cc [~InvisibleProgrammer] {code:java} is that a backward incompatibility issue? older beeline clients are unable to perform any SELECT operations?{code} [~dkuzmenko] I think this is a backward incompatibility issue. For older beeline clients(4.1.0), while they can perform SELECT query operations, the query latency is significantly higher compared to new beeline clients(4.2.0). For example, when executing a query like `SELECT * FROM table` using `FETCH TASK` (with `set hive.fetch.task.conversion=more`), the old client takes over 1 second to return results, and error messages appear in the HS2 logs. In contrast, the new client only requires 0.1 seconds to return results. When using the old client, this creates a misconception for users that Hive is experiencing performance issues. Regarding the two exceptions reported above, is it confirmed that both are caused by HIVE-19996? Does reverting HIVE-19996 make all errors go away? Generally mixing older vs newer binaries is not something really recommended and its not officially supported either. To exaggerate a bit someone may come and say that Beeline 0.11.0 cannot connect to HS2 4.2.0 but I don't think we would consider this a big deal. In a perfect world, everything should be backward and forward compatible but this is rarely the case in most software releases. Anyways, if somebody sees an easy way to fix the issue reported here then we can definitely do it and include it in the release but personally I wouldn't consider this a release blocker. Introducing backward incompatibility in MINOR releases doesn\u2019t seem appropriate Hi, I tried to reproduce the issue but couldn't: * Picked up a 4.1.0 beeline client * Built the current master * ran minihs2 cluter * ran the scripts provided (as an extra, created the database testdb) Got no error at all. It worked fine, no errors. {noformat} Connecting to jdbc:hive2://localhost:10000/ Connected to: Apache Hive (version 4.2.0-SNAPSHOT) Driver: Hive JDBC (version 4.1.0) Transaction isolation: TRANSACTION_REPEATABLE_READ Beeline version 4.1.0 by Apache Hive{noformat} [~InvisibleProgrammer] I did not use the minihs2 cluster mode; I used the standard local deployment mode, with the backend metadata stored in MySQL. I'm not sure if the deployment differences are causing this. But I have tested many times and this issue persists. [~zabetak] You make a valid point. It seems we never explicitly promised that older beeline clients would be fully compatible with newer versions of the HS2 cluster. However, I believe that in practice, many users will still use older beeline clients to access newer HS2 versions, as users generally assume that beeline clients offer compatibility guarantees\u2014similar to how MySQL clients can typically connect to different versions of MySQL servers. Therefore, I think we should strive to ensure that beeline clients can access different versions of HS2 as compatibly as possible, in order to provide a better user experience. Sorry, this issue might be related to my mixed use of HMS metadata from versions 4.1.0 and 4.2.0. Since I assumed that no new metadata fields were added in the recent 4.2.0 release, I deployed both 4.1.0 and 4.2.0 clusters sharing a single metadata database (version 4.2.0). This might have caused some anomalies. Therefore, you folks can temporarily set this issue aside and i will continue investigating my local metadata configuration. If it\u2019s confirmed to be solely due to the mixed use of metadata, I will update and close this ticket. Additionally, this ticket does not block the release of version 4.2.0, so you can skip this issue for now. Thank you. I have deployed Hive 4.0.1, 4.1.0, and 4.2.0 locally, each using its own HMS metadata to avoid issues caused by mixed metadata usage. After debugging the beeline/jdbc modules of these three versions, I finally identified the root cause of this issue. Prior to version 4.1.0, the *Rows::isPrimaryKey* method would call `{*}HiveResultSetMetaData::getTableName{*}` to retrieve the table name: [https://github.com/apache/hive/blob/2d1405e7feed176aeed337581292b8438cf13326/beeline/src/java/org/apache/hive/beeline/Rows.java#L86] However, before version 4.1.0, the `HiveResultSetMetaData::getTableName` method was not implemented: [https://github.com/apache/hive/blob/2d1405e7feed176aeed337581292b8438cf13326/jdbc/src/java/org/apache/hive/jdbc/HiveResultSetMetaData.java#L102] As a result, the `{*}Rows::isPrimaryKey{*}` method would catch an exception and terminate, without sending a Thrift RPC request to call `{*}HiveDatabaseMetaData::getPrimaryKeys(String catalog, String schema, String table)`{*} to retrieve primary key information related to the table. In other words, before version 4.1.0, the beeline client never actually executed the `HiveDatabaseMetaData::getPrimaryKeys` method. {color:#de350b}It is worth noting that HIVE-19996 mentions \"Beeline performance poor with drivers having slow DatabaseMetaData.getPrimaryKeys impl.\" I believe this issue should not have existed before version 4.1.0, as I mentioned above{*},{*} the beeline client never actually executed the `HiveDatabaseMetaData::getPrimaryKeys` method.{color} In version 4.1.0, HIVE-27887 implemented `{*}HiveResultSetMetaData::getTableName{*}`: [https://github.com/apache/hive/blob/75e40b7537c91a70ccaa31c397d21823c7528eeb/jdbc/src/java/org/apache/hive/jdbc/HiveResultSetMetaData.java#L113] Thus, the `{*}Rows::isPrimaryKey{*}` method executed normally and sent a Thrift RPC request to call `{*}HiveDatabaseMetaData::getPrimaryKeys(String catalog, String schema, String table){*}` to retrieve primary key information related to the table: [https://github.com/apache/hive/blob/75e40b7537c91a70ccaa31c397d21823c7528eeb/beeline/src/java/org/apache/hive/beeline/Rows.java#L92] However, in the Beeline module, there is currently no way to retrieve the catalog and schema. The catalog is an empty string, and the schema (i.e., db_name) is null. A null schema (db_name) causes the RPC method call to throw an exception because the Thrift definition prohibits db_name from being null: [https://github.com/apache/hive/blob/75e40b7537c91a70ccaa31c397d21823c7528eeb/standalone-metastore/metastore-common/src/main/thrift/hive_metastore.thrift#L754] The error message is as follows: {code:java} 2025-09-18T16:22:34,121 ERROR [HiveServer2-Handler-Pool: Thread-166] thrift.ThriftCLIService: Failed to get primary keys [request: TGetPrimaryKeysReq(sessionHandle:TSessionHandle(sessionId:THandleIdentifier(guid:56 22 37 10 00 2D 4B BB B6 FE 42 49 E2 25 75 1F, secret:C7 E6 BF 96 D1 82 4E F5 82 36 AC A7 A1 3E 7A A6)), catalogName:, tableName:test12)] org.apache.hive.service.cli.HiveSQLException: org.apache.thrift.protocol.TProtocolException: Required field 'db_name' is unset! Struct:PrimaryKeysRequest(db_name:null, tbl_name:test12, catName:hive){code} *Therefore, in version 4.1.0, HIVE-27887 inadvertently triggered the execution of `HiveDatabaseMetaData::getPrimaryKeys`, leading to this exception.* Then, in version 4.2.0, HIVE-29118 optimized the method call of `{*}Rows::isPrimaryKey{*}`, ensuring that `{*}Rows::isPrimaryKey{*}` and `{*}HiveDatabaseMetaData::getPrimaryKeys{*}` are only triggered when the `--color` option is specified, i.e.: {code:java} ./apache-hive-4.2.0-bin/bin/beeline -u jdbc:hive2://127.0.0.1:10000/default hive --color=true{code} {color:#de350b}*Thus, in version 4.2.0, if you use `beeline --color` to execute a SELECT query, the same exception will occur.*{color} In summary, before version 4.1.0 (HIVE-27887), we never actually retrieved the primary key of a table on the beeline side. After version 4.1.0 (HIVE-27887), since we cannot retrieve the actual db_name and catalog on the beeline side, the call to `{*}HiveDatabaseMetaData::getPrimaryKeys(String catalog, String schema, String table){*}` throws an exception. I believe retrieving the primary key of a table on the beeline side is meaningless (the original purpose was only for coloring the beeline output). {color:#de350b}*We do not need to investigate how to retrieve the correct db_name to ensure the method executes properly. I believe removing this code logic would suffice.*{color} Let me continue with some additional points. I just noticed that it's not only Hive that uses the Beeline client; other components like Impala and HBase also utilize the Beeline component. Therefore, the relevant primary key code in Beeline\u2014specifically `DatabaseMetaData.getPrimaryKeys`\u2014though never used by Hive prior to Hive 4.1.0, might have been consistently used by HBase. This could very well be the source of the issue reported in HIVE-19996. This makes it quite interesting: the Beeline codes related to primary key in Hive that has never been utilized by Hive itself but is actively used by other components. Nevertheless, I still recommend removing the usage of `DatabaseMetaData.getPrimaryKeys` in Beeline. First, fixing this exception on the Hive side is neither easy nor meaningful; Second, as mentioned above, the purpose of retrieving primary keys in Beeline is solely for coloring the output results. Removing this code would not cause any compatibility issues and would actually improve Beeline's performance, as it avoids the heavyweight Thrift RPC call to the server for fetching primary keys. Created a new ticket HIVE-29296 to remove the getPrimaryKeys codes from beeline module Hi, Thank you for bringing up this issue. Firstly, I totally agree with removing that functionality from Beeline entirely. I suggested the same when we met with the performance problem here: [https://lists.apache.org/thread/mk7b51bwcd53k5jcdsyyyvbrogsqm2bw] I hope https://issues.apache.org/jira/browse/HIVE-29296 will pass through the review. Secondly, I'm not arguing, just want to get a clear picture of the requirements of backward compatibility: If I understood the situation correctly, a change made just today, can break up the functionality with a version delivered yesterday. In software development, it is pretty standard. I know Beeline can be a special case, as it could be a standalone software, delivered independently from Hive. My question is: do we have some boundaries about how many releases Beeline should support backward? Other question: what is your opinion, does it make sense to detach Beeline on the repository level from Hive? If we don't only use it for Hive, and a given Beeline release has to support multiple versions of Hive, HBase, Impala, etc, maybe it is easier to do development and testing if it can be compiled without Hive.", "created": "2025-09-18T08:58:24.000+0000", "updated": "2025-10-31T10:24:59.000+0000", "derived": {"summary_task": "Summarize this issue: HIVE-19996 fixed a performance issue related to beeline. However, this fix causes a warning exception \"Failed to get primary keys\" to appear in the HS2 logs when using a beeline client that does not include this patch (such as Hive 4.1.0) to submit SQL queries to Hive 4.2.0. Here is a simple test procedure: # Use the Hive 4.1.0 beeline client to connect to the latest version of Hive 4.2.0 (which i", "classification_task": "Classify the issue priority and type: HIVE-19996 fixed a performance issue related to beeline. However, this fix causes a warning exception \"Failed to get primary keys\" to appear in the HS2 logs when using a beeline client that does not include this patch (such as Hive 4.1.0) to submit SQL queries to Hive 4.2.0. Here is a simple test procedure: # Use the Hive 4.1.0 beeline client to connect to the latest version of Hive 4.2.0 (which i", "qna_task": "Question: What is this issue about?\nAnswer: HS2 report 'Failed to get primary keys' if using old beeline client"}}
{"id": "13629248", "key": "HIVE-29211", "project": "HIVE", "summary": "Add LDAP group filtering support for Kerberos-authenticated users", "description": "Currently, HS2 and HMS support LDAP authentication with group filtering, but when users authenticate via Kerberos, LDAP group filters are not applied. This creates an inconsistency where authorization policies differ based on the authentication method used. We need to add the capability to optionally enforce LDAP group membership checks for Kerberos-authenticated users in both HS2 and HMS.", "comments": "[~hazeljiang] - Thanks for the contribution. The patch has been merged into the master branch.", "created": "2025-09-17T20:08:23.000+0000", "updated": "2025-09-26T19:15:41.000+0000", "derived": {"summary_task": "Summarize this issue: Currently, HS2 and HMS support LDAP authentication with group filtering, but when users authenticate via Kerberos, LDAP group filters are not applied. This creates an inconsistency where authorization policies differ based on the authentication method used. We need to add the capability to optionally enforce LDAP group membership checks for Kerberos-authenticated users in both HS2 and HMS.", "classification_task": "Classify the issue priority and type: Currently, HS2 and HMS support LDAP authentication with group filtering, but when users authenticate via Kerberos, LDAP group filters are not applied. This creates an inconsistency where authorization policies differ based on the authentication method used. We need to add the capability to optionally enforce LDAP group membership checks for Kerberos-authenticated users in both HS2 and HMS.", "qna_task": "Question: What is this issue about?\nAnswer:  Add LDAP group filtering support for Kerberos-authenticated users"}}
{"id": "13629232", "key": "HIVE-29210", "project": "HIVE", "summary": "Minor compaction produces duplicates conditionally in case of HMS instance running initiator crash", "description": "In a case, with multiple HiveServer2 (HS2) instances, one of the HS2 instances may run on the same host as the Hive Metastore (HMS). In this setup, the initiator runs within HMS, while the compaction worker threads run within HS2. If the HMS instance unexpectedly crashes, the method revokeFromLocalWorkers() is invoked. This method resets all compaction jobs back to the initiated state, provided they were running on the same host. We believe this behavior is by design: if both HMS and HS2(running workers) were to crash simultaneously, and jobs were not reset, those compactions could remain stalled until revokeTimedoutWorkers() eventually reclaims them. However, in the case where HMS crashes but the HS2 instance survives, the reset still occurs. As a result, the job is made available for reassignment even though the original HS2 worker is still actively processing it. This can lead to a scenario where another HS2 worker picks up the same compaction task, causing two workers to run the same minor compaction job concurrently. This race condition can intermittently result in duplicate records being written to the table.", "comments": "Thanks a lot [~tanishqchugh] for the fix.", "created": "2025-09-17T17:13:26.000+0000", "updated": "2025-10-09T11:46:34.000+0000", "derived": {"summary_task": "Summarize this issue: In a case, with multiple HiveServer2 (HS2) instances, one of the HS2 instances may run on the same host as the Hive Metastore (HMS). In this setup, the initiator runs within HMS, while the compaction worker threads run within HS2. If the HMS instance unexpectedly crashes, the method revokeFromLocalWorkers() is invoked. This method resets all compaction jobs back to the initiated state, provided th", "classification_task": "Classify the issue priority and type: In a case, with multiple HiveServer2 (HS2) instances, one of the HS2 instances may run on the same host as the Hive Metastore (HMS). In this setup, the initiator runs within HMS, while the compaction worker threads run within HS2. If the HMS instance unexpectedly crashes, the method revokeFromLocalWorkers() is invoked. This method resets all compaction jobs back to the initiated state, provided th", "qna_task": "Question: What is this issue about?\nAnswer: Minor compaction produces duplicates conditionally in case of HMS instance running initiator crash"}}
{"id": "13629231", "key": "HIVE-29209", "project": "HIVE", "summary": "Remove unnecessary usage of LoginException", "description": "{{LoginException}} is not necessary in many classes, we can clean it.", "comments": "Committed to master. Thanx [~wechar] for the contribution & [~InvisibleProgrammer] for the review!!!", "created": "2025-09-17T17:10:33.000+0000", "updated": "2025-09-27T16:57:48.000+0000", "derived": {"summary_task": "Summarize this issue: {{LoginException}} is not necessary in many classes, we can clean it.", "classification_task": "Classify the issue priority and type: {{LoginException}} is not necessary in many classes, we can clean it.", "qna_task": "Question: What is this issue about?\nAnswer: Remove unnecessary usage of LoginException"}}
{"id": "13629205", "key": "HIVE-29208", "project": "HIVE", "summary": "Infinite loop while compiling query with filter predicate containing disjuncts on the same expression", "description": "Repro {code} CREATE TABLE t1 (Date_ STRING); SELECT * FROM t1 WHERE ( ( MINUTE(`date_`) = 2 OR MINUTE(`date_`) = 10 ) OR (MINUTE(`date_`) IS NULL) ); {code} The expression {{MINUTE(`date_`)}} is compared in each disjunct hence search operator is not used", "comments": "Merged to master. Thanks [~zabetak], [~dkuzmenko], [~Dayakar] for the review.", "created": "2025-09-17T13:16:43.000+0000", "updated": "2025-09-26T09:16:16.000+0000", "derived": {"summary_task": "Summarize this issue: Repro {code} CREATE TABLE t1 (Date_ STRING); SELECT * FROM t1 WHERE ( ( MINUTE(`date_`) = 2 OR MINUTE(`date_`) = 10 ) OR (MINUTE(`date_`) IS NULL) ); {code} The expression {{MINUTE(`date_`)}} is compared in each disjunct hence search operator is not used", "classification_task": "Classify the issue priority and type: Repro {code} CREATE TABLE t1 (Date_ STRING); SELECT * FROM t1 WHERE ( ( MINUTE(`date_`) = 2 OR MINUTE(`date_`) = 10 ) OR (MINUTE(`date_`) IS NULL) ); {code} The expression {{MINUTE(`date_`)}} is compared in each disjunct hence search operator is not used", "qna_task": "Question: What is this issue about?\nAnswer: Infinite loop while compiling query with filter predicate containing disjuncts on the same expression"}}
{"id": "13629133", "key": "HIVE-29207", "project": "HIVE", "summary": "Remove check-spelling CI action", "description": "The check-spelling action reports many false positives that require additional work and extra commits to address. Addressing the errors requires additional work from contributors and extra resources from CI since all tests are triggered again on new commits. Occasionally it also detects valid typos but at this stage the negatives outweigh the positives.", "comments": "The removal of the action is discussed on the mailing list: [https://lists.apache.org/thread/bb2ncb5ytk50j73fcwzw0wbdsblkw9x3] [https://lists.apache.org/thread/n7k808bkxtclww95fhgkznfsol32f0mn] Fixed in https://github.com/apache/hive/commit/f374f39ec7205d98ad5605be6890564fc58eea0c", "created": "2025-09-17T08:25:19.000+0000", "updated": "2025-09-18T07:16:31.000+0000", "derived": {"summary_task": "Summarize this issue: The check-spelling action reports many false positives that require additional work and extra commits to address. Addressing the errors requires additional work from contributors and extra resources from CI since all tests are triggered again on new commits. Occasionally it also detects valid typos but at this stage the negatives outweigh the positives.", "classification_task": "Classify the issue priority and type: The check-spelling action reports many false positives that require additional work and extra commits to address. Addressing the errors requires additional work from contributors and extra resources from CI since all tests are triggered again on new commits. Occasionally it also detects valid typos but at this stage the negatives outweigh the positives.", "qna_task": "Question: What is this issue about?\nAnswer: Remove check-spelling CI action"}}
{"id": "13629104", "key": "HIVE-29206", "project": "HIVE", "summary": "Remove commons-lang from hive-exec shaded jar", "description": "[HIVE-7145|https://issues.apache.org/jira/browse/HIVE-7145] and [HIVE-22653|https://issues.apache.org/jira/browse/HIVE-22653] removed commons-lang from the dependencies. However, it is being included in hive-exec shaded jar and needs to be removed.", "comments": "", "created": "2025-09-17T05:08:26.000+0000", "updated": "2025-09-22T23:57:37.000+0000", "derived": {"summary_task": "Summarize this issue: [HIVE-7145|https://issues.apache.org/jira/browse/HIVE-7145] and [HIVE-22653|https://issues.apache.org/jira/browse/HIVE-22653] removed commons-lang from the dependencies. However, it is being included in hive-exec shaded jar and needs to be removed.", "classification_task": "Classify the issue priority and type: [HIVE-7145|https://issues.apache.org/jira/browse/HIVE-7145] and [HIVE-22653|https://issues.apache.org/jira/browse/HIVE-22653] removed commons-lang from the dependencies. However, it is being included in hive-exec shaded jar and needs to be removed.", "qna_task": "Question: What is this issue about?\nAnswer: Remove commons-lang from hive-exec shaded jar"}}
{"id": "13629094", "key": "HIVE-29205", "project": "HIVE", "summary": "Iceberg: Upgrade iceberg version to 1.10.1", "description": "", "comments": "", "created": "2025-09-16T23:44:37.000+0000", "updated": "2025-09-22T17:46:38.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Iceberg: Upgrade iceberg version to 1.10.1"}}
{"id": "13629028", "key": "HIVE-29204", "project": "HIVE", "summary": "Hive-site: cleanup attachments and links to attachments", "description": "Some links to attachments lead to a 404 Not found, e.g. [attachments/40509928/42696874-txt|https://hive.apache.org/attachments/40509928/42696874-txt] in [SQL Standard Based Hive Authorization|https://hive.apache.org/docs/latest/language/sql-standard-based-hive-authorization/#hive-013]. Some link texts replace the dot with a dash (e.g., content/community/resources/presentations.md). In general, it would be better to use the title of the document instead of numbers as file name and link text. {code:java} 50:* [attachments/27362054/35193149-pptx](/attachments/27362054/35193149.pptx) (Ashutosh Chauhan){code} A few shell commands that might be helpful: {code:java} find themes/hive/static/attachments -type f | sed 's#themes/hive/static/##' | sort -u > available-attachments.txt rg \"attachments/\" | sed 's#attachments/#\\nattachments/#g;' | grep '^attachments' | sed 's/\\([?\"<> )]\\|\\]\\).*//' | sort -u > needed-attachments.txt {code} There are also some duplicate files: {code:java} $ cat available-attachments.txt| sed 's#^#themes/hive/static/#' | xargs md5sum | sort ... f9f26fe37b0c5276d0b63f98e1188324 themes/hive/static/attachments/27362075/34177489.pdf f9f26fe37b0c5276d0b63f98e1188324 themes/hive/static/attachments/27362075/34177517.pdf f9f26fe37b0c5276d0b63f98e1188324 themes/hive/static/attachments/27362075/35193010.pdf f9f26fe37b0c5276d0b63f98e1188324 themes/hive/static/attachments/27362075/35193011.pdf ... {code}", "comments": "", "created": "2025-09-16T10:57:14.000+0000", "updated": "2025-09-16T10:57:14.000+0000", "derived": {"summary_task": "Summarize this issue: Some links to attachments lead to a 404 Not found, e.g. [attachments/40509928/42696874-txt|https://hive.apache.org/attachments/40509928/42696874-txt] in [SQL Standard Based Hive Authorization|https://hive.apache.org/docs/latest/language/sql-standard-based-hive-authorization/#hive-013]. Some link texts replace the dot with a dash (e.g., content/community/resources/presentations.md). In general, it ", "classification_task": "Classify the issue priority and type: Some links to attachments lead to a 404 Not found, e.g. [attachments/40509928/42696874-txt|https://hive.apache.org/attachments/40509928/42696874-txt] in [SQL Standard Based Hive Authorization|https://hive.apache.org/docs/latest/language/sql-standard-based-hive-authorization/#hive-013]. Some link texts replace the dot with a dash (e.g., content/community/resources/presentations.md). In general, it ", "qna_task": "Question: What is this issue about?\nAnswer: Hive-site: cleanup attachments and links to attachments"}}
{"id": "13629016", "key": "HIVE-29203", "project": "HIVE", "summary": "get_aggr_stats_for doesn't aggregate stats when direct sql batch retrieve is enabled", "description": "In case of metastore.direct.sql.batch.size > 0, and number of partition names or columns in get_aggr_stats_for is bigger than the metastore.direct.sql.batch.size, then the AggrStats from the call get_aggr_stats_for might have un-merged stats for the same column, so the aggregated stats is not correct, which may make CBO generate an outdated execution plan.", "comments": "If we remove the batch processing from [https://github.com/apache/hive/blob/4bb08099d91acbefee73a449a36abb1ecd2b5925/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java#L1882-L1891,] perhaps we need to consider the memory usage when enableBitVector || enableKll, and the restrictions on IN list size in aggrStatsUseDB. [~ramitg254] please set the affected version", "created": "2025-09-16T08:28:59.000+0000", "updated": "2025-10-28T04:47:37.000+0000", "derived": {"summary_task": "Summarize this issue: In case of metastore.direct.sql.batch.size > 0, and number of partition names or columns in get_aggr_stats_for is bigger than the metastore.direct.sql.batch.size, then the AggrStats from the call get_aggr_stats_for might have un-merged stats for the same column, so the aggregated stats is not correct, which may make CBO generate an outdated execution plan.", "classification_task": "Classify the issue priority and type: In case of metastore.direct.sql.batch.size > 0, and number of partition names or columns in get_aggr_stats_for is bigger than the metastore.direct.sql.batch.size, then the AggrStats from the call get_aggr_stats_for might have un-merged stats for the same column, so the aggregated stats is not correct, which may make CBO generate an outdated execution plan.", "qna_task": "Question: What is this issue about?\nAnswer: get_aggr_stats_for doesn't aggregate stats when direct sql batch retrieve is enabled"}}
{"id": "13628986", "key": "HIVE-29202", "project": "HIVE", "summary": "Add HiveAuthzContext support to HiveMetaStoreAuthorizableEvent for enhanced authorization", "description": "Currently, HiveMetaStoreAuthorizableEvent and its implementations (like ReadDatabaseEvent) don't pass through the full HiveAuthzContext when creating HiveMetaStoreAuthzInfo. This limits the ability of custom authorization plugins to access contextual information during authorization decisions.", "comments": "", "created": "2025-09-15T20:39:19.000+0000", "updated": "2025-09-26T22:16:39.000+0000", "derived": {"summary_task": "Summarize this issue: Currently, HiveMetaStoreAuthorizableEvent and its implementations (like ReadDatabaseEvent) don't pass through the full HiveAuthzContext when creating HiveMetaStoreAuthzInfo. This limits the ability of custom authorization plugins to access contextual information during authorization decisions.", "classification_task": "Classify the issue priority and type: Currently, HiveMetaStoreAuthorizableEvent and its implementations (like ReadDatabaseEvent) don't pass through the full HiveAuthzContext when creating HiveMetaStoreAuthzInfo. This limits the ability of custom authorization plugins to access contextual information during authorization decisions.", "qna_task": "Question: What is this issue about?\nAnswer: Add HiveAuthzContext support to HiveMetaStoreAuthorizableEvent for enhanced authorization"}}
{"id": "13628948", "key": "HIVE-29201", "project": "HIVE", "summary": "Fix flaky test query_iceberg_metadata_of_unpartitioned_table.q", "description": "The test in itself uses SORT_QUERY_RESULTS to keep a deterministic ordering of queries. But there is still scope of non determinism, as SORT_QUERY_RESULTS sorts the output of each query lexicographically, and if the present masked values change, then the output ordering changes as well. Hence, we need to add explicit order by on queries.", "comments": "PR: https://github.com/apache/hive/pull/6075 The flakiness has been occurring for months affecting builds in master and PRs: * https://ci.hive.apache.org/job/hive-precommit/job/master/2565/ run on Jun 19, 2025 * https://ci.hive.apache.org/job/hive-precommit/job/PR-6092/1/ run on Sep 22, 2025", "created": "2025-09-15T13:51:08.000+0000", "updated": "2025-10-01T20:39:21.000+0000", "derived": {"summary_task": "Summarize this issue: The test in itself uses SORT_QUERY_RESULTS to keep a deterministic ordering of queries. But there is still scope of non determinism, as SORT_QUERY_RESULTS sorts the output of each query lexicographically, and if the present masked values change, then the output ordering changes as well. Hence, we need to add explicit order by on queries.", "classification_task": "Classify the issue priority and type: The test in itself uses SORT_QUERY_RESULTS to keep a deterministic ordering of queries. But there is still scope of non determinism, as SORT_QUERY_RESULTS sorts the output of each query lexicographically, and if the present masked values change, then the output ordering changes as well. Hence, we need to add explicit order by on queries.", "qna_task": "Question: What is this issue about?\nAnswer: Fix flaky test query_iceberg_metadata_of_unpartitioned_table.q"}}
{"id": "13628945", "key": "HIVE-29200", "project": "HIVE", "summary": "CI Spell checking errors in ObjectInspectorUtils and TeradataBinarySerde", "description": "The CI spell checking action [currently fails|https://github.com/apache/hive/actions/runs/17731661136/job/50383947095] in master due to the following errors {noformat} Warning: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java: line 659, columns 70-83, Warning - `bucketedtables` is not a recognized word. (unrecognized-spelling) Warning: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java: line 659, columns 51-64, Warning - `languagemanual` is not a recognized word. (unrecognized-spelling) Warning: serde/src/java/org/apache/hadoop/hive/serde2/teradata/TeradataBinarySerde.java: line 80, columns 45-63, Warning - `teradatabinaryserde` is not a recognized word. (unrecognized-spelling) {noformat}", "comments": "Fixed in https://github.com/apache/hive/commit/38236c704f8229b6a52bdbedd87f51638758d792", "created": "2025-09-15T13:17:10.000+0000", "updated": "2025-09-16T09:36:26.000+0000", "derived": {"summary_task": "Summarize this issue: The CI spell checking action [currently fails|https://github.com/apache/hive/actions/runs/17731661136/job/50383947095] in master due to the following errors {noformat} Warning: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java: line 659, columns 70-83, Warning - `bucketedtables` is not a recognized word. (unrecognized-spelling) Warning: serde/src/java/org/apach", "classification_task": "Classify the issue priority and type: The CI spell checking action [currently fails|https://github.com/apache/hive/actions/runs/17731661136/job/50383947095] in master due to the following errors {noformat} Warning: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java: line 659, columns 70-83, Warning - `bucketedtables` is not a recognized word. (unrecognized-spelling) Warning: serde/src/java/org/apach", "qna_task": "Question: What is this issue about?\nAnswer: CI Spell checking errors in ObjectInspectorUtils and TeradataBinarySerde"}}
{"id": "13633087", "key": "SPARK-54131", "project": "SPARK", "summary": "Update pandas to 2.3.3", "description": "New version, support for python 3.14 https://pandas.pydata.org/pandas-docs/version/2.3/whatsnew/v2.3.3.html", "comments": "", "created": "2025-11-01T16:54:40.000+0000", "updated": "2025-11-01T16:54:40.000+0000", "derived": {"summary_task": "Summarize this issue: New version, support for python 3.14 https://pandas.pydata.org/pandas-docs/version/2.3/whatsnew/v2.3.3.html", "classification_task": "Classify the issue priority and type: New version, support for python 3.14 https://pandas.pydata.org/pandas-docs/version/2.3/whatsnew/v2.3.3.html", "qna_task": "Question: What is this issue about?\nAnswer: Update pandas to 2.3.3"}}
{"id": "13633084", "key": "SPARK-54130", "project": "SPARK", "summary": "Add detailed error messages for catalog assertion failures", "description": "There are many instances where we simply assert without providing a valid error message. This complicates debugging. We should implement proper error messages to make debugging easier.", "comments": "", "created": "2025-11-01T14:59:49.000+0000", "updated": "2025-11-01T15:28:20.000+0000", "derived": {"summary_task": "Summarize this issue: There are many instances where we simply assert without providing a valid error message. This complicates debugging. We should implement proper error messages to make debugging easier.", "classification_task": "Classify the issue priority and type: There are many instances where we simply assert without providing a valid error message. This complicates debugging. We should implement proper error messages to make debugging easier.", "qna_task": "Question: What is this issue about?\nAnswer: Add detailed error messages for catalog assertion failures"}}
{"id": "13633078", "key": "SPARK-54129", "project": "SPARK", "summary": "Extract dropping table from catalog from DataFrameWriter in case of overwriting table during creation", "description": "Currently, if we call DataFrameWriter.saveAsTable(tableName) with the SaveMode equal to Overwrite and the DataSource is V1, the table is dropped from catalog directly in the DataFrameWriter and only then the CreateTable command is created. In case if we want to somehow change the default behavior using Spark extensions, for instance, postpone the table dropping to decrease the time interval, when the old table is dropped and the new one is not created, such hardcoded behavior in DataFrameWriter makes it almost impossible if we want to use the original saveAsTable method. For instance, if we call df.write.mode(\"overwrite\").insertInto(table_name) then it's possible to control the table removal logic if we provide our own implementation of the spark.sql.sources.commitProtocolClass. In case of saveAsTable it makes no sense, because a table is unconditionally dropped before handling the CreateTable command. Therefore, is it possible to extract the table dropping logic from DataFrameWriter to the corresponding LogicalPlan commands (as it was done for V2 Datasources) in order to give more flexibility to Spark extensions?", "comments": "", "created": "2025-11-01T09:55:16.000+0000", "updated": "2025-11-01T09:55:16.000+0000", "derived": {"summary_task": "Summarize this issue: Currently, if we call DataFrameWriter.saveAsTable(tableName) with the SaveMode equal to Overwrite and the DataSource is V1, the table is dropped from catalog directly in the DataFrameWriter and only then the CreateTable command is created. In case if we want to somehow change the default behavior using Spark extensions, for instance, postpone the table dropping to decrease the time interval, when ", "classification_task": "Classify the issue priority and type: Currently, if we call DataFrameWriter.saveAsTable(tableName) with the SaveMode equal to Overwrite and the DataSource is V1, the table is dropped from catalog directly in the DataFrameWriter and only then the CreateTable command is created. In case if we want to somehow change the default behavior using Spark extensions, for instance, postpone the table dropping to decrease the time interval, when ", "qna_task": "Question: What is this issue about?\nAnswer: Extract dropping table from catalog from DataFrameWriter in case of overwriting table during creation"}}
{"id": "13633075", "key": "SPARK-54128", "project": "SPARK", "summary": "Improve Error Handling for Spark Connect", "description": "This is an umbrella Jira to migrate exceptions in Spark Connect from plain runtime exceptions to proper Spark Exceptions with error class.", "comments": "", "created": "2025-11-01T08:47:45.000+0000", "updated": "2025-11-01T08:47:45.000+0000", "derived": {"summary_task": "Summarize this issue: This is an umbrella Jira to migrate exceptions in Spark Connect from plain runtime exceptions to proper Spark Exceptions with error class.", "classification_task": "Classify the issue priority and type: This is an umbrella Jira to migrate exceptions in Spark Connect from plain runtime exceptions to proper Spark Exceptions with error class.", "qna_task": "Question: What is this issue about?\nAnswer: Improve Error Handling for Spark Connect"}}
{"id": "13633071", "key": "SPARK-54127", "project": "SPARK", "summary": "Fix sbt inconsistent shading package", "description": "", "comments": "", "created": "2025-11-01T04:20:38.000+0000", "updated": "2025-11-01T04:29:06.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Fix sbt inconsistent shading package"}}
{"id": "13633068", "key": "SPARK-54126", "project": "SPARK", "summary": "SHOW TBLPROPERTIES AS JSON", "description": "", "comments": "", "created": "2025-11-01T00:18:18.000+0000", "updated": "2025-11-01T00:20:36.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: SHOW TBLPROPERTIES AS JSON"}}
{"id": "13633067", "key": "SPARK-54125", "project": "SPARK", "summary": "SHOW DATABASES AS JSON", "description": "", "comments": "", "created": "2025-11-01T00:14:14.000+0000", "updated": "2025-11-01T00:20:55.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: SHOW DATABASES AS JSON"}}
{"id": "13633066", "key": "SPARK-54124", "project": "SPARK", "summary": "SHOW VIEWS AS JSON", "description": "", "comments": "", "created": "2025-11-01T00:13:05.000+0000", "updated": "2025-11-01T00:21:04.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: SHOW VIEWS AS JSON"}}
{"id": "13633065", "key": "SPARK-54123", "project": "SPARK", "summary": "Add timezone to make timestamp absolute time.", "description": "", "comments": "", "created": "2025-11-01T00:05:48.000+0000", "updated": "2025-11-01T00:25:46.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Add timezone to make timestamp absolute time."}}
{"id": "13633064", "key": "SPARK-54122", "project": "SPARK", "summary": "Improve the testing experience for TransformWithState", "description": "Currently it's hard to write unit tests for user's implementation of StatefulProcessor used in TransformWithState. User either needs to test it by running actual streaming query, or they need to refactor business logic into a separate function (that will be called from handleInputRows), and write unit tests for that function. I propose to add a simple testing framework for TransformWithState that will allow users easily test their business logic inside StatefulProcessor. On high-level, it's a class TwsTester that takes StatefulProcessor and allows to feed in input rows and immediately returns what rows would be produced by TWS. It also allows to set and inspect state. This can be used in unit tests without having to run streaming query and it won't need RocksDB (it will use in-memory state store). I will start with implementing this for Scala users, potentially making it available for Python users later.", "comments": "", "created": "2025-11-01T00:03:04.000+0000", "updated": "2025-11-01T01:12:01.000+0000", "derived": {"summary_task": "Summarize this issue: Currently it's hard to write unit tests for user's implementation of StatefulProcessor used in TransformWithState. User either needs to test it by running actual streaming query, or they need to refactor business logic into a separate function (that will be called from handleInputRows), and write unit tests for that function. I propose to add a simple testing framework for TransformWithState that ", "classification_task": "Classify the issue priority and type: Currently it's hard to write unit tests for user's implementation of StatefulProcessor used in TransformWithState. User either needs to test it by running actual streaming query, or they need to refactor business logic into a separate function (that will be called from handleInputRows), and write unit tests for that function. I propose to add a simple testing framework for TransformWithState that ", "qna_task": "Question: What is this issue about?\nAnswer: Improve the testing experience for TransformWithState"}}
{"id": "13633061", "key": "SPARK-54121", "project": "SPARK", "summary": "Automatic Snapshot Repair for State store", "description": "Today, the engine currently treats both the changelog and snapshot files with the same importance, so when a state store needs to be loaded it reads the latest snapshot and applies the subsequent changes on it. If the snapshot is bad or corrupted, then the query will fail and be completely down and blocked, needing manual intervention. This leads to user clearing their query checkpoint and having to do full recomputation. This shouldn\u2019t be the case. The changelog should be treated as the \u201csource of truth\u201d and the snapshot is just a disposable materialization of the log. Introducing Automatic snapshot repair, which will automatically repair the checkpoint by skipping bad snapshots and rebuilding the current state from the last good snapshot (works even if there\u2019s none) and applying the changelogs on it. This eliminates the need for manual intervention and unblocks the pipeline to keep it running. Also emit metrics about number of state stores that were auto repaired in a given batch, so that you can build alert and dashboard for it.", "comments": "", "created": "2025-10-31T22:49:00.000+0000", "updated": "2025-11-01T04:54:12.000+0000", "derived": {"summary_task": "Summarize this issue: Today, the engine currently treats both the changelog and snapshot files with the same importance, so when a state store needs to be loaded it reads the latest snapshot and applies the subsequent changes on it. If the snapshot is bad or corrupted, then the query will fail and be completely down and blocked, needing manual intervention. This leads to user clearing their query checkpoint and having ", "classification_task": "Classify the issue priority and type: Today, the engine currently treats both the changelog and snapshot files with the same importance, so when a state store needs to be loaded it reads the latest snapshot and applies the subsequent changes on it. If the snapshot is bad or corrupted, then the query will fail and be completely down and blocked, needing manual intervention. This leads to user clearing their query checkpoint and having ", "qna_task": "Question: What is this issue about?\nAnswer: Automatic Snapshot Repair for State store"}}
{"id": "13633056", "key": "SPARK-54120", "project": "SPARK", "summary": "Update assertGeneratedCRDMatchesHelmChart to include diff", "description": "", "comments": "Issue resolved by pull request 414 [https://github.com/apache/spark-kubernetes-operator/pull/414]", "created": "2025-10-31T21:24:37.000+0000", "updated": "2025-11-01T09:44:53.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Update assertGeneratedCRDMatchesHelmChart to include diff"}}
{"id": "13633054", "key": "SPARK-54119", "project": "SPARK", "summary": "Metrics & semantic modeling in Spark", "description": "SPIP: https://docs.google.com/document/d/1xVTLijvDTJ90lZ_ujwzf9HvBJgWg0mY6cYM44Fcghl0/edit?tab=t.0#heading=h.4iogryr5qznc", "comments": "", "created": "2025-10-31T20:49:11.000+0000", "updated": "2025-10-31T21:33:37.000+0000", "derived": {"summary_task": "Summarize this issue: SPIP: https://docs.google.com/document/d/1xVTLijvDTJ90lZ_ujwzf9HvBJgWg0mY6cYM44Fcghl0/edit?tab=t.0#heading=h.4iogryr5qznc", "classification_task": "Classify the issue priority and type: SPIP: https://docs.google.com/document/d/1xVTLijvDTJ90lZ_ujwzf9HvBJgWg0mY6cYM44Fcghl0/edit?tab=t.0#heading=h.4iogryr5qznc", "qna_task": "Question: What is this issue about?\nAnswer: Metrics & semantic modeling in Spark"}}
{"id": "13633050", "key": "SPARK-54118", "project": "SPARK", "summary": "Improve the put/merge operation in ListState when t here are multiple values", "description": "In SS TWS, when we do the {{put(array)}} operation in liststate, we put the first element and then merge the remaining elements one by one. so if we want to put an array with 100 elements, it means we need do 1 put + 99 merges. This can result in worse performance than a single put operation for the entire array. Similar, we have the same issue in {{merge(array)}}", "comments": "Issue resolved by pull request 52820 [https://github.com/apache/spark/pull/52820]", "created": "2025-10-31T20:24:12.000+0000", "updated": "2025-11-01T04:42:03.000+0000", "derived": {"summary_task": "Summarize this issue: In SS TWS, when we do the {{put(array)}} operation in liststate, we put the first element and then merge the remaining elements one by one. so if we want to put an array with 100 elements, it means we need do 1 put + 99 merges. This can result in worse performance than a single put operation for the entire array. Similar, we have the same issue in {{merge(array)}}", "classification_task": "Classify the issue priority and type: In SS TWS, when we do the {{put(array)}} operation in liststate, we put the first element and then merge the remaining elements one by one. so if we want to put an array with 100 elements, it means we need do 1 put + 99 merges. This can result in worse performance than a single put operation for the entire array. Similar, we have the same issue in {{merge(array)}}", "qna_task": "Question: What is this issue about?\nAnswer: Improve the put/merge operation in ListState when t here are multiple values"}}
{"id": "13633035", "key": "SPARK-54117", "project": "SPARK", "summary": "Throw better error to indicate that TWS is only supported with RocksDB state store provider", "description": "When user tries to use TWS with HDFS as state store, they should get an error explicitly requiring to use RocksDB state provider.", "comments": "I am working on this.", "created": "2025-10-31T17:09:35.000+0000", "updated": "2025-10-31T23:11:36.000+0000", "derived": {"summary_task": "Summarize this issue: When user tries to use TWS with HDFS as state store, they should get an error explicitly requiring to use RocksDB state provider.", "classification_task": "Classify the issue priority and type: When user tries to use TWS with HDFS as state store, they should get an error explicitly requiring to use RocksDB state provider.", "qna_task": "Question: What is this issue about?\nAnswer: Throw better error to indicate that TWS is only supported with RocksDB state store provider"}}
{"id": "13632999", "key": "SPARK-54116", "project": "SPARK", "summary": "Add off-heap mode support for LongHashedRelation", "description": "LongHashedRelation now only supports on-heap mode. The task is for adding off-heap mode support for the relation.", "comments": "", "created": "2025-10-31T12:13:28.000+0000", "updated": "2025-10-31T12:24:50.000+0000", "derived": {"summary_task": "Summarize this issue: LongHashedRelation now only supports on-heap mode. The task is for adding off-heap mode support for the relation.", "classification_task": "Classify the issue priority and type: LongHashedRelation now only supports on-heap mode. The task is for adding off-heap mode support for the relation.", "qna_task": "Question: What is this issue about?\nAnswer: Add off-heap mode support for LongHashedRelation"}}
{"id": "13632988", "key": "SPARK-54115", "project": "SPARK", "summary": "Display connect server execution threads first in thread dump page", "description": "", "comments": "", "created": "2025-10-31T09:50:13.000+0000", "updated": "2025-10-31T17:04:26.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Display connect server execution threads first in thread dump page"}}
{"id": "13632982", "key": "SPARK-54114", "project": "SPARK", "summary": "Support getColumns for SparkConnectDatabaseMetaData", "description": "", "comments": "", "created": "2025-10-31T09:03:00.000+0000", "updated": "2025-10-31T09:03:00.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Support getColumns for SparkConnectDatabaseMetaData"}}
{"id": "13632981", "key": "SPARK-54113", "project": "SPARK", "summary": "Support getTables for SparkConnectDatabaseMetaData", "description": "", "comments": "", "created": "2025-10-31T09:02:33.000+0000", "updated": "2025-10-31T09:02:33.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Support getTables for SparkConnectDatabaseMetaData"}}
{"id": "13632980", "key": "SPARK-54112", "project": "SPARK", "summary": "Support getSchemas for SparkConnectDatabaseMetaData", "description": "", "comments": "", "created": "2025-10-31T09:02:03.000+0000", "updated": "2025-10-31T17:46:19.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Support getSchemas for SparkConnectDatabaseMetaData"}}
{"id": "13632979", "key": "SPARK-54111", "project": "SPARK", "summary": "Support getCatalogs for SparkConnectDatabaseMetaData", "description": "", "comments": "", "created": "2025-10-31T09:01:36.000+0000", "updated": "2025-10-31T09:34:27.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Support getCatalogs for SparkConnectDatabaseMetaData"}}
{"id": "13632975", "key": "SPARK-54110", "project": "SPARK", "summary": "Introduce type encoders for Geography and Geometry types", "description": "", "comments": "Work in progress: https://github.com/apache/spark/pull/52813.", "created": "2025-10-31T08:32:57.000+0000", "updated": "2025-11-01T09:22:54.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Introduce type encoders for Geography and Geometry types"}}
{"id": "13632967", "key": "SPARK-54109", "project": "SPARK", "summary": "Avoid function conflicts in test_pandas_grouped_map", "description": "", "comments": "Issue resolved by pull request 52811 [https://github.com/apache/spark/pull/52811]", "created": "2025-10-31T07:47:22.000+0000", "updated": "2025-10-31T09:38:25.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Avoid function conflicts in test_pandas_grouped_map"}}
{"id": "13632957", "key": "SPARK-54108", "project": "SPARK", "summary": "Revise execute methods of SparkConnectStatement", "description": "", "comments": "", "created": "2025-10-31T06:24:19.000+0000", "updated": "2025-10-31T07:22:06.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Revise execute methods of SparkConnectStatement"}}
{"id": "13632954", "key": "SPARK-54107", "project": "SPARK", "summary": "Use `4.1.0-preview3-java21-scala` image for preview examples", "description": "", "comments": "Issue resolved by pull request 413 [https://github.com/apache/spark-kubernetes-operator/pull/413]", "created": "2025-10-31T05:41:26.000+0000", "updated": "2025-10-31T06:10:40.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Use `4.1.0-preview3-java21-scala` image for preview examples "}}
{"id": "13632952", "key": "SPARK-54106", "project": "SPARK", "summary": "State Store Row Checksum implementation", "description": "Introduce row checksum creation and verification for state store, both {*}HDFS and RocksDB state store{*}. This will help detect corruption at the row level and help prevent corruption.", "comments": "-Issue resolved by pull request 52809- [https://github.com/apache/spark/pull/52809] Reverted. https://github.com/apache/spark/pull/52827", "created": "2025-10-31T05:23:24.000+0000", "updated": "2025-11-01T17:34:27.000+0000", "derived": {"summary_task": "Summarize this issue: Introduce row checksum creation and verification for state store, both {*}HDFS and RocksDB state store{*}. This will help detect corruption at the row level and help prevent corruption.", "classification_task": "Classify the issue priority and type: Introduce row checksum creation and verification for state store, both {*}HDFS and RocksDB state store{*}. This will help detect corruption at the row level and help prevent corruption.", "qna_task": "Question: What is this issue about?\nAnswer: State Store Row Checksum implementation"}}
{"id": "13632951", "key": "SPARK-54105", "project": "SPARK", "summary": "Update `integration-test-ubuntu-spark41` to use Spark `4.1.0-preview3-java21`", "description": "", "comments": "Issue resolved by pull request 259 [https://github.com/apache/spark-connect-swift/pull/259]", "created": "2025-10-31T05:17:51.000+0000", "updated": "2025-10-31T05:38:38.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Update `integration-test-ubuntu-spark41` to use Spark `4.1.0-preview3-java21`"}}
{"id": "13632944", "key": "SPARK-54104", "project": "SPARK", "summary": "Disallow casting geospatial types to/from other data types", "description": "", "comments": "Work in progress: https://github.com/apache/spark/pull/52806. Issue resolved by pull request 52806 [https://github.com/apache/spark/pull/52806]", "created": "2025-10-31T02:25:41.000+0000", "updated": "2025-10-31T15:32:09.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Disallow casting geospatial types to/from other data types"}}
{"id": "13632938", "key": "SPARK-54103", "project": "SPARK", "summary": "Introduce client-side Geography and Geometry classes", "description": "", "comments": "Work in progress: https://github.com/apache/spark/pull/52804. Issue resolved by pull request 52804 [https://github.com/apache/spark/pull/52804]", "created": "2025-10-31T00:00:31.000+0000", "updated": "2025-10-31T06:51:19.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Introduce client-side Geography and Geometry classes"}}
{"id": "13632935", "key": "SPARK-54102", "project": "SPARK", "summary": "Spark 4.0.1 still throws \"String length (20054016) exceeds the maximum length (20000000)\" and \"from_json\" fails on a very large JSON with a jackson_core parse error", "description": "Based on JIRA *SPARK-49872* and the implementation in [{{JsonProtocol.scala}}|https://github.com/apache/spark/blob/29434ea766b0fc3c3bf6eaadb43a8f931133649e/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala#L71], the relevant limit was removed in {*}4.0.1{*}. However, in our environment we can still reliably reproduce: # When generating/processing a very large string: {code:java} Caused by: com.fasterxml.jackson.core.exc.StreamConstraintsException: String value length (20040525) exceeds the maximum allowed (20000000, from `StreamReadConstraints.getMaxStringLength()`){code} # When using {{from_json}} on a _valid_ very large single-line JSON (no missing comma), Jackson throws at around column {*}20,271,838{*}: {code:java} Caused by: com.fasterxml.jackson.core.JsonParseException: Unexpected character ('1' (code 49)): was expecting comma to separate Object entries at [Source: UNKNOWN; line: 1, column: 20271838]{code} I'm sure this is not a formatting issue. If I truncate the JSON to below column {*}20,271,838{*}, it parses successfully. Here is my parsing code: {code:java} raw_df.withColumn(\"parsed_item\", f.from_json(f.col(\"item\"), my_schema){code}", "comments": "", "created": "2025-10-30T23:41:04.000+0000", "updated": "2025-10-30T23:41:04.000+0000", "derived": {"summary_task": "Summarize this issue: Based on JIRA *SPARK-49872* and the implementation in [{{JsonProtocol.scala}}|https://github.com/apache/spark/blob/29434ea766b0fc3c3bf6eaadb43a8f931133649e/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala#L71], the relevant limit was removed in {*}4.0.1{*}. However, in our environment we can still reliably reproduce: # When generating/processing a very large string: {code:java} Caused ", "classification_task": "Classify the issue priority and type: Based on JIRA *SPARK-49872* and the implementation in [{{JsonProtocol.scala}}|https://github.com/apache/spark/blob/29434ea766b0fc3c3bf6eaadb43a8f931133649e/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala#L71], the relevant limit was removed in {*}4.0.1{*}. However, in our environment we can still reliably reproduce: # When generating/processing a very large string: {code:java} Caused ", "qna_task": "Question: What is this issue about?\nAnswer: Spark 4.0.1 still throws \"String length (20054016) exceeds the maximum length (20000000)\" and \"from_json\" fails on a very large JSON with a jackson_core parse error"}}
{"id": "13632933", "key": "SPARK-54101", "project": "SPARK", "summary": "Introduce the framework for adding ST functions in Scala", "description": "", "comments": "Work in progress: https://github.com/apache/spark/pull/52803. Issue resolved by pull request 52803 [https://github.com/apache/spark/pull/52803]", "created": "2025-10-30T22:53:14.000+0000", "updated": "2025-10-31T09:20:51.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Introduce the framework for adding ST functions in Scala"}}
{"id": "13632931", "key": "SPARK-54100", "project": "SPARK", "summary": "Remove `ignore.symbol.file` Javac option", "description": "", "comments": "Issue resolved by pull request 52802 [https://github.com/apache/spark/pull/52802]", "created": "2025-10-30T22:09:49.000+0000", "updated": "2025-10-31T03:55:13.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Remove `ignore.symbol.file` Javac option"}}
{"id": "13632930", "key": "SPARK-54099", "project": "SPARK", "summary": "XML to Variant conversion throws ArithmeticException on decimals with extreme exponents", "description": "When parsing XML data with `parse_xml` that contains decimal numbers with very large exponents (e.g., \"1E+2147483647\"), the conversion to Variant type fails with: ``` java.lang.ArithmeticException: BigInteger would overflow supported range at java.base/java.math.BigDecimal.setScale(BigDecimal.java:3000) at org.apache.spark.sql.catalyst.xml.StaxXmlParser$.org$apache$spark$sql$catalyst$xml$StaxXmlParser$$appendXMLCharacterToVariant(StaxXmlParser.scala:1285) ```", "comments": "", "created": "2025-10-30T21:30:34.000+0000", "updated": "2025-10-31T00:30:02.000+0000", "derived": {"summary_task": "Summarize this issue: When parsing XML data with `parse_xml` that contains decimal numbers with very large exponents (e.g., \"1E+2147483647\"), the conversion to Variant type fails with: ``` java.lang.ArithmeticException: BigInteger would overflow supported range at java.base/java.math.BigDecimal.setScale(BigDecimal.java:3000) at org.apache.spark.sql.catalyst.xml.StaxXmlParser$.org$apache$spark$sql$catalyst$xml$StaxXmlPa", "classification_task": "Classify the issue priority and type: When parsing XML data with `parse_xml` that contains decimal numbers with very large exponents (e.g., \"1E+2147483647\"), the conversion to Variant type fails with: ``` java.lang.ArithmeticException: BigInteger would overflow supported range at java.base/java.math.BigDecimal.setScale(BigDecimal.java:3000) at org.apache.spark.sql.catalyst.xml.StaxXmlParser$.org$apache$spark$sql$catalyst$xml$StaxXmlPa", "qna_task": "Question: What is this issue about?\nAnswer: XML to Variant conversion throws ArithmeticException on decimals with extreme exponents"}}
{"id": "13632929", "key": "SPARK-54098", "project": "SPARK", "summary": "Set `--sun-misc-unsafe-memory-access=allow` for `JavaExec` Gradle tasks", "description": "", "comments": "", "created": "2025-10-30T21:25:47.000+0000", "updated": "2025-10-30T21:45:58.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Set `--sun-misc-unsafe-memory-access=allow` for `JavaExec` Gradle tasks"}}
{"id": "13632928", "key": "SPARK-54097", "project": "SPARK", "summary": "Upgrade `Gradle` to 9.2.0", "description": "", "comments": "Issue resolved by pull request 411 [https://github.com/apache/spark-kubernetes-operator/pull/411]", "created": "2025-10-30T21:10:46.000+0000", "updated": "2025-10-30T21:23:53.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Upgrade `Gradle` to 9.2.0"}}
{"id": "13632923", "key": "SPARK-54096", "project": "SPARK", "summary": "Support Spatial Reference System mapping in PySpark", "description": "", "comments": "Work in progress: https://github.com/apache/spark/pull/52799. Issue resolved by pull request 52799 [https://github.com/apache/spark/pull/52799]", "created": "2025-10-30T19:29:49.000+0000", "updated": "2025-10-31T07:54:59.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Support Spatial Reference System mapping in PySpark"}}
{"id": "13632914", "key": "SPARK-54095", "project": "SPARK", "summary": "Release Spark Connect Swift Client 0.6.0", "description": "", "comments": "", "created": "2025-10-30T17:49:49.000+0000", "updated": "2025-10-30T17:50:32.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Release Spark Connect Swift Client 0.6.0"}}
{"id": "13632913", "key": "SPARK-54094", "project": "SPARK", "summary": "Extract common methods to KafkaOffsetReaderBase", "description": "When reviewing https://github.com/apache/spark/pull/52729, I found that KafkaOffsetReaderConsumer and KafkaOffsetReaderAdmin have the exactly same getOffsetRangesFromResolvedOffsets method. The method is actually long so seems good to extract them to common one.", "comments": "Issue resolved by pull request 52788 [https://github.com/apache/spark/pull/52788]", "created": "2025-10-30T17:45:28.000+0000", "updated": "2025-10-30T22:08:46.000+0000", "derived": {"summary_task": "Summarize this issue: When reviewing https://github.com/apache/spark/pull/52729, I found that KafkaOffsetReaderConsumer and KafkaOffsetReaderAdmin have the exactly same getOffsetRangesFromResolvedOffsets method. The method is actually long so seems good to extract them to common one.", "classification_task": "Classify the issue priority and type: When reviewing https://github.com/apache/spark/pull/52729, I found that KafkaOffsetReaderConsumer and KafkaOffsetReaderAdmin have the exactly same getOffsetRangesFromResolvedOffsets method. The method is actually long so seems good to extract them to common one.", "qna_task": "Question: What is this issue about?\nAnswer: Extract common methods to KafkaOffsetReaderBase"}}
{"id": "13632907", "key": "SPARK-54093", "project": "SPARK", "summary": "Release Spark Kubernetes Operator 0.7.0", "description": "", "comments": "", "created": "2025-10-30T15:49:36.000+0000", "updated": "2025-10-30T15:49:54.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Release Spark Kubernetes Operator 0.7.0"}}
{"id": "13632904", "key": "SPARK-54092", "project": "SPARK", "summary": "Use Java-friendly `KubernetesClientUtils` APIs", "description": "", "comments": "Issue resolved by pull request 410 [https://github.com/apache/spark-kubernetes-operator/pull/410]", "created": "2025-10-30T15:24:09.000+0000", "updated": "2025-10-30T17:27:48.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Use Java-friendly `KubernetesClientUtils` APIs"}}
{"id": "13632888", "key": "SPARK-54091", "project": "SPARK", "summary": "Implement the ST_Srid expression in SQL", "description": "", "comments": "Work in progress: https://github.com/apache/spark/pull/52795. Issue resolved by pull request 52795 [https://github.com/apache/spark/pull/52795]", "created": "2025-10-30T13:20:13.000+0000", "updated": "2025-10-31T04:42:39.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Implement the ST_Srid expression in SQL"}}
{"id": "13632875", "key": "SPARK-54090", "project": "SPARK", "summary": "AssertDataframeEqual carries rows when showing differences", "description": "When we try to do assertDataFrameEqual, on two dataframes that have not the same amount of rows, the output gets cascading from the difference till the end. Why this is happening: [https://github.com/apache/spark/blob/067969ff946712eeabf47040415f25000837cd87/python/pyspark/testing/utils.py#L1036] {code:java} def assert_rows_equal( rows1: List[Row], rows2: List[Row], maxErrors: int = None, showOnlyDiff: bool = False ): __tracebackhide__ = True zipped = list(zip_longest(rows1, rows2)) diff_rows_cnt = 0 diff_rows = [] has_diff_rows = False rows_str1 = \"\" rows_str2 = \"\" # count different rows for r1, r2 in zipped: if not compare_rows(r1, r2): diff_rows_cnt += 1 has_diff_rows = True if includeDiffRows: diff_rows.append((r1, r2)) rows_str1 += str(r1) + \"\\n\" rows_str2 += str(r2) + \"\\n\" if maxErrors is not None and diff_rows_cnt >= maxErrors: break elif not showOnlyDiff: rows_str1 += str(r1) + \"\\n\" rows_str2 += str(r2) + \"\\n\" generated_diff = _context_diff( actual=rows_str1.splitlines(), expected=rows_str2.splitlines(), n=len(zipped) ) if has_diff_rows: error_msg = \"Results do not match: \" percent_diff = (diff_rows_cnt / len(zipped)) * 100 error_msg += \"( %.5f %% )\" % percent_diff error_msg += \"\\n\" + \"\\n\".join(generated_diff) data = diff_rows if includeDiffRows else None raise PySparkAssertionError( errorClass=\"DIFFERENT_ROWS\", messageParameters={\"error_msg\": error_msg}, data=data ){code} The problem lies in the way that we zip the lines {code:java} zipped = list(zip_longest(rows1, rows2)){code} With zip longest we assume that the rows are in order and we do position by position comparison but it does not work well with checkRowOrder which defaults to False. If I have 1 line difference in 100 line dataframe the result percentage won't be 1% but the amount of rows that cascade towards on from that difference. The best solution here would be to have a set based comparison and return the percentage and the rows over that. A sample of what zip_longest is doing: {code:java} from itertools import zip_longest rows1 = [ 'A', 'B', 'C', 'D', 'E'] rows2 = [ 'A', 'C', 'D',] zipped = list(zip_longest(rows1, rows2))zipped {code} Result: {code:java} [('A', 'A'), ('B', 'C'), ('C', 'D'), ('D', None), ('E', None)]{code} So in this case we would have 80% rows failure. This comes directly with the implementation of CheckRowOrder, when it is True we do not sort and it makes sense to use the zip_longest, when it is False it makes sense to use set based comparison since we have already sorted.", "comments": "", "created": "2025-10-30T10:44:10.000+0000", "updated": "2025-10-30T13:09:58.000+0000", "derived": {"summary_task": "Summarize this issue: When we try to do assertDataFrameEqual, on two dataframes that have not the same amount of rows, the output gets cascading from the difference till the end. Why this is happening: [https://github.com/apache/spark/blob/067969ff946712eeabf47040415f25000837cd87/python/pyspark/testing/utils.py#L1036] {code:java} def assert_rows_equal( rows1: List[Row], rows2: List[Row], maxErrors: int = None, showOnly", "classification_task": "Classify the issue priority and type: When we try to do assertDataFrameEqual, on two dataframes that have not the same amount of rows, the output gets cascading from the difference till the end. Why this is happening: [https://github.com/apache/spark/blob/067969ff946712eeabf47040415f25000837cd87/python/pyspark/testing/utils.py#L1036] {code:java} def assert_rows_equal( rows1: List[Row], rows2: List[Row], maxErrors: int = None, showOnly", "qna_task": "Question: What is this issue about?\nAnswer: AssertDataframeEqual carries rows when showing differences"}}
{"id": "13632871", "key": "SPARK-54089", "project": "SPARK", "summary": "Add off-heap mode support for on-heap-only memory consumers", "description": "There are a few memory consumers that only support on-heap mode. Including: # LongToUnsafeRowMap (for long key hash join) # ExternalSorter (for non-serializable sort-based shuffle) It's ideal to make them support off-heap when Spark is running with off-heap memory mode. For 3rd accelerator plugins that leverage Spark off-heap memory, having the large allocations in vanilla Spark supporting off-heap mode will significantly ease the memory settings for running the mixed query plan that contains both vanilla Spark and offloaded computations.", "comments": "", "created": "2025-10-30T10:32:48.000+0000", "updated": "2025-10-31T15:02:55.000+0000", "derived": {"summary_task": "Summarize this issue: There are a few memory consumers that only support on-heap mode. Including: # LongToUnsafeRowMap (for long key hash join) # ExternalSorter (for non-serializable sort-based shuffle) It's ideal to make them support off-heap when Spark is running with off-heap memory mode. For 3rd accelerator plugins that leverage Spark off-heap memory, having the large allocations in vanilla Spark supporting off-hea", "classification_task": "Classify the issue priority and type: There are a few memory consumers that only support on-heap mode. Including: # LongToUnsafeRowMap (for long key hash join) # ExternalSorter (for non-serializable sort-based shuffle) It's ideal to make them support off-heap when Spark is running with off-heap memory mode. For 3rd accelerator plugins that leverage Spark off-heap memory, having the large allocations in vanilla Spark supporting off-hea", "qna_task": "Question: What is this issue about?\nAnswer: Add off-heap mode support for on-heap-only memory consumers"}}
{"id": "13632858", "key": "SPARK-54088", "project": "SPARK", "summary": "When TaskSchedulerImpl searches for an executor to schedule an unschedulable task, it should exclude executors that are pending to remove", "description": "Call kill executor !image-2025-10-30-17-22-25-127.png|width=1255,height=241! Container 18 didn't kill it !image-2025-10-30-17-24-03-632.png|width=1087,height=183! Pending container causing task can't be scheduled .", "comments": "", "created": "2025-10-30T09:14:40.000+0000", "updated": "2025-11-01T07:25:22.000+0000", "derived": {"summary_task": "Summarize this issue: Call kill executor !image-2025-10-30-17-22-25-127.png|width=1255,height=241! Container 18 didn't kill it !image-2025-10-30-17-24-03-632.png|width=1087,height=183! Pending container causing task can't be scheduled .", "classification_task": "Classify the issue priority and type: Call kill executor !image-2025-10-30-17-22-25-127.png|width=1255,height=241! Container 18 didn't kill it !image-2025-10-30-17-24-03-632.png|width=1087,height=183! Pending container causing task can't be scheduled .", "qna_task": "Question: What is this issue about?\nAnswer: When TaskSchedulerImpl searches for an executor to schedule an unschedulable task, it should exclude executors that are pending to remove"}}
{"id": "13632855", "key": "SPARK-54087", "project": "SPARK", "summary": "Spark Executor launch task failed should return task killed message", "description": "When not enable speculation ,executor launch task failed caused by OOM or other issue, DAGScheduler won;t know task failed and won't scheduler again, causing application stuck !image-2025-10-30-16-52-22-524.png|width=589,height=233!", "comments": "", "created": "2025-10-30T08:50:01.000+0000", "updated": "2025-10-30T09:02:56.000+0000", "derived": {"summary_task": "Summarize this issue: When not enable speculation ,executor launch task failed caused by OOM or other issue, DAGScheduler won;t know task failed and won't scheduler again, causing application stuck !image-2025-10-30-16-52-22-524.png|width=589,height=233!", "classification_task": "Classify the issue priority and type: When not enable speculation ,executor launch task failed caused by OOM or other issue, DAGScheduler won;t know task failed and won't scheduler again, causing application stuck !image-2025-10-30-16-52-22-524.png|width=589,height=233!", "qna_task": "Question: What is this issue about?\nAnswer: Spark Executor launch task failed should return task killed message"}}
{"id": "13632852", "key": "SPARK-54086", "project": "SPARK", "summary": "Support IO_URING Netty IO Mode", "description": "", "comments": "", "created": "2025-10-30T08:30:49.000+0000", "updated": "2025-10-31T22:47:01.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Support IO_URING Netty IO Mode"}}
{"id": "13632846", "key": "SPARK-54085", "project": "SPARK", "summary": "Fix `initialize` to add `CREATE` option additionally in `DriverRunner`", "description": "When submitting jobs to standalone cluster using restful api, we get errors like: ``` 25/10/30 16:02:59 INFO DriverRunner: Killing driver process! 25/10/30 16:02:59 INFO CommandUtils: Redirection to /Users/ericgao/workspace/open_source/spark/work/driver-20251030160259-0020/stdout closed: Stream closed 25/10/30 16:02:59 WARN Worker: Driver driver-20251030160259-0020 failed with unrecoverable exception: java.nio.file.NoSuchFileException: /Users/ericgao/workspace/open_source/spark/work/driver-20251030160259-0020/stderr ``` We need to fix the usage of `Files.writeString` in DriverRunner", "comments": "I'd like to fix it. Issue resolved by pull request 52789 [https://github.com/apache/spark/pull/52789]", "created": "2025-10-30T08:12:22.000+0000", "updated": "2025-10-30T15:52:51.000+0000", "derived": {"summary_task": "Summarize this issue: When submitting jobs to standalone cluster using restful api, we get errors like: ``` 25/10/30 16:02:59 INFO DriverRunner: Killing driver process! 25/10/30 16:02:59 INFO CommandUtils: Redirection to /Users/ericgao/workspace/open_source/spark/work/driver-20251030160259-0020/stdout closed: Stream closed 25/10/30 16:02:59 WARN Worker: Driver driver-20251030160259-0020 failed with unrecoverable except", "classification_task": "Classify the issue priority and type: When submitting jobs to standalone cluster using restful api, we get errors like: ``` 25/10/30 16:02:59 INFO DriverRunner: Killing driver process! 25/10/30 16:02:59 INFO CommandUtils: Redirection to /Users/ericgao/workspace/open_source/spark/work/driver-20251030160259-0020/stdout closed: Stream closed 25/10/30 16:02:59 WARN Worker: Driver driver-20251030160259-0020 failed with unrecoverable except", "qna_task": "Question: What is this issue about?\nAnswer: Fix `initialize` to add `CREATE` option additionally in `DriverRunner`"}}
{"id": "13632831", "key": "SPARK-54084", "project": "SPARK", "summary": "Publish Apache Spark 4.1.0-preview3 to docker registry", "description": "", "comments": "This is resolved via https://github.com/apache/spark-docker/pull/97", "created": "2025-10-30T05:22:24.000+0000", "updated": "2025-10-31T03:59:01.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Publish Apache Spark 4.1.0-preview3 to docker registry"}}
{"id": "13632830", "key": "SPARK-54083", "project": "SPARK", "summary": "Use `4.1.0-preview3` instead of `RC1`", "description": "", "comments": "Issue resolved by pull request 258 [https://github.com/apache/spark-connect-swift/pull/258]", "created": "2025-10-30T05:06:34.000+0000", "updated": "2025-10-30T16:04:02.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Use `4.1.0-preview3` instead of `RC1`"}}
{"id": "13632829", "key": "SPARK-54082", "project": "SPARK", "summary": "Upgrade Spark to `4.1.0-preview3`", "description": "", "comments": "Issue resolved by pull request 409 [https://github.com/apache/spark-kubernetes-operator/pull/409]", "created": "2025-10-30T04:59:24.000+0000", "updated": "2025-10-30T14:28:45.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Upgrade Spark to `4.1.0-preview3`"}}
{"id": "13632818", "key": "SPARK-54081", "project": "SPARK", "summary": "Add `word-count-preview.yaml` Example", "description": "", "comments": "Issue resolved by pull request 408 [https://github.com/apache/spark-kubernetes-operator/pull/408]", "created": "2025-10-29T23:45:58.000+0000", "updated": "2025-10-30T04:15:51.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Add `word-count-preview.yaml` Example"}}
{"id": "13632805", "key": "SPARK-54080", "project": "SPARK", "summary": "Use Swift 6.2 as the minimum supported version", "description": "", "comments": "Issue resolved by pull request 257 [https://github.com/apache/spark-connect-swift/pull/257]", "created": "2025-10-29T18:04:09.000+0000", "updated": "2025-10-29T18:32:26.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Use Swift 6.2 as the minimum supported version"}}
{"id": "13632803", "key": "SPARK-54079", "project": "SPARK", "summary": "Introduce the framework for adding ST expressions in Catalyst", "description": "", "comments": "Work in progress: https://github.com/apache/spark/pull/52784. Issue resolved by pull request 52784 [https://github.com/apache/spark/pull/52784]", "created": "2025-10-29T17:47:59.000+0000", "updated": "2025-10-31T02:35:09.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Introduce the framework for adding ST expressions in Catalyst"}}
{"id": "13632796", "key": "SPARK-54078", "project": "SPARK", "summary": "Deflake StateStoreSuite `SPARK-40492: maintenance before unload`", "description": "`SPARK-40492: maintenance before unload` in StateStoreSuite is flaky due to timing issues. Fix this by adding hooks to pause maintenance.", "comments": "Issue resolved by pull request 52783 [https://github.com/apache/spark/pull/52783]", "created": "2025-10-29T16:40:18.000+0000", "updated": "2025-10-30T17:13:36.000+0000", "derived": {"summary_task": "Summarize this issue: `SPARK-40492: maintenance before unload` in StateStoreSuite is flaky due to timing issues. Fix this by adding hooks to pause maintenance.", "classification_task": "Classify the issue priority and type: `SPARK-40492: maintenance before unload` in StateStoreSuite is flaky due to timing issues. Fix this by adding hooks to pause maintenance.", "qna_task": "Question: What is this issue about?\nAnswer: Deflake StateStoreSuite `SPARK-40492: maintenance before unload`"}}
{"id": "13632795", "key": "SPARK-54077", "project": "SPARK", "summary": "Enable all disabled tests on Linux", "description": "", "comments": "Issue resolved by pull request 256 [https://github.com/apache/spark-connect-swift/pull/256]", "created": "2025-10-29T16:37:05.000+0000", "updated": "2025-10-29T17:24:39.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Enable all disabled tests on Linux"}}
{"id": "13632785", "key": "SPARK-54076", "project": "SPARK", "summary": "Fix Flaky Test:`OracleJoinPushdownIntegrationSuite`", "description": "Recently, `OracleJoinPushdownIntegrationSuite` frequently fails. https://github.com/beliefer/spark/actions/runs/18904457292/job/53959322233#logs {code:java} [info] org.apache.spark.sql.jdbc.v2.join.OracleJoinPushdownIntegrationSuite *** ABORTED *** (10 minutes, 15 seconds) [info] The code passed to eventually never returned normally. Attempted 599 times over 10.012648813883333 minutes. Last failure message: ORA-12541: Cannot connect. No listener at host 10.1.0.201 port 44551. (CONNECTION_ID=5rB4vZNTQR2sFHpk9xQC1A==) [info] https://docs.oracle.com/error-help/db/ora-12541/. (DockerJDBCIntegrationSuite.scala:214) [info] org.scalatest.exceptions.TestFailedDueToTimeoutException: [info] at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:219) [info] at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226) [info] at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313) [info] at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312) [info] at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.eventually(DockerJDBCIntegrationSuite.scala:103) [info] at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$1(DockerJDBCIntegrationSuite.scala:214) [info] at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled(DockerIntegrationFunSuite.scala:49) [info] at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled$(DockerIntegrationFunSuite.scala:47) [info] at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.runIfTestsEnabled(DockerJDBCIntegrationSuite.scala:103) [info] at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.beforeAll(DockerJDBCIntegrationSuite.scala:133) [info] at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212) [info] at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210) [info] at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208) [info] at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68) [info] at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321) [info] at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517) [info] at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414) [info] at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) [info] at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [info] at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [info] at java.base/java.lang.Thread.run(Thread.java:840) [info] Cause: java.sql.SQLException: ORA-12541: Cannot connect. No listener at host 10.1.0.201 port 44551. (CONNECTION_ID=5rB4vZNTQR2sFHpk9xQC1A==) [info] https://docs.oracle.com/error-help/db/ora-12541/ [info] at oracle.jdbc.driver.T4CConnection.handleLogonNetException(T4CConnection.java:1631) [info] at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:1151) [info] at oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:1189) [info] at oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:106) [info] at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:895) [info] at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:702) [info] at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:681) [info] at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:190) [info] at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.getConnection(DockerJDBCIntegrationSuite.scala:250) [info] at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$8(DockerJDBCIntegrationSuite.scala:215) [info] at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18) [info] at org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184) [info] at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196) [info] at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226) [info] at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313) [info] at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312) [info] at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.eventually(DockerJDBCIntegrationSuite.scala:103) [info] at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$1(DockerJDBCIntegrationSuite.scala:214) [info] at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled(DockerIntegrationFunSuite.scala:49) [info] at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled$(DockerIntegrationFunSuite.scala:47) [info] at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.runIfTestsEnabled(DockerJDBCIntegrationSuite.scala:103) [info] at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.beforeAll(DockerJDBCIntegrationSuite.scala:133) [info] at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212) [info] at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210) [info] at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208) [info] at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68) [info] at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321) [info] at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517) [info] at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414) [info] at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) [info] at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [info] at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [info] at java.base/java.lang.Thread.run(Thread.java:840) [info] Cause: oracle.net.ns.NetException: ORA-12541: Cannot connect. No listener at host 10.1.0.201 port 44551. (CONNECTION_ID=5rB4vZNTQR2sFHpk9xQC1A==) [info] https://docs.oracle.com/error-help/db/ora-12541/ [info] at oracle.net.nt.TcpNTAdapter.handleEstablishSocketException(TcpNTAdapter.java:418) [info] at oracle.net.nt.TcpNTAdapter.establishSocket(TcpNTAdapter.java:350) [info] at oracle.net.nt.TcpNTAdapter.connect(TcpNTAdapter.java:228) [info] at oracle.net.nt.ConnOption.connect(ConnOption.java:333) [info] at oracle.net.nt.ConnStrategy.executeConnOption(ConnStrategy.java:1223) [info] at oracle.net.nt.ConnStrategy.execute(ConnStrategy.java:762) [info] at oracle.net.resolver.AddrResolution.resolveAndExecute(AddrResolution.java:712) [info] at oracle.net.ns.NSProtocol.establishConnection(NSProtocol.java:960) [info] at oracle.net.ns.NSProtocol.connect(NSProtocol.java:329) [info] at oracle.jdbc.driver.T4CConnection.connectNetworkSessionProtocol(T4CConnection.java:3462) [info] at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:1030) [info] at oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:1189) [info] at oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:106) [info] at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:895) [info] at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:702) [info] at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:681) [info] at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:190) [info] at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.getConnection(DockerJDBCIntegrationSuite.scala:250) [info] at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$8(DockerJDBCIntegrationSuite.scala:215) [info] at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18) [info] at org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184) [info] at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196) [info] at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226) [info] at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313) [info] at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312) [info] at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.eventually(DockerJDBCIntegrationSuite.scala:103) [info] at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$1(DockerJDBCIntegrationSuite.scala:214) [info] at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled(DockerIntegrationFunSuite.scala:49) [info] at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled$(DockerIntegrationFunSuite.scala:47) [info] at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.runIfTestsEnabled(DockerJDBCIntegrationSuite.scala:103) [info] at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.beforeAll(DockerJDBCIntegrationSuite.scala:133) [info] at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212) [info] at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210) [info] at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208) [info] at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68) [info] at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321) [info] at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517) [info] at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414) [info] at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) [info] at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [info] at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [info] at java.base/java.lang.Thread.run(Thread.java:840) [info] Cause: java.net.ConnectException: Connection refused [info] at java.base/sun.nio.ch.Net.pollConnect(Native Method) [info] at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672) [info] at java.base/sun.nio.ch.SocketChannelImpl.finishTimedConnect(SocketChannelImpl.java:1141) [info] at java.base/sun.nio.ch.SocketChannelImpl.blockingConnect(SocketChannelImpl.java:1183) [info] at java.base/sun.nio.ch.SocketAdaptor.connect(SocketAdaptor.java:98) [info] at oracle.net.nt.TimeoutSocketChannel.doConnect(TimeoutSocketChannel.java:289) [info] at oracle.net.nt.TimeoutSocketChannel.initializeSocketChannel(TimeoutSocketChannel.java:269) [info] at oracle.net.nt.TimeoutSocketChannel.connect(TimeoutSocketChannel.java:236) [info] at oracle.net.nt.TimeoutSocketChannel.<init>(TimeoutSocketChannel.java:203) [info] at oracle.net.nt.TcpNTAdapter.establishSocket(TcpNTAdapter.java:339) [info] at oracle.net.nt.TcpNTAdapter.connect(TcpNTAdapter.java:228) [info] at oracle.net.nt.ConnOption.connect(ConnOption.java:333) [info] at oracle.net.nt.ConnStrategy.executeConnOption(ConnStrategy.java:1223) [info] at oracle.net.nt.ConnStrategy.execute(ConnStrategy.java:762) [info] at oracle.net.resolver.AddrResolution.resolveAndExecute(AddrResolution.java:712) [info] at oracle.net.ns.NSProtocol.establishConnection(NSProtocol.java:960) [info] at oracle.net.ns.NSProtocol.connect(NSProtocol.java:329) [info] at oracle.jdbc.driver.T4CConnection.connectNetworkSessionProtocol(T4CConnection.java:3462) [info] at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:1030) [info] at oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:1189) [info] at oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:106) [info] at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:895) [info] at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:702) [info] at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:681) [info] at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:190) [info] at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.getConnection(DockerJDBCIntegrationSuite.scala:250) [info] at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$8(DockerJDBCIntegrationSuite.scala:215) [info] at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18) [info] at org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184) [info] at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196) [info] at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226) [info] at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313) [info] at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312) [info] at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.eventually(DockerJDBCIntegrationSuite.scala:103) [info] at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$1(DockerJDBCIntegrationSuite.scala:214) [info] at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled(DockerIntegrationFunSuite.scala:49) [info] at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled$(DockerIntegrationFunSuite.scala:47) [info] at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.runIfTestsEnabled(DockerJDBCIntegrationSuite.scala:103) [info] at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.beforeAll(DockerJDBCIntegrationSuite.scala:133) [info] at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212) [info] at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210) [info] at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208) [info] at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68) [info] at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321) [info] at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517) [info] at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414) [info] at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) [info] at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [info] at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [info] at java.base/java.lang.Thread.run(Thread.java:840) {code}", "comments": "Issue resolved by pull request 52780 [https://github.com/apache/spark/pull/52780]", "created": "2025-10-29T14:56:32.000+0000", "updated": "2025-10-30T04:48:07.000+0000", "derived": {"summary_task": "Summarize this issue: Recently, `OracleJoinPushdownIntegrationSuite` frequently fails. https://github.com/beliefer/spark/actions/runs/18904457292/job/53959322233#logs {code:java} [info] org.apache.spark.sql.jdbc.v2.join.OracleJoinPushdownIntegrationSuite *** ABORTED *** (10 minutes, 15 seconds) [info] The code passed to eventually never returned normally. Attempted 599 times over 10.012648813883333 minutes. Last failur", "classification_task": "Classify the issue priority and type: Recently, `OracleJoinPushdownIntegrationSuite` frequently fails. https://github.com/beliefer/spark/actions/runs/18904457292/job/53959322233#logs {code:java} [info] org.apache.spark.sql.jdbc.v2.join.OracleJoinPushdownIntegrationSuite *** ABORTED *** (10 minutes, 15 seconds) [info] The code passed to eventually never returned normally. Attempted 599 times over 10.012648813883333 minutes. Last failur", "qna_task": "Question: What is this issue about?\nAnswer: Fix Flaky Test:`OracleJoinPushdownIntegrationSuite`"}}
{"id": "13632784", "key": "SPARK-54075", "project": "SPARK", "summary": "Make ResolvedCollation evaluable", "description": "In this PR I propose to make ResolvedCollation evaluable. By making ResolvedCollation evaluable, it can now pass the canEvaluateWithinJoin check, allowing the optimizer to use efficient hash joins for queries with inline COLLATE in join conditions (before this change we would fallback to BroadcastNestedLoopJoin).", "comments": "Hi, [~mihailoale-db] . Apache Spark community has a policy which manages `Fix Version` and `Target Version` like the following. So, please don't set it when you file a JIRA issue. - https://spark.apache.org/contributing.html {quote}Do not set the following fields: - Fix Version. This is assigned by committers only when resolved. - Target Version. This is assigned by committers to indicate a PR has been accepted for possible fix by the target version. {quote} Issue resolved by pull request 52779 [https://github.com/apache/spark/pull/52779]", "created": "2025-10-29T14:48:46.000+0000", "updated": "2025-10-29T17:35:50.000+0000", "derived": {"summary_task": "Summarize this issue: In this PR I propose to make ResolvedCollation evaluable. By making ResolvedCollation evaluable, it can now pass the canEvaluateWithinJoin check, allowing the optimizer to use efficient hash joins for queries with inline COLLATE in join conditions (before this change we would fallback to BroadcastNestedLoopJoin).", "classification_task": "Classify the issue priority and type: In this PR I propose to make ResolvedCollation evaluable. By making ResolvedCollation evaluable, it can now pass the canEvaluateWithinJoin check, allowing the optimizer to use efficient hash joins for queries with inline COLLATE in join conditions (before this change we would fallback to BroadcastNestedLoopJoin).", "qna_task": "Question: What is this issue about?\nAnswer: Make ResolvedCollation evaluable"}}
{"id": "13632750", "key": "SPARK-54074", "project": "SPARK", "summary": "Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when offset(s) in planned topic partitions got unavailable during subsequent batches`", "description": "Similar to `Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches` resolved in SPARK-54058, `Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite` also seems flaky. https://github.com/micheal-o/spark/actions/runs/18895266750/job/53931238163 {code} [info] - Query with Trigger.AvailableNow should throw error when offset(s) in planned topic partitions got unavailable during subsequent batches *** FAILED *** (1 minute) [info] java.lang.AssertionError: assertion failed: Exception tree doesn't contain the expected exception with message: Some of partitions in Kafka topic(s) report available offset which is less than end offset during running query with Trigger.AvailableNow. [info] org.scalatest.exceptions.TestFailedException: isPropagated was false Partition [topic-42, 1] metadata not propagated after timeout [info] at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472) [info] at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471) [info] at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231) [info] at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295) [info] at org.apache.spark.sql.kafka010.KafkaTestUtils.$anonfun$waitUntilMetadataIsPropagated$1(KafkaTestUtils.scala:614) [info] at org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184) [info] at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196) [info] at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226) [info] at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:348) [info] at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:347) [info] at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:457) [info] at org.apache.spark.sql.kafka010.KafkaTestUtils.waitUntilMetadataIsPropagated(KafkaTestUtils.scala:613) [info] at org.apache.spark.sql.kafka010.KafkaTestUtils.$anonfun$createTopic$1(KafkaTestUtils.scala:378) [info] at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:256) [info] at org.apache.spark.sql.kafka010.KafkaTestUtils.createTopic(KafkaTestUtils.scala:377) [info] at org.apache.spark.sql.kafka010.KafkaMicroBatchSourceSuiteBase.$anonfun$new$16(KafkaMicroBatchSourceSuite.scala:406) [info] at org.apache.spark.sql.kafka010.KafkaMicroBatchSourceSuiteBase.$anonfun$new$16$adapted(KafkaMicroBatchSourceSuite.scala:403) [info] at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.callBatchWriter(ForeachBatchSink.scala:56) [info] at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:49) [info] at org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runBatch$19(MicroBatchExecution.scala:1063) [info] at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:176) [info] at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:284) [info] at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:138) [info] at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94) [info] at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112) [info] at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106) [info] at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111) [info] at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:138) [info] at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:307) [info] at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:137) [info] at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804) [info] at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:91) [info] at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:249) [info] at org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runBatch$18(MicroBatchExecution.scala:1054) [info] at org.apache.spark.sql.execution.streaming.runtime.ProgressContext.reportTimeTaken(ProgressReporter.scala:200) [info] at org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.runBatch(MicroBatchExecution.scala:1054) [info] at org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$executeOneBatch$2(MicroBatchExecution.scala:513) [info] at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18) [info] at org.apache.spark.sql.execution.streaming.runtime.ProgressContext.reportTimeTaken(ProgressReporter.scala:200) [info] at org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.executeOneBatch(MicroBatchExecution.scala:478) [info] at org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:458) [info] at org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runActivatedStream$1$adapted(MicroBatchExecution.scala:458) [info] at org.apache.spark.sql.execution.streaming.runtime.TriggerExecutor.runOneBatch(TriggerExecutor.scala:40) [info] at org.apache.spark.sql.execution.streaming.runtime.TriggerExecutor.runOneBatch$(TriggerExecutor.scala:38) [info] at org.apache.spark.sql.execution.streaming.runtime.MultiBatchExecutor.runOneBatch(TriggerExecutor.scala:60) [info] at org.apache.spark.sql.execution.streaming.runtime.MultiBatchExecutor.execute(TriggerExecutor.scala:65) [info] at org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:458) [info] at org.apache.spark.sql.execution.streaming.runtime.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:347) [info] at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18) [info] at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804) [info] at org.apache.spark.sql.execution.streaming.runtime.StreamExecution.org$apache$spark$sql$execution$streaming$runtime$StreamExecution$$runStream(StreamExecution.scala:307) [info] at org.apache.spark.sql.execution.streaming.runtime.StreamExecution$$anon$1.run(StreamExecution.scala:230) [info] at scala.Predef$.assert(Predef.scala:279) [info] at org.apache.spark.TestUtils$.assertExceptionMsg(TestUtils.scala:198) [info] at org.apache.spark.sql.kafka010.KafkaMicroBatchSourceSuiteBase.$anonfun$new$14(KafkaMicroBatchSourceSuite.scala:428) [info] at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18) [info] at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127) [info] at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282) [info] at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231) [info] at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230) [info] at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:68) [info] at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:154) [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85) [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83) [info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) [info] at org.scalatest.Transformer.apply(Transformer.scala:22) [info] at org.scalatest.Transformer.apply(Transformer.scala:20) [info] at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226) [info] at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:226) [info] at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224) [info] at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236) [info] at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306) [info] at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236) [info] at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218) [info] at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:68) [info] at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234) [info] at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227) [info] at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:68) [info] at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269) [info] at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413) [info] at scala.collection.immutable.List.foreach(List.scala:323) [info] at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401) [info] at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396) [info] at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475) [info] at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269) [info] at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268) [info] at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564) [info] at org.scalatest.Suite.run(Suite.scala:1114) [info] at org.scalatest.Suite.run$(Suite.scala:1096) [info] at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564) [info] at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273) [info] at org.scalatest.SuperEngine.runImpl(Engine.scala:535) [info] at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273) [info] at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272) [info] at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:68) [info] at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213) [info] at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210) [info] at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208) [info] at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68) [info] at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321) [info] at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517) [info] at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414) [info] at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) [info] at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [info] at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [info] at java.base/java.lang.Thread.run(Thread.java:840) {code}", "comments": "Issue resolved by pull request 52777 [https://github.com/apache/spark/pull/52777]", "created": "2025-10-29T10:36:24.000+0000", "updated": "2025-10-29T16:27:25.000+0000", "derived": {"summary_task": "Summarize this issue: Similar to `Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches` resolved in SPARK-54058, `Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite` also seems flaky. https://github.com/micheal-o/spark/actions/runs/18895266750/job/53931238163 {code} [info] - Query with Trigger.AvailableNow should throw error when offset(s) in planned ", "classification_task": "Classify the issue priority and type: Similar to `Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches` resolved in SPARK-54058, `Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite` also seems flaky. https://github.com/micheal-o/spark/actions/runs/18895266750/job/53931238163 {code} [info] - Query with Trigger.AvailableNow should throw error when offset(s) in planned ", "qna_task": "Question: What is this issue about?\nAnswer: Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when offset(s) in planned topic partitions got unavailable during subsequent batches`"}}
{"id": "13632725", "key": "SPARK-54073", "project": "SPARK", "summary": "Improve `ConfOptionDocGenerator` to generate a sorted doc by config key", "description": "", "comments": "Issue resolved by pull request 407 [https://github.com/apache/spark-kubernetes-operator/pull/407]", "created": "2025-10-29T04:41:02.000+0000", "updated": "2025-10-29T15:07:17.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Improve `ConfOptionDocGenerator` to generate a sorted doc by config key"}}
{"id": "13632721", "key": "SPARK-54072", "project": "SPARK", "summary": "Make sure we don't upload empty files in RocksDB snapshot", "description": "This change is to make sure the RocksDB snapshot zip file doesn't contain empty files (except RocksDB log file). Other files such as OPTION, manifest, metadata are not expected to be empty in any situation. Introduced a conf {{verifyNonEmptyFilesInZip}} to make sure we can turn this off if needed.", "comments": "Issue resolved by pull request 52774 [https://github.com/apache/spark/pull/52774]", "created": "2025-10-29T02:42:37.000+0000", "updated": "2025-10-29T21:08:09.000+0000", "derived": {"summary_task": "Summarize this issue: This change is to make sure the RocksDB snapshot zip file doesn't contain empty files (except RocksDB log file). Other files such as OPTION, manifest, metadata are not expected to be empty in any situation. Introduced a conf {{verifyNonEmptyFilesInZip}} to make sure we can turn this off if needed.", "classification_task": "Classify the issue priority and type: This change is to make sure the RocksDB snapshot zip file doesn't contain empty files (except RocksDB log file). Other files such as OPTION, manifest, metadata are not expected to be empty in any situation. Introduced a conf {{verifyNonEmptyFilesInZip}} to make sure we can turn this off if needed.", "qna_task": "Question: What is this issue about?\nAnswer: Make sure we don't upload empty files in RocksDB snapshot"}}
{"id": "13632718", "key": "SPARK-54071", "project": "SPARK", "summary": "Spark Structured Streaming Filesink can not generate open lineage with output details", "description": "h2. Environment details h3. OpenLineage version {quote}io.openlineage:openlineage-spark_2.13:1.39.0 Technology and package versions Python: 3.13.3 Scala: 2.13.16 Java: OpenJDK 64-Bit Server VM, 17.0.16 pip freeze py4j==0.10.9.9 pyspark==4.0.1{quote} For the openlineage set up, I used the default setting: {quote}$ git clone https://github.com/MarquezProject/marquez.git && cd marquez $ ./docker/up.sh{quote} h3. Spark Deployment details I used native spark on local machine. There is no managed services involved. Problem details h2. Issue details When using Spark structured streaming to write parquet file to file systems, * File sink will only generate openlineage event with streaming processing type with output information as empty. * Foreachbatch sink will generate openlineage event with both streaming processing type and batch processing type. The batch processing type will have valid output information. The bug is that File sink in Spark structured streaming does not generate open lineage event with output details. More details about the sample code and sample events are following. File sink: Sample code: {quote} query = streaming_df.writeStream \\ .format('parquet') \\ .outputMode('append') \\ .partitionBy('year', 'month', 'day') \\ .option('checkpointLocation', checkpoint_path) \\ .option('path', output_path) \\ .queryName('filesink') \\ .start() {quote} Sample event for \"processingType\":\"STREAMING\" {quote} 25/10/29 00:49:02 DEBUG wire: http-outgoing-52 >> \"{\"eventTime\":\"2025-10-28T13:45:34.282Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"019a2b14-4e9d-7574-95a9-55182f07591d\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-0/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"019a2b10-5218-7ba3-9cf4-1ce4b1800752\"},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"filesink\"},\"root\":{\"run\":{\"runId\":\"019a2b10-5218-7ba3-9cf4-1ce4b1800752\"},\"job\":{\"namespace\":\"airflow_namespace\",\"name\":\"airflow_dag.airflow_task\"}}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"filesink\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"4.0.1\",\"name\":\"spark\",\"openlineageAdapterVersion\":\"1.39.0\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"filesink.project\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-3/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"STREAMING\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\"{quote} foreachbatch sink sample code {quote} def write_to_file(batch_df, batch_id): if batch_df.count() > 0: batch_df.write \\ .mode(\"append\") \\ .partitionBy(\"year\", \"month\", \"day\") \\ .parquet(output_path) {quote} {quote} query = streaming_df \\ .writeStream \\ .outputMode(\"append\") \\ .foreachBatch(write_to_file) \\ .option(\"checkpointLocation\", checkpoint_path) \\ .trigger(processingTime='10 seconds') \\ .start() {quote} The above code with generate both streaming and batch processing type event. Sample streaming type event {quote} 25/10/29 01:04:45 DEBUG wire: http-outgoing-1 >> \"{\"eventTime\":\"2025-10-28T14:04:43.373Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"019a2b22-b364-79ca-be87-2d173c25c16c\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-0/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"019a2b22-9e86-770e-8af9-f9136ec6594c\"},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"foreachbatchsink\"},\"root\":{\"run\":{\"runId\":\"019a2b22-9e86-770e-8af9-f9136ec6594c\"},\"job\":{\"namespace\":\"airflow_namespace\",\"name\":\"airflow_dag.airflow_task\"}}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"foreachbatchsink\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"4.0.1\",\"name\":\"spark\",\"openlineageAdapterVersion\":\"1.39.0\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"foreachbatchsink.project\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-3/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"STREAMING\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\" {quote} Sample batch type event {quote} 25/10/29 01:07:26 DEBUG wire: http-outgoing-33 >> \"{\"eventTime\":\"2025-10-28T14:07:20.711Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"019a2b25-28f6-7fa0-a4e2-2aaba4f61d7e\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-0/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"019a2b22-9e86-770e-8af9-f9136ec6594c\"},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"foreachbatchsink\"},\"root\":{\"run\":{\"runId\":\"019a2b22-9e86-770e-8af9-f9136ec6594c\"},\"job\":{\"namespace\":\"airflow_namespace\",\"name\":\"airflow_dag.airflow_task\"}}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"foreachbatchsink\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"4.0.1\",\"name\":\"spark\",\"openlineageAdapterVersion\":\"1.39.0\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"foreachbatchsink.execute_insert_into_hadoop_fs_relation_command.tests_output_foreachbatchsink\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-3/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"file\",\"name\":\"/Users/xxxx/venvs/tests/output_foreachbatchsink\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"file\",\"uri\":\"file\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"long\"},{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"timestamp\",\"type\":\"timestamp\"},{\"name\":\"value\",\"type\":\"double\"},{\"name\":\"year\",\"type\":\"integer\"},{\"name\":\"month\",\"type\":\"integer\"},{\"name\":\"day\",\"type\":\"integer\"}]}},\"outputFacets\":{\"outputStatistics\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-2/OutputStatisticsOutputDatasetFacet.json#/$defs/OutputStatisticsOutputDatasetFacet\",\"rowCount\":10,\"size\":13259,\"fileCount\":10}}}]}\"{quote} What you think should happen instead File sink for spark structured streaming should create open lineage event with valid output details as the information is in the spark query logic plan. Here is the logic plan for streaming query using file sink. {quote} == Analyzed Logical Plan == id: bigint, name: string, timestamp: timestamp, value: double, year: int, month: int, day: int ~WriteToMicroBatchDataSourceV1 FileSink[file:/Users/xxx/venvs/tests/output_filesink], 753bdc9a-07cd-4788-a17d-27ff622ababc, [checkpointLocation=checkpoint_filesink, path=output_filesink, queryName=filesink], Append, 1 +- ~Project [id#2L, name#3, timestamp#0, value#4, year#5, month#6, dayofmonth(cast(timestamp#0 as date)) AS day#7] +- ~Project [id#2L, name#3, timestamp#0, value#4, year#5, month(cast(timestamp#0 as date)) AS month#6] +- ~Project [id#2L, name#3, timestamp#0, value#4, year(cast(timestamp#0 as date)) AS year#5] +- ~Project [(value#1L % cast(1000 as bigint)) AS id#2L, concat(user_, cast((value#1L % cast(100 as bigint)) as string)) AS name#3, timestamp#0, (rand(-1344458628259366487) * cast(100 as double)) AS value#4] +- ~StreamingDataSourceV2ScanRelation[timestamp#0, value#1L] RateStream(rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=12) {quote} How to reproduce * step 1: install pyspark on your local machine https://spark.apache.org/docs/latest/api/python/getting_started/install.html * step 2: install openlineage server on your local machine https://openlineage.io/getting-started * step 3: refer to following spark-submit command to run file_sink.py and foreachbatch_sink.py. You will see the open lineage event in the debug logs. {quote} spark-submit --packages io.openlineage:openlineage-spark_2.13:1.39.0 --conf \"spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener\" --conf \"spark.openlineage.transport.type=http\" --conf \"spark.openlineage.transport.url=http://localhost:5000\" --conf \"spark.openlineage.namespace=spark_namespace\" --conf \"spark.openlineage.parentJobNamespace=airflow_namespace\" --conf \"spark.openlineage.parentJobName=airflow_dag.airflow_task\" --conf \"spark.openlineage.parentRunId=xxxx-xxxx-xxxx-xxxx\" [filename].py {quote}", "comments": "", "created": "2025-10-28T23:15:44.000+0000", "updated": "2025-10-29T00:01:44.000+0000", "derived": {"summary_task": "Summarize this issue: h2. Environment details h3. OpenLineage version {quote}io.openlineage:openlineage-spark_2.13:1.39.0 Technology and package versions Python: 3.13.3 Scala: 2.13.16 Java: OpenJDK 64-Bit Server VM, 17.0.16 pip freeze py4j==0.10.9.9 pyspark==4.0.1{quote} For the openlineage set up, I used the default setting: {quote}$ git clone https://github.com/MarquezProject/marquez.git && cd marquez $ ./docker/up.s", "classification_task": "Classify the issue priority and type: h2. Environment details h3. OpenLineage version {quote}io.openlineage:openlineage-spark_2.13:1.39.0 Technology and package versions Python: 3.13.3 Scala: 2.13.16 Java: OpenJDK 64-Bit Server VM, 17.0.16 pip freeze py4j==0.10.9.9 pyspark==4.0.1{quote} For the openlineage set up, I used the default setting: {quote}$ git clone https://github.com/MarquezProject/marquez.git && cd marquez $ ./docker/up.s", "qna_task": "Question: What is this issue about?\nAnswer: Spark Structured Streaming Filesink can not generate open lineage with output details"}}
{"id": "13632715", "key": "SPARK-54070", "project": "SPARK", "summary": "Add spark.logConf configuration to operator docs", "description": "", "comments": "Issue resolved by pull request 406 [https://github.com/apache/spark-kubernetes-operator/pull/406]", "created": "2025-10-28T21:58:38.000+0000", "updated": "2025-10-29T06:29:07.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Add spark.logConf configuration to operator docs"}}
{"id": "13632709", "key": "SPARK-54069", "project": "SPARK", "summary": "Skip `test_to_feather` in Python 3.14", "description": "", "comments": "Issue resolved by pull request 52771 [https://github.com/apache/spark/pull/52771]", "created": "2025-10-28T20:56:28.000+0000", "updated": "2025-10-29T03:15:29.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Skip `test_to_feather` in Python 3.14"}}
{"id": "13632708", "key": "SPARK-54068", "project": "SPARK", "summary": "Fix `pyspark.pandas.tests.connect.io.test_parity_feather.FeatherParityTests.test_to_feather` in Python 3.14", "description": "{code} ====================================================================== ERROR [1.960s]: test_to_feather (pyspark.pandas.tests.connect.io.test_parity_feather.FeatherParityTests.test_to_feather) ---------------------------------------------------------------------- Traceback (most recent call last): File \"/__w/spark/spark/python/pyspark/pandas/tests/io/test_feather.py\", line 43, in test_to_feather self.psdf.to_feather(path2) ~~~~~~~~~~~~~~~~~~~~^^^^^^^ File \"/__w/spark/spark/python/pyspark/pandas/frame.py\", line 2702, in to_feather return validate_arguments_and_invoke_function( self._to_internal_pandas(), self.to_feather, pd.DataFrame.to_feather, args ) File \"/__w/spark/spark/python/pyspark/pandas/utils.py\", line 592, in validate_arguments_and_invoke_function return pandas_func(**args) File \"/usr/local/lib/python3.14/dist-packages/pandas/core/frame.py\", line 2949, in to_feather to_feather(self, path, **kwargs) ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^ File \"/usr/local/lib/python3.14/dist-packages/pandas/io/feather_format.py\", line 65, in to_feather feather.write_feather(df, handles.handle, **kwargs) ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/usr/local/lib/python3.14/dist-packages/pyarrow/feather.py\", line 156, in write_feather table = Table.from_pandas(df, preserve_index=preserve_index) File \"pyarrow/table.pxi\", line 4795, in pyarrow.lib.Table.from_pandas File \"/usr/local/lib/python3.14/dist-packages/pyarrow/pandas_compat.py\", line 663, in dataframe_to_arrays pandas_metadata = construct_metadata( columns_to_convert, df, column_names, index_columns, index_descriptors, preserve_index, types, column_field_names=column_field_names ) File \"/usr/local/lib/python3.14/dist-packages/pyarrow/pandas_compat.py\", line 281, in construct_metadata b'pandas': json.dumps({ ~~~~~~~~~~^^ 'index_columns': index_descriptors, ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ...<7 lines>... 'pandas_version': _pandas_api.version ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ }).encode('utf8') ^^ File \"/usr/lib/python3.14/json/__init__.py\", line 231, in dumps return _default_encoder.encode(obj) ~~~~~~~~~~~~~~~~~~~~~~~^^^^^ File \"/usr/lib/python3.14/json/encoder.py\", line 200, in encode chunks = self.iterencode(o, _one_shot=True) File \"/usr/lib/python3.14/json/encoder.py\", line 261, in iterencode return _iterencode(o, 0) File \"/usr/lib/python3.14/json/encoder.py\", line 180, in default raise TypeError(f'Object of type {o.__class__.__name__} ' f'is not JSON serializable') TypeError: Object of type PlanMetrics is not JSON serializable when serializing list item 0 when serializing dict item 'metrics' when serializing dict item 'attributes' {code}", "comments": "", "created": "2025-10-28T20:55:17.000+0000", "updated": "2025-10-28T20:55:17.000+0000", "derived": {"summary_task": "Summarize this issue: {code} ====================================================================== ERROR [1.960s]: test_to_feather (pyspark.pandas.tests.connect.io.test_parity_feather.FeatherParityTests.test_to_feather) ---------------------------------------------------------------------- Traceback (most recent call last): File \"/__w/spark/spark/python/pyspark/pandas/tests/io/test_feather.py\", line 43, in test_to_fea", "classification_task": "Classify the issue priority and type: {code} ====================================================================== ERROR [1.960s]: test_to_feather (pyspark.pandas.tests.connect.io.test_parity_feather.FeatherParityTests.test_to_feather) ---------------------------------------------------------------------- Traceback (most recent call last): File \"/__w/spark/spark/python/pyspark/pandas/tests/io/test_feather.py\", line 43, in test_to_fea", "qna_task": "Question: What is this issue about?\nAnswer: Fix `pyspark.pandas.tests.connect.io.test_parity_feather.FeatherParityTests.test_to_feather` in Python 3.14"}}
{"id": "13632707", "key": "SPARK-54067", "project": "SPARK", "summary": "Improve `SparkSubmit` to invoke `exitFn` with the root cause instead of `SparkUserAppException`", "description": "I hit this when I ran a pipeline that had no flows: ``` org.apache.spark.SparkUserAppException: User application exited with 1 at org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:127) at org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:569) at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52) at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1028) at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:203) at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:226) at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:95) at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1166) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1175) at org.apache.spark.deploy.SparkPipelines$.main(SparkPipelines.scala:42) at org.apache.spark.deploy.SparkPipelines.main(SparkPipelines.scala) ``` This is not information that's relevant to the user.", "comments": "Issue resolved by pull request 52770 [https://github.com/apache/spark/pull/52770] I collected this as a subtask of SPARK-51727 in order to improve the visibility.", "created": "2025-10-28T20:32:23.000+0000", "updated": "2025-10-30T17:47:37.000+0000", "derived": {"summary_task": "Summarize this issue: I hit this when I ran a pipeline that had no flows: ``` org.apache.spark.SparkUserAppException: User application exited with 1 at org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:127) at org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorIm", "classification_task": "Classify the issue priority and type: I hit this when I ran a pipeline that had no flows: ``` org.apache.spark.SparkUserAppException: User application exited with 1 at org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:127) at org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorIm", "qna_task": "Question: What is this issue about?\nAnswer: Improve `SparkSubmit` to invoke `exitFn` with the root cause instead of `SparkUserAppException`"}}
{"id": "13632705", "key": "SPARK-54066", "project": "SPARK", "summary": "Skip `test_in_memory_data_source` in Python 3.14", "description": "", "comments": "Issue resolved by pull request 52769 [https://github.com/apache/spark/pull/52769]", "created": "2025-10-28T19:51:29.000+0000", "updated": "2025-10-29T03:14:52.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Skip `test_in_memory_data_source` in Python 3.14"}}
{"id": "13632704", "key": "SPARK-54065", "project": "SPARK", "summary": "Fix `test_in_memory_data_source` in Python 3.14", "description": "{code} ====================================================================== ERROR [0.007s]: test_in_memory_data_source (pyspark.sql.tests.test_python_datasource.PythonDataSourceTests.test_in_memory_data_source) ---------------------------------------------------------------------- Traceback (most recent call last): File \"/__w/spark/spark/python/pyspark/serializers.py\", line 460, in dumps return cloudpickle.dumps(obj, pickle_protocol) ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^ File \"/__w/spark/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 1537, in dumps cp.dump(obj) ~~~~~~~^^^^^ File \"/__w/spark/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 1303, in dump return super().dump(obj) ~~~~~~~~~~~~^^^^^ TypeError: cannot pickle '_abc._abc_data' object when serializing dict item '_abc_impl' when serializing tuple item 0 when serializing cell reconstructor arguments when serializing cell object when serializing tuple item 0 when serializing dict item '__closure__' when serializing tuple item 1 when serializing function state when serializing function object when serializing dict item '__annotate_func__' when serializing tuple item 0 when serializing abc.ABCMeta state when serializing abc.ABCMeta object when serializing tuple item 0 when serializing cell reconstructor arguments when serializing cell object when serializing tuple item 0 when serializing dict item '__closure__' when serializing tuple item 1 when serializing function state when serializing function object when serializing dict item 'reader' when serializing tuple item 0 when serializing abc.ABCMeta state when serializing abc.ABCMeta object During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"/__w/spark/spark/python/pyspark/sql/tests/test_python_datasource.py\", line 283, in test_in_memory_data_source self.spark.dataSource.register(InMemoryDataSource) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^ File \"/__w/spark/spark/python/pyspark/sql/datasource.py\", line 1197, in register wrapped = _wrap_function(sc, dataSource) File \"/__w/spark/spark/python/pyspark/sql/udf.py\", line 59, in _wrap_function pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command) ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^ File \"/__w/spark/spark/python/pyspark/core/rdd.py\", line 5121, in _prepare_for_python_RDD pickled_command = ser.dumps(command) File \"/__w/spark/spark/python/pyspark/serializers.py\", line 470, in dumps raise pickle.PicklingError(msg) _pickle.PicklingError: Could not serialize object: TypeError: cannot pickle '_abc._abc_data' object {code} {code} ====================================================================== ERROR [0.014s]: test_in_memory_data_source (pyspark.sql.tests.connect.test_parity_python_datasource.PythonDataSourceParityTests.test_in_memory_data_source) ---------------------------------------------------------------------- Traceback (most recent call last): File \"/__w/spark/spark/python/pyspark/serializers.py\", line 460, in dumps return cloudpickle.dumps(obj, pickle_protocol) ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^ File \"/__w/spark/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 1537, in dumps cp.dump(obj) ~~~~~~~^^^^^ File \"/__w/spark/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 1303, in dump return super().dump(obj) ~~~~~~~~~~~~^^^^^ TypeError: cannot pickle '_abc._abc_data' object when serializing dict item '_abc_impl' when serializing tuple item 0 when serializing cell reconstructor arguments when serializing cell object when serializing tuple item 0 when serializing dict item '__closure__' when serializing tuple item 1 when serializing function state when serializing function object when serializing dict item '__annotate_func__' when serializing tuple item 0 when serializing abc.ABCMeta state when serializing abc.ABCMeta object when serializing tuple item 0 when serializing cell reconstructor arguments when serializing cell object when serializing tuple item 0 when serializing dict item '__closure__' when serializing tuple item 1 when serializing function state when serializing function object when serializing dict item 'reader' when serializing tuple item 0 when serializing abc.ABCMeta state when serializing abc.ABCMeta object During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"/__w/spark/spark/python/pyspark/sql/tests/test_python_datasource.py\", line 283, in test_in_memory_data_source self.spark.dataSource.register(InMemoryDataSource) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^ File \"/__w/spark/spark/python/pyspark/sql/connect/datasource.py\", line 45, in register self.sparkSession._client.register_data_source(dataSource) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^ File \"/__w/spark/spark/python/pyspark/sql/connect/client/core.py\", line 863, in register_data_source ).to_data_source_proto(self) ~~~~~~~~~~~~~~~~~~~~^^^^^^ File \"/__w/spark/spark/python/pyspark/sql/connect/plan.py\", line 2833, in to_data_source_proto plan.python_data_source.CopyFrom(self._data_source.to_plan(session)) ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^ File \"/__w/spark/spark/python/pyspark/sql/connect/plan.py\", line 2807, in to_plan ds.command = CloudPickleSerializer().dumps(self._data_source) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^ File \"/__w/spark/spark/python/pyspark/serializers.py\", line 470, in dumps raise pickle.PicklingError(msg) _pickle.PicklingError: Could not serialize object: TypeError: cannot pickle '_abc._abc_data' object ---------------------------------------------------------------------- {code}", "comments": "", "created": "2025-10-28T19:49:33.000+0000", "updated": "2025-10-28T19:54:26.000+0000", "derived": {"summary_task": "Summarize this issue: {code} ====================================================================== ERROR [0.007s]: test_in_memory_data_source (pyspark.sql.tests.test_python_datasource.PythonDataSourceTests.test_in_memory_data_source) ---------------------------------------------------------------------- Traceback (most recent call last): File \"/__w/spark/spark/python/pyspark/serializers.py\", line 460, in dumps return ", "classification_task": "Classify the issue priority and type: {code} ====================================================================== ERROR [0.007s]: test_in_memory_data_source (pyspark.sql.tests.test_python_datasource.PythonDataSourceTests.test_in_memory_data_source) ---------------------------------------------------------------------- Traceback (most recent call last): File \"/__w/spark/spark/python/pyspark/serializers.py\", line 460, in dumps return ", "qna_task": "Question: What is this issue about?\nAnswer: Fix `test_in_memory_data_source` in Python 3.14"}}
{"id": "13632703", "key": "SPARK-54064", "project": "SPARK", "summary": "Simplify recursion detection of `cloudpickle`", "description": "", "comments": "", "created": "2025-10-28T18:58:28.000+0000", "updated": "2025-10-29T03:29:24.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Simplify recursion detection of `cloudpickle`"}}
{"id": "13632699", "key": "SPARK-54063", "project": "SPARK", "summary": "Trigger snapshot generation for next batch when lag is detected", "description": "We have done work to detect snapshot lag. Now we need to trigger snapshot generation for the next microbatch, when snapshot lag is detected.", "comments": "", "created": "2025-10-28T18:32:08.000+0000", "updated": "2025-10-31T20:44:51.000+0000", "derived": {"summary_task": "Summarize this issue: We have done work to detect snapshot lag. Now we need to trigger snapshot generation for the next microbatch, when snapshot lag is detected.", "classification_task": "Classify the issue priority and type: We have done work to detect snapshot lag. Now we need to trigger snapshot generation for the next microbatch, when snapshot lag is detected.", "qna_task": "Question: What is this issue about?\nAnswer: Trigger snapshot generation for next batch when lag is detected"}}
{"id": "13632698", "key": "SPARK-54062", "project": "SPARK", "summary": "MergeScalarSubqueries code cleanup", "description": "", "comments": "Issue resolved by pull request 52763 [https://github.com/apache/spark/pull/52763] I collected this as a subtask of SPARK-51166 in order to improve the visibility of this improvement.", "created": "2025-10-28T17:47:04.000+0000", "updated": "2025-10-29T17:56:04.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: MergeScalarSubqueries code cleanup"}}
{"id": "13632696", "key": "SPARK-54061", "project": "SPARK", "summary": "Wrap IllegalArgumentException with proper error code for invalid datetime patterns", "description": "When `TO_TIMESTAMP` is called with an invalid datetime pattern (e.g., `'yyyyMMddHHMIss'` with invalid letter `'I'`), the `IllegalArgumentException` from Java's `DateTimeFormatterBuilder` escapes during constant folding optimization, resulting in a raw exception without proper error codes or `SQLSTATE`.", "comments": "I am working ok this Issue resolved by pull request 52762 [https://github.com/apache/spark/pull/52762]", "created": "2025-10-28T17:28:02.000+0000", "updated": "2025-10-30T05:14:34.000+0000", "derived": {"summary_task": "Summarize this issue: When `TO_TIMESTAMP` is called with an invalid datetime pattern (e.g., `'yyyyMMddHHMIss'` with invalid letter `'I'`), the `IllegalArgumentException` from Java's `DateTimeFormatterBuilder` escapes during constant folding optimization, resulting in a raw exception without proper error codes or `SQLSTATE`.", "classification_task": "Classify the issue priority and type: When `TO_TIMESTAMP` is called with an invalid datetime pattern (e.g., `'yyyyMMddHHMIss'` with invalid letter `'I'`), the `IllegalArgumentException` from Java's `DateTimeFormatterBuilder` escapes during constant folding optimization, resulting in a raw exception without proper error codes or `SQLSTATE`.", "qna_task": "Question: What is this issue about?\nAnswer: Wrap IllegalArgumentException with proper error code for invalid datetime patterns"}}
{"id": "13632692", "key": "SPARK-54060", "project": "SPARK", "summary": "Introduce Geometry and Geography in-memory wrapper formats", "description": "", "comments": "Issue is resolved by: [https://github.com/apache/spark/pull/52761.] cc [~cloud_fan]", "created": "2025-10-28T16:50:42.000+0000", "updated": "2025-10-29T16:32:58.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Introduce Geometry and Geography in-memory wrapper formats"}}
{"id": "13632686", "key": "SPARK-54059", "project": "SPARK", "summary": "Reduce default page size by LONG_ARRAY_OFFSET if ZGC or ShenandoahGC and ON_HEAP are used", "description": "", "comments": "Issue resolved by pull request 52754 [https://github.com/apache/spark/pull/52754]", "created": "2025-10-28T15:37:39.000+0000", "updated": "2025-10-29T09:51:00.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Reduce default page size by LONG_ARRAY_OFFSET if ZGC or ShenandoahGC and ON_HEAP are used"}}
{"id": "13632675", "key": "SPARK-54058", "project": "SPARK", "summary": "Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches`", "description": "Recently, `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches` frequently fails. [https://github.com/apache/spark/actions/runs/18872699886/job/53854858890] {code:java} [info] - Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches *** FAILED *** (1 minute) [info] java.lang.AssertionError: assertion failed: Exception tree doesn't contain the expected exception with message: Some of partitions in Kafka topic(s) have been lost during running query with Trigger.AvailableNow. [info] org.scalatest.exceptions.TestFailedException: isPropagated was false Partition [topic-41, 1] metadata not propagated after timeout [info] at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472) [info] at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471) [info] at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231) [info] at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295) [info] at org.apache.spark.sql.kafka010.KafkaTestUtils.$anonfun$waitUntilMetadataIsPropagated$1(KafkaTestUtils.scala:614) [info] at org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184) [info] at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196) [info] at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226) [info] at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:348) [info] at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:347) [info] at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:457) [info] at org.apache.spark.sql.kafka010.KafkaTestUtils.waitUntilMetadataIsPropagated(KafkaTestUtils.scala:613) [info] at org.apache.spark.sql.kafka010.KafkaTestUtils.$anonfun$createTopic$1(KafkaTestUtils.scala:378) [info] at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:256) [info] at org.apache.spark.sql.kafka010.KafkaTestUtils.createTopic(KafkaTestUtils.scala:377) [info] at org.apache.spark.sql.kafka010.KafkaMicroBatchSourceSuiteBase.$anonfun$new$11(KafkaMicroBatchSourceSuite.scala:352) [info] at org.apache.spark.sql.kafka010.KafkaMicroBatchSourceSuiteBase.$anonfun$new$11$adapted(KafkaMicroBatchSourceSuite.scala:349) [info] at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.callBatchWriter(ForeachBatchSink.scala:56) [info] at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:49) [info] at org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runBatch$19(MicroBatchExecution.scala:1063) [info] at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:176) [info] at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:284) [info] at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:138) [info] at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94) [info] at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112) [info] at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106) [info] at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111) [info] at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:138) [info] at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:307) [info] at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:137) [info] at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804) [info] at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:91) [info] at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:249) [info] at org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runBatch$18(MicroBatchExecution.scala:1054) [info] at org.apache.spark.sql.execution.streaming.runtime.ProgressContext.reportTimeTaken(ProgressReporter.scala:200) [info] at org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.runBatch(MicroBatchExecution.scala:1054) [info] at org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$executeOneBatch$2(MicroBatchExecution.scala:513) [info] at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18) [info] at org.apache.spark.sql.execution.streaming.runtime.ProgressContext.reportTimeTaken(ProgressReporter.scala:200) [info] at org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.executeOneBatch(MicroBatchExecution.scala:478) [info] at org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:458) [info] at org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runActivatedStream$1$adapted(MicroBatchExecution.scala:458) [info] at org.apache.spark.sql.execution.streaming.runtime.TriggerExecutor.runOneBatch(TriggerExecutor.scala:40) [info] at org.apache.spark.sql.execution.streaming.runtime.TriggerExecutor.runOneBatch$(TriggerExecutor.scala:38) [info] at org.apache.spark.sql.execution.streaming.runtime.MultiBatchExecutor.runOneBatch(TriggerExecutor.scala:60) [info] at org.apache.spark.sql.execution.streaming.runtime.MultiBatchExecutor.execute(TriggerExecutor.scala:65) [info] at org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:458) [info] at org.apache.spark.sql.execution.streaming.runtime.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:347) [info] at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18) [info] at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804) [info] at org.apache.spark.sql.execution.streaming.runtime.StreamExecution.org$apache$spark$sql$execution$streaming$runtime$StreamExecution$$runStream(StreamExecution.scala:307) [info] at org.apache.spark.sql.execution.streaming.runtime.StreamExecution$$anon$1.run(StreamExecution.scala:230) [info] at scala.Predef$.assert(Predef.scala:279) [info] at org.apache.spark.TestUtils$.assertExceptionMsg(TestUtils.scala:198) [info] at org.apache.spark.sql.kafka010.KafkaMicroBatchSourceSuiteBase.$anonfun$new$9(KafkaMicroBatchSourceSuite.scala:374) [info] at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18) [info] at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127) [info] at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282) [info] at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231) [info] at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230) [info] at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:68) [info] at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:154) [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85) [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83) [info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) [info] at org.scalatest.Transformer.apply(Transformer.scala:22) [info] at org.scalatest.Transformer.apply(Transformer.scala:20) [info] at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226) [info] at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:226) [info] at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224) [info] at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236) [info] at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306) [info] at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236) [info] at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218) [info] at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:68) [info] at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234) [info] at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227) [info] at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:68) [info] at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269) [info] at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413) [info] at scala.collection.immutable.List.foreach(List.scala:323) [info] at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401) [info] at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396) [info] at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475) [info] at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269) [info] at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268) [info] at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564) [info] at org.scalatest.Suite.run(Suite.scala:1114) [info] at org.scalatest.Suite.run$(Suite.scala:1096) [info] at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564) [info] at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273) [info] at org.scalatest.SuperEngine.runImpl(Engine.scala:535) [info] at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273) [info] at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272) [info] at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:68) [info] at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213) [info] at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210) [info] at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208) [info] at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68) [info] at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321) [info] at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517) [info] at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414) [info] at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) [info] at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [info] at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [info] at java.base/java.lang.Thread.run(Thread.java:840) {code}", "comments": "Issue resolved by pull request 52766 [https://github.com/apache/spark/pull/52766]", "created": "2025-10-28T14:01:43.000+0000", "updated": "2025-10-28T19:07:45.000+0000", "derived": {"summary_task": "Summarize this issue: Recently, `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches` frequently fails. [https://github.com/apache/spark/actions/runs/18872699886/job/53854858890] {code:java} [info] - Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches ***", "classification_task": "Classify the issue priority and type: Recently, `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches` frequently fails. [https://github.com/apache/spark/actions/runs/18872699886/job/53854858890] {code:java} [info] - Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches ***", "qna_task": "Question: What is this issue about?\nAnswer: Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches`"}}
{"id": "13632667", "key": "SPARK-54057", "project": "SPARK", "summary": "Retrieve dependency version from pom.xml for DependencyOverrides in SparkBuild.scala", "description": "Currently, version strings for some dependencies are hard coded in DependencyOverrides in SparkBuild, so developers need to keep consistent with version strings specified in pom.xml manually.", "comments": "Issue resolved by pull request 52760 [https://github.com/apache/spark/pull/52760]", "created": "2025-10-28T12:10:10.000+0000", "updated": "2025-10-28T15:33:55.000+0000", "derived": {"summary_task": "Summarize this issue: Currently, version strings for some dependencies are hard coded in DependencyOverrides in SparkBuild, so developers need to keep consistent with version strings specified in pom.xml manually.", "classification_task": "Classify the issue priority and type: Currently, version strings for some dependencies are hard coded in DependencyOverrides in SparkBuild, so developers need to keep consistent with version strings specified in pom.xml manually.", "qna_task": "Question: What is this issue about?\nAnswer: Retrieve dependency version from pom.xml for DependencyOverrides in SparkBuild.scala"}}
{"id": "13632664", "key": "SPARK-54056", "project": "SPARK", "summary": "Enable substitution for SQLConf settings", "description": "Values set for custom catalogs are not being substituted: {code:java} spark.sql.catalog.mssql=org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalog spark.sql.catalog.mssql.user=${env:MSSQL__USER} {code} Fails with: {code:java} spark.sql(\"show tables in mssql\").show() com.microsoft.sqlserver.jdbc.SQLServerException: Login failed for user '${env:MSSQL__USER}' {code}", "comments": "PR: https://github.com/apache/spark/pull/52759", "created": "2025-10-28T11:13:47.000+0000", "updated": "2025-11-01T14:27:00.000+0000", "derived": {"summary_task": "Summarize this issue: Values set for custom catalogs are not being substituted: {code:java} spark.sql.catalog.mssql=org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalog spark.sql.catalog.mssql.user=${env:MSSQL__USER} {code} Fails with: {code:java} spark.sql(\"show tables in mssql\").show() com.microsoft.sqlserver.jdbc.SQLServerException: Login failed for user '${env:MSSQL__USER}' {code}", "classification_task": "Classify the issue priority and type: Values set for custom catalogs are not being substituted: {code:java} spark.sql.catalog.mssql=org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalog spark.sql.catalog.mssql.user=${env:MSSQL__USER} {code} Fails with: {code:java} spark.sql(\"show tables in mssql\").show() com.microsoft.sqlserver.jdbc.SQLServerException: Login failed for user '${env:MSSQL__USER}' {code}", "qna_task": "Question: What is this issue about?\nAnswer: Enable substitution for SQLConf settings"}}
{"id": "13632655", "key": "SPARK-54055", "project": "SPARK", "summary": "Spark Connect sessions leak pyspark UDF daemon processes and threads", "description": "Each Spark Connect session that uses Python UDFs seems to leak one PySpark `daemon` process. Over time, these can accumulate in the 100s until the corresponding node or container goes OOM (although it can take a while as each process doesn't use a lot of memory and some of those pages are shared). {code:java} spark 263 0.0 0.0 121424 59504 ? S 05:00 0:01 \\_ /opt/spark/.venv/bin/python3 -m pyspark.daemon spark 1515 0.0 0.0 121324 60148 ? S 05:04 0:01 \\_ /opt/spark/.venv/bin/python3 -m pyspark.daemon spark 1525 0.0 0.0 121324 60400 ? S 05:04 0:01 \\_ /opt/spark/.venv/bin/python3 -m pyspark.daemon spark 1568 0.0 0.0 121324 60280 ? S 05:04 0:01 \\_ /opt/spark/.venv/bin/python3 -m pyspark.daemon{code} In addition there are also threads leaking - e.g., here is a thread dump histogram from a sample executor with 200+ leaked daemon processes: {code:java} # These threads seem to be leaking 226 threads Idle Worker Monitor for /opt/spark/.venv/bin/python3 226 threads process reaper 226 threads stderr reader for /opt/spark/.venv/bin/python3 226 threads stdout reader for /opt/spark/.venv/bin/python3 250 threads Worker Monitor for /opt/spark/.venv/bin/python3 # These threads seem fine, Spark is configured with 24 cores/executor 21 threads stdout writer for /opt/spark/.venv/bin/python3 21 threads Writer Monitor for /opt/spark/.venv/bin/python3{code} With Spark Connect, each session always has a `SPARK_JOB_ARTIFACT_UUID`, even if there are no artifacts, so the UDF environment built by `BasePythonRunner.compute` is always different, and each session ends up with its own `PythonWorkerFactory` and hence its own daemon process. `PythonWorkerFactory` has a `stop` method that stops the daemon, but there does not seem to be anyone that calls `PythonWorkerFactory.stop`, except at shutdown in `SparkEnv.stop`. This can be reproduced by running a bunch of Spark Connect sessions: {code:java} parallel -n0 .venv/bin/python3 dummy.py ::: {1..200} {code} with `dummy.py`: {code:java} from collections.abc import Iterable import pandas as pd from pyspark.sql import SparkSession def _udf(iterator: Iterable[pd.DataFrame]) -> Iterable[pd.DataFrame]: yield from iterator if __name__ == \"__main__\": spark = SparkSession.builder.remote(\"...\").getOrCreate() df = spark.range(128) df.mapInPandas(_udf, df.schema).count() {code}", "comments": "", "created": "2025-10-28T09:52:58.000+0000", "updated": "2025-10-28T16:23:06.000+0000", "derived": {"summary_task": "Summarize this issue: Each Spark Connect session that uses Python UDFs seems to leak one PySpark `daemon` process. Over time, these can accumulate in the 100s until the corresponding node or container goes OOM (although it can take a while as each process doesn't use a lot of memory and some of those pages are shared). {code:java} spark 263 0.0 0.0 121424 59504 ? S 05:00 0:01 \\_ /opt/spark/.venv/bin/python3 -m pyspark.", "classification_task": "Classify the issue priority and type: Each Spark Connect session that uses Python UDFs seems to leak one PySpark `daemon` process. Over time, these can accumulate in the 100s until the corresponding node or container goes OOM (although it can take a while as each process doesn't use a lot of memory and some of those pages are shared). {code:java} spark 263 0.0 0.0 121424 59504 ? S 05:00 0:01 \\_ /opt/spark/.venv/bin/python3 -m pyspark.", "qna_task": "Question: What is this issue about?\nAnswer: Spark Connect sessions leak pyspark UDF daemon processes and threads"}}
{"id": "13632636", "key": "SPARK-54054", "project": "SPARK", "summary": "Support row position for SparkConnectResultSet", "description": "", "comments": "Issue resolved in https://github.com/apache/spark/pull/52756", "created": "2025-10-28T05:28:22.000+0000", "updated": "2025-10-29T06:50:21.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Support row position for SparkConnectResultSet"}}
{"id": "13632635", "key": "SPARK-54053", "project": "SPARK", "summary": "Use Swift 6.2 for all Linux CIs", "description": "", "comments": "Issue resolved by pull request 255 [https://github.com/apache/spark-connect-swift/pull/255]", "created": "2025-10-28T05:00:55.000+0000", "updated": "2025-10-28T14:47:51.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Use Swift 6.2 for all Linux CIs"}}
{"id": "13632634", "key": "SPARK-54052", "project": "SPARK", "summary": "Add SparkThrowable wrapper to workaround Py4J limitation", "description": "We could have a wrapper class to work around the problem in https://issues.apache.org/jira/browse/SPARK-52022", "comments": "", "created": "2025-10-28T03:42:55.000+0000", "updated": "2025-10-30T20:51:33.000+0000", "derived": {"summary_task": "Summarize this issue: We could have a wrapper class to work around the problem in https://issues.apache.org/jira/browse/SPARK-52022", "classification_task": "Classify the issue priority and type: We could have a wrapper class to work around the problem in https://issues.apache.org/jira/browse/SPARK-52022", "qna_task": "Question: What is this issue about?\nAnswer: Add SparkThrowable wrapper to workaround Py4J limitation"}}
{"id": "13632633", "key": "SPARK-54051", "project": "SPARK", "summary": "Keep coverage data when running pip tests", "description": "", "comments": "Issue resolved by pull request 51552 [https://github.com/apache/spark/pull/51552]", "created": "2025-10-28T03:38:34.000+0000", "updated": "2025-10-29T04:55:07.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Keep coverage data when running pip tests"}}
{"id": "13632632", "key": "SPARK-54050", "project": "SPARK", "summary": "Update the documents of arrow-batching related configures", "description": "", "comments": "Issue resolved by pull request 52753 [https://github.com/apache/spark/pull/52753]", "created": "2025-10-28T03:02:22.000+0000", "updated": "2025-10-28T04:58:38.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Update the documents of arrow-batching related configures"}}
{"id": "13632629", "key": "SPARK-54049", "project": "SPARK", "summary": "spark-network-common no longer shades all of Guava", "description": "Shading of Guava in `spark-network-common` has changed from 3.x to 4.x such that conflicting classes prevent downstream code from using Guava. In spark jars prior to 4.x, `com/google/thirdparty/publicsuffix/PublicSuffixPatterns.class` was not a class included in the jar. Post 4.x, Guava is still shaded to `sparkproject` but `com/google/thirdparty/publicsuffix/PublicSuffixPatterns.class`, used in Guava's `InternetDomainName` and elsewhere, is now present in the jar. {code:java} $ jar tf spark-network-common_2.13-4.0.0.jar | rg PublicSuffixPatterns com/google/thirdparty/publicsuffix/PublicSuffixPatterns.class {code} If a library depends on spark but also depends on Guava, calls to `InternetDomainName` that call `PublicSuffixPatterns` will fail with `Exception in thread \"main\" java.lang.NoSuchFieldError: EXACT`. Inspecting the code locations in such a library via `classOf[InternetDomainName].getProtectionDomain.getCodeSource.getLocation` and `classOf[PublicSuffixPatterns].getProtectionDomain.getCodeSource.getLocation` reveals that `InternetDomainName` is sourced from Guava, `target/bg-jobs/sbt_2062a9c3/target/5fcb43b5/1685140132000/guava-32.0.0-jre.jar`, while `PublicSuffixPatterns` is sourced instead from spark jar, `target/bg-jobs/sbt_2062a9c3/target/db746978/1747651686000/spark-network-common_2.13-4.0.0.jar`.", "comments": "", "created": "2025-10-28T01:04:16.000+0000", "updated": "2025-11-01T15:50:54.000+0000", "derived": {"summary_task": "Summarize this issue: Shading of Guava in `spark-network-common` has changed from 3.x to 4.x such that conflicting classes prevent downstream code from using Guava. In spark jars prior to 4.x, `com/google/thirdparty/publicsuffix/PublicSuffixPatterns.class` was not a class included in the jar. Post 4.x, Guava is still shaded to `sparkproject` but `com/google/thirdparty/publicsuffix/PublicSuffixPatterns.class`, used in G", "classification_task": "Classify the issue priority and type: Shading of Guava in `spark-network-common` has changed from 3.x to 4.x such that conflicting classes prevent downstream code from using Guava. In spark jars prior to 4.x, `com/google/thirdparty/publicsuffix/PublicSuffixPatterns.class` was not a class included in the jar. Post 4.x, Guava is still shaded to `sparkproject` but `com/google/thirdparty/publicsuffix/PublicSuffixPatterns.class`, used in G", "qna_task": "Question: What is this issue about?\nAnswer: spark-network-common no longer shades all of Guava"}}
{"id": "13632625", "key": "SPARK-54048", "project": "SPARK", "summary": "Update `dev/requirements.txt` to install `torch/torchvision` in Python 3.14", "description": "", "comments": "Issue resolved by pull request 52750 [https://github.com/apache/spark/pull/52750]", "created": "2025-10-27T23:55:25.000+0000", "updated": "2025-10-28T01:47:01.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Update `dev/requirements.txt` to install `torch/torchvision` in Python 3.14"}}
{"id": "13632624", "key": "SPARK-54047", "project": "SPARK", "summary": "Use difference error message when kill on idle timeout", "description": "", "comments": "Issue resolved by pull request 52749 [https://github.com/apache/spark/pull/52749]", "created": "2025-10-27T23:19:35.000+0000", "updated": "2025-10-28T23:48:47.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Use difference error message when kill on idle timeout"}}
{"id": "13632623", "key": "SPARK-54046", "project": "SPARK", "summary": "Upgrade PyArrow to 22.0.0", "description": "", "comments": "", "created": "2025-10-27T22:45:17.000+0000", "updated": "2025-10-30T05:09:54.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Upgrade PyArrow to 22.0.0"}}
{"id": "13632620", "key": "SPARK-54045", "project": "SPARK", "summary": "Upgrade `FlatBuffers` to v25.9.23", "description": "", "comments": "Issue resolved by pull request 254 [https://github.com/apache/spark-connect-swift/pull/254]", "created": "2025-10-27T22:24:51.000+0000", "updated": "2025-10-28T04:45:59.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Upgrade `FlatBuffers` to v25.9.23"}}
{"id": "13632617", "key": "SPARK-54044", "project": "SPARK", "summary": "Upgrade `gRPC Swift NIO Transport` to 2.2.0", "description": "", "comments": "Issue resolved by pull request 253 [https://github.com/apache/spark-connect-swift/pull/253]", "created": "2025-10-27T21:57:06.000+0000", "updated": "2025-10-27T22:20:09.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Upgrade `gRPC Swift NIO Transport` to 2.2.0"}}
{"id": "13632608", "key": "SPARK-54043", "project": "SPARK", "summary": "Update `Spark Connect`-generated `Swift` source code with `4.1.0-preview3` RC1", "description": "", "comments": "Issue resolved by pull request 252 [https://github.com/apache/spark-connect-swift/pull/252]", "created": "2025-10-27T20:05:42.000+0000", "updated": "2025-10-27T21:16:43.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Update `Spark Connect`-generated `Swift` source code with `4.1.0-preview3` RC1"}}
{"id": "13632593", "key": "SPARK-54042", "project": "SPARK", "summary": "Use 4.1.0-preview3 in `integration-test-(token|mac-spark41)`", "description": "", "comments": "Issue resolved by pull request 251 [https://github.com/apache/spark-connect-swift/pull/251]", "created": "2025-10-27T16:11:43.000+0000", "updated": "2025-10-27T21:46:39.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Use 4.1.0-preview3 in `integration-test-(token|mac-spark41)`"}}
{"id": "13632592", "key": "SPARK-54041", "project": "SPARK", "summary": "Refactor ParameterizedQuery arguments validation", "description": "* In this issue I propose to refactor ParameterizedQuery arguments validation to ParameterizedQueryArgumentsValidator so it can be reused between single-pass and fixed-point analyzer implementations. We also remove one redundant case from the ParameterizedQueryArgumentsValidator.isNotAllowed (Alias shouldn't call isNotAllowed again as it introduces unnecessary overhead to the method) to improve performance.", "comments": "Issue resolved by pull request 52744 [https://github.com/apache/spark/pull/52744]", "created": "2025-10-27T16:09:06.000+0000", "updated": "2025-10-29T12:28:01.000+0000", "derived": {"summary_task": "Summarize this issue: * In this issue I propose to refactor ParameterizedQuery arguments validation to ParameterizedQueryArgumentsValidator so it can be reused between single-pass and fixed-point analyzer implementations. We also remove one redundant case from the ParameterizedQueryArgumentsValidator.isNotAllowed (Alias shouldn't call isNotAllowed again as it introduces unnecessary overhead to the method) to improve pe", "classification_task": "Classify the issue priority and type: * In this issue I propose to refactor ParameterizedQuery arguments validation to ParameterizedQueryArgumentsValidator so it can be reused between single-pass and fixed-point analyzer implementations. We also remove one redundant case from the ParameterizedQueryArgumentsValidator.isNotAllowed (Alias shouldn't call isNotAllowed again as it introduces unnecessary overhead to the method) to improve pe", "qna_task": "Question: What is this issue about?\nAnswer: Refactor ParameterizedQuery arguments validation"}}
{"id": "13632587", "key": "SPARK-54040", "project": "SPARK", "summary": "Remove unused commons-collections 3.x", "description": "", "comments": "Issue resolved by pull request 52743 [https://github.com/apache/spark/pull/52743]", "created": "2025-10-27T14:47:43.000+0000", "updated": "2025-10-27T18:21:49.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Remove unused commons-collections 3.x"}}
{"id": "13632571", "key": "SPARK-54039", "project": "SPARK", "summary": "SS | `Add TaskContext information to KafkaDataConsumer release() logs for better debugging`", "description": "Currently, log messages in the {{release()}} method of {{KafkaDataConsumer}} do not include Spark task context information (e.g., TaskID). This change adds task context details to the {{release()}} logs to improve traceability and debugging of Kafka consumer lifecycle events. We previously added similar context in {{{}KafkaBatchReaderFactory.createReader{}}}, which has proven useful for debugging. Including this information in {{release()}} helps correlate consumer release metrics with the specific Spark tasks that borrowed the consumer.", "comments": "Issue resolved by pull request 52745 [https://github.com/apache/spark/pull/52745]", "created": "2025-10-27T12:15:05.000+0000", "updated": "2025-10-31T08:44:20.000+0000", "derived": {"summary_task": "Summarize this issue: Currently, log messages in the {{release()}} method of {{KafkaDataConsumer}} do not include Spark task context information (e.g., TaskID). This change adds task context details to the {{release()}} logs to improve traceability and debugging of Kafka consumer lifecycle events. We previously added similar context in {{{}KafkaBatchReaderFactory.createReader{}}}, which has proven useful for debugging.", "classification_task": "Classify the issue priority and type: Currently, log messages in the {{release()}} method of {{KafkaDataConsumer}} do not include Spark task context information (e.g., TaskID). This change adds task context details to the {{release()}} logs to improve traceability and debugging of Kafka consumer lifecycle events. We previously added similar context in {{{}KafkaBatchReaderFactory.createReader{}}}, which has proven useful for debugging.", "qna_task": "Question: What is this issue about?\nAnswer: SS | `Add TaskContext information to KafkaDataConsumer release() logs for better debugging`"}}
{"id": "13632550", "key": "SPARK-54038", "project": "SPARK", "summary": "Support getSQLKeywords for SparkConnectDatabaseMetaData", "description": "", "comments": "Issue resolved by pull request 52757 [https://github.com/apache/spark/pull/52757]", "created": "2025-10-27T09:41:10.000+0000", "updated": "2025-10-30T07:26:51.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Support getSQLKeywords for SparkConnectDatabaseMetaData"}}
{"id": "13632534", "key": "SPARK-54037", "project": "SPARK", "summary": "Throughput deteriorated after migration from spark 3.5.5 to spark 4.0.0", "description": "My team recently updated spark dependency version from 3.5.5 to 4.0.0 This included use of spark-4.0.0-bin-hadoop3.tgz, update in pom.xml files and change of import statements (org.apache.spark.sql -> org.apache.spark.sql.classic). After this change our throughput (calculated as rows transferred per second) has significantly dropped for our both scenarios: 1. read from file, write to database and 2. read from database, write to database. I have performed comparison between application versions with spark 3.5.5 and 4.0.0 in cluster mode, local mode and one comparison (with use of synthetic file) using spark-shell only. In case of spark-shell I had more or less the same throughput for 3.5.5 and 4.0.0 but in case of our app used in cluster / local mode - both of these scenarios had better throughput with 3.5.5. I have observed that with 4.0.0 there are longer delays (when compared with 3.5.5) between log lines \"Running task x in stage y\" and \"Finished task x in stage y\". Is this throughput degradation a known issue? Could it be related to this task - [SPARK-48456] [M1] Performance benchmark - ASF JIRA ? (I'll also mention that we are using checkpointing (in case it might be important here))", "comments": "", "created": "2025-10-27T08:21:21.000+0000", "updated": "2025-10-27T08:21:59.000+0000", "derived": {"summary_task": "Summarize this issue: My team recently updated spark dependency version from 3.5.5 to 4.0.0 This included use of spark-4.0.0-bin-hadoop3.tgz, update in pom.xml files and change of import statements (org.apache.spark.sql -> org.apache.spark.sql.classic). After this change our throughput (calculated as rows transferred per second) has significantly dropped for our both scenarios: 1. read from file, write to database and ", "classification_task": "Classify the issue priority and type: My team recently updated spark dependency version from 3.5.5 to 4.0.0 This included use of spark-4.0.0-bin-hadoop3.tgz, update in pom.xml files and change of import statements (org.apache.spark.sql -> org.apache.spark.sql.classic). After this change our throughput (calculated as rows transferred per second) has significantly dropped for our both scenarios: 1. read from file, write to database and ", "qna_task": "Question: What is this issue about?\nAnswer: Throughput deteriorated after migration from spark 3.5.5 to spark 4.0.0"}}
{"id": "13632516", "key": "SPARK-54036", "project": "SPARK", "summary": "Add `build_python_3.11_macos26.yml` GitHub Action job", "description": "", "comments": "Issue resolved by pull request 52740 [https://github.com/apache/spark/pull/52740]", "created": "2025-10-27T03:00:13.000+0000", "updated": "2025-10-27T03:19:34.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Add `build_python_3.11_macos26.yml` GitHub Action job"}}
{"id": "13632514", "key": "SPARK-54035", "project": "SPARK", "summary": "Construct FileStatus from the executor side directly", "description": "https://github.com/apache/spark/pull/50765#discussion_r2357607758", "comments": "", "created": "2025-10-27T02:33:18.000+0000", "updated": "2025-10-27T02:33:18.000+0000", "derived": {"summary_task": "Summarize this issue: https://github.com/apache/spark/pull/50765#discussion_r2357607758", "classification_task": "Classify the issue priority and type: https://github.com/apache/spark/pull/50765#discussion_r2357607758", "qna_task": "Question: What is this issue about?\nAnswer: Construct FileStatus from the executor side directly"}}
{"id": "13632511", "key": "SPARK-54034", "project": "SPARK", "summary": "Fix `Utils.isBindCollision` to detect port conflict `NativeIoException` correctly", "description": "", "comments": "Issue resolved by pull request 52738 [https://github.com/apache/spark/pull/52738]", "created": "2025-10-26T22:27:03.000+0000", "updated": "2025-10-27T15:29:41.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Fix `Utils.isBindCollision` to detect port conflict `NativeIoException` correctly"}}
{"id": "13632510", "key": "SPARK-54033", "project": "SPARK", "summary": "Introduce Catalyst server-side geospatial execution classes", "description": "", "comments": "Work in progress: https://github.com/apache/spark/pull/52737. Issue resolved by pull request 52737 [https://github.com/apache/spark/pull/52737]", "created": "2025-10-26T21:09:17.000+0000", "updated": "2025-10-31T07:53:32.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Introduce Catalyst server-side geospatial execution classes"}}
{"id": "13632507", "key": "SPARK-54032", "project": "SPARK", "summary": "Prefer to use native Netty transports by default", "description": "", "comments": "Issue resolved by pull request 52736 [https://github.com/apache/spark/pull/52736]", "created": "2025-10-26T20:07:43.000+0000", "updated": "2025-10-27T02:42:36.000+0000", "derived": {"summary_task": "Summarize this issue: ", "classification_task": "Classify the issue priority and type: ", "qna_task": "Question: What is this issue about?\nAnswer: Prefer to use native Netty transports by default"}}
